[
    {
        "name": "Graham Neubig",
        "publications": [
            {
                "title": "Poster: Learning to Mine Parallel Natural Language/Source Code Corpora from Stack Overflow",
                "link": "https://ieeexplore.ieee.org/document/8449590/",
                "date_of_publication": "30 August 2018",
                "doi": null,
                "citations": "66",
                "abstract": "For tasks like code synthesis from natural language, code retrieval, and code summarization, data-driven models have shown great promise. However, creating these models requires parallel data between natural language (NL) and code with fine-grained alignments. Stack Overflow (SO) is a promising source to create such a data set but existing heuristic methods are limited both in their coverage and the correctness of the NL-code pairs obtained. In this paper, we propose a method to mine high-quality aligned data from SO by training a classifier using two sets of features: hand-crafted features considering the structure of the extracted snippets, and correspondence features obtained by training a neural network model to capture the correlation between NL and code. Experiments using Python and Java as test beds show that the proposed method greatly expands coverage and accuracy over existing mining methods, even when using only a small number of labeled examples.",
                "ieee_keywords": [
                    "Feature extraction",
                    "Data mining",
                    "Natural languages",
                    "Training",
                    "Python",
                    "Java",
                    "Data models"
                ],
                "author_keywords": [
                    "Code Mining",
                    "Stack Overflow",
                    "Neural Networks",
                    "Bimodal Modeling"
                ]
            },
            {
                "title": "Universal Phone Recognition with a Multilingual Allophone System",
                "link": "https://ieeexplore.ieee.org/document/9054362/",
                "date_of_publication": "09 April 2020",
                "doi": "10.1109/ICASSP40776.2020.9054362",
                "citations": "27",
                "abstract": "Multilingual models can improve language processing, particularly for low resource situations, by sharing parameters across languages. Multilingual acoustic models, however, generally ignore the difference between phonemes (sounds that can support lexical contrasts in a particular language) and their corresponding phones (the sounds that are actually spoken, which are language independent). This can lead to performance degradation when combining a variety of training languages, as identically annotated phonemes can actually correspond to several different underlying phonetic realizations. In this work, we propose a joint model of both language-independent phone and language-dependent phoneme distributions. In multilingual ASR experiments over 11 languages, we find that this model improves testing performance by 2% phoneme error rate absolute in low-resource conditions. Additionally, because we are explicitly modeling language-independent phones, we can build a (nearly-)universal phone recognizer that, when combined with the PHOIBLE [1] large, manually curated database of phone inventories, can be customized into 2,000 language dependent recognizers. Experiments on two low-resourced indigenous languages, Inuktitut and Tusom, show that our recognizer achieves phone accuracy improvements of more than 17%, moving a step closer to speech recognition for all languages in the world. 1",
                "ieee_keywords": [
                    "Training",
                    "Error analysis",
                    "Speech recognition",
                    "Signal processing",
                    "Phonetics",
                    "Acoustics",
                    "Speech processing"
                ],
                "author_keywords": [
                    "multilingual speech recognition",
                    "universal phone recognition",
                    "phonology"
                ]
            },
            {
                "title": "Linguistic Unit Discovery from Multi-Modal Inputs in Unwritten Languages: Summary of the “Speaking Rosetta” JSALT 2017 Workshop",
                "link": "https://ieeexplore.ieee.org/document/8461761/",
                "date_of_publication": "13 September 2018",
                "doi": "10.1109/ICASSP.2018.8461761",
                "citations": "12",
                "abstract": "We summarize the accomplishments of a multi-disciplinary workshop exploring the computational and scientific issues surrounding the discovery of linguistic units (subwords and words) in a language without orthography. We study the replacement of orthographic transcriptions by images and/or translated text in a well-resourced language to help unsupervised discovery from raw speech.",
                "ieee_keywords": [
                    "Task analysis",
                    "Feature extraction",
                    "Acoustics",
                    "Hidden Markov models",
                    "Visualization",
                    "Linguistics",
                    "Training"
                ],
                "author_keywords": [
                    "unwritten languages",
                    "multi-modal data",
                    "unsupervised unit discovery",
                    "image retrieval",
                    "machine translation"
                ]
            },
            {
                "title": "EXCALIBUR: Encouraging and Evaluating Embodied Exploration",
                "link": "https://ieeexplore.ieee.org/document/10204393/",
                "date_of_publication": "22 August 2023",
                "doi": "10.1109/CVPR52729.2023.01434",
                "citations": "3",
                "abstract": "Experience precedes understanding. Humans constantly explore and learn about their environment out of curiosity, gather information, and update their models of the world. On the other hand, machines are either trained to learn passively from static and fixed datasets, or taught to complete specific goal-conditioned tasks. To encourage the development of exploratory interactive agents, we present the EXCALIBUR benchmark. EXCALIBUR allows agents to explore their environment for long durations and then query their understanding of the physical world via inquiries like: “is the small heavy red bowl made from glass?” or “is there a silver spoon heavier than the egg?”. This design encourages agents to perform free-form home exploration without myopia induced by goal conditioning. Once the agents have answered a series of questions, they can renter the scene to refine their knowledge, update their beliefs, and improve their performance on the questions. Our experiments demonstrate the challenges posed by this dataset for the present-day state-of-the-art embodied systems and the headroom afforded to develop new innovative methods. Finally, we present a virtual reality interface that enables humans to seamlessly interact within the simulated world and use it to gather human performance measures. EXCALIBUR affords unique challenges in comparison to presentday benchmarks and represents the next frontier for embodied AI research.",
                "ieee_keywords": [
                    "Solid modeling",
                    "Silver",
                    "Computer vision",
                    "Virtual reality",
                    "Glass",
                    "Benchmark testing",
                    "Pattern recognition"
                ],
                "author_keywords": [
                    "Embodied vision: Active agents",
                    "simulation"
                ]
            },
            {
                "title": "Speech Technology for Unwritten Languages",
                "link": "https://ieeexplore.ieee.org/document/8998182/",
                "date_of_publication": null,
                "doi": "10.1109/TASLP.2020.2973896",
                "citations": "12",
                "abstract": "Speech technology plays an important role in our everyday life. Among others, speech is used for human-computer interaction, for instance for information retrieval and on-line shopping. In the case of an unwritten language, however, speech technology is unfortunately difficult to create, because it cannot be created by the standard combination of pre-trained speech-to-text and text-to-speech subsystems. The research presented in this article takes the first steps towards speech technology for unwritten languages. Specifically, the aim of this work was 1) to learn speech-to-meaning representations without using text as an intermediate representation, and 2) to test the sufficiency of the learned representations to regenerate speech or translated text, or to retrieve images that depict the meaning of an utterance in an unwritten language. The results suggest that building systems that go directly from speech-to-meaning and from meaning-to-speech, bypassing the need for text, is possible.",
                "ieee_keywords": [
                    "Task analysis",
                    "Speech processing",
                    "Semantics",
                    "Speech recognition",
                    "Acoustics",
                    "Neural networks",
                    "Databases"
                ],
                "author_keywords": [
                    "Speech processing",
                    "automatic speech recognition",
                    "unsupervised learning",
                    "speech synthesis",
                    "image retrieval"
                ]
            },
            {
                "title": "DIRE: A Neural Approach to Decompiled Identifier Naming",
                "link": "https://ieeexplore.ieee.org/document/8952404/",
                "date_of_publication": "09 January 2020",
                "doi": "10.1109/ASE.2019.00064",
                "citations": "24",
                "abstract": "The decompiler is one of the most common tools for examining binaries without corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Decompilers can reconstruct much of the information that is lost during the compilation process (e.g., structure and type information). Unfortunately, they do not reconstruct semantically meaningful variable names, which are known to increase code understandability. We propose the Decompiled Identifier Renaming Engine (DIRE), a novel probabilistic technique for variable name recovery that uses both lexical and structural information recovered by the decompiler. We also present a technique for generating corpora suitable for training and evaluating models of decompiled code renaming, which we use to create a corpus of 164,632 unique x86-64 binaries generated from C projects mined from GitHub. Our results show that on this corpus DIRE can predict variable names identical to the names in the original source code up to 74.3% of the time.",
                "ieee_keywords": [
                    "Tools",
                    "Recurrent neural networks",
                    "Reverse engineering",
                    "Training",
                    "Software",
                    "Analytical models"
                ],
                "author_keywords": [
                    "Decompilation",
                    "Deep learning"
                ]
            },
            {
                "title": "VarCLR: Variable Semantic Representation Pre-training via Contrastive Learning",
                "link": "https://ieeexplore.ieee.org/document/9793917/",
                "date_of_publication": "20 June 2022",
                "doi": "10.1145/3510003.3510162",
                "citations": "3",
                "abstract": "Variable names are critical for conveying intended program behavior. Machine learning-based program analysis methods use variable name representations for a wide range of tasks, such as suggesting new variable names and bug detection. Ideally, such methods could capture semantic relationships between names beyond syntactic similarity, e.g., the fact that the names average and mean are similar. Unfortunately, previous work has found that even the best of previous representation approaches primarily capture “relatedness” (whether two variables are linked at all), rather than “similarity” (whether they actually have the same meaning). We propose Varclr, a new approach for learning semantic representations of variable names that effectively captures variable similarity in this stricter sense. We observe that this problem is an excellent fit for contrastive learning, which aims to minimize the distance between explicitly similar inputs, while maximizing the distance between dissimilar inputs. This requires labeled training data, and thus we construct a novel, weakly-supervised variable renaming dataset mined from GitHub edits. We show that Varclr enables the effective application of sophisticated, general-purpose language models like BERT, to variable name representation and thus also to related downstream tasks like variable name similarity search or spelling correction. Varclr produces models that significantly outperform the state-of-the-art on IDBENCH, an existing benchmark that explicitly captures variable similarity (as distinct from relatedness). Finally, we contribute a release of all data, code, and pre-trained models, aiming to provide a drop-in replacement for variable representations used in either existing or future program analyses that rely on variable names.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow",
                "link": "https://ieeexplore.ieee.org/document/8595231/",
                "date_of_publication": "30 December 2018",
                "doi": null,
                "citations": "431",
                "abstract": "For tasks like code synthesis from natural language, code retrieval, and code summarization, data-driven models have shown great promise. However, creating these models require parallel data between natural language (NL) and code with fine-grained alignments. StackOverflow (SO) is a promising source to create such a data set: the questions are diverse and most of them have corresponding answers with high-quality code snippets. However, existing heuristic methods (e.g. pairing the title of a post with the code in the accepted answer) are limited both in their coverage and the correctness of the NL-code pairs obtained. In this paper, we propose a novel method to mine high-quality aligned data from SO using two sets of features: hand-crafted features considering the structure of the extracted snippets, and correspondence features obtained by training a probabilistic model to capture the correlation between NL and code using neural networks. These features are fed into a classifier that determines the quality of mined NL-code pairs. Experiments using Python and Java as test beds show that the proposed method greatly expands coverage and accuracy over existing mining methods, even when using only a small number of labeled examples. Further, we find that reasonable results are achieved even when training the classifier on one language and testing on another, showing promise for scaling NL-code mining to a wide variety of programming languages beyond those for which we are able to annotate data.",
                "ieee_keywords": [
                    "Data mining",
                    "Python",
                    "Feature extraction",
                    "Java",
                    "Natural languages",
                    "Training"
                ],
                "author_keywords": [
                    "Code Mining",
                    "Stack Overflow",
                    "Neural Networks"
                ]
            },
            {
                "title": "An investigation of how to design control parameters for statistical voice timbre control",
                "link": "https://ieeexplore.ieee.org/document/8282283/",
                "date_of_publication": "08 February 2018",
                "doi": "10.1109/APSIPA.2017.8282283",
                "citations": "47",
                "abstract": "Multiple-regression Gaussian mixture models (MR-GMM) allow for control of voice timbre along several axes each described by a voice timbre expression word. To create these axes, perceptual scores corresponding to multiple voice timbre expression words are manually assigned to individual pre-stored target speakers as the voice timbre control parameters, and then acoustic basis vectors corresponding to the individual control parameters are learned. The voice timbre expression words are usually selected from various words using factor analysis so that the voice timbre control parameters are independent of each other. However, the resulting basis vectors are not often orthogonal to each other, and they practically cause difficulties in intuitively controlling the converted voice timbre. Towards the development of the MR-GMM capable of intuitively controlling converted voice timbre, we investigate how to design the voice timbre control parameters so that not only the voice timbre control parameters but also the corresponding acoustic basis vectors are independent of each other. Experimental results demonstrate that 1) a method for annotation of the voice timbre control parameters using the converted voices rather than natural voices is effective, and 2) the independences of the voice timbre control parameters and acoustic basis vectors is helpful for improving the converted voice timbre controllability of the MR-GMM.",
                "ieee_keywords": [
                    "Timbre",
                    "Controllability",
                    "Speech",
                    "Training",
                    "Feature extraction",
                    "Correlation"
                ],
                "author_keywords": []
            },
            {
                "title": "Postfilters to Modify the Modulation Spectrum for Statistical Parametric Speech Synthesis",
                "link": "https://ieeexplore.ieee.org/document/7393764/",
                "date_of_publication": null,
                "doi": "10.1109/TASLP.2016.2522655",
                "citations": "41",
                "abstract": "This paper presents novel approaches based on modulation spectrum (MS) for high-quality statistical parametric speech synthesis, including text-to-speech (TTS) and voice conversion (VC). Although statistical parametric speech synthesis offers various advantages over concatenative speech synthesis, the synthetic speech quality is still not as good as that of concatenative speech synthesis or the quality of natural speech. One of the biggest issues causing the quality degradation is the over-smoothing effect often observed in the generated speech parameter trajectories. Global variance (GV) is known as a feature well correlated with the over-smoothing effect, and the effectiveness of keeping the GV of the generated speech parameter trajectories similar to those of natural speech has been confirmed. However, the quality gap between natural speech and synthetic speech is still large. In this paper, we propose using the MS of the generated speech parameter trajectories as a new feature to effectively quantify the over-smoothing effect. Moreover, we propose postfilters to modify the MS utterance by utterance or segment by segment to make the MS of synthetic speech close to that of natural speech. The proposed postfilters are applicable to various synthesizers based on statistical parametric speech synthesis. We first perform an evaluation of the proposed method in the framework of hidden Markov model (HMM)-based TTS, examining its properties from different perspectives. Furthermore, effectiveness of the proposed postfilters are also evaluated in Gaussian mixture model (GMM)-based VC and classification and regression trees (CART)-based TTS (a.k.a., CLUSTERGEN). The experimental results demonstrate that 1) the proposed utterance-level postfilter achieves quality comparable to the conventional generation algorithm considering the GV, and yields significant improvements by applying to the GV-based generation algorithm in HMM-based TTS, 2) the proposed segment-level postfilter... (Show More)",
                "ieee_keywords": [
                    "Speech",
                    "Hidden Markov models",
                    "Natural languages",
                    "Speech synthesis",
                    "Modulation",
                    "Yttrium",
                    "Probability density function"
                ],
                "author_keywords": [
                    "Statistical parametric speech synthesis",
                    "oversmoothing",
                    "post-filter",
                    "global variance",
                    "modulation spectrum",
                    "GMM-based voice conversion",
                    "CLUSTERGEN",
                    "Statistical parametric speech synthesis",
                    "over-smoothing",
                    "post-filter",
                    "global variance",
                    "modulation spectrum",
                    "HMM-based text-to-speech",
                    "GMM-based voice conversion",
                    "CLUSTERGEN"
                ]
            }
        ]
    },
    {
        "name": "David Woodruff",
        "publications": [
            {
                "title": "Active Linear Regression for ℓp Norms and Beyond",
                "link": "https://ieeexplore.ieee.org/document/9996923/",
                "date_of_publication": "28 December 2022",
                "doi": "10.1109/FOCS54457.2022.00076",
                "citations": "76",
                "abstract": "We study active sampling algorithms for linear regression, which aim to query only a small number of entries of a target vector and output a near minimizer to the objective function. For ℓ p norm regression for any 0<p<∞ , we give an algorithm based on Lewis weight sampling which outputs a(1+ϵ) -approximate solution using just O ~ (d/ ϵ 2 ) queries to b for p∈(0,1) , O ~ (d/ϵ) queries for p∈(1,2) , and O ~ ( d p/2 / ϵ p ) queries for p∈(2, ∞) . For p∈(0,2) , our bounds are optimal up to logarithmic factors, thus settling the query complexity for this range of p. For p∈(2, ∞) , our dependence on d is optimal, while our dependence on ϵ is off by at most a single ϵ factor, up to logarithmic factors. Our result resolves an open question of Chen and Dereziński, who gave near optimal bounds for the ℓ 1 norm, but required at least d 2 / ϵ 2 samples for ℓ p regression with p∈(1,2) , and gave no bounds for p∈(2, ∞) or p∈(0,1) . We also provide the first total sensitivity upper bound for loss functions with at most degree p polynomial growth. This improves a recent result of Tukan, Maalouf, and Feldman. By combining this with our techniques for ℓ p regression, we obtain the first active regression algorithms for such loss functions, including the important cases of the Tukey and Huber losses. This answers another question of Chen and Dereziński. Our sensitivity bounds also give improvements to a variety of previous results using sensitivity sampling, including Orlicz norm subspace embeddings, robust subspace approximation, and dimension reduction for smoothed p-norms. Finally, our active sampling results give the first sublinear time algorithms for Kronecker product regression under every ℓ p norm. Previous results required reading the entire b vector in the kernel feature space. 1 1 Extende... (Show More)",
                "ieee_keywords": [
                    "Dimensionality reduction",
                    "Computer science",
                    "Sensitivity",
                    "Upper bound",
                    "Linear regression",
                    "Approximation algorithms",
                    "Linear programming"
                ],
                "author_keywords": [
                    "active learning",
                    "linear regression"
                ]
            },
            {
                "title": "Perfect Lp Sampling in a Data Stream",
                "link": "https://ieeexplore.ieee.org/document/8555136/",
                "date_of_publication": "02 December 2018",
                "doi": "10.1109/FOCS.2018.00058",
                "citations": "7",
                "abstract": "In this paper, we resolve the one-pass space complexity of L p sampling for p ∈ (0, 2). Given a stream of updates (insertions and deletions) to the coordinates of an underlying vector f ∈ ℝ n , a perfect Lp sampler must output an index i with probability |f i | p /||f|| p p , and is allowed to fail with some probability δ. So far, for p > 0 no algorithm has been shown to solve the problem exactly using poly(log n)-bits of space. In 2010, Monemizadeh and Woodruff introduced an approximate L p sampler, which outputs i with probability (1 ± ν)|f i | p /||f|| p p , using space polynomial in ν -1 and log(n). The space complexity was later reduced by Jowhari, Saglam, and Tardos to roughly O(ν -p log 2 n log δ -1 ) for p ∈ (0, 2), which tightly matches the Ω(log 2 n log δ -1 ) lower bound in terms of n and δ, but is loose in terms of ν. Given these nearly tight bounds, it is perhaps surprising that no lower bound at all exists in terms of ν-not even a bound of Ω(ν -1 ) is known. In this paper, we explain this phenomenon by demonstrating the existence of an O(log 2 n log δ -1 )-bit perfect L p sampler for p ∈ (0, 2). This shows that ν need not factor into the space of an L p sampler, which completely closes the complexity of the problem for this range of p. For p = 2, our bound is O(log 3 n log δ -1 )-bits, which matches the prior best known upper bound of O(ν -2 log 3 n log δ -1 ), but has no dependence on ν. Finally, we show that a (1 ± ε) relative error estimate of the frequency f i of the sampled index i can be obtained using an additional O(ε -p log n)-bits of space for p <; 2, and O(ε -2 log 2 n) bits for p = 2, which was possible before only by running the prior algorithms with ν = ε.",
                "ieee_keywords": [
                    "Indexes",
                    "Complexity theory",
                    "Upper bound",
                    "Estimation",
                    "Computer science",
                    "Approximation algorithms",
                    "Frequency estimation"
                ],
                "author_keywords": [
                    "streaming",
                    "sampling",
                    "sketching",
                    "hashing"
                ]
            },
            {
                "title": "Tight Bounds for Adversarially Robust Streams and Sliding Windows via Difference Estimators",
                "link": "https://ieeexplore.ieee.org/document/9719698/",
                "date_of_publication": "04 March 2022",
                "doi": "10.1109/FOCS52979.2021.00116",
                "citations": "2",
                "abstract": "In the adversarially robust streaming model, a stream of elements is presented to an algorithm and is allowed to depend on the output of the algorithm at earlier times during the stream. In the classic insertion-only model of data streams, Ben-Eliezer et al. (PODS 2020, best paper award) show how to convert a non-robust algorithm into a robust one with a roughly 1/ε factor overhead. This was subsequently improved to a 1/ ε √ factor overhead by Hassidim et al. (NeurIPS 2020, oral presentation), suppressing logarithmic factors. For general functions the latter is known to be best-possible, by a result of Kaplan et al. (CRYPTO 2021). We show how to bypass this impossibility result by developing data stream algorithms for a large class of streaming problems, with no overhead in the approximation factor. Our class of streaming problems includes the most well-studied problems such as the L 2 -heavy hitters problem, F p -moment estimation, as well as empirical entropy estimation. We substantially improve upon all prior work on these problems, giving the first optimal dependence on the approximation factor. As in previous work, we obtain a general transformation that applies to any non-robust streaming algorithm and depends on the so-called flip number. However, the key technical innovation is that we apply the transformation to what we call a difference estimator for the streaming problem, rather than an estimator for the streaming prob-lem itself. We then develop the first difference estimators for a wide range of problems. Our difference estimator methodology is not only applicable to the adversarially ro-bust model, but to other streaming models where temporal properties of the data play a central role. To demonstrate the generality of our technique, we additionally introduce a general framework for the related sliding window model of data streams and resolve longstanding open questions in that model, obtaining a drastic impro... (Show More)",
                "ieee_keywords": [
                    "Computer science",
                    "Technological innovation",
                    "Computational modeling",
                    "Estimation",
                    "Approximation algorithms",
                    "Data models",
                    "Entropy"
                ],
                "author_keywords": []
            },
            {
                "title": "High-Dimensional Geometric Streaming in Polynomial Space",
                "link": "https://ieeexplore.ieee.org/document/9996775/",
                "date_of_publication": "28 December 2022",
                "doi": "10.1109/FOCS54457.2022.00075",
                "citations": "47",
                "abstract": "Many existing algorithms for streaming geometric data analysis have been plagued by exponential dependencies in the space complexity, which are undesirable for processing high-dimensional data sets, i.e., large d. In particular, once d≥logn , there are no known non-trivial streaming algorithms for problems such as maintaining convex hulls and Löwner-John ellipsoids of n points, despite a long line of work in high-dimensional streaming computational geometry since [2]. We simultaneously improve all of these results to poly (d, logn) bits of space by trading off with a poly (d, logn) factor distortion. We achieve these results in a unified manner, by designing the first streaming algorithm for maintaining a coreset for ℓ ∞ subspace embeddings with poly (d, logn) space and poly (d, logn) distortion. Our algorithm also gives similar guarantees in the online coreset model. Along the way, we sharpen known results for online numerical linear algebra by replacing a log condition number dependence with a logn dependence, answering an open question of [13]. Our techniques provide a novel connection between leverage scores, a fundamental object in numerical linear algebra, and computational geometry. For ℓ p subspace embeddings, our improvements in online numerical linear algebra yield nearly optimal tradeoffs between space and distortion for one-pass streaming algorithms. For instance, we obtain a deterministic coreset using o( d 2 logn) space and o((dlogn ) 1 2 − 1 p ) distortion for p>2 , whereas previous deterministic algorithms incurred a poly (n) factor in the space or the distortion [26]. Our techniques have implications also in the offline setting, where we give optimal trade-offs between the space complexity and distortion of a subspace sketch data structure, which preprocesses an n×d matrix A and outputs ∥Ax ∥ p up to a poly (d) factor distortion for an... (Show More)",
                "ieee_keywords": [
                    "Computer science",
                    "Computational geometry",
                    "Data analysis",
                    "Computational modeling",
                    "Linear algebra",
                    "Distortion",
                    "Data structures"
                ],
                "author_keywords": [
                    "computational geometry",
                    "streaming"
                ]
            },
            {
                "title": "Near Optimal Linear Algebra in the Online and Sliding Window Models",
                "link": "https://ieeexplore.ieee.org/document/9317959/",
                "date_of_publication": "19 January 2021",
                "doi": "10.1109/FOCS46700.2020.00055",
                "citations": "6",
                "abstract": "We initiate the study of numerical linear algebra in the sliding window model, where only the most recent W updates in a stream form the underlying data set. Although many existing algorithms in the sliding window model use or borrow elements from the smooth histogram framework (Braverman and Ostrovsky, FOCS 2007), we show that many interesting linear-algebraic problems, including spectral and vector induced matrix norms, generalized regression, and low-rank approximation, are not amenable to this approach in the row-arrival model. To overcome this challenge, we first introduce a unified row-sampling based framework that gives randomized algorithms for spectral approximation, low-rank approximation/projection-cost preservation, and ℓ 1 -subspace embeddings in the sliding window model, which often use nearly optimal space and achieve nearly input sparsity runtime. Our algorithms are based on “reverse online” versions of offline sampling distributions such as (ridge) leverage scores, ℓ 1 sensitivities, and Lewis weights to quantify both the importance and the recency of a row; our structural results on these distributions may be of independent interest for future algorithmic design. Although our techniques initially address numerical linear algebra in the sliding window model, our row-sampling framework rather surprisingly implies connections to the well-studied online model; our structural results also give the first sample optimal (up to lower order terms) online algorithm for low-rank approximation/projection-cost preservation. Using this powerful primitive, we give online algorithms for column/row subset selection and principal component analysis that resolves the main open question of Bhaskara et al. (FOCS 2019). We also give the first online algorithm for ℓ 1 -subspace embeddings. We further formalize the connection between the online model and the sliding window model by introducing an additional unified framework for deterministic algorithms using a merge and... (Show More)",
                "ieee_keywords": [
                    "Approximation algorithms",
                    "Data models",
                    "Numerical models",
                    "Principal component analysis",
                    "Analytical models",
                    "Linear algebra",
                    "Computational modeling"
                ],
                "author_keywords": [
                    "streaming algorithms",
                    "online algorithms",
                    "sliding window model",
                    "numerical linear algebra"
                ]
            },
            {
                "title": "Optimal Lower Bounds for Universal Relation, and for Samplers and Finding Duplicates in Streams",
                "link": "https://ieeexplore.ieee.org/document/8104082/",
                "date_of_publication": "13 November 2017",
                "doi": "10.1109/FOCS.2017.50",
                "citations": "6",
                "abstract": "In the communication problem UR (universal relation) [25], Alice and Bob respectively receive x, y ∈ {0, 1} n with the promise that x ≠ y. The last player to receive a message must output an index i such that x i ≠ y i . We prove that the randomized one-way communication complexity of this problem in the public coin model is exactly Θ(min{n, log(1/δ) log 2 (n/log(1/δ) )}) for failure probability δ. Our lower bound holds even if promised support(y) ⊂ support(x). As a corollary, we obtain optimal lower bounds for ℓ p -sampling in strict turnstile streams for 0 ≤ p <; 2, as well as for the problem of finding duplicates in a stream. Our lower bounds do not need to use large weights, and hold even if promised x ∈ {0, 1} n at all points in the stream. We give two different proofs of our main result. The first proof demonstrates that any algorithm A solving sampling problems in turnstile streams in low memory can be used to encode subsets of [n] of certain sizes into a number of bits below the information theoretic minimum. Our encoder makes adaptive queries to A throughout its execution, but done carefully so as to not violate correctness. This is accomplished by injecting random noise into the encoder's interactions with A, which is loosely motivated by techniques in differential privacy. Our correctness analysis involves understanding the ability of A to correctly answer adaptive queries which have positive but bounded mutual information with A's internal randomness, and may be of independent interest in the newly emerging area of adaptive data analysis with a theoretical computer science lens. Our second proof is via a novel randomized reduction from Augmented Indexing [30] which needs to interact with A adaptively. To handle the adaptivity we identify certain likely interaction patterns and union bound over them to guarantee correct interaction on all of them. To guarantee correctness, it is important that the interaction hides some of its randomness from A in the reduction.",
                "ieee_keywords": [
                    "Upper bound",
                    "Heuristic algorithms",
                    "Indexes",
                    "Complexity theory",
                    "Data structures",
                    "Computer science"
                ],
                "author_keywords": [
                    "streaming",
                    "lower bounds",
                    "ℓp-sampling"
                ]
            },
            {
                "title": "Testing Positive Semidefiniteness Using Linear Measurements",
                "link": "https://ieeexplore.ieee.org/document/9996743/",
                "date_of_publication": "28 December 2022",
                "doi": "10.1109/FOCS54457.2022.00016",
                "citations": "57",
                "abstract": "We study the problem of testing whether a symmetric d×d input matrix A is symmetric positive semidefinite (PSD), or is ϵ -far from the PSD cone, meaning that λ min (A)≤−ϵ∥A ∥ p , where ∥A ∥ p is the Schatten-p norm of A. In applications one often needs to quickly tell if an input matrix is PSD, and a small distance from the PSD cone may be tolerable. We consider two well-studied query models for measuring efficiency, namely, the matrix-vector and vector-matrix-vector query models. We first consider one-sided testers, which are testers that correctly classify any PSD input, but may fail on a non-PSD input with a tiny failure probability. Up to logarithmic factors, in the matrix-vector query model we show a tight Θ ~ (1/ ϵ p/(2p+1) ) bound, while in the vector-matrix-vector query model we show a tight Θ ~ ( d 1−1/p /ϵ) bound, for every p≥1 . We also show a strong separation between one-sided and two-sided testers in the vector-matrix-vector model, where a two-sided tester can fail on both PSD and non-PSD inputs with a tiny failure probability. In particular, for the important case of the Frobenius norm, we show that any one-sided tester requires Ω ~ ( d − − √ /ϵ) queries. However we introduce a bilinear sketch for two-sided testing from which we construct a Frobenius norm tester achieving the optimal O ~ (1/ ϵ 2 ) queries. We also give a number of additional separations between adaptive and non-adaptive testers. Our techniques have implications beyond testing, providing new methods to approximate the spectrum of a matrix with Frobenius norm error using dimensionality reduction in a way that preserves the signs of eigenvalues.",
                "ieee_keywords": [
                    "Dimensionality reduction",
                    "Computer science",
                    "Symmetric matrices",
                    "Eigenvalues and eigenfunctions",
                    "Testing"
                ],
                "author_keywords": []
            },
            {
                "title": "How to Reduce Dimension With PCA and Random Projections?",
                "link": "https://ieeexplore.ieee.org/document/9537789/",
                "date_of_publication": null,
                "doi": "10.1109/TIT.2021.3112821",
                "citations": "6",
                "abstract": "In our “big data” age, the size and complexity of data is steadily increasing. Methods for dimension reduction are ever more popular and useful. Two distinct types of dimension reduction are “data-oblivious” methods such as random projections and sketching, and “data-aware” methods such as principal component analysis (PCA). Both have their strengths, such as speed for random projections, and data-adaptivity for PCA. In this work, we study how to combine them to get the best of both. We study “sketch and solve” methods that take a random projection (or sketch) first, and compute PCA after. We compute the performance of several popular sketching methods (random iid projections, random sampling, subsampled Hadamard transform, CountSketch, etc) in a general “signal-plus-noise” (or spiked) data model. Compared to well-known works, our results are: 1) give asymptotically exact results; 2) apply when the signal components are only slightly above the noise, but the projection dimension is non-negligible. We also study stronger signals allowing more general covariance structures. We find that: 1) signal strength decreases under projection in a delicate way depending on the structure of the data and the sketching method; 2) orthogonal projections are slightly more accurate; 3) randomization does not hurt too much, due to concentration of measure; 4) the CountSketch can be somewhat improved by a normalization method. Our results have implications for statistical learning and data analysis. We also illustrate that the results are highly accurate in simulations and in analyzing empirical data.",
                "ieee_keywords": [
                    "Principal component analysis",
                    "Data models",
                    "Transforms",
                    "Fans",
                    "Dimensionality reduction",
                    "Covariance matrices",
                    "Tools"
                ],
                "author_keywords": [
                    "Dimension reduction",
                    "principal component analysis",
                    "sketching",
                    "random projection",
                    "random matrix theory"
                ]
            },
            {
                "title": "Recovery from Non-Decomposable Distance Oracles",
                "link": "https://ieeexplore.ieee.org/document/10164646/",
                "date_of_publication": null,
                "doi": "10.1109/TIT.2023.3289981",
                "citations": "30",
                "abstract": "A line of work has looked at the problem of recovering an input from distance queries. In this setting, there is an unknown sequence s ∈ {0, 1} ≤n , and one chooses a set of queries y ∈ {0, 1} O(n) and receives d ( s , y ) for a distance function d . The goal is to make as few queries as possible to recover s . Although this problem is well-studied for decomposable distances, i.e., distances of the form d ( s , y ) = Σ n i=1 f ( si , yi ) for some function f , which includes the important cases of Hamming distance, ℓ p -norms, and M -estimators, to the best of our knowledge this problem has not been studied for non-decomposable distances, for which there are important instances including edit distance, dynamic time warping (DTW), Fréchet distance, earth mover’s distance, and others. We initiate the study and develop a general framework for such distances. Interestingly, for some distances such as DTW or Fréchet, exact recovery of the sequence s is provably impossible, and so we show by allowing the characters in y to be drawn from a slightly larger alphabet this then becomes possible. In a number of cases we obtain optimal or near-optimal query complexity. One motivation for understanding non-adaptivity is that the query sequence can be fixed and provide a non-linear embedding of the input, which can be used in downstream applications involving, e.g., neural networks for natural language processing.",
                "ieee_keywords": [
                    "Complexity theory",
                    "Encoding",
                    "Upper bound",
                    "Testing",
                    "Symbols",
                    "Robustness",
                    "Perturbation methods"
                ],
                "author_keywords": [
                    "Sequence Recovery",
                    "Edit Distance",
                    "DTW Distance",
                    "Fréchet Distance"
                ]
            },
            {
                "title": "Sublinear Time Low-Rank Approximation of Positive Semidefinite Matrices",
                "link": "https://ieeexplore.ieee.org/document/8104100/",
                "date_of_publication": "13 November 2017",
                "doi": "10.1109/FOCS.2017.68",
                "citations": "3",
                "abstract": "We show how to compute a relative-error low-rank approximation to any positive semidefinite (PSD) matrix in sublinear time, i.e., for any n x n PSD matrix A, in Õ(n · poly(k/ε)) time we output a rank-k matrix B, in factored form, for which ||A - B|| 2 F ≤ (1 + ε)||A - A k || F 2 , where Ak is the best rank-k approximation to A. When k and 1/ε are not too large compared to the sparsity of A, our algorithm does not need to read all entries of the matrix. Hence, we significantly improve upon previous nnz(A) time algorithms based on oblivious subspace embeddings, and bypass an nnz(A) time lower bound for general matrices (where nnz(A) denotes the number of non-zero entries in the matrix). We prove time lower bounds for low-rank approximation of PSD matrices, showing that our algorithm is close to optimal. Finally, we extend our techniques to give sublinear time algorithms for lowrank approximation of A in the (often stronger) spectral norm metric ||A - B|| 2 2 and for ridge regression on PSD matrices.",
                "ieee_keywords": [
                    "Approximation algorithms",
                    "Symmetric matrices",
                    "Covariance matrices",
                    "Runtime",
                    "Matrix decomposition",
                    "Kernel",
                    "Standards"
                ],
                "author_keywords": [
                    "low-rank approximation",
                    "leverage score sampling",
                    "sublinear time algorithms",
                    "matrix sketching"
                ]
            }
        ]
    },
    {
        "name": "Abhinav Gupta",
        "publications": [
            {
                "title": "Cross-Stitch Networks for Multi-task Learning",
                "link": "https://ieeexplore.ieee.org/document/7780802/",
                "date_of_publication": "12 December 2016",
                "doi": "10.1109/CVPR.2016.433",
                "citations": "498",
                "abstract": "Multi-task learning in Convolutional Networks has displayed remarkable success in the field of recognition. This success can be largely attributed to learning shared representations from multiple supervisory tasks. However, existing multi-task approaches rely on enumerating multiple network architectures specific to the tasks at hand, that do not generalize. In this paper, we propose a principled approach to learn shared representations in ConvNets using multitask learning. Specifically, we propose a new sharing unit: \"cross-stitch\" unit. These units combine the activations from multiple networks and can be trained end-to-end. A network with cross-stitch units can learn an optimal combination of shared and task-specific representations. Our proposed method generalizes across multiple tasks and shows dramatically improved performance over baseline methods for categories with few training examples.",
                "ieee_keywords": [
                    "Computer architecture",
                    "Computer vision",
                    "Network architecture",
                    "Estimation",
                    "Semantics",
                    "Face",
                    "Training"
                ],
                "author_keywords": []
            },
            {
                "title": "Affordance Diffusion: Synthesizing Hand-Object Interactions",
                "link": "https://ieeexplore.ieee.org/document/10204191/",
                "date_of_publication": "22 August 2023",
                "doi": "10.1109/CVPR52729.2023.02153",
                "citations": "Abstract",
                "abstract": "Recent successes in image synthesis are powered by large-scale diffusion models. However, most methods are currently limited to either text- or image-conditioned generation for synthesizing an entire image, texture transfer or inserting objects into a user-specified region. In contrast, in this work we focus on synthesizing complex interactions (i.e., an articulated hand) with a given object. Given an RGB image of an object, we aim to hallucinate plausible images of a human hand interacting with it. We propose a two-step generative approach: a LayoutNet that samples an articulation-agnostic hand-object-interaction layout, and a ContentNet that synthesizes images of a hand grasping the object given the predicted layout. Both are built on top of a large-scale pretrained diffusion model to make use of its latent representation. Compared to baselines, the proposed method is shown to generalize better to novel objects and perform surprisingly well on out-of-distribution in-the-wild scenes of portable-sized objects. The resulting system allows us to predict descriptive affordance information, such as hand articulation and approaching orientation.",
                "ieee_keywords": [
                    "Layout",
                    "Three-dimensional displays",
                    "Affordances",
                    "Noise measurement",
                    "Data models",
                    "Training",
                    "Visualization"
                ],
                "author_keywords": [
                    "Image and video synthesis and generation",
                    "Media",
                    "More Like This",
                    "Exploratory Visualization of Surgical Training Databases for Improving Skill Acquisition",
                    "IEEE Computer Graphics and Applications",
                    "Published: 2012",
                    "Design of Mathematical Online Training Interactive Framework based on AI-assisted Data Modeling Algorithm Visualization Process",
                    "2022 International Conference on Inventive Computation Technologies (ICICT)",
                    "Published: 2022",
                    "Show More"
                ]
            },
            {
                "title": "Visual features for context-aware speech recognition",
                "link": "https://ieeexplore.ieee.org/document/7953112/",
                "date_of_publication": "19 June 2017",
                "doi": "10.1109/ICASSP.2017.7953112",
                "citations": "15",
                "abstract": "Automatic transcriptions of consumer generated multi-media content such as “Youtube” videos still exhibit high word error rates. Such data typically occupies a very broad domain, has been recorded in challenging conditions, with cheap hardware and a focus on the visual modality, and may have been post-processed or edited.",
                "ieee_keywords": [
                    "Adaptation models",
                    "Videos",
                    "Visualization",
                    "Acoustics",
                    "Feature extraction",
                    "Context",
                    "Training"
                ],
                "author_keywords": [
                    "audio-visual speech recognition",
                    "multimodal processing",
                    "deep learning"
                ]
            },
            {
                "title": "Learn-to-Race: A Multimodal Control Environment for Autonomous Racing",
                "link": "https://ieeexplore.ieee.org/document/9710502/",
                "date_of_publication": "28 February 2022",
                "doi": "10.1109/ICCV48922.2021.00965",
                "citations": "6",
                "abstract": "Existing research on autonomous driving primarily focuses on urban driving, which is insufficient for characterising the complex driving behaviour underlying high-speed racing. At the same time, existing racing simulation frameworks struggle in capturing realism, with respect to visual rendering, vehicular dynamics, and task objectives, inhibiting the transfer of learning agents to real-world contexts. We introduce a new environment, where agents Learn-to-Race (L2R) in simulated competition-style racing, using multimodal information—from virtual cameras to a comprehensive array of inertial measurement sensors. Our environment, which includes a simulator and an interfacing training framework, accurately models vehicle dynamics and racing conditions. In this paper, we release the Arrival simulator for autonomous racing. Next, we propose the L2R task with challenging metrics, inspired by learning-to-drive challenges, Formula-style racing, and multimodal trajectory prediction for autonomous driving. Additionally, we provide the L2R framework suite, facilitating simulated racing on high-precision models of real-world tracks. Finally, we provide an official L2R task dataset of expert demonstrations, as well as a series of baseline experiments and reference implementations. We make all code available: https://github.com/learn-to-race/l2r.",
                "ieee_keywords": [
                    "Measurement",
                    "Training",
                    "Visualization",
                    "Trajectory",
                    "Sensors",
                    "Task analysis",
                    "Vehicle dynamics"
                ],
                "author_keywords": [
                    "Datasets and evaluation",
                    "Vision + other modalities",
                    "Vision for robotics and autonomous vehicles"
                ]
            },
            {
                "title": "Cutting through the clutter: Task-relevant features for image matching",
                "link": "https://ieeexplore.ieee.org/document/7477576/",
                "date_of_publication": "26 May 2016",
                "doi": "10.1109/WACV.2016.7477576",
                "citations": "135",
                "abstract": "Where do we focus our attention in an image? Humans have an amazing ability to cut through the clutter to the parts of an image most relevant to the task at hand. Consider the task of geo-localizing tourist photos by retrieving other images taken at that location. Such photos naturally contain friends and family, and perhaps might even be nearly filled by a person's face if it is a selfie. Humans have no trouble ignoring these `distractions' and recognizing the parts that are indicative of location (e.g., the towers of Neuschwanstein Castle instead of their friend's face, a tree, or a car). In this paper, we investigate learning this ability automatically. At training-time, we learn how informative a region is for localization. At test-time, we use this learned model to determine what parts of a query image to use for retrieval. We introduce a new dataset, People at Landmarks, that contains large amounts of clutter in query images. Our system is able to outperform the existing state of the art approach to retrieval by more than 10% mAP, as well as improve results on a standard dataset without heavy occluders (Oxford5K).",
                "ieee_keywords": [
                    "Face",
                    "Clutter",
                    "Visualization",
                    "Standards",
                    "Heating",
                    "Feature extraction",
                    "Training"
                ],
                "author_keywords": []
            },
            {
                "title": "A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection",
                "link": "https://ieeexplore.ieee.org/document/8099807/",
                "date_of_publication": "09 November 2017",
                "doi": "10.1109/CVPR.2017.324",
                "citations": "310",
                "abstract": "How do we learn an object detector that is invariant to occlusions and deformations? Our current solution is to use a data-driven strategy - collect large-scale datasets which have object instances under different conditions. The hope is that the final classifier can use these examples to learn invariances. But is it really possible to see all the occlusions in a dataset? We argue that like categories, occlusions and object deformations also follow a long-tail. Some occlusions and deformations are so rare that they hardly happen, yet we want to learn a model invariant to such occurrences. In this paper, we propose an alternative solution. We propose to learn an adversarial network that generates examples with occlusions and deformations. The goal of the adversary is to generate examples that are difficult for the object detector to classify. In our framework both the original detector and adversary are learned in a joint manner. Our experimental results indicate a 2.3% mAP boost on VOC07 and a 2.6% mAP boost on VOC2012 object detection challenge compared to the Fast-RCNN pipeline.",
                "ieee_keywords": [
                    "Detectors",
                    "Strain",
                    "Training",
                    "Object detection",
                    "Feature extraction",
                    "Proposals"
                ],
                "author_keywords": []
            },
            {
                "title": "NEIL: Extracting Visual Knowledge from Web Data",
                "link": "https://ieeexplore.ieee.org/document/6751285/",
                "date_of_publication": "03 March 2014",
                "doi": "10.1109/ICCV.2013.178",
                "citations": "252",
                "abstract": "We propose NEIL (Never Ending Image Learner), a computer program that runs 24 hours per day and 7 days per week to automatically extract visual knowledge from Internet data. NEIL uses a semi-supervised learning algorithm that jointly discovers common sense relationships (e.g., \"Corolla is a kind of/looks similar to Car\", \"Wheel is a part of Car\") and labels instances of the given visual categories. It is an attempt to develop the world's largest visual structured knowledge base with minimum human labeling effort. As of 10th October 2013, NEIL has been continuously running for 2.5 months on 200 core cluster (more than 350K CPU hours) and has an ontology of 1152 object categories, 1034 scene categories and 87 attributes. During this period, NEIL has discovered more than 1700 relationships and has labeled more than 400K visual instances.",
                "ieee_keywords": [
                    "Visualization",
                    "Detectors",
                    "Semantics",
                    "Data mining",
                    "Computers",
                    "Knowledge based systems",
                    "Semisupervised learning"
                ],
                "author_keywords": [
                    "never ending learning",
                    "visual knowledge base",
                    "common sense relationships",
                    "macro vision",
                    "object detection",
                    "scene classification",
                    "attributes",
                    "semi-supervised learning"
                ]
            },
            {
                "title": "Enriching Visual Knowledge Bases via Object Discovery and Segmentation",
                "link": "https://ieeexplore.ieee.org/document/6909658/",
                "date_of_publication": "25 September 2014",
                "doi": "10.1109/CVPR.2014.261",
                "citations": "73",
                "abstract": "There have been some recent efforts to build visual knowledge bases from Internet images. But most of these approaches have focused on bounding box representation of objects. In this paper, we propose to enrich these knowledge bases by automatically discovering objects and their segmentations from noisy Internet images. Specifically, our approach combines the power of generative modeling for segmentation with the effectiveness of discriminative models for detection. The key idea behind our approach is to learn and exploit top-down segmentation priors based on visual subcategories. The strong priors learned from these visual subcategories are then combined with discriminatively trained detectors and bottom up cues to produce clean object segmentations. Our experimental results indicate state-of-the-art performance on the difficult dataset introduced by [29] Rubinstein et al. We have integrated our algorithm in NEIL for enriching its knowledge base [5]. As of 14th April 2014, NEIL has automatically generated approximately 500K segmentations using web data.",
                "ieee_keywords": [
                    "Image segmentation",
                    "Visualization",
                    "Noise measurement",
                    "Detectors",
                    "Internet",
                    "Joints",
                    "Semantics"
                ],
                "author_keywords": []
            },
            {
                "title": "Train Offline, Test Online: A Real Robot Learning Benchmark",
                "link": "https://ieeexplore.ieee.org/document/10160594/",
                "date_of_publication": "04 July 2023",
                "doi": "10.1109/ICRA48891.2023.10160594",
                "citations": "51",
                "abstract": "Three challenges limit the progress of robot learning research: robots are expensive (few labs can participate), everyone uses different robots (findings do not generalize across labs), and we lack internet-scale robotics data. We take on these challenges via a new benchmark: Train Offline, Test Online (TOTO). TOTO provides remote users with access to shared robots for evaluating methods on common tasks and an open-source dataset of these tasks for offline training. Its manipulation task suite requires challenging generalization to unseen objects, positions, and lighting. We present initial results on TOTO comparing five pretrained visual representations and four offline policy learning baselines, remotely contributed by five institutions. The real promise of TOTO, however, lies in the future: we release the benchmark for additional submissions from any user, enabling easy, direct comparison to several methods without the need to obtain hardware or collect data.",
                "ieee_keywords": [
                    "Training",
                    "Visualization",
                    "Automation",
                    "Lighting",
                    "Benchmark testing",
                    "Robot learning",
                    "Hardware"
                ],
                "author_keywords": []
            },
            {
                "title": "Neural Topological SLAM for Visual Navigation",
                "link": "https://ieeexplore.ieee.org/document/9157610/",
                "date_of_publication": "05 August 2020",
                "doi": "10.1109/CVPR42600.2020.01289",
                "citations": "9",
                "abstract": "This paper studies the problem of image-goal navigation which involves navigating to the location indicated by a goal image in a novel previously unseen environment. To tackle this problem, we design topological representations for space that effectively leverage semantics and afford approximate geometric reasoning. At the heart of our representations are nodes with associated semantic features, that are interconnected using coarse geometric information. We describe supervised learning-based algorithms that can build, maintain and use such representations under noisy actuation. Experimental study in visually and physically realistic simulation suggests that our method builds effective representations that capture structural regularities and efficiently solve long-horizon navigation problems. We observe a relative improvement of more than 50% over existing methods that study this task.",
                "ieee_keywords": [
                    "Navigation",
                    "Task analysis",
                    "Semantics",
                    "Measurement",
                    "Training",
                    "Visualization",
                    "Simultaneous localization and mapping"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Zico Kolter",
        "publications": [
            {
                "title": "Model predictive control of industrial loads and energy storage for demand response",
                "link": "https://ieeexplore.ieee.org/document/7741228/",
                "date_of_publication": "14 November 2016",
                "doi": "10.1109/PESGM.2016.7741228",
                "citations": "17",
                "abstract": "With the potential to enhance the power system's operational flexibility in a cost-effective way, demand response is gaining increased attention worldwide. Industrial loads such as cement crushing plants consume large amounts of electric energy and therefore are prime candidates for the provision of significant amounts of demand response. They have the capability to turn on/off an arbitrary number of their crushers thereby adjusting their electric power consumption. However, the change in power consumption by cement crushing plants and also other industrial loads are often not granular enough to provide valuable ancillary services such as regulation and load following. In this paper, we propose a coordination method based on model predictive control to overcome the granularity restriction with the help of an energy storage.",
                "ieee_keywords": [
                    "Switches",
                    "Energy storage",
                    "Load management",
                    "Power demand",
                    "Load modeling",
                    "Predictive models"
                ],
                "author_keywords": [
                    "Demand response",
                    "industrial loads",
                    "model predictive control",
                    "ARMA prediction",
                    "regulation",
                    "load following"
                ]
            },
            {
                "title": "Deep Equilibrium Optical Flow Estimation",
                "link": "https://ieeexplore.ieee.org/document/9880309/",
                "date_of_publication": "27 September 2022",
                "doi": "10.1109/CVPR52688.2022.00070",
                "citations": "10",
                "abstract": "Many recent state-of-the-art (SOTA) optical flow models use finite-step recurrent update operations to emulate traditional algorithms by encouraging iterative refinements toward a stable flow estimation. However, these RNNs impose large computation and memory overheads, and are not directly trained to model such “stable estimation”. They can converge poorly and thereby suffer from performance degradation. To combat these drawbacks, we propose deep equilibrium (DEQ)flow estimators, an approach that directly solves for the flow as the infinite-level fixed point of an implicit layer (using any black-box solver) [3], and differentiates through this fixed point analytically (thus requiring O(1) training memory). This implicit-depth approach is not predicated on any specific model, and thus can be applied to a wide range of SOTA flow estimation model designs (e.g., RAFT [1] and GMA [2]). The use of these DEQflow estimators allows us to compute the flow faster using, e.g., fixed-point reuse and inexact gradients, consumes 4∼6×less training memory than the recurrent counterpart, and achieves better results with the same computation budget. In addition, we propose a novel, sparse fixed-point correction scheme to stabilize our DEQ flow estimators, which addresses a longstanding challenge for DEQ models in general. We test our approach in various realistic settings and show that it improves SOTA methods on Sintel and KITTI datasets with substantially better computational and memory efficiency.",
                "ieee_keywords": [
                    "Training",
                    "Degradation",
                    "Computer vision",
                    "Image motion analysis",
                    "Computational modeling",
                    "Memory management",
                    "Estimation"
                ],
                "author_keywords": [
                    "Deep learning architectures and techniques; Computational photography; Machine learning"
                ]
            },
            {
                "title": "Demand Response of Ancillary Service From Industrial Loads Coordinated With Energy Storage",
                "link": "https://ieeexplore.ieee.org/document/7930447/",
                "date_of_publication": null,
                "doi": "10.1109/TPWRS.2017.2704524",
                "citations": "80",
                "abstract": "As one of the featured initiatives in smart grids, demand response is enabling active participation of electricity consumers in the supply/demand balancing process, thereby enhancing the power system's operational flexibility in a cost-effective way. Industrial load plays an important role in demand response because of its intense power consumption, already existing advanced monitoring, and control infrastructure, and its strong economic incentive due to the high energy costs. As typical industrial loads, cement plants are able to quickly adjust their power consumption rate by switching on/off the crushers. However, in the cement plant as well as other industrial loads, switching on/off the loading units only achieves discrete power changes, which restricts the load from offering valuable ancillary services such as regulation and load following, as continuous power changes are required for these services. In this paper, we overcome this restriction of poor granularity by proposing methods that enable these loads to provide regulation or load following with the support of an onsite energy storage system.",
                "ieee_keywords": [
                    "Load management",
                    "Energy storage",
                    "Switches",
                    "Power demand",
                    "Optimal scheduling",
                    "Real-time systems"
                ],
                "author_keywords": [
                    "Demand response",
                    "energy storage system (ESS)",
                    "industrial load",
                    "load following",
                    "model predictive control",
                    "regulation"
                ]
            },
            {
                "title": "Large-scale probabilistic forecasting in energy systems using sparse Gaussian conditional random fields",
                "link": "https://ieeexplore.ieee.org/document/6760016/",
                "date_of_publication": "10 March 2014",
                "doi": "10.1109/CDC.2013.6760016",
                "citations": "14",
                "abstract": "Short-term forecasting is a ubiquitous practice in a wide range of energy systems, including forecasting demand, renewable generation, and electricity pricing. Although it is known that probabilistic forecasts (which give a distribution over possible future outcomes) can improve planning and control, many forecasting systems in practice are just used as “point forecast” tools, as it is challenging to represent high-dimensional non-Gaussian distributions over multiple spatial and temporal points. In this paper, we apply a recently-proposed algorithm for modeling high-dimensional conditional Gaussian distributions to forecasting wind power and extend it to the non-Gaussian case using the copula transform. On a wind power forecasting task, we show that this probabilistic model greatly outperforms other methods on the task of accurately modeling potential distributions of power (as would be necessary in a stochastic dispatch problem, for example).",
                "ieee_keywords": [
                    "Aggregates",
                    "Indium tin oxide",
                    "Predictive models",
                    "Probabilistic logic",
                    "Hafnium compounds",
                    "Optimization",
                    "Wind forecasting"
                ],
                "author_keywords": []
            },
            {
                "title": "Neural Network Virtual Sensors for Fuel Injection Quantities with Provable Performance Specifications",
                "link": "https://ieeexplore.ieee.org/document/9304765/",
                "date_of_publication": "08 January 2021",
                "doi": "10.1109/IV47402.2020.9304765",
                "citations": "1",
                "abstract": "Recent work has shown that it is possible to learn neural networks with provable guarantees on the output of the model when subject to input perturbations, however these works have focused primarily on defending against adversarial examples for image classifiers. In this paper, we study how these provable guarantees can be naturally applied to other real world settings, namely getting performance specifications for robust virtual sensors measuring fuel injection quantities within an engine. We first demonstrate that, in this setting, even simple neural network models are highly susceptible to reasonable levels of adversarial sensor noise, which are capable of increasing the mean relative error of a standard neural network from 6.6% to 43.8%. We then leverage methods for learning provably robust networks and verifying robustness properties, resulting in a robust model which we can provably guarantee has at most 16.5% mean relative error under any sensor noise. Additionally, we show how specific intervals of fuel injection quantities can be targeted to maximize robustness for certain ranges, allowing us to train a virtual sensor for fuel injection which is provably guaranteed to have at most 10.69% relative error under noise while maintaining 3% relative error on non-adversarial data within normalized fuel injection ranges of 0.6 to 1.0.",
                "ieee_keywords": [
                    "Sensors",
                    "Fuels",
                    "Neural networks",
                    "Training",
                    "Perturbation methods",
                    "Standards",
                    "Safety"
                ],
                "author_keywords": []
            },
            {
                "title": "DeepSplit: Scalable Verification of Deep Neural Networks via Operator Splitting",
                "link": "https://ieeexplore.ieee.org/document/9811356/",
                "date_of_publication": null,
                "doi": "10.1109/OJCSYS.2022.3187429",
                "citations": "907",
                "abstract": "Analyzing the worst-case performance of deep neural networks against input perturbations amounts to solving a large-scale non-convex optimization problem, for which several past works have proposed convex relaxations as a promising alternative. However, even for reasonably-sized neural networks, these relaxations are not tractable, and so must be replaced by even weaker relaxations in practice. In this work, we propose a novel operator splitting method that can directly solve a convex relaxation of the problem to high accuracy, by splitting it into smaller sub-problems that often have analytical solutions. The method is modular, scales to very large problem instances, and compromises of operations that are amenable to fast parallelization with GPU acceleration. We demonstrate our method in bounding the worst-case performance of large convolutional networks in image classification and reinforcement learning settings, and in reachability analysis of neural network dynamical systems. Topic: Intersection of Machine Learning with Control",
                "ieee_keywords": [],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Howie Choset",
        "publications": [
            {
                "title": "Snake Robot Urban Search After the 2017 Mexico City Earthquake",
                "link": "https://ieeexplore.ieee.org/document/8468633/",
                "date_of_publication": "20 September 2018",
                "doi": "10.1109/SSRR.2018.8468633",
                "citations": "41",
                "abstract": "The Carnegie Mellon University Biorobotics Laboratory was invited to bring snake robots to Mexico City to assist with search and rescue efforts in the wake of the September 2017 earthquake. We travelled with the Mexican Red Cross to collapsed building sites, and deployed a snake robot within one building to obtain a camera view in two voids that conventional search cameras could not access. We confirmed that the an open area within the building was unoccupied. In this paper we describe our experiences during the deployment and the limitations of snake robot platform encountered along the way.",
                "ieee_keywords": [
                    "Robot kinematics",
                    "Buildings",
                    "Snake robots",
                    "Cameras",
                    "Robot vision systems",
                    "Urban areas"
                ],
                "author_keywords": []
            },
            {
                "title": "Modeling rolling gaits of a snake robot",
                "link": "https://ieeexplore.ieee.org/document/7139719/",
                "date_of_publication": "02 July 2015",
                "doi": "10.1109/ICRA.2015.7139719",
                "citations": "13",
                "abstract": "Successful deployment of a snake robot in search and rescue tasks requires the capability of generating controls which can adapt to unknown environments in real-time. However, available motion generation techniques can be computationally expensive and lack the ability to adapt to the surroundings. This work considers modeling the rolling motion of a snake robot by applying the Bellows model with computation reduction techniques. One benefit of this is that controllers are defined with physically meaningful parameters, which in turn allows for higher level control of the robot. Another benefit is that it allows controllers to be defined by “composing shapes”, which enables developing controllers that can adapt to the surroundings. Using shape composition, we implemented a novel gait, named rolling hump, which forms a contour-fitting hump to negotiate obstacles. The efficacy of a snake robot climbing over obstacles by using the rolling hump is experimentally evaluated. An autonomous control strategy is presented and realized in simulation.",
                "ieee_keywords": [
                    "Robots",
                    "Joints",
                    "Shape",
                    "Mathematical model",
                    "Computational modeling",
                    "Bellows",
                    "Trajectory"
                ],
                "author_keywords": []
            },
            {
                "title": "Visual sensing for developing autonomous behavior in snake robots",
                "link": "https://ieeexplore.ieee.org/document/6907257/",
                "date_of_publication": "29 September 2014",
                "doi": "10.1109/ICRA.2014.6907257",
                "citations": "26",
                "abstract": "Snake robots are uniquely qualified to investigate a large variety of settings including archaeological sites, natural disaster zones, and nuclear power plants. For these applications, modular snake robots have been tele-operated to perform specific tasks using images returned to it from an onboard camera in the robots head. In order to give the operator an even richer view of the environment and to enable the robot to perform autonomous tasks we developed a structured light sensor that can make three-dimensional maps of the environment. This paper presents a sensor that is uniquely qualified to meet the severe constraints in size, power and computational footprint of snake robots. Using range data, in the form of 3D pointclouds, we show that it is possible to pair high-level planning with mid-level control to accomplish complex tasks without operator intervention.",
                "ieee_keywords": [
                    "Robot sensing systems",
                    "Cameras",
                    "Three-dimensional displays",
                    "Head",
                    "Robot kinematics",
                    "Planning"
                ],
                "author_keywords": []
            },
            {
                "title": "Optimal control for geometric motion planning of a robot diver",
                "link": "https://ieeexplore.ieee.org/document/7759702/",
                "date_of_publication": "01 December 2016",
                "doi": "10.1109/IROS.2016.7759702",
                "citations": "3",
                "abstract": "Inertial reorientation of airborne articulated bodies has been an active area of research in the robotics community, as this behavior can help guide dynamic robots to a safe landing with minimal damage. The main objective of this work is emulating the aggressive and large angle correction maneuvers, like somersaults, that are performed by human divers. To this end, a planar three link robot, called DiverBot, is proposed. By considering a gravity-free scenario, a local connection is obtained between joint angles and the body orientation, resulting in a reduction in the system dynamics. An optimal control policy applied on this reduced configuration space yielded diving maneuvers that are dynamically feasible. Numerical results show that the DiverBot can execute one somersault without drift and multiple somersaults with minimal drift.",
                "ieee_keywords": [
                    "Optimal control",
                    "Shape",
                    "Optimization",
                    "Robot kinematics",
                    "Dynamics",
                    "Numerical models"
                ],
                "author_keywords": []
            },
            {
                "title": "Design and architecture of a series elastic snake robot",
                "link": "https://ieeexplore.ieee.org/document/6943219/",
                "date_of_publication": "06 November 2014",
                "doi": "10.1109/IROS.2014.6943219",
                "citations": "65",
                "abstract": "This paper details the design and architecture of a series elastic actuated snake robot, the SEA Snake. The robot consists of a series chain of 1-DOF modules that are capable of torque, velocity and position control. Additionally, each module includes a high-speed Ethernet communications bus, internal IMU, modular electro-mechanical interface, and ARM based on-board control electronics.",
                "ieee_keywords": [
                    "Robot sensing systems",
                    "Torque",
                    "Cameras",
                    "Gears",
                    "Connectors",
                    "Springs"
                ],
                "author_keywords": []
            },
            {
                "title": "Design of a Biomimetic Tactile Sensor for Material Classification",
                "link": "https://ieeexplore.ieee.org/document/9811543/",
                "date_of_publication": "12 July 2022",
                "doi": "10.1109/ICRA46639.2022.9811543",
                "citations": "1",
                "abstract": "Tactile sensing typically involves active exploration of unknown surfaces and objects, making it especially effective at processing the characteristics of materials and textures. A key property extracted by human tactile perception in material classification is surface roughness, which relies on measuring vibratory signals using the multi-layered fingertip structure. Existing robotic systems lack tactile sensors that are able to provide high dynamic sensing ranges, perceive material properties, and maintain a low hardware cost. In this work, we introduce the reference design and fabrication procedure of a miniature and low-cost tactile sensor consisting of a biomimetic cutaneous structure, including the artificial fingerprint, dermis, epidermis, and an embedded magnet-sensor structure which serves as a mechanoreceptor for converting mechanical information to digital signals. The presented sensor is capable of detecting high-resolution magnetic field data through the Hall effect and creating high-dimensional time-frequency domain features for material texture classification. Additionally, we investigate the effects of different superficial sensor fingerprint patterns for classifying materials through both simulation and physical experimentation. After extracting time series and frequency domain features, we assess a k-nearest neighbors classifier for distinguishing between different materials. The results from our experiments show that our biomimetic tactile sensors with fingerprint ridges can classify materials with more than 7.7% higher accuracy and lower variability than ridge-less sensors. These results, along with the low cost and customizability of our sensor, demonstrate high potential for lowering the barrier to entry for a wide array of robotic applications, including modelless tactile sensing for texture classification, material inspection, and object recognition.",
                "ieee_keywords": [
                    "Tactile sensors",
                    "Fingerprint recognition",
                    "Feature extraction",
                    "Surface roughness",
                    "Sensors",
                    "Rough surfaces",
                    "Object recognition"
                ],
                "author_keywords": []
            },
            {
                "title": "Minimalistic, dynamic, tube climbing robot",
                "link": "https://ieeexplore.ieee.org/document/5509948/",
                "date_of_publication": "15 July 2010",
                "doi": "10.1109/ROBOT.2010.5509948",
                "citations": "8",
                "abstract": "This video shows the investigation of a novel minimalistic, dynamic climbing robot which can climb up tubes of different shapes using a simple dc motor. The motor moves an eccentric mass in a constant velocity. The location of the eccentric mass relative to the contact point determines the stability and the direction of the climbing motion. We present the analysis of this mechanism, simulation and experimental results.",
                "ieee_keywords": [
                    "Climbing robots",
                    "Friction",
                    "Structural rings",
                    "DC motors",
                    "Anisotropic magnetoresistance",
                    "Stability",
                    "Torque",
                    "Robotics and automation",
                    "USA Councils",
                    "Videoconference"
                ],
                "author_keywords": []
            },
            {
                "title": "A Tunable Magnet-based Tactile Sensor Framework",
                "link": "https://ieeexplore.ieee.org/document/9278634/",
                "date_of_publication": "09 December 2020",
                "doi": "10.1109/SENSORS47125.2020.9278634",
                "citations": "2",
                "abstract": "Tactile sensing enables controllable interactions as robots enter unknown and unstructured environments. However, existing tactile sensors suffer from limited form factors and durability, lack of dynamic range, and are not cost-effective. Magnet-elastomer-based tactile sensors offer a potential solution to compensate for these deficiencies. In this paper, we present a generalizable design approach, a first-order model based on the magnetic field and elastomer fundamentals, and an automated calibration routine to create custom tactile sensors. To demonstrate the performance and versatility of the proposed sensor architecture, we selected two sample design configurations: a small form factor that was optimized for tumor localization (Pinky) and a larger form factor for robust contact estimation on legged robots (Foot). These two unique cases demonstrate the breadth of sensors that can be designed using this approach as well as how changes in sensor parameters can be used to tune the range and resolution for different applications.",
                "ieee_keywords": [
                    "Robot sensing systems",
                    "Magnetomechanical effects",
                    "Magnetic hysteresis",
                    "Robots",
                    "Foot",
                    "Tactile sensors",
                    "Strain"
                ],
                "author_keywords": []
            },
            {
                "title": "A Compact and Infrastructure-free Confined Space Sensor for 3D Scanning and SLAM",
                "link": "https://ieeexplore.ieee.org/document/9278586/",
                "date_of_publication": "09 December 2020",
                "doi": "10.1109/SENSORS47125.2020.9278586",
                "citations": "1",
                "abstract": "High-precision inspection and metrology in short-range and tight spaces are challenging due to the lack of a commercial off-the-shelf (COTS) 3D scanner that is compact and does not rely on any external infrastructure (e.g., fiducial markers, motion-capture cameras, or laser tracking interferometer) to provide positioning or localization support. This paper presents a hardware and software design framework for creating a low-cost, miniature, yet intelligent sensor that is able to capture visual imagery, reconstruct 3D geometry, and most importantly, perform Simultaneous Localization and Mapping (SLAM) without sensory feedback from external devices. We further present an example sensor design that follows this framework, with experiments to evaluate and compare the sensor performance against a state-of-the-art COTS sensor, under both feature-sparse and feature-dense scenarios to simulate industrial and household 3D scanning application scenarios.",
                "ieee_keywords": [
                    "Three-dimensional displays",
                    "Lasers",
                    "Robot sensing systems",
                    "Simultaneous localization and mapping",
                    "Cameras",
                    "Image reconstruction",
                    "Software"
                ],
                "author_keywords": []
            },
            {
                "title": "Toward Robotically Automated Femoral Vascular Access",
                "link": "https://ieeexplore.ieee.org/document/9661560/",
                "date_of_publication": "03 January 2022",
                "doi": "10.1109/ISMR48346.2021.9661560",
                "citations": "1",
                "abstract": "Advanced resuscitative technologies, such as Extra Corporeal Membrane Oxygenation (ECMO) cannulation or Resuscitative Endovascular Balloon Occlusion of the Aorta (REBOA), are technically difficult even for skilled medical personnel. This paper describes the core technologies that comprise a teleoperated system capable of granting femoral vascular access, an essential step in these procedures, and a significant roadblock in their broader use in the field. These technologies include a kinematic manipulator, various sensing modalities, and a user interface. In addition, we evaluate our system on a surgical phantom as well as in-vivo porcine experiments. To the best of our knowledge, these resulted in the first robot-assisted arterial catheterizations, a significant step towards our eventual goal of automatic catheter insertion through the Seldinger technique.",
                "ieee_keywords": [
                    "Surface reconstruction",
                    "Ultrasonic imaging",
                    "Three-dimensional displays",
                    "Wires",
                    "User interfaces",
                    "Robot sensing systems",
                    "Needles"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Deva Ramanan",
        "publications": [
            {
                "title": "Are we Asking the Right Questions in MovieQA?",
                "link": "https://ieeexplore.ieee.org/document/9022572/",
                "date_of_publication": "05 March 2020",
                "doi": "10.1109/ICCVW.2019.00235",
                "citations": "4",
                "abstract": "Joint vision and language tasks like visual question answering are fascinating because they explore high-level understanding, but at the same time, can be more prone to language biases. In this paper, we explore the biases in the MovieQA dataset and propose a strikingly simple model which can exploit them. We find that using the right word embedding is of utmost importance. By using an appropriately-trained word embedding, about half the Question-Answers (QAs) can be answered by looking at the questions and answers alone, completely ignoring narrative context from video clips, subtitles, and movie scripts. Compared to the best published papers on the leaderboard, our simple question+answer only model improves accuracy by 5% for video + subtitle category, 5% for subtitle, 15% for DVS and 6% higher for scripts.",
                "ieee_keywords": [
                    "Motion pictures",
                    "Task analysis",
                    "Visualization",
                    "Google",
                    "Information services",
                    "Electronic publishing",
                    "Internet"
                ],
                "author_keywords": [
                    "joint language vision"
                ]
            },
            {
                "title": "4D Visualization of Dynamic Events From Unconstrained Multi-View Videos",
                "link": "https://ieeexplore.ieee.org/document/9156775/",
                "date_of_publication": "05 August 2020",
                "doi": "10.1109/CVPR42600.2020.00541",
                "citations": "28",
                "abstract": "We present a data-driven approach for 4D space-time visualization of dynamic events from videos captured by hand-held multiple cameras. Key to our approach is the use of self-supervised neural networks specific to the scene to compose static and dynamic aspects of an event. Though captured from discrete viewpoints, this model enables us to move around the space-time of the event continuously. This model allows us to create virtual cameras that facilitate: (1) freezing the time and exploring views; (2) freezing a view and moving through time; and (3) simultaneously changing both time and view. We can also edit the videos and reveal occluded objects for a given view if it is visible in any of the other views. We validate our approach on challenging in-the-wild events captured using up to 15 mobile cameras.",
                "ieee_keywords": [
                    "Cameras",
                    "Three-dimensional displays",
                    "Videos",
                    "Image reconstruction",
                    "Computational modeling",
                    "Estimation",
                    "Visualization"
                ],
                "author_keywords": []
            },
            {
                "title": "Safe Local Motion Planning with Self-Supervised Freespace Forecasting",
                "link": "https://ieeexplore.ieee.org/document/9577522/",
                "date_of_publication": "02 November 2021",
                "doi": "10.1109/CVPR46437.2021.01254",
                "citations": "9",
                "abstract": "Safe local motion planning for autonomous driving in dynamic environments requires forecasting how the scene evolves. Practical autonomy stacks adopt a semantic object-centric representation of a dynamic scene and build object detection, tracking, and prediction modules to solve forecasting. However, training these modules comes at an enormous human cost of manually annotated objects across frames. In this work, we explore future freespace as an alternative representation to support motion planning. Our key intuition is that it is important to avoid straying into occupied space regardless of what is occupying it. Importantly, computing ground-truth future freespace is annotation-free. First, we explore freespace forecasting as a self-supervised learning task. We then demonstrate how to use forecasted freespace to identify collision-prone plans from off-the-shelf motion planners. Finally, we propose future freespace as an additional source of annotation-free supervision. We demonstrate how to integrate such supervision into the learning-based planners. Experimental results on nuScenes and CARLA suggest both approaches lead to a significant reduction in collision rates. 1",
                "ieee_keywords": [
                    "Training",
                    "Costs",
                    "Tracking",
                    "Dynamics",
                    "Semantics",
                    "Object detection",
                    "Planning"
                ],
                "author_keywords": []
            },
            {
                "title": "Forecasting from LiDAR via Future Object Detection",
                "link": "https://ieeexplore.ieee.org/document/9880029/",
                "date_of_publication": "27 September 2022",
                "doi": "10.1109/CVPR52688.2022.01669",
                "citations": "6",
                "abstract": "Object detection and forecasting are fundamental components of embodied perception. These two problems, how-ever, are largely studied in isolation by the community. In this paper, we propose an end-to-end approachfor detection and motion forecasting based on raw sensor measurement as opposed to ground truth tracks. Instead of predicting the current frame locations and forecasting forward in time, we directly predict future object locations and backcast to determine where each trajectory began. Our approach not only improves overall accuracy compared to other modular or end-to-end baselines, it also prompts us to rethink the role of explicit tracking for embodied perception. Additionally, by linking future and current locations in a many-to-one manner, our approach is able to reason about multiple futures, a capability that was previously considered difficult for end-to-end approaches. We conduct extensive experi-ments on the popular nuScenes dataset and demonstrate the empirical effectiveness of our approach. In addition, we investigate the appropriateness of reusing standard forecasting metrics for an end-to-end setup, and find a number of limitations which allow us to build simple baselines to game these metrics. We address this issue with a novel set of joint forecasting and detection metrics that extend the commonly used AP metrics from the detection community to measuring forecasting accuracy. Our code is available on GitHub.",
                "ieee_keywords": [
                    "Measurement",
                    "Analytical models",
                    "Laser radar",
                    "Tracking",
                    "Object detection",
                    "Predictive models",
                    "Trajectory"
                ],
                "author_keywords": [
                    "Navigation and autonomous driving; Motion and tracking"
                ]
            },
            {
                "title": "Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models",
                "link": "https://ieeexplore.ieee.org/document/10205126/",
                "date_of_publication": "22 August 2023",
                "doi": "10.1109/CVPR52729.2023.01852",
                "citations": "2",
                "abstract": "The ability to quickly learn a new task with minimal instruction - known as few-shot learning - is a central aspect of intelligent agents. Classical few-shot benchmarks make use of few-shot samples from a single modality, but such samples may not be sufficient to characterize an entire concept class. In contrast, humans use cross-modal information to learn new concepts efficiently. In this work, we demonstrate that one can indeed build a better visual dog classifier by reading about dogs and listening to them bark. To do so, we exploit the fact that recent multimodal foundation models such as CLIP are inherently cross-modal, mapping different modalities to the same representation space. Specifically, we propose a simple cross-modal adaptation approach that learns from few-shot examples spanning different modalities. By repurposing class names as additional one-shot training samples, we achieve SOTA results with an embarrassingly simple linear classifier for vision-language adaptation. Furthermore, we show that our approach can benefit existing methods such as prefix tuning, adapters, and classifier ensembling. Finally, to explore other modalities beyond vision and language, we construct the first (to our knowledge) audiovisual few-shot benchmark and use cross-modal training to improve the performance of both image and audio classification. Project site at link.",
                "ieee_keywords": [
                    "Training",
                    "Adaptation models",
                    "Visualization",
                    "Computer vision",
                    "Dogs",
                    "Benchmark testing",
                    "Pattern recognition"
                ],
                "author_keywords": [
                    "Recognition: Categorization",
                    "detection",
                    "retrieval"
                ]
            },
            {
                "title": "Opening up Open World Tracking",
                "link": "https://ieeexplore.ieee.org/document/9880207/",
                "date_of_publication": "27 September 2022",
                "doi": "10.1109/CVPR52688.2022.01846",
                "citations": "4",
                "abstract": "Tracking and detecting any object, including ones never-seen-before during model training, is a crucial but elusive capability of autonomous systems. An autonomous agent that is blind to never-seen-before objects poses a safety hazard when operating in the real world - and yet this is how almost all current systems work. One of the main obstacles towards advancing tracking any object is that this task is notoriously difficult to evaluate. A benchmark that would allow us to perform an apples-to-apples comparison of existing efforts is a crucial first step towards advancing this important research field. This paper addresses this evaluation deficit and lays out the landscape and evaluation methodology for detecting and tracking both known and unknown objects in the open-world setting. We propose a new benchmark, TAOOW. Tracking Any Object in an Open World, analyze existing efforts in multi-object tracking, and construct a baseline for this task while highlighting future challenges. We hope to open a new front in multi-object tracking research that will hopefully bring us a step closer to intelligent systems that can operate safely in the real world.",
                "ieee_keywords": [
                    "Training",
                    "Heart",
                    "Computer vision",
                    "Autonomous systems",
                    "Benchmark testing",
                    "Hazards",
                    "Pattern recognition"
                ],
                "author_keywords": [
                    "Motion and tracking; Datasets and evaluation"
                ]
            },
            {
                "title": "Need for Speed: A Benchmark for Higher Frame Rate Object Tracking",
                "link": "https://ieeexplore.ieee.org/document/8237390/",
                "date_of_publication": "25 December 2017",
                "doi": "10.1109/ICCV.2017.128",
                "citations": "218",
                "abstract": "In this paper, we propose the first higher frame rate video dataset (called Need for Speed - NfS) and benchmark for visual object tracking. The dataset consists of 100 videos (380K frames) captured with now commonly available higher frame rate (240 FPS) cameras from real world scenarios. All frames are annotated with axis aligned bounding boxes and all sequences are manually labelled with nine visual attributes - such as occlusion, fast motion, background clutter, etc. Our benchmark provides an extensive evaluation of many recent and state-of-the-art trackers on higher frame rate sequences. We ranked each of these trackers according to their tracking accuracy and real-time performance. One of our surprising conclusions is that at higher frame rates, simple trackers such as correlation filters outperform complex methods based on deep networks. This suggests that for practical applications (such as in robotics or embedded vision), one needs to carefully tradeoff bandwidth constraints associated with higher frame rate acquisition, computational costs of real-time analysis, and the required application accuracy. Our dataset and benchmark allows for the first time (to our knowledge) systematic exploration of such issues, and will be made available to allow for further research in this space.",
                "ieee_keywords": [
                    "Visualization",
                    "Cameras",
                    "Object tracking",
                    "Correlation",
                    "Benchmark testing",
                    "Real-time systems"
                ],
                "author_keywords": []
            },
            {
                "title": "3D-aware Conditional Image Synthesis",
                "link": "https://ieeexplore.ieee.org/document/10203565/",
                "date_of_publication": "22 August 2023",
                "doi": "10.1109/CVPR52729.2023.00431",
                "citations": "1",
                "abstract": "We propose pix2pix3D, a 3D-aware conditional generative model for controllable photorealistic image synthesis. Given a 2D label map, such as a segmentation or edge map, our model learns to synthesize a corresponding image from different viewpoints. To enable explicit 3D user control, we extend conditional generative models with neural radiance fields. Given widely-available posed monocular image and label map pairs, our model learns to assign a label to every 3D point in addition to color and density, which enables it to render the image and pixel-aligned label map simultaneously. Finally, we build an interactive system that allows users to edit the label map from different viewpoints and generate outputs accordingly.",
                "ieee_keywords": [
                    "Solid modeling",
                    "Image segmentation",
                    "Computer vision",
                    "Three-dimensional displays",
                    "Image synthesis",
                    "Image color analysis",
                    "Interactive systems"
                ],
                "author_keywords": [
                    "Image and video synthesis and generation"
                ]
            },
            {
                "title": "Point Cloud Forecasting as a Proxy for 4D Occupancy Forecasting",
                "link": "https://ieeexplore.ieee.org/document/10204018/",
                "date_of_publication": "22 August 2023",
                "doi": "10.1109/CVPR52729.2023.00114",
                "citations": "3",
                "abstract": "Predicting how the world can evolve in the future is crucial for motion planning in autonomous systems. Classical methods are limited because they rely on costly human annotations in the form of semantic class labels, bounding boxes, and tracks or HD maps of cities to plan their motion - and thus are difficult to scale to large unlabeled datasets. One promising self-supervised task is 3D point cloud forecasting [11, 18–20] from unannotated LiDAR sequences. We show that this task requires algorithms to implicitly capture (1) sensor extrinsics (i.e., the egomotion of the autonomous vehicle), (2) sensor intrinsics (i.e., the sampling pattern specific to the particular LiDAR sensor), and (3) the shape and motion of other objects in the scene. But autonomous systems should make predictions about the world and not their sensors! To this end, we factor out (1) and (2) by recasting the task as one of spacetime (4D) occupancy forecasting. But because it is expensive to obtain ground-truth 4D occupancy, we “render” point cloud data from 4D occupancy predictions given sensor extrinsics and intrinsics, allowing one to train and test occupancy algorithms with unannotated LiDAR sequences. This also allows one to evaluate and compare point cloud forecasting algorithms across diverse datasets, sensors, and vehicles.",
                "ieee_keywords": [
                    "Point cloud compression",
                    "Laser radar",
                    "Three-dimensional displays",
                    "Shape",
                    "Tracking",
                    "Urban areas",
                    "Prediction algorithms"
                ],
                "author_keywords": [
                    "3D from multi-view and sensors"
                ]
            },
            {
                "title": "Reconstructing Animatable Categories from Videos",
                "link": "https://ieeexplore.ieee.org/document/10204503/",
                "date_of_publication": "22 August 2023",
                "doi": "10.1109/CVPR52729.2023.01630",
                "citations": "4",
                "abstract": "Building animatable 3D models is challenging due to the need for 3D scans, laborious registration, and rigging. Recently, differentiable rendering provides a pathway to obtain high-quality 3D models from monocular videos, but these are limited to rigid categories or single instances. We present RAC, a method to build category-level 3D models from monocular videos, disentangling variations over instances and motion over time. Three key ideas are introduced to solve this problem: (1) specializing a category-level skeleton to instances, (2) a method for latent space regularization that encourages shared structure across a category while maintaining instance details, and (3) using 3D background models to disentangle objects from the background. We build 3D models for humans, cats and dogs given monocular videos. Project page: https://gengshan-y.github.io/rac-www/.",
                "ieee_keywords": [
                    "Solid modeling",
                    "Computer vision",
                    "Three-dimensional displays",
                    "Computational modeling",
                    "Buildings",
                    "Dogs",
                    "Rendering (computer graphics)"
                ],
                "author_keywords": [
                    "3D from single images"
                ]
            }
        ]
    },
    {
        "name": "martial hebert",
        "publications": [
            {
                "title": "Visual sensing for developing autonomous behavior in snake robots",
                "link": "https://ieeexplore.ieee.org/document/6907257/",
                "date_of_publication": "29 September 2014",
                "doi": "10.1109/ICRA.2014.6907257",
                "citations": "26",
                "abstract": "Snake robots are uniquely qualified to investigate a large variety of settings including archaeological sites, natural disaster zones, and nuclear power plants. For these applications, modular snake robots have been tele-operated to perform specific tasks using images returned to it from an onboard camera in the robots head. In order to give the operator an even richer view of the environment and to enable the robot to perform autonomous tasks we developed a structured light sensor that can make three-dimensional maps of the environment. This paper presents a sensor that is uniquely qualified to meet the severe constraints in size, power and computational footprint of snake robots. Using range data, in the form of 3D pointclouds, we show that it is possible to pair high-level planning with mid-level control to accomplish complex tasks without operator intervention.",
                "ieee_keywords": [
                    "Robot sensing systems",
                    "Cameras",
                    "Three-dimensional displays",
                    "Head",
                    "Robot kinematics",
                    "Planning"
                ],
                "author_keywords": []
            },
            {
                "title": "An integrated system for autonomous robotics manipulation",
                "link": "https://ieeexplore.ieee.org/document/6385888/",
                "date_of_publication": "20 December 2012",
                "doi": "10.1109/IROS.2012.6385888",
                "citations": "65",
                "abstract": "We describe the software components of a robotics system designed to autonomously grasp objects and perform dexterous manipulation tasks with only high-level supervision. The system is centered on the tight integration of several core functionalities, including perception, planning and control, with the logical structuring of tasks driven by a Behavior Tree architecture. The advantage of the implementation is to reduce the execution time while integrating advanced algorithms for autonomous manipulation. We describe our approach to 3-D perception, real-time planning, force compliant motions, and audio processing. Performance results for object grasping and complex manipulation tasks of in-house tests and of an independent evaluation team are presented.",
                "ieee_keywords": [
                    "Robot sensing systems",
                    "Planning",
                    "Software",
                    "Computer architecture",
                    "Grasping",
                    "Software algorithms"
                ],
                "author_keywords": []
            },
            {
                "title": "Physical querying with multi-modal sensing",
                "link": "https://ieeexplore.ieee.org/document/6836103/",
                "date_of_publication": "23 June 2014",
                "doi": "10.1109/WACV.2014.6836103",
                "citations": "98",
                "abstract": "We present Marvin, a system that can search physical objects using a mobile or wearable device. It integrates HOG-based object recognition, SURF-based localization information, automatic speech recognition, and user feedback information with a probabilistic model to recognize the “object of interest” at high accuracy and at interactive speeds. Once the object of interest is recognized, the information that the user is querying, e.g. reviews, options, etc., is displayed on the user's mobile or wearable device. We tested this prototype in a real-world retail store during business hours, with varied degree of background noise and clutter. We show that this multi-modal approach achieves superior recognition accuracy compared to using a vision system alone, especially in cluttered scenes where a vision system would be unable to distinguish which object is of interest to the user without additional input. It is computationally able to scale to large numbers of objects by focusing compute-intensive resources on the objects most likely to be of interest, inferred from user speech and implicit localization information. We present the system architecture, the probabilistic model that integrates the multi-modal information, and empirical results showing the benefits of multi-modal integration.",
                "ieee_keywords": [
                    "Speech",
                    "Object recognition",
                    "Feature extraction",
                    "Speech recognition",
                    "Computer architecture",
                    "Context",
                    "Visualization"
                ],
                "author_keywords": []
            },
            {
                "title": "Map-supervised road detection",
                "link": "https://ieeexplore.ieee.org/document/7535374/",
                "date_of_publication": "08 August 2016",
                "doi": "10.1109/IVS.2016.7535374",
                "citations": "57",
                "abstract": "We propose an approach to detect drivable road area in monocular images. It is a self-supervised approach which doesn't require any human road annotations on images to train the road detection algorithm. Our approach reduces human labeling effort and makes training scalable. We combine the best of both supervised and unsupervised methods in our approach. First, we automatically generate training road annotations for images using OpenStreetMap 1 , vehicle pose estimation sensors, and camera parameters. Next, we train a Convolutional Neural Network (CNN) for road detection using these annotations. We show that we are able to generate reasonably accurate training annotations in KITTI data-set [1]. We achieve state-of-the-art performance among the methods which do not require human annotation effort.",
                "ieee_keywords": [
                    "Roads",
                    "Training",
                    "Labeling",
                    "Vehicles",
                    "Sensors",
                    "Cameras",
                    "Noise measurement"
                ],
                "author_keywords": []
            },
            {
                "title": "MAPPER: Multi-Agent Path Planning with Evolutionary Reinforcement Learning in Mixed Dynamic Environments",
                "link": "https://ieeexplore.ieee.org/document/9340876/",
                "date_of_publication": "10 February 2021",
                "doi": "10.1109/IROS45743.2020.9340876",
                "citations": "28",
                "abstract": "Multi-agent navigation in dynamic environments is of great industrial value when deploying a large scale fleet of robot to real-world applications. This paper proposes a decentralized partially observable multi-agent path planning with evolutionary reinforcement learning (MAPPER) method to learn an effective local planning policy in mixed dynamic environments. Reinforcement learning-based methods usually suffer performance degradation on long-horizon tasks with goal-conditioned sparse rewards, so we decompose the long-range navigation task into many easier sub-tasks under the guidance of a global planner, which increases agents' performance in large environments. Moreover, most existing multi-agent planning approaches assume either perfect information of the surrounding environment or homogeneity of nearby dynamic agents, which may not hold in practice. Our approach models dynamic obstacles' behavior with an image-based representation and trains a policy in mixed dynamic environments without homogeneity assumption. To ensure multi-agent training stability and performance, we propose an evolutionary training approach that can be easily scaled to large and complex environments. Experiments show that MAPPER is able to achieve higher success rates and more stable performance when exposed to a large number of non-cooperative dynamic obstacles compared with traditional reaction-based planner LRA* and the state-of-the-art learning-based method.",
                "ieee_keywords": [
                    "Training",
                    "Learning systems",
                    "Navigation",
                    "Reinforcement learning",
                    "Path planning",
                    "Planning",
                    "Task analysis"
                ],
                "author_keywords": []
            },
            {
                "title": "First-Person Vision",
                "link": "https://ieeexplore.ieee.org/document/6232429/",
                "date_of_publication": null,
                "doi": "10.1109/JPROC.2012.2200554",
                "citations": "78",
                "abstract": "For understanding the behavior, intent, and environment of a person, the surveillance metaphor is traditional; that is, install cameras and observe the subject, and his/her interaction with other people and the environment. Instead, we argue that the first-person vision (FPV), which senses the environment and the subject's activities from a wearable sensor, is more advantageous with images about the subject's environment as taken from his/her view points, and with readily available information about head motion and gaze through eye tracking. In this paper, we review key research challenges that need to be addressed to develop such FPV systems, and describe our ongoing work to address them using examples from our prototype systems.",
                "ieee_keywords": [
                    "Surveillance",
                    "Image matching",
                    "Object recognition",
                    "Wearable sensors",
                    "Tracking",
                    "Videos",
                    "Computer vision",
                    "Service robots",
                    "Intelligent systems",
                    "Quality assessment",
                    "Behavioral science"
                ],
                "author_keywords": [
                    "Computer vision",
                    "eye tracking",
                    "object recognition",
                    "quality of life technologies"
                ]
            },
            {
                "title": "Learning monocular reactive UAV control in cluttered natural environments",
                "link": "https://ieeexplore.ieee.org/document/6630809/",
                "date_of_publication": "17 October 2013",
                "doi": "10.1109/ICRA.2013.6630809",
                "citations": "199",
                "abstract": "Autonomous navigation for large Unmanned Aerial Vehicles (UAVs) is fairly straight-forward, as expensive sensors and monitoring devices can be employed. In contrast, obstacle avoidance remains a challenging task for Micro Aerial Vehicles (MAVs) which operate at low altitude in cluttered environments. Unlike large vehicles, MAVs can only carry very light sensors, such as cameras, making autonomous navigation through obstacles much more challenging. In this paper, we describe a system that navigates a small quadrotor helicopter autonomously at low altitude through natural forest environments. Using only a single cheap camera to perceive the environment, we are able to maintain a constant velocity of up to 1.5m/s. Given a small set of human pilot demonstrations, we use recent state-of-the-art imitation learning techniques to train a controller that can avoid trees by adapting the MAVs heading. We demonstrate the performance of our system in a more controlled environment indoors, and in real natural forest environments outdoors.",
                "ieee_keywords": [
                    "Training",
                    "Cameras",
                    "Trajectory",
                    "Optical imaging",
                    "Sensors",
                    "Visualization",
                    "Vegetation"
                ],
                "author_keywords": []
            },
            {
                "title": "PCN: Point Completion Network",
                "link": "https://ieeexplore.ieee.org/document/8491026/",
                "date_of_publication": "14 October 2018",
                "doi": "10.1109/3DV.2018.00088",
                "citations": "346",
                "abstract": "Shape completion, the problem of estimating the complete geometry of objects from partial observations, lies at the core of many vision and robotics applications. In this work, we propose Point Completion Network (PCN), a novel learning-based approach for shape completion. Unlike existing shape completion methods, PCN directly operates on raw point clouds without any structural assumption (e.g. symmetry) or annotation (e.g. semantic class) about the underlying shape. It features a decoder design that enables the generation of fine-grained completions while maintaining a small number of parameters. Our experiments show that PCN produces dense, complete point clouds with realistic structures in the missing regions on inputs with various levels of incompleteness and noise, including cars from LiDAR scans in the KITTI dataset.",
                "ieee_keywords": [
                    "Three-dimensional displays",
                    "Shape",
                    "Decoding",
                    "Geometry",
                    "Automobiles",
                    "Solid modeling",
                    "Neural networks"
                ],
                "author_keywords": [
                    "shape completion",
                    "learning on point clouds",
                    "3D reconstruction",
                    "point cloud registration"
                ]
            },
            {
                "title": "Prop-free pointing detection in dynamic cluttered environments",
                "link": "https://ieeexplore.ieee.org/document/5771428/",
                "date_of_publication": "19 May 2011",
                "doi": "10.1109/FG.2011.5771428",
                "citations": "15",
                "abstract": "Vision-based prop-free pointing detection is challenging both from an algorithmic and a systems standpoint. From a computer vision perspective, accurately determining where multiple users are pointing is difficult in cluttered environments with dynamic scene content. Standard approaches relying on appearance models or background subtraction to segment users operate poorly in this domain. We propose a method that focuses on motion analysis to detect pointing gestures and robustly estimate the pointing direction. Our algorithm is self-initializing; as the user points, we analyze the observed motion from two cameras and infer rotation centers that best explain the observed motion. From these, we group pixel-level flow into dominant pointing vectors that each originate from a rotation center and merge across views to obtain 3D pointing vectors. However, our proposed algorithm is computationally expensive, posing systems challenges even with current computing infrastructure. We achieve interactive speeds by exploiting coarse-grained parallelization over a cluster of computers. In unconstrained environments, we obtain an average angular precision of 2.7°.",
                "ieee_keywords": [
                    "Cameras",
                    "Three dimensional displays",
                    "Tracking",
                    "Robustness",
                    "Trajectory",
                    "Noise measurement",
                    "Streaming media"
                ],
                "author_keywords": []
            },
            {
                "title": "Inferring door locations from a teammate's trajectory in stealth human-robot team operations",
                "link": "https://ieeexplore.ieee.org/document/7354127/",
                "date_of_publication": "17 December 2015",
                "doi": "10.1109/IROS.2015.7354127",
                "citations": "1",
                "abstract": "Robot perception is generally viewed as the interpretation of data from various types of sensors such as cameras. In this paper, we study indirect perception where a robot can perceive new information by making inferences from non-visual observations of human teammates. As a proof-of-concept study, we specifically focus on a door detection problem in a stealth mission setting where a team operation must not be exposed to the visibility of the team's opponents. We use a special type of the Noisy-OR model known as BN2O model of Bayesian inference network to represent the inter-visibility and to infer the locations of the doors, i.e., potential locations of the opponents. Experimental results on both synthetic data and real person tracking data achieve an F-measure of over .9 on average, suggesting further investigation on the use of non-visual perception in human-robot team operations.",
                "ieee_keywords": [
                    "Buildings",
                    "Noise measurement",
                    "Robot sensing systems",
                    "Bayes methods",
                    "Visualization"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Elaine Shi",
        "publications": [
            {
                "title": "Locally Differentially Private Sparse Vector Aggregation",
                "link": "https://ieeexplore.ieee.org/document/9833635/",
                "date_of_publication": "27 July 2022",
                "doi": "10.1109/SP46214.2022.9833635",
                "citations": "4",
                "abstract": "Vector mean estimation is a central primitive in federated analytics. In vector mean estimation, each user i∈[n] holds a real-valued vector v i ∈[−1,1 ] d , and a server wants to estimate the mean of all n vectors; we would additionally like to protect each user’s privacy. In this paper, we consider the k-sparse version of the vector mean estimation problem. That is, suppose each user’s vector has at most k non-zero coordinates in its d-dimensional vector, and moreover, k≪d . In practice, since the universe size d can be very large (e.g., the space of all possible URLs), we would like the per-user communication to be succinct, i.e., independent of or (poly-)logarithmic in the universe size.In this paper, we show matching upper- and lower-bounds for the k-sparse vector mean estimation problem under local differential privacy (LDP). Specifically, we construct new mechanisms that achieve asymptotically optimal error as well as succinct communication, either under user-level-LDP or event-level-LDP. We implement our algorithms and evaluate them on synthetic and real-world datasets. Our experiments show that we can often achieve one or two orders of magnitude reduction in error compared with prior work under typical choices of parameters, while incurring insignificant communication cost.",
                "ieee_keywords": [
                    "Privacy",
                    "Differential privacy",
                    "Costs",
                    "Estimation",
                    "Servers",
                    "Security"
                ],
                "author_keywords": []
            },
            {
                "title": "XCRYPT: Accelerating Lattice Based Cryptography with Memristor Crossbar Arrays",
                "link": "https://ieeexplore.ieee.org/document/10051721/",
                "date_of_publication": null,
                "doi": "10.1109/MM.2023.3248080",
                "citations": "83",
                "abstract": "This paper makes a case for accelerating lattice-based post quantum cryptography with memristor-based crossbars. We map the polynomial multiplications in a representative algorithm, SABER, and show that analog dot-products can yield 1.7 − 32.5× performance and energy efficiency improvement, compared to recent hardware proposals. We introduce several additional techniques to address the bottlenecks in this initial design. First, we show that software techniques used in SABER, that are effective on CPU platforms, are unhelpful in crossbars. Relying on simpler algorithms further improves our efficiency by 1.3 − 3.6×. Second, modular arithmetic in SABER offers an opportunity to drop most significant bits, enabling techniques that exploit a few variable precision ADCs, and yielding up to 1.8× higher efficiency. Third, to further reduce ADC pressure, we propose a simple analog Shift-and-Add technique, demonstrating a 1.3 − 6.3× improvement. Overall, XCRYPT achieve 3 − 15× higher efficiency over the initial design and highlight the importance of algorithm-accelerator co-design.",
                "ieee_keywords": [
                    "Memristors",
                    "Computer architecture",
                    "Encryption",
                    "Microprocessors",
                    "Servers",
                    "Voltage",
                    "Protocols"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Zhiwei Steven Wu",
        "publications": [
            {
                "title": "Private Hypothesis Selection",
                "link": "https://ieeexplore.ieee.org/document/9316923/",
                "date_of_publication": null,
                "doi": "10.1109/TIT.2021.3049802",
                "citations": "3",
                "abstract": "We provide a differentially private algorithm for hypothesis selection. Given samples from an unknown probability distribution P and a set of m probability distributions H, the goal is to output, in a ε-differentially private manner, a distribution from H whose total variation distance to P is comparable to that of the best such distribution (which we denote by α). The sample complexity of our basic algorithm is O(log m/α 2 + log m/αε), representing a minimal cost for privacy when compared to the non-private algorithm. We also can handle infinite hypothesis classes H by relaxing to (ε, δ)-differential privacy. We apply our hypothesis selection algorithm to give learning algorithms for a number of natural distribution classes, including Gaussians, product distributions, sums of independent random variables, piecewise polynomials, and mixture classes. Our hypothesis selection procedure allows us to generically convert a cover for a class to a learning algorithm, complementing known learning lower bounds which are in terms of the size of the packing number of the class. As the covering and packing numbers are often closely related, for constant α, our algorithms achieve the optimal sample complexity for many classes of interest. Finally, we describe an application to private distribution-free PAC learning.",
                "ieee_keywords": [
                    "Complexity theory",
                    "Privacy",
                    "Differential privacy",
                    "Approximation algorithms",
                    "TV",
                    "Picture archiving and communication systems",
                    "Internet"
                ],
                "author_keywords": [
                    "Differential privacy",
                    "hypothesis selection",
                    "density estimation"
                ]
            }
        ]
    },
    {
        "name": "Chris Harrison",
        "publications": [
            {
                "title": "Achieving Ubiquity: The New Third Wave",
                "link": "https://ieeexplore.ieee.org/document/5532578/",
                "date_of_publication": null,
                "doi": "10.1109/MMUL.2010.53",
                "citations": "4",
                "abstract": "In this article, Chris Harrison, Jason Wiese, and Anind K. Dey discuss the predictions of Mark Weiser, the father of ubiquitous computing, who envisioned that we would have smart personal environments, with numerous computational devices embedded within each environment. The authors point out that, rather than this happening, what we have currently are personalized computational devices, for example, smart phones, tied to users rather than embedded in the environment. The interesting development of this observation is the crux of their article. Even though multimedia, per se, is not specifically addressed in the article, what the authors have to say is certainly relevant to our community, as smart computational devices and sensors of various sorts are certainly siblings under the skin.-William I. Grosky",
                "ieee_keywords": [
                    "Pervasive computing",
                    "Ubiquitous computing",
                    "Embedded computing",
                    "Microprocessors",
                    "DVD",
                    "Costs",
                    "Microcomputers",
                    "TV"
                ],
                "author_keywords": [
                    "ubiquitous",
                    "pervasive",
                    "quality",
                    "quantity",
                    "computing",
                    "ubicomp",
                    "tab",
                    "pad",
                    "mobile devices",
                    "smart environments",
                    "multimedia and graphics",
                    "Media Impact"
                ]
            },
            {
                "title": "Ultrasonic Doppler Sensing in HCI",
                "link": "https://ieeexplore.ieee.org/document/6133264/",
                "date_of_publication": null,
                "doi": "10.1109/MPRV.2012.17",
                "citations": "40",
                "abstract": "Several properties differentiate ultrasonic Doppler sensing from other sensing techniques-high frame rate, low computational overhead, instantaneous velocity readings, and range independence. Also, because it isn't vision-based, it might open doors to sensing in once taboo locations.",
                "ieee_keywords": [
                    "Doppler effect",
                    "Receivers",
                    "Ultrasonic imaging",
                    "Acoustics",
                    "Speech processing",
                    "Sensor phenomena and characterization"
                ],
                "author_keywords": [
                    "sensors",
                    "ultrasonic",
                    "Doppler",
                    "ubiquitous computing",
                    "gestures",
                    "recognition",
                    "identification",
                    "privacy"
                ]
            },
            {
                "title": "Appropriated Interaction Surfaces",
                "link": "https://ieeexplore.ieee.org/document/5481943/",
                "date_of_publication": null,
                "doi": "10.1109/MC.2010.158",
                "citations": "15",
                "abstract": "We can make small devices easier to use by enabling them to \"steal\" surface area from the environment.",
                "ieee_keywords": [
                    "Smart phones",
                    "Portable computers",
                    "Batteries",
                    "Keyboards",
                    "Usability",
                    "Surface fitting",
                    "Computer aided manufacturing",
                    "Computer vision",
                    "Humans",
                    "Portable media players"
                ],
                "author_keywords": [
                    "Invisible computing",
                    "Ubiquitous computing"
                ]
            },
            {
                "title": "LRAir: Non-contact Haptics Using Synthetic Jets",
                "link": "https://ieeexplore.ieee.org/document/9765565/",
                "date_of_publication": "05 May 2022",
                "doi": "10.1109/HAPTICS52432.2022.9765565",
                "citations": "379",
                "abstract": "We propose a new scalable, non-contact haptic actuation technique based on a speaker in a ported enclosure which can deliver air pulses to the skin. The technique is low cost, low voltage, and uses existing electronics. We detail a prototype device's design and construction, and validate a multiple domain impedance model with current, voltage, and pressure measurements. A non-linear phenomenon at the port creates pulsed zero-net-mass-flux flows, so-called “synthetic jets”. Our prototype is capable of 10 mN time averaged thrusts at an air velocity of 10.4 m/s (4.3W input power). A perception study reveals that tactile effects can be detected 25 mm away with only 380 mVrms applied voltage, and 19 mWrms input power.",
                "ieee_keywords": [
                    "Low voltage",
                    "Costs",
                    "Atmospheric modeling",
                    "Prototypes",
                    "Skin",
                    "Haptic interfaces",
                    "Pressure measurement"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Jiawei Han",
        "publications": [
            {
                "title": "Pattern Discovery for Wide-Window Open Information Extraction in Biomedical Literature",
                "link": "https://ieeexplore.ieee.org/document/8621375/",
                "date_of_publication": "24 January 2019",
                "doi": "10.1109/BIBM.2018.8621375",
                "citations": "5",
                "abstract": "Open information extraction is an important task in Biomedical domain. The goal of the OpenIE is to automatically extract structured information from unstructured text with no or little supervision. It aims to extract all the relation tuples from the corpus without requiring pre-specified relation types. The existing tools may extract ill-structured or incomplete information, or fail on the Biomedical literature due to the long and complicated sentences. In this paper, we propose a novel pattern-based information extraction method for the wide-window entities (WW-PIE). WW-PIE utilizes dependency parsing to break down the long sentences first and then utilizes frequent textual patterns to extract the high-quality information. The pattern hierarchical grouping organize and structure the extractions to be straightforward and precise. Consequently, comparing with the existing OpenIE tools, WW-PIE produces structured output that can be directly used for downstream applications. The proposed WW-PIE is also capable in extracting n-ary and nested relation structures, which is less studied in the existing methods. Extensive experiments on real-world biomedical corpus from PubMed abstracts demonstrate the power of WW-PIE at extracting precise and well-structured information.",
                "ieee_keywords": [
                    "Chemicals",
                    "Data mining",
                    "Information retrieval",
                    "Task analysis",
                    "Diseases",
                    "Bioinformatics",
                    "Tools"
                ],
                "author_keywords": []
            },
            {
                "title": "Bringing Semantics to Spatiotemporal Data Mining: Challenges, Methods, and Applications",
                "link": "https://ieeexplore.ieee.org/document/7930108/",
                "date_of_publication": "18 May 2017",
                "doi": "10.1109/ICDE.2017.210",
                "citations": "2",
                "abstract": "The pervasiveness of GPS-equipped mobile devices has been nurturing an unprecedented amount of semanticsrich spatiotemporal data. The confluence of spatiotemporal and semantic information offers new opportunities for extracting valuable knowledge about people's behaviors, but meanwhile also introduces its unique challenges that render conventional spatiotemporal data mining techniques inadequate. Consequently, mining semantics-rich spatiotemporal data has attracted significant research attention from the data mining community in the past few years. In this tutorial, we start with reviewing classic spatiotemporal data mining tasks and identifying the new opportunities introduced by semantics-rich spatiotemporal data. Subsequently, we provide a comprehensive introduction of existing techniques for mining semantics-rich spatiotemporal data, covering topics including spatiotemporal activity mining, spatiotemporal event discovery, and spatiotemporal mobility modeling. Finally, we discuss about the limitations of existing research and identify several important future directions.",
                "ieee_keywords": [
                    "Spatiotemporal phenomena",
                    "Data mining",
                    "Tutorials",
                    "Semantics",
                    "Trajectory",
                    "Computer science",
                    "Event detection"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Jian Peng",
        "publications": [
            {
                "title": "Roadmap of Spin–Orbit Torques",
                "link": "https://ieeexplore.ieee.org/document/9427163/",
                "date_of_publication": null,
                "doi": "10.1109/TMAG.2021.3078583",
                "citations": "139",
                "abstract": "Spin–orbit torque (SOT) is an emerging technology that enables the efficient manipulation of spintronic devices. The initial processes of interest in SOTs involved electric fields, spin–orbit coupling, conduction electron spins, and magnetization. More recently, interest has grown to include a variety of other processes that include phonons, magnons, or heat. Over the past decade, many materials have been explored to achieve a larger SOT efficiency. Recently, holistic design to maximize the performance of SOT devices has extended material research from a nonmagnetic layer to a magnetic layer. The rapid development of SOT has spurred a variety of SOT-based applications. In this article, we first review the theories of SOTs by introducing the various mechanisms thought to generate or control SOTs, such as the spin Hall effect, the Rashba-Edelstein effect, the orbital Hall effect, thermal gradients, magnons, and strain effects. Then, we discuss the materials that enable these effects, including metals, metallic alloys, topological insulators, 2-D materials, and complex oxides. We also discuss the important roles in SOT devices of different types of magnetic layers, such as magnetic insulators, antiferromagnets, and ferrimagnets. Afterward, we discuss device applications utilizing SOTs. We discuss and compare three- and two-terminal SOT-magnetoresistive random access memories (MRAMs); we mention various schemes to eliminate the need for an external field. We provide technological application considerations for SOT-MRAM and give perspectives on SOT-based neuromorphic devices and circuits. In addition to SOT-MRAM, we present SOT-based spintronic terahertz generators, nano-oscillators, and domain-wall and skyrmion racetrack memories. This article aims to achieve a comprehensive review of SOT theory, materials, and applications, guiding future SOT development in both the academic and industrial sectors.",
                "ieee_keywords": [],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Wen-mei W. Hwu",
        "publications": [
            {
                "title": "A Tiling-Scheme Viterbi Decoder in Software Defined Radio for GPUs",
                "link": "https://ieeexplore.ieee.org/document/6036680/",
                "date_of_publication": "10 October 2011",
                "doi": "10.1109/wicom.2011.6036680",
                "citations": "13",
                "abstract": "In this paper, we propose a parallel design of Viterbi decoder for Software-Defined Radio (SDR). Our method implements a divide-and-conquer approach by tiling decoding sequences, performing independent speculated Viterbi decoding, and merging partial candidate paths into the final path. For each independent Viterbi decoding, the best path is selected by calculating Hamming distances trellis-by-trellis in parallel. Our method shows up to 14.6x speedup on an NVIDIA 8800 GTX over a sequential C implementation on a 2.4GHz Intel Core 2 CPU. Also, compared with the existing GPU-based implementation, our method outperforms up to 2.5x.",
                "ieee_keywords": [
                    "Graphics processing unit",
                    "Viterbi algorithm",
                    "Instruction sets",
                    "Maximum likelihood decoding",
                    "Convolutional codes",
                    "Merging"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Svetlana Lazebnik",
        "publications": [
            {
                "title": "Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models",
                "link": "https://ieeexplore.ieee.org/document/7410660/",
                "date_of_publication": "18 February 2016",
                "doi": "10.1109/ICCV.2015.303",
                "citations": "461",
                "abstract": "The Flickr30k dataset has become a standard benchmark for sentence-based image description. This paper presents Flickr30k Entities, which augments the 158k captions from Flickr30k with 244k coreference chains linking mentions of the same entities in images, as well as 276k manually annotated bounding boxes corresponding to each entity. Such annotation is essential for continued progress in automatic image description and grounded language understanding. We present experiments demonstrating the usefulness of our annotations for text-to-image reference resolution, or the task of localizing textual entity mentions in an image, and for bidirectional image-sentence retrieval. These experiments confirm that we can further improve the accuracy of state-of-the-art retrieval methods by training with explicit region-to-phrase correspondence, but at the same time, they show that accurately inferring this correspondence given an image and a caption remains really challenging.",
                "ieee_keywords": [
                    "Standards",
                    "Benchmark testing",
                    "Image resolution",
                    "Grounding",
                    "Glass",
                    "Training",
                    "Image color analysis"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Yuan Zhou",
        "publications": [
            {
                "title": "Characterizing logging practices in open-source software",
                "link": "https://ieeexplore.ieee.org/document/6227202/",
                "date_of_publication": "28 June 2012",
                "doi": "10.1109/ICSE.2012.6227202",
                "citations": "148",
                "abstract": "Software logging is a conventional programming practice. While its efficacy is often important for users and developers to understand what have happened in the production run, yet software logging is often done in an arbitrary manner. So far, there have been little study for understanding logging practices in real world software. This paper makes the first attempt (to the best of our knowledge) to provide a quantitative characteristic study of the current log messages within four pieces of large open-source software. First, we quantitatively show that software logging is pervasive. By examining developers' own modifications to the logging code in the revision history, we find that they often do not make the log messages right in their first attempts, and thus need to spend a significant amount of efforts to modify the log messages as after-thoughts. Our study further provides several interesting findings on where developers spend most of their efforts in modifying the log messages, which can give insights for programmers, tool developers, and language and compiler designers to improve the current logging practice. To demonstrate the benefit of our study, we built a simple checker based on one of our findings and effectively detected 138 pieces of new problematic logging code from studied software (24 of them are already confirmed and fixed by developers).",
                "ieee_keywords": [
                    "Open source software",
                    "Servers",
                    "History",
                    "Production",
                    "Programming",
                    "Printing"
                ],
                "author_keywords": [
                    "log message",
                    "log quality",
                    "empirical study",
                    "failure diagnosis"
                ]
            }
        ]
    },
    {
        "name": "Saugata Ghose",
        "publications": [
            {
                "title": "Google Neural Network Models for Edge Devices: Analyzing and Mitigating Machine Learning Inference Bottlenecks",
                "link": "https://ieeexplore.ieee.org/document/9563028/",
                "date_of_publication": "18 October 2021",
                "doi": "10.1109/PACT52795.2021.00019",
                "citations": "21",
                "abstract": "Emerging edge computing platforms often contain machine learning (ML) accelerators that can accelerate inference for a wide range of neural network (NN) models. These models are designed to fit within the limited area and energy constraints of the edge computing platforms, each targeting various applications (e.g., face detection, speech recognition, translation, image captioning, video analytics). To understand how edge ML accelerators perform, we characterize the performance of a commercial Google Edge TPU, using 24 Google edge NN models (which span a wide range of NN model types) and analyzing each NN layer within each model. We find that the Edge TPU suffers from three major shortcomings: (1) it operates significantly below peak computational throughput, (2) it operates significantly below its theoretical energy efficiency, and (3) its memory system is a large energy and performance bottleneck. Our characterization reveals that the one-size-fits-all, monolithic design of the Edge TPU ignores the high degree of heterogeneity both across different NN models and across different NN layers within the same NN model, leading to the shortcomings we observe. We propose a new acceleration framework called Mensa. Mensa incorporates multiple heterogeneous edge ML accelerators (including both on-chip and near-data accelerators), each of which caters to the characteristics of a particular subset of NN models and layers. During NN inference, for each NN layer, Mensa decides which accelerator to schedule the layer on, taking into account both the optimality of each accelerator for the layer and layer-to-layer communication costs. Our comprehensive analysis of the Google edge NN models shows that all of the layers naturally group into a small number of clusters, which allows us to design an efficient implementation of Mensa for these models with only three specialized accelerators. Averaged across all 24 Google edge NN models, Mensa improves energy efficiency and throughput by ... (Show More)",
                "ieee_keywords": [
                    "Analytical models",
                    "Computational modeling",
                    "Image edge detection",
                    "Visual analytics",
                    "Artificial neural networks",
                    "Machine learning",
                    "Throughput"
                ],
                "author_keywords": []
            },
            {
                "title": "Polynesia: Enabling High-Performance and Energy-Efficient Hybrid Transactional/Analytical Databases with Hardware/Software Co-Design",
                "link": "https://ieeexplore.ieee.org/document/9835628/",
                "date_of_publication": "02 August 2022",
                "doi": "10.1109/ICDE53745.2022.00270",
                "citations": "4",
                "abstract": "A growth in data volume, combined with increasing demand for real-time analysis (using the most recent data), has resulted in the emergence of database systems that concurrently support transactions and data analytics. These hybrid transactional and analytical processing (HTAP) database systems can support real-time data analysis without the high costs of synchronizing across separate single-purpose databases. Unfortunately, for many applications that perform a high rate of data updates, state-of-the-art HTAP systems incur significant losses in transactional (up to 74.6%) and/or analytical (up to 49.8%) throughput compared to performing only transactional or only analytical queries in isolation, due to (1) data movement be-tween the CPU and memory, (2) data update propagation from transactional to analytical workloads, and (3) the cost to main-tain a consistent view of data across the system. We propose Polynesia, a hardware-software co-designed system for in-memory HTAP databases that avoids the large throughput losses of traditional HTAP systems. Polynesia (1) di-vides the HTAP system into transactional and analytical pro-cessing islands, (2) implements new custom hardware that un-locks software optimizations to reduce the costs of update prop-agation and consistency, and (3) exploits processing-in-memory for the analytical islands to alleviate data movement overheads. Our evaluation shows that Polynesia outperforms three state-of-the-art HTAP systems, with average transactional/analytical throughput improvements of 1.7×/3.7×, and reduces energy consumption by 48% over the prior lowest-energy HTAP sys-tem.",
                "ieee_keywords": [
                    "Costs",
                    "Data analysis",
                    "Conferences",
                    "Throughput",
                    "Propagation losses",
                    "Real-time systems",
                    "Database systems"
                ],
                "author_keywords": [
                    "databases",
                    "hybrid-transactional/analytical-processing-(HTAP)",
                    "hardware-software-co-design",
                    "processing-in-memory",
                    "near-data-processing",
                    "accelerator"
                ]
            }
        ]
    },
    {
        "name": "Darko Marinov",
        "publications": [
            {
                "title": "Ballerina: Automatic generation and clustering of efficient random unit tests for multithreaded code",
                "link": "https://ieeexplore.ieee.org/document/6227145/",
                "date_of_publication": "28 June 2012",
                "doi": "10.1109/ICSE.2012.6227145",
                "citations": "43",
                "abstract": "Testing multithreaded code is hard and expensive. A multithreaded unit test creates two or more threads, each executing one or more methods on shared objects of the class under test. Such unit tests can be generated at random, but basic random generation produces tests that are either slow or do not trigger concurrency bugs. Worse, such tests have many false alarms, which require human effort to filter out. We present Ballerina, a novel technique for automated random generation of efficient multithreaded tests that effectively trigger concurrency bugs. Ballerina makes tests efficient by having only two threads, each executing a single, randomly selected method. Ballerina increases chances that such simple parallel code finds bugs by appending it to more complex, randomly generated sequential code. We also propose a clustering technique to reduce the manual effort in inspecting failures of automatically generated multithreaded tests. We evaluate Ballerina on 14 real-world bugs from six popular codebases: Groovy, JDK, JFreeChart, Apache Log4j, Apache Lucene, and Apache Pool. The experiments show that tests generated by Ballerina find bugs on average 2×-10× faster than basic random generation, and our clustering technique reduces the number of inspected failures on average 4×-8×. Using Ballerina, we found three previously unknown bugs, two of which were already confirmed and fixed.",
                "ieee_keywords": [
                    "Computer bugs",
                    "Instruction sets",
                    "Concurrent computing",
                    "Testing",
                    "Receivers",
                    "Inspection"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Timothy Bretl",
        "publications": [
            {
                "title": "Manipulation planning with contacts for an extensible elastic rod by sampling on the submanifold of static equilibrium configurations",
                "link": "https://ieeexplore.ieee.org/document/7139627/",
                "date_of_publication": "02 July 2015",
                "doi": "10.1109/ICRA.2015.7139627",
                "citations": "19",
                "abstract": "We consider the manipulation planning problem of an extensible elastic rod in collision-free or contact space. We assume the rod can be handled by grippers either at both or at only one of its extremities and during the manipulation, the grasped end may change. We show that the use of both quasi-static and dynamic models can be coupled efficiently with sampling-based methods. By sampling directly on the submanifold of static equilibrium and contact-free configurations, we can take advantage of the dynamic model to improve the exploration in the state space. We show the necessity of considering contacts for this type of problems with several simulation experiments on various scenarios.",
                "ieee_keywords": [
                    "Planning",
                    "Grippers",
                    "Dynamics",
                    "Robots",
                    "Optimal control",
                    "Strain",
                    "Extremities"
                ],
                "author_keywords": []
            },
            {
                "title": "Efficient motion planning for quasi-static elastic rods using geometry neighborhood approximation",
                "link": "https://ieeexplore.ieee.org/document/6878215/",
                "date_of_publication": "14 August 2014",
                "doi": "10.1109/AIM.2014.6878215",
                "citations": "4",
                "abstract": "We present a motion planning algorithm for a quasi-static Kirchhoff elastic rod in complex environments. As the set of quasi-static deformations defines a finite dimensional manifold that can be parameterized by a single chart, the configuration space formulation extends nicely to this deformation space. This parameterization is computationally expensive and our algorithm takes advantage of its linearization to perform fast collision checking in its neighborhood. In the context of physically realistic deformable rods, the efficiency of this approximation can be coupled with motion planning techniques to obtain significant performance improvements. We demonstrate the effectiveness of our approach on various toy and industrial scenarios.",
                "ieee_keywords": [
                    "Approximation methods",
                    "Planning",
                    "Geometry",
                    "Robots",
                    "Computational modeling",
                    "Manifolds",
                    "Deformable models"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Marco Caccamo",
        "publications": [
            {
                "title": "Real-Time Communication for Multicore Systems with Multi-domain Ring Buses",
                "link": "https://ieeexplore.ieee.org/document/5591279/",
                "date_of_publication": "30 September 2010",
                "doi": "10.1109/RTCSA.2010.32",
                "citations": "192",
                "abstract": "We address the problem of scheduling real-time data transactions on a multicore processor bus. In particular, to in-crease system predictability and tighten WCET estimation, we propose to employ a software-controllable Multi-Domain Ring Bus (MDRB) architecture. The problem of scheduling periodic real-time transactions on MDRB is challenging because the bus allows multiple non-overlapping transactions to be executed concurrently, and because the degree of concurrency depends on the topology of the bus and of executed transactions. We propose a practical abstraction mechanism for the scheduling problem together with two novel scheduling algorithms. The first algorithm is optimal for transaction sets under restrictive assumptions while the second one induces a competitive sufficient schedulable utilization bound for more general transaction sets.",
                "ieee_keywords": [
                    "Real time systems",
                    "Schedules",
                    "Scheduling",
                    "Scheduling algorithm",
                    "Multicore processing"
                ],
                "author_keywords": [
                    "Real-time scheduling",
                    "Multi-domain ring bus",
                    "Real-time systems"
                ]
            }
        ]
    },
    {
        "name": "Olgica Milenkovic",
        "publications": [
            {
                "title": "The Postdoc Problem under the Mallows Model",
                "link": "https://ieeexplore.ieee.org/document/9517855/",
                "date_of_publication": "01 September 2021",
                "doi": "10.1109/ISIT45174.2021.9517855",
                "citations": "73",
                "abstract": "The well-known secretary problem in sequential analysis and optimal stopping theory asks one to maximize the probability of finding the optimal candidate in a sequentially examined list under the constraint that accept/reject decisions are made in real-time. The problem is related to practical questions arising in online search, data streaming, daily purchase modeling and multi-arm bandit mechanisms. An extension is the postdoc problem, for which one aims to identify the second-best candidate with highest possible probability of success. We solve the postdoc problem for the nontraditional setting where the candidates are not presented uniformly at random but rather according to permutations drawn from the Mallows distribution. The optimal stopping criteria depend on the choice of the Mallows model parameter θ : For θ>1 , we reject the first k ′ (θ) candidates and then accept the next left-to-right second-best candidate (second-best ranked when comparing with all appeared candidates). This coincides with the optimal strategy for the classical postdoc problem, where the rankings being drawn uniformly at random (i.e.θ=1) . For 0<θ⩽1/2 , we reject the first k ′′ (θ) candidates and then accept the next left-to-right best candidate; if no selection is made before the last candidate, then the last candidate is accepted. For 1/2<θ<1 , we reject the first k 1 (θ) candidates and then accept the next left-to-right maximum, or reject the first k 2 (θ)⩾ k 1 (θ) candidates and then accept the next left-to-right second-maximum, whichever comes first.",
                "ieee_keywords": [
                    "Sequential analysis",
                    "Search problems",
                    "Real-time systems",
                    "Data models",
                    "Information theory"
                ],
                "author_keywords": [
                    "secretary problem",
                    "postdoc problem",
                    "Mallows model"
                ]
            }
        ]
    },
    {
        "name": "Nuno Vasconcelos",
        "publications": [
            {
                "title": "Catastrophic Child's Play: Easy to Perform, Hard to Defend Adversarial Attacks",
                "link": "https://ieeexplore.ieee.org/document/8954169/",
                "date_of_publication": "09 January 2020",
                "doi": "10.1109/CVPR.2019.00945",
                "citations": "2",
                "abstract": "The problem of adversarial CNN attacks is considered, with an emphasis on attacks that are trivial to perform but difficult to defend. A framework for the study of such attacks is proposed, using real world object manipulations. Unlike most works in the past, this framework supports the design of attacks based on both small and large image perturbations, implemented by camera shake and pose variation. A setup is proposed for the collection of such perturbations and determination of their perceptibility. It is argued that perceptibility depends on context, and a distinction is made between imperceptible and semantically imperceptible perturbations. While the former survives image comparisons, the latter are perceptible but have no impact on human object recognition. A procedure is proposed to determine the perceptibility of perturbations using Turk experiments, and a dataset of both perturbation classes which enables replicable studies of object manipulation attacks, is assembled. Experiments using defenses based on many datasets, CNN models, and algorithms from the literature elucidate the difficulty of defending these attacks -- in fact, none of the existing defenses is found effective against them. Better results are achieved with real world data augmentation, but even this is not foolproof. These results confirm the hypothesis that current CNNs are vulnerable to attacks implementable even by a child, and that such attacks may prove difficult to defend.",
                "ieee_keywords": [],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Hao Su",
        "publications": [
            {
                "title": "Experimental Demonstration of Crosstalk Reduction to Achieve Turbulence-Resilient Multiple-OAM-Beam Free-Space Optical Communications using Pilot Tones to Mix Beams at the Receiver",
                "link": "https://ieeexplore.ieee.org/document/9192963/",
                "date_of_publication": "10 September 2020",
                "doi": null,
                "citations": "115",
                "abstract": "We experimentally demonstrate a 4-Gbit/s turbulence-resilient two-OAM-mode-multiplexed FSO link using pilot tones at different wavelengths to mix beams at the receiver. Inter-modal crosstalk is measured to be resilient to turbulence for different OAM mode selections.",
                "ieee_keywords": [
                    "Crosstalk",
                    "Laser beams",
                    "Distortion",
                    "Optical mixing",
                    "Optical receivers",
                    "Adaptive optics"
                ],
                "author_keywords": []
            },
            {
                "title": "Demonstration of Turbulence Mitigation in a 200-Gbit/s Orbital-Angular-Momentum Multiplexed Free-Space Optical Link using Simple Power Measurements on a Probe Wavelength",
                "link": "https://ieeexplore.ieee.org/document/9573141/",
                "date_of_publication": "29 October 2021",
                "doi": null,
                "citations": "52",
                "abstract": "We experimentally demonstrate turbulence mitigation in a 200-Gbit/s OAM-multiplexed free-space optical link with data channels on 1552 nm using simple power measurements on a probe wavelength of 1550 nm without interrupting the data transmission. We observe a crosstalk reduction of up to 25 dB.",
                "ieee_keywords": [
                    "Multiplexing",
                    "Power measurement",
                    "Power lasers",
                    "Optical vortices",
                    "Measurement by laser beam",
                    "Optical variables measurement",
                    "Optical crosstalk"
                ],
                "author_keywords": [
                    "(060.2605) Free-space optical communications",
                    "(010.1330) Atmospheric turbulence",
                    "(050.4865) Optical vortices"
                ]
            },
            {
                "title": "Demonstration of Turbulence Resiliency in a Mode-, Polarization-, and Wavelength-Multiplexed Free-Space Optical Link using Pilot Tones and Optoelectronic Wave Mixing",
                "link": "https://ieeexplore.ieee.org/document/9333301/",
                "date_of_publication": "04 February 2021",
                "doi": "10.1109/ECOC48923.2020.9333301",
                "citations": "2",
                "abstract": "We experimentally demonstrate the turbulence resiliency in a mode-, polarization, and wavelength-multiplexed free-space optical link using pilot tones and optoelectronic wave mixing. 8 multiplexed channels combined with 2 OAM modes, 2 polarizations, and 2 wavelengths are demonstrated in a 4-Gbit/s QPSK link.",
                "ieee_keywords": [
                    "Multiplexing",
                    "Phase shift keying",
                    "Europe",
                    "Optical fiber communication",
                    "Resilience"
                ],
                "author_keywords": []
            },
            {
                "title": "Experimental Demonstration of a 100-Gbit/s 16-QAM Free-Space Optical Link Using a Structured Optical \"Bottle Beam\" to Circumvent Obstructions",
                "link": "https://ieeexplore.ieee.org/document/9606019/",
                "date_of_publication": "22 November 2021",
                "doi": "10.1109/ECOC52684.2021.9606019",
                "citations": "1",
                "abstract": "We experimentally demonstrate a 100-Gbit/s 16-QAM free-space optical link using/tailoring an optical bottle beam to circumvent obstructions with different sizes and locations. Experimental results show ~10-dB less obstruction-induced power penalty compared to using a Gaussian beam for the obstruction with a diameter of ~4.5 mm.",
                "ieee_keywords": [
                    "Europe",
                    "Optical fiber communication",
                    "Optical beams"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Daniel Kane",
        "publications": [
            {
                "title": "TOI-431/HIP 26013: a super-Earth and a sub-Neptune transiting a bright, early K dwarf, with a third RV planet",
                "link": "https://ieeexplore.ieee.org/document/9584212/",
                "date_of_publication": null,
                "doi": "10.1093/mnras/stab2313",
                "citations": "Abstract",
                "abstract": "We present the bright (V mag = 9.12), multiplanet system TOI-431, characterized with photometry and radial velocities (RVs). We estimate the stellar rotation period to be 30.5 ± 0.7 d using archival photometry and RVs. Transiting Exoplanet Survey Satellite (TESS) objects of Interest (TOI)-431 b is a super-Earth with a period of 0.49 d, a radius of 1.28 ± 0.04 R ⊕ , a mass of 3.07 ± 0.35 M ⊕ , and a density of 8.0 ± 1.0 g cm −3 ; TOI-431 d is a sub-Neptune with a period of 12.46 d, a radius of 3.29 ± 0.09 R ⊕ , a mass of 9.90 +1.53 −1.49 M ⊕ , and a density of 1.36 ± 0.25 g cm −3 . We find a third planet, TOI-431 c, in the High Accuracy Radial velocity Planet Searcher RV data, but it is not seen to transit in the TESS light curves. It has an Msin i of 2.83 +0.41 −0.34 M ⊕ , and a period of 4.85 d. TOI-431 d likely has an extended atmosphere and is one of the most well-suited TESS discoveries for atmospheric characterization, while the super-Earth TOI-431 b may be a stripped core. These planets straddle the radius gap, presenting an interesting case-study for atmospheric evolution, and TOI-431 b is a prime TESS discovery for the study of rocky planet phase curves.",
                "ieee_keywords": [],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Dinesh Bharadia",
        "publications": [
            {
                "title": "A Fully-Reflective Wi-Fi-Compatible Backscatter Communication System With Retro-Reflective MIMO Gain for Improved Range",
                "link": "https://ieeexplore.ieee.org/document/10149215/",
                "date_of_publication": null,
                "doi": "10.1109/JSSC.2023.3281553",
                "citations": "268",
                "abstract": "This article presents an integrated circuit (IC) designed to enable low-power long-range backscatter communication with commodity Wi-Fi transceivers. The proposed chip endeavors to improve the most critical and difficult specification in Wi-Fi backscatter systems: range. It does so through two proposed techniques: 1) a fully-reflective single-antenna backscatter solution, whereby the termination of a power combiner always has a reflection coefficient near 1 to ensure high reflected power while enabling single-sideband (SSB) quadrature phase shift keying (QPSK) modulation with frequency-translation to separate Wi-Fi channel and 2) a retro-reflective multiple-in–multiple-out (MIMO) approach that redirects incident Wi-Fi signals, after SSB QPSK modulation, back to a colocated access point (AP) with MIMO gain. The proposed chip also implemented a counter-based wake-up scheme within a synchronization receiver (RX) to achieve standards-compatible wake-up with high synchronization accuracy. Implemented in 65-nm CMOS, the wake-up RX consumes 4.5 μW and achieves a sensitivity of −43.5 dBm, while the synchronization RX consumes an average power of 4.8 μW and achieves a synchronization accuracy of at least 150 ns for input power of −35 dBm or better. During backscattering, the IC consumes 32 and 38 μW and attains an AP-to-tag range of 13 and 23 m for the fully-reflective and retro-reflective MIMO approaches, respectively.",
                "ieee_keywords": [
                    "Backscatter",
                    "Wireless fidelity",
                    "Amplitude modulation",
                    "Switches",
                    "Modulation",
                    "Synchronization",
                    "Radio frequency"
                ],
                "author_keywords": [
                    "Backscatter communication",
                    "Internet-of-Things (IoT)",
                    "low-power wireless system",
                    "range improvement",
                    "wake-up radios",
                    "Wi-Fi"
                ]
            },
            {
                "title": "A Low-Power Backscatter Modulation System Communicating Across Tens of Meters With Standards-Compliant Wi-Fi Transceivers",
                "link": "https://ieeexplore.ieee.org/document/9205267/",
                "date_of_publication": null,
                "doi": "10.1109/JSSC.2020.3023956",
                "citations": "11",
                "abstract": "This article presents the first integrated circuit designed to enable low-power backscatter communication with commodity Wi-Fi transceivers. The developed chip operates by receiving a series of packets generated from a Wi-Fi access point (AP), which feeds into a low-power energy-detecting wakeup receiver that determines when backscatter communication should commence. Then, the Wi-Fi AP sends an additional packet that is intended to be backscatter modulated. To accomplish this, the antenna receiving the incident Wi-Fi packet is terminated by a dynamically varying collection of complex impedances via a crystal-stabilized multi-phase local oscillator driven by a single-sideband (SSB) mixer, which ultimately performs SSB quadrature phase shift-keying (QPSK) modulation with frequency-translation to a separate Wi-Fi channel for reception by a second Wi-Fi AP. Implemented in 65-nm CMOS, the downlink wake-up receiver consumes 2.8 μW and achieves a sensitivity of -42.5 dBm, which is good enough for >30 m wakeup range, while the backscattering uplink consumes 28 μW and achieves 17 dB of image rejection. Wireless tests reveal a range of 21 m when the developed IC is placed symmetrically between Wi-Fi access points (APs), and a range of >90 m when the developed IC is placed within 1 m of the transmitting Wi-Fi AP.",
                "ieee_keywords": [
                    "Backscatter",
                    "Wireless fidelity",
                    "Amplitude modulation",
                    "Frequency modulation",
                    "Phase shift keying"
                ],
                "author_keywords": [
                    "24-GHz band",
                    "backscatter communication",
                    "image rejection",
                    "Internet-of-Things (IoT)",
                    "low-power wireless",
                    "RFID",
                    "single-sideband (SSB)",
                    "wake-up radios",
                    "wake-up receivers (WuRXs)",
                    "Wi-Fi"
                ]
            },
            {
                "title": "A Hierarchical Self-Interference Canceller for Full-Duplex LPWAN Applications Achieving 52–70-dB RF Cancellation",
                "link": "https://ieeexplore.ieee.org/document/9873897/",
                "date_of_publication": null,
                "doi": "10.1109/JSSC.2022.3200369",
                "citations": "698",
                "abstract": "We present a radio frequency self-interference (SI) canceller chip for low-power wide area network (LPWAN) basestations. To enhance the cancellation capability without necessitating unreasonable resolution, power consumption, and area, we introduce a hierarchical cancellation technique using a nested vector modulator (VM) implementation. Nesting a 7-b and two 6-b stages, a 16-b theoretical and >13-b measured resolution is obtained per tap. LPWAN channels have large group delay and group delay spread in tens of ns, and a requirement for >100 dB of analog SI cancellation. To enable large ON-chip group delay, we leverage frequency translation circuits, and in comparison to prior art, decouple impedance matching requirement from tap delay implementation to realize range/resolution from 16 ns/33 ps to 80 ns/150 ps. We demonstrate 64 dB of RF SI cancellation over 0.8-MHz bandwidth (BW) with a three-tap implementation for an LPWAN wireless channel. For more controlled environments, up to 70-dB cancellation is shown. Each tap occupies 0.21 mm2 of area and consumes 12.3 mA of current with a 1.2-V supply voltage. The whole chip occupies 1.2 mm2 and consumes 44.28 mW of power in a 65-nm CMOS process.",
                "ieee_keywords": [
                    "Low-power wide area networks",
                    "Interference cancellation",
                    "Radio frequency",
                    "Delays",
                    "Antennas",
                    "Wireless communication",
                    "Logic gates"
                ],
                "author_keywords": [
                    "Full-duplex (FD) radio",
                    "in-band FD",
                    "self-interference (SI) cancellation (SIC)",
                    "simultaneous transmission and reception (STR)",
                    "vector modulator (VM)"
                ]
            }
        ]
    },
    {
        "name": "Michael C. Yip",
        "publications": [
            {
                "title": "Image-based Pose Estimation and Shape Reconstruction for Robot Manipulators and Soft, Continuum Robots via Differentiable Rendering",
                "link": "https://ieeexplore.ieee.org/document/10161066/",
                "date_of_publication": "04 July 2023",
                "doi": "10.1109/ICRA48891.2023.10161066",
                "citations": "318",
                "abstract": "State estimation from measured data is crucial for robotic applications as autonomous systems rely on sensors to capture the motion and localize in the 3D world. Among sensors that are designed for measuring a robot's pose, or for soft robots, their shape, vision sensors are favorable because they are information-rich, easy to set up, and cost-effective. With recent advancements in computer vision, deep learning-based methods no longer require markers for identifying feature points on the robot. However, learning-based methods are data-hungry and hence not suitable for soft and prototyping robots, as building such bench-marking datasets is usually infeasible. In this work, we achieve image-based robot pose estimation and shape reconstruction from camera images. Our method requires no precise robot meshes, but rather utilizes a differentiable renderer and primitive shapes. It hence can be applied to robots for which CAD models might not be available or are crude. Our parameter estimation pipeline is fully differentiable. The robot shape and pose are estimated iteratively by back-propagating the image loss to update the parameters. We demonstrate that our method of using geometrical shape primitives can achieve high accuracy in shape reconstruction for a soft continuum robot and pose estimation for a robot manipulator.",
                "ieee_keywords": [
                    "Learning systems",
                    "Shape",
                    "Shape measurement",
                    "Pose estimation",
                    "Robot vision systems",
                    "Rendering (computer graphics)",
                    "Manipulators"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Arun Kumar",
        "publications": [
            {
                "title": "The Role of Water on the Interfacial Adhesion in Si Solar Modules",
                "link": "https://ieeexplore.ieee.org/document/9519053/",
                "date_of_publication": "26 August 2021",
                "doi": "10.1109/PVSC43889.2021.9519053",
                "citations": "1",
                "abstract": "Delamination of solar module interfaces often occurs in field-tested solar modules after decades of service due to environmental stressors such as humidity. As water diffuses into the module, failure mechanisms like corrosion and delamination are significantly affected. In the presence of water, the interfaces between EVA and the cell, glass, and backsheet all experience losses of adhesion exposing the module to accelerated degradation. Understanding the relation between interfacial adhesion and water content inside PV modules can help mitigate detrimental power losses. Water content measurements via short wave infrared reflectometry combined with 180° peel tests were used to study and quantify the effect of water ingress and egress on adhesion. Changes in adhesion strength for different module interfaces are quantified, correlating spatial distribution of water content to adhesion for damp heat and dry heat exposed samples. After 1000 hours of damp heat exposure, decreases in adhesion strength of approximately 1 N/mm were noted for all interfaces.",
                "ieee_keywords": [
                    "Photovoltaic systems",
                    "Graphical models",
                    "Adhesives",
                    "Water heating",
                    "Failure analysis",
                    "Humidity",
                    "Glass"
                ],
                "author_keywords": [
                    "delamination",
                    "adhesion",
                    "peel",
                    "water"
                ]
            },
            {
                "title": "Code-Domain Multiplexing for Shared IF/LO Interfaces in Millimeter-Wave MIMO Arrays",
                "link": "https://ieeexplore.ieee.org/document/8998273/",
                "date_of_publication": null,
                "doi": "10.1109/JSSC.2020.2967538",
                "citations": "7",
                "abstract": "Millimeter-wave (mm-wave) multi-input-multi-output (MIMO) systems with the digitization of every element enable spatial multiplexing, virtual arrays for radar, and digital beamforming for high mobility scenarios. However, per-element digitization results in a formidable I/O challenge in large-scale tiled MIMO mm-wave arrays. This article analyzes a code-domain approach to multiplex signals from different elements as well as the LO on a single interface. System considerations are presented, and the approach is demonstrated through a 28 GHz four-element MIMO receiver in 65-nm CMOS. Measurements show the feasibility of digital beamforming after de-multiplexing of the baseband signals from the IF/LO interface. Each element in the array achieves 16-dB conversion gain while consuming 60 mA from 1.2 V. The IC occupies 5.75 mm 2 in 65-nm CMOS, achieving small size and compatibility with λ/2 × λ/2 antenna spacing due to the absence of phase shifters/combiners required in phased arrays.",
                "ieee_keywords": [
                    "Multiplexing",
                    "MIMO communication",
                    "Phased arrays",
                    "Millimeter wave radar",
                    "Bandwidth",
                    "Correlation",
                    "Array signal processing"
                ],
                "author_keywords": [
                    "Code-domain",
                    "input matching",
                    "multi-input multi-output (MIMO)",
                    "millimeter-wave (mm-wave) arrays",
                    "phased arrays",
                    "Walsh code"
                ]
            }
        ]
    },
    {
        "name": "Ryan Kastner",
        "publications": [
            {
                "title": "Property Specific Information Flow Analysis for Hardware Security Verification",
                "link": "https://ieeexplore.ieee.org/document/8587741/",
                "date_of_publication": "03 January 2019",
                "doi": "10.1145/3240765.3240839",
                "citations": "14",
                "abstract": "Hardware information flow analysis detects security vulnerabilities resulting from unintended design flaws, timing channels, and hardware Trojans. These information flow models are typically generated in a general way, which includes a significant amount of redundancy that is irrelevant to the specified security properties. In this work, we propose a property specific approach for information flow security. We create information flow models tailored to the properties to be verified by performing a property specific search to identify security critical paths. This helps find suspicious signals that require closer inspection and quickly eliminates portions of the design that are free of security violations. Our property specific trimming technique reduces the complexity of the security model; this accelerates security verification and restricts potential security violations to a smaller region which helps quickly pinpoint hardware security vulnerabilities.",
                "ieee_keywords": [
                    "Security",
                    "Hardware",
                    "Trojan horses",
                    "Mathematical model",
                    "Timing",
                    "Complexity theory",
                    "Acceleration"
                ],
                "author_keywords": [
                    "Hardware security",
                    "security verification",
                    "information flow analysis",
                    "security property",
                    "design methodology"
                ]
            }
        ]
    },
    {
        "name": "Rajesh K. Gupta",
        "publications": [
            {
                "title": "Keynote presentations",
                "link": "https://ieeexplore.ieee.org/document/6657016/",
                "date_of_publication": "07 November 2013",
                "doi": "10.1109/ICCD.2013.6657016",
                "citations": "62",
                "abstract": "These keynote presentations discuss the following: architecture research: simulation versus rapid prototyping; accelerator-rich architectures - computing beyond processors; combating dark silicon: it takes a village; and from crash-and-recover to sense-and-adapt: our evolving models of computing machines.",
                "ieee_keywords": [],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Yuanyuan Zhou",
        "publications": [
            {
                "title": "Characterizing logging practices in open-source software",
                "link": "https://ieeexplore.ieee.org/document/6227202/",
                "date_of_publication": "28 June 2012",
                "doi": "10.1109/ICSE.2012.6227202",
                "citations": "148",
                "abstract": "Software logging is a conventional programming practice. While its efficacy is often important for users and developers to understand what have happened in the production run, yet software logging is often done in an arbitrary manner. So far, there have been little study for understanding logging practices in real world software. This paper makes the first attempt (to the best of our knowledge) to provide a quantitative characteristic study of the current log messages within four pieces of large open-source software. First, we quantitatively show that software logging is pervasive. By examining developers' own modifications to the logging code in the revision history, we find that they often do not make the log messages right in their first attempts, and thus need to spend a significant amount of efforts to modify the log messages as after-thoughts. Our study further provides several interesting findings on where developers spend most of their efforts in modifying the log messages, which can give insights for programmers, tool developers, and language and compiler designers to improve the current logging practice. To demonstrate the benefit of our study, we built a simple checker based on one of our findings and effectively detected 138 pieces of new problematic logging code from studied software (24 of them are already confirmed and fixed by developers).",
                "ieee_keywords": [
                    "Open source software",
                    "Servers",
                    "History",
                    "Production",
                    "Programming",
                    "Printing"
                ],
                "author_keywords": [
                    "log message",
                    "log quality",
                    "empirical study",
                    "failure diagnosis"
                ]
            }
        ]
    },
    {
        "name": "William G. Griswold",
        "publications": [
            {
                "title": "WitchDoctor: IDE support for real-time auto-completion of refactorings",
                "link": "https://ieeexplore.ieee.org/document/6227191/",
                "date_of_publication": "28 June 2012",
                "doi": "10.1109/ICSE.2012.6227191",
                "citations": "48",
                "abstract": "Integrated Development Environments (IDEs) have come to perform a wide variety of tasks on behalf of the programmer, refactoring being a classic example. These operations have undeniable benefits, yet their large (and growing) number poses a cognitive scalability problem. Our main contribution is WitchDoctor - a system that can detect, on the fly, when a programmer is hand-coding a refactoring. The system can then complete the refactoring in the background and propose it to the user long before the user can complete it. This implies a number of technical challenges. The algorithm must be 1) highly efficient, 2) handle unparseable programs, 3) tolerate the variety of ways programmers may perform a given refactoring, 4) use the IDE's proven and familiar refactoring engine to perform the refactoring, even though the the refactoring has already begun, and 5) support the wide range of refactorings present in modern IDEs. Our techniques for overcoming these challenges are the technical contributions of this paper. We evaluate WitchDoctor's design and implementation by simulating over 5,000 refactoring operations across three open-source projects. The simulated user is faster and more efficient than an average human user, yet WitchDoctor can detect more than 90% of refactoring operations as they are being performed - and can complete over a third of refactorings before the simulated user does. All the while, WitchDoctor remains robust in the face of non-parseable programs and unpredictable refactoring scenarios. We also show that WitchDoctor is efficient enough to perform computation on a keystroke-by-keystroke basis, adding an average overhead of only 15 milliseconds per keystroke.",
                "ieee_keywords": [
                    "Syntactics",
                    "History",
                    "Real time systems",
                    "Software",
                    "Engines",
                    "Data mining",
                    "Context"
                ],
                "author_keywords": [
                    "refactoring",
                    "IDE",
                    "change detection",
                    "repository mining"
                ]
            }
        ]
    },
    {
        "name": "Antonio Torralba",
        "publications": [
            {
                "title": "Intelligent Carpet: Inferring 3D Human Pose from Tactile Signals",
                "link": "https://ieeexplore.ieee.org/document/9577856/",
                "date_of_publication": "02 November 2021",
                "doi": "10.1109/CVPR46437.2021.01110",
                "citations": "15",
                "abstract": "Daily human activities, e.g., locomotion, exercises, and resting, are heavily guided by the tactile interactions between the human and the ground. In this work, leveraging such tactile interactions, we propose a 3D human pose estimation approach using the pressure maps recorded by a tactile carpet as input. We build a low-cost, high-density, large-scale intelligent carpet, which enables the real-time recordings of human-floor tactile interactions in a seamless manner. We collect a synchronized tactile and visual dataset on various human activities. Employing a state-of-the-art camera-based pose estimation model as supervision, we design and implement a deep neural network model to infer 3D human poses using only the tactile information. Our pipeline can be further scaled up to multi-person pose estimation. We evaluate our system and demonstrate its potential applications in diverse fields.",
                "ieee_keywords": [
                    "Visualization",
                    "Solid modeling",
                    "Three-dimensional displays",
                    "Machine vision",
                    "Pose estimation",
                    "Smart homes",
                    "Skeleton"
                ],
                "author_keywords": []
            },
            {
                "title": "ShadowCam: Real-Time Detection of Moving Obstacles Behind A Corner For Autonomous Vehicles",
                "link": "https://ieeexplore.ieee.org/document/8569569/",
                "date_of_publication": "09 December 2018",
                "doi": "10.1109/ITSC.2018.8569569",
                "citations": "6",
                "abstract": "Moving obstacles occluded by corners are a potential source for collisions in mobile robotics applications such as autonomous vehicles. In this paper, we address the problem of anticipating such collisions by proposing a vision-based detection algorithm for obstacles which are outside of a vehicle's direct line of sight. Our method detects shadows of obstacles hidden around corners and automatically classifies these unseen obstacles as “dynamic” or “static”. We evaluate our proposed detection algorithm on real-world corners and a large variety of simulated environments to assess generalizability in different challenging surface and lighting conditions. The mean classification accuracy on simulated data is around 80% and on real-world corners approximately 70%. Additionally, we integrate our detection system on a full-scale autonomous wheelchair and demonstrate its feasibility as an additional safety mechanism through real-world experiments. We release our real-time-capable implementation of the proposed ShadowCam algorithm and the dataset containing simulated and real-world data under an open-source license.",
                "ieee_keywords": [
                    "Heuristic algorithms",
                    "Vehicle dynamics",
                    "Cameras",
                    "Autonomous vehicles",
                    "Wheelchairs",
                    "Classification algorithms",
                    "Robots"
                ],
                "author_keywords": []
            },
            {
                "title": "Turning Corners into Cameras: Principles and Methods",
                "link": "https://ieeexplore.ieee.org/document/8237511/",
                "date_of_publication": "25 December 2017",
                "doi": "10.1109/ICCV.2017.249",
                "citations": "75",
                "abstract": "We show that walls, and other obstructions with edges, can be exploited as naturally-occurring “cameras” that reveal the hidden scenes beyond them. In particular, we demonstrate methods for using the subtle spatio-temporal radiance variations that arise on the ground at the base of a wall's edge to construct a one-dimensional video of the hidden scene behind the wall. The resulting technique can be used for a variety of applications in diverse physical settings. From standard RGB video recordings, we use edge cameras to recover 1-D videos that reveal the number and trajectories of people moving in an occluded scene. We further show that adjacent wall edges, such as those that arise in the case of an open doorway, yield a stereo camera from which the 2-D location of hidden, moving objects can be recovered. We demonstrate our technique in a number of indoor and outdoor environments involving varied floor surfaces and illumination conditions.",
                "ieee_keywords": [
                    "Cameras",
                    "Image edge detection",
                    "Observers",
                    "Lighting",
                    "Standards",
                    "Image color analysis",
                    "Trajectory"
                ],
                "author_keywords": []
            },
            {
                "title": "Propagation Networks for Model-Based Control Under Partial Observation",
                "link": "https://ieeexplore.ieee.org/document/8793509/",
                "date_of_publication": "12 August 2019",
                "doi": "10.1109/ICRA.2019.8793509",
                "citations": "35",
                "abstract": "There has been an increasing interest in learning dynamics simulators for model-based control. Compared with off-the-shelf physics engines, a learnable simulator can quickly adapt to unseen objects, scenes, and tasks. However, existing models like interaction networks only work for fully observable systems; they also only consider pairwise interactions within a single time step, both restricting their use in practical systems. We introduce Propagation Networks (PropNet), a differentiable, learnable dynamics model that handles partially observable scenarios and enables instantaneous propagation of signals beyond pairwise interactions. With these innovations, our propagation networks not only outperform current learnable physics engines in forward simulation, but also achieves superior performance on various control tasks. Compared with existing deep reinforcement learning algorithms, model-based control with propagation networks is more accurate, efficient, and generalizable to novel, partially observable scenes and tasks.",
                "ieee_keywords": [
                    "Engines",
                    "Task analysis",
                    "Force",
                    "Robots",
                    "Computational modeling",
                    "Adaptation models"
                ],
                "author_keywords": []
            },
            {
                "title": "Inferring Light Fields from Shadows",
                "link": "https://ieeexplore.ieee.org/document/8578754/",
                "date_of_publication": "16 December 2018",
                "doi": "10.1109/CVPR.2018.00656",
                "citations": "32",
                "abstract": "We present a method for inferring a 4D light field of a hidden scene from 2D shadows cast by a known occluder on a diffuse wall. We do this by determining how light naturally reflected off surfaces in the hidden scene interacts with the occluder. By modeling the light transport as a linear system, and incorporating prior knowledge about light field structures, we can invert the system to recover the hidden scene. We demonstrate results of our inference method across simulations and experiments with different types of occluders. For instance, using the shadow cast by a real house plant, we are able to recover low resolution light fields with different levels of texture and parallax complexity. We provide two experimental results: a human subject and two planar elements at different depths.",
                "ieee_keywords": [
                    "Two dimensional displays",
                    "Image reconstruction",
                    "Nonlinear optics",
                    "Geometry",
                    "Sparse matrices",
                    "Cameras"
                ],
                "author_keywords": []
            },
            {
                "title": "Learning Words by Drawing Images",
                "link": "https://ieeexplore.ieee.org/document/8954412/",
                "date_of_publication": "09 January 2020",
                "doi": "10.1109/CVPR.2019.00213",
                "citations": "4",
                "abstract": "We propose a framework for learning through drawing. Our goal is to learn the correspondence between spoken words and abstract visual attributes, from a dataset of spoken descriptions of images. Building upon recent findings that GAN representations can be manipulated to edit semantic concepts in the generated output, we propose a new method to use such GAN-generated images to train a model using a triplet loss. To apply the method, we develop Audio CLEVRGAN, a new dataset of audio descriptions of GAN-generated CLEVR images, and we describe a training procedure that creates a curriculum of GAN-generated images that focuses training on image pairs that differ in a specific, informative way. Training is done without additional supervision beyond the spoken captions and the GAN. We find that training that takes advantage of GAN-generated edited examples results in improvements in the model's ability to learn attributes compared to previous results. Our proposed learning framework also results in models that can associate spoken words with some abstract visual concepts such as color and size.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Dynamic Modeling of Hand-Object Interactions via Tactile Sensing",
                "link": "https://ieeexplore.ieee.org/document/9636361/",
                "date_of_publication": "16 December 2021",
                "doi": "10.1109/IROS51168.2021.9636361",
                "citations": "4",
                "abstract": "Tactile sensing is critical for humans to perform everyday tasks. While significant progress has been made in analyzing object grasping from vision, it remains unclear how we can utilize tactile sensing to reason about and model the dynamics of hand-object interactions. In this work, we employ a high-resolution tactile glove to perform four different interactive activities on a diversified set of objects. We propose a framework aiming at predicting the 3d locations of both the hand and the object purely from the touch data by combining a predictive model and a contrastive learning module. This framework can reason about the interaction patterns from the tactile data, hallucinate the changes in the environment, esti-mate the uncertainty of the prediction, and generalize to unseen objects. We also provide detailed ablation studies regarding different system designs as well as visualizations of the predicted trajectories. This work takes a step on dynamics modeling in hand-object interactions from dense tactile sensing, which opens the door for future applications in activity learning, human-computer interactions, and imitation learning for robotics.",
                "ieee_keywords": [
                    "Human computer interaction",
                    "Uncertainty",
                    "Three-dimensional displays",
                    "Predictive models",
                    "Robot sensing systems",
                    "Sensors",
                    "Trajectory"
                ],
                "author_keywords": []
            },
            {
                "title": "Exploiting Occlusion in Non-Line-of-Sight Active Imaging",
                "link": "https://ieeexplore.ieee.org/document/8345763/",
                "date_of_publication": null,
                "doi": "10.1109/TCI.2018.2829599",
                "citations": "44",
                "abstract": "Active non-line-of-sight imaging systems are of growing interest for diverse applications. The most commonly proposed approaches to date rely on exploiting time-resolved measurements, i.e., measuring the time it takes for short-duration light pulses to transit the scene. This typically requires expensive, specialized, ultrafast lasers, and detectors that must be carefully calibrated. We develop an alternative approach that exploits the valuable role that natural occluders in a scene play in enabling accurate and practical image formation in such settings without such hardware complexity. In particular, we demonstrate that the presence of occluders in the hidden scene can obviate the need for collecting time-resolved measurements, and develop an accompanying analysis for such systems and their generalizations. Ultimately, the results suggest the potential to develop increasingly sophisticated future systems that are able to identify and exploit diverse structural features of the environment to reconstruct scenes hidden from view.",
                "ieee_keywords": [
                    "Cameras",
                    "Lighting",
                    "Nonlinear optics",
                    "Measurement by laser beam",
                    "Laser beams",
                    "Optical surface waves"
                ],
                "author_keywords": [
                    "Computational imaging",
                    "computer vision",
                    "LIDAR",
                    "non-line-of-sight imaging",
                    "time-of-flight cameras"
                ]
            },
            {
                "title": "SUN database: Large-scale scene recognition from abbey to zoo",
                "link": "https://ieeexplore.ieee.org/document/5539970/",
                "date_of_publication": "05 August 2010",
                "doi": "10.1109/CVPR.2010.5539970",
                "citations": "1449",
                "abstract": "Scene categorization is a fundamental problem in computer vision. However, scene understanding research has been constrained by the limited scope of currently-used databases which do not capture the full variety of scene categories. Whereas standard databases for object categorization contain hundreds of different classes of objects, the largest available dataset of scene categories contains only 15 classes. In this paper we propose the extensive Scene UNderstanding (SUN) database that contains 899 categories and 130,519 images. We use 397 well-sampled categories to evaluate numerous state-of-the-art algorithms for scene recognition and establish new bounds of performance. We measure human scene classification performance on the SUN database and compare this with computational methods. Additionally, we study a finer-grained scene representation to detect scenes embedded inside of larger scenes.",
                "ieee_keywords": [
                    "Sun",
                    "Large-scale systems",
                    "Layout",
                    "Humans",
                    "Image databases",
                    "Computer vision",
                    "Anthropometry",
                    "Bridges",
                    "Legged locomotion",
                    "Spatial databases"
                ],
                "author_keywords": []
            },
            {
                "title": "Scene Parsing through ADE20K Dataset",
                "link": "https://ieeexplore.ieee.org/document/8100027/",
                "date_of_publication": "09 November 2017",
                "doi": "10.1109/CVPR.2017.544",
                "citations": "1111",
                "abstract": "Scene parsing, or recognizing and segmenting objects and stuff in an image, is one of the key problems in computer vision. Despite the communitys efforts in data collection, there are still few image datasets covering a wide range of scenes and object categories with dense and detailed annotations for scene parsing. In this paper, we introduce and analyze the ADE20K dataset, spanning diverse annotations of scenes, objects, parts of objects, and in some cases even parts of parts. A scene parsing benchmark is built upon the ADE20K with 150 object and stuff classes included. Several segmentation baseline models are evaluated on the benchmark. A novel network design called Cascade Segmentation Module is proposed to parse a scene into stuff, objects, and object parts in a cascade and improve over the baselines. We further show that the trained scene parsing networks can lead to applications such as image content removal and scene synthesis 1 .",
                "ieee_keywords": [
                    "Image segmentation",
                    "Semantics",
                    "Sun",
                    "Labeling",
                    "Visualization",
                    "Neural networks",
                    "Computer vision"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Wojciech Matusik",
        "publications": [
            {
                "title": "Seamless-walk: Novel Natural Virtual Reality Locomotion Method with a High-Resolution Tactile Sensor",
                "link": "https://ieeexplore.ieee.org/document/9757510/",
                "date_of_publication": "20 April 2022",
                "doi": "10.1109/VRW55335.2022.00199",
                "citations": "377",
                "abstract": "Published: 2023 Show More References References is not available for this document.",
                "ieee_keywords": [
                    "Legged locomotion",
                    "Three-dimensional displays",
                    "Conferences",
                    "Virtual environments",
                    "Tactile sensors",
                    "Machine learning",
                    "User interfaces"
                ],
                "author_keywords": [
                    "Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Virtual reality"
                ]
            },
            {
                "title": "Intelligent Carpet: Inferring 3D Human Pose from Tactile Signals",
                "link": "https://ieeexplore.ieee.org/document/9577856/",
                "date_of_publication": "02 November 2021",
                "doi": "10.1109/CVPR46437.2021.01110",
                "citations": "15",
                "abstract": "Daily human activities, e.g., locomotion, exercises, and resting, are heavily guided by the tactile interactions between the human and the ground. In this work, leveraging such tactile interactions, we propose a 3D human pose estimation approach using the pressure maps recorded by a tactile carpet as input. We build a low-cost, high-density, large-scale intelligent carpet, which enables the real-time recordings of human-floor tactile interactions in a seamless manner. We collect a synchronized tactile and visual dataset on various human activities. Employing a state-of-the-art camera-based pose estimation model as supervision, we design and implement a deep neural network model to infer 3D human poses using only the tactile information. Our pipeline can be further scaled up to multi-person pose estimation. We evaluate our system and demonstrate its potential applications in diverse fields.",
                "ieee_keywords": [
                    "Visualization",
                    "Solid modeling",
                    "Three-dimensional displays",
                    "Machine vision",
                    "Pose estimation",
                    "Smart homes",
                    "Skeleton"
                ],
                "author_keywords": []
            },
            {
                "title": "ChainQueen: A Real-Time Differentiable Physical Simulator for Soft Robotics",
                "link": "https://ieeexplore.ieee.org/document/8794333/",
                "date_of_publication": "12 August 2019",
                "doi": "10.1109/ICRA.2019.8794333",
                "citations": "76",
                "abstract": "Physical simulators have been widely used in robot planning and control. Among them, differentiable simulators are particularly favored, as they can be incorporated into gradient-based optimization algorithms that are efficient in solving inverse problems such as optimal control and motion planning. Therefore, rigid body simulators and recently their differentiable variants are studied extensively. Simulating deformable objects is, however, more challenging compared to rigid body dynamics. The underlying physical laws of deformable objects are more complex, and the resulting systems have orders of magnitude more degrees of freedom and there-fore they are significantly more computationally expensive to simulate. Computing gradients with respect to physical design or controller parameters is typically even more computationally challenging. In this paper, we propose a real-time, differentiable hybrid Lagrangian-Eulerian physical simulator for deformable objects, ChainQueen, based on the Moving Least Squares Material Point Method (MLS-MPM). MLS-MPM can simulate deformable objects with collisions and can be seamlessly incorporated into soft robotic systems. We demonstrate that our simulator achieves high precision in both forward simulation and backward gradient computation. We have successfully employed it in a diverse set of inference, control and co-design tasks for soft robotics.",
                "ieee_keywords": [
                    "Graphics processing units",
                    "Computational modeling",
                    "Soft robotics",
                    "Three-dimensional displays",
                    "Planning",
                    "Inverse problems"
                ],
                "author_keywords": []
            },
            {
                "title": "Dynamic Modeling of Hand-Object Interactions via Tactile Sensing",
                "link": "https://ieeexplore.ieee.org/document/9636361/",
                "date_of_publication": "16 December 2021",
                "doi": "10.1109/IROS51168.2021.9636361",
                "citations": "4",
                "abstract": "Tactile sensing is critical for humans to perform everyday tasks. While significant progress has been made in analyzing object grasping from vision, it remains unclear how we can utilize tactile sensing to reason about and model the dynamics of hand-object interactions. In this work, we employ a high-resolution tactile glove to perform four different interactive activities on a diversified set of objects. We propose a framework aiming at predicting the 3d locations of both the hand and the object purely from the touch data by combining a predictive model and a contrastive learning module. This framework can reason about the interaction patterns from the tactile data, hallucinate the changes in the environment, esti-mate the uncertainty of the prediction, and generalize to unseen objects. We also provide detailed ablation studies regarding different system designs as well as visualizations of the predicted trajectories. This work takes a step on dynamics modeling in hand-object interactions from dense tactile sensing, which opens the door for future applications in activity learning, human-computer interactions, and imitation learning for robotics.",
                "ieee_keywords": [
                    "Human computer interaction",
                    "Uncertainty",
                    "Three-dimensional displays",
                    "Predictive models",
                    "Robot sensing systems",
                    "Sensors",
                    "Trajectory"
                ],
                "author_keywords": []
            },
            {
                "title": "Graph Grammar-Based Automatic Design for Heterogeneous Fleets of Underwater Robots",
                "link": "https://ieeexplore.ieee.org/document/9811808/",
                "date_of_publication": "12 July 2022",
                "doi": "10.1109/ICRA46639.2022.9811808",
                "citations": "1",
                "abstract": "Autonomous underwater vehicles (AUVs) are spe-cialized robots that are commonly used for seafloor surveying and ocean water sampling. Computational design approaches have emerged to reduce the effort required to design both individual AUVs as well as fleets. As the number and scale of underwater missions increases beyond the capabilities of a single vehicle, fleet level design will become more important. Depending on the mission, the optimal fleet may consist of multiple distinct types of AUVs designed to a variety of specifications. Moreover, the AUVs may differ in both continuous parameters (such as battery capacity) and discrete parameters (such as number and model of thrusters). In this work, we present a computational pipeline for designing these heterogeneous AUV fleets. Using a novel shape design space based on a graph grammar and deformation cages, we can express a variety of AUV architectures with different topologies, component selections, and dimensions. We search this space using a combination of discrete graph search and gradient-based continuous optimization, enabled by a differentiable AUV simulator. Finally, we formulate heterogeneous fleet design as a modified knapsack problem, and solve it using an efficient backtracking-based algorithm. We evaluate our pipeline on a simulated mission with nonuniform design requirements-surveying a section of seafloor with varying depth-and show that the best heterogeneous fleet outperforms the best fleet composed of a single vehicle type.",
                "ieee_keywords": [
                    "Autonomous underwater vehicles",
                    "Shape",
                    "Sea floor",
                    "Pipelines",
                    "Computer architecture",
                    "Topology",
                    "Grammar"
                ],
                "author_keywords": []
            },
            {
                "title": "Automatic Co-Design of Aerial Robots Using a Graph Grammar",
                "link": "https://ieeexplore.ieee.org/document/9982013/",
                "date_of_publication": "26 December 2022",
                "doi": "10.1109/IROS47612.2022.9982013",
                "citations": "326",
                "abstract": "Unmanned aerial vehicles (UAVs) have broad applications including disaster response, transportation, photography, and mapping. A significant bottleneck in the development of UAVs is the limited availability of automatic tools for task-specific co-design of a UAV's shape and controller. The development of such tools is particularly challenging as UAVs can take many forms, including fixed-wing planes, radial copters, and hybrid topologies, with each class of topology showing different advantages. In this work, we present a computational design pipeline for UAVs based on a graph grammar that can search across a wide range of topologies. Graphs generated by the grammar encode different topologies and component selections, while continuous parameters encode the dimensions and properties of each component. We further augment the shape representation with deformation cages, which allow expressing a variety of wing shapes. Each UAV design is associated with an LQR controller with tunable continuous parameters. To search over this complex discrete and continuous design space, we develop a hybrid algorithm that combines discrete graph search strategies and gradient-based continuous optimization methods using a differentiable UAV simulator. We evaluate our pipeline on a set of simulated flight tasks requiring dynamic motions, showing that it discovers novel UAV designs that outperform canonical UAVs typically made by engineers.",
                "ieee_keywords": [
                    "Shape",
                    "Pipelines",
                    "Transportation",
                    "Autonomous aerial vehicles",
                    "Search problems",
                    "Topology",
                    "Grammar"
                ],
                "author_keywords": []
            },
            {
                "title": "An Integrated Design Pipeline for Tactile Sensing Robotic Manipulators",
                "link": "https://ieeexplore.ieee.org/document/9812335/",
                "date_of_publication": "12 July 2022",
                "doi": "10.1109/ICRA46639.2022.9812335",
                "citations": "224",
                "abstract": "Traditional robotic manipulator design methods require extensive, time-consuming, and manual trial and error to produce a viable design. During this process, engineers often spend their time redesigning or reshaping components as they discover better topologies for the robotic manipula-tor. Tactile sensors, while useful, often complicate the design due to their bulky form factor. We propose an integrated design pipeline to streamline the design and manufacturing of robotic manipulators with knitted, glove-like tactile sensors. The proposed pipeline allows a designer to assemble a collection of modular, open-source components by applying predefined graph grammar rules. The end result is an intuitive design paradigm that allows the creation of new virtual designs of manipulators in a matter of minutes. Our framework allows the designer to fine-tune the manipulator's shape through cage-based geometry deformation. Finally, the designer can select surfaces for adding tactile sensing. Once the manipulator design is finished, the program will automatically generate 3D printing and knitting files for manufacturing. We demonstrate the utility of this pipeline by creating four custom manipulators tested on real-world tasks: screwing in a wing nut, pouring water from a bottle, picking up an egg, and cutting paper with scissors.",
                "ieee_keywords": [
                    "Pipelines",
                    "Tactile sensors",
                    "Integrated design",
                    "Manipulators",
                    "Three-dimensional printing",
                    "Sensors",
                    "Manufacturing"
                ],
                "author_keywords": []
            },
            {
                "title": "Co-Learning of Task and Sensor Placement for Soft Robotics",
                "link": "https://ieeexplore.ieee.org/document/9345345/",
                "date_of_publication": null,
                "doi": "10.1109/LRA.2021.3056369",
                "citations": "14",
                "abstract": "Unlike rigid robots which operate with compact degrees of freedom, soft robots must reason about an infinite dimensional state space. Mapping this continuum state space presents significant challenges, especially when working with a finite set of discrete sensors. Reconstructing the robot's state from these sparse inputs is challenging, especially since sensor location has a profound downstream impact on the richness of learned models for robotic tasks. In this work, we present a novel representation for co-learning sensor placement and complex tasks. Specifically, we present a neural architecture which processes on-board sensor information to learn a salient and sparse selection of placements for optimal task performance. We evaluate our model and learning algorithm on six soft robot morphologies for various supervised learning tasks, including tactile sensing and proprioception. We also highlight applications to soft robot motion subspace visualization and control. Our method demonstrates superior performance in task learning to algorithmic and human baselines while also learning sensor placements and latent spaces that are semantically meaningful.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Multi-Objective Graph Heuristic Search for Terrestrial Robot Design",
                "link": "https://ieeexplore.ieee.org/document/9561818/",
                "date_of_publication": "18 October 2021",
                "doi": "10.1109/ICRA48506.2021.9561818",
                "citations": "9",
                "abstract": "We present methods for co-designing rigid robots over control and morphology (including discrete topology) over multiple objectives. Previous work has addressed problems in single-objective robot co-design or multi-objective control. However, the joint multi-objective co-design problem is extremely important for generating capable, versatile, algorithmically designed robots. In this work, we present Multi-Objective Graph Heuristic Search, which extends a single-objective graph heuristic search from previous work to enable a highly efficient multi-objective search in a combinatorial design topology space. Core to this approach, we introduce a new universal, multiobjective heuristic function based on graph neural networks that is able to effectively leverage learned information between different task trade-offs. We demonstrate our approach on six combinations of seven terrestrial locomotion and design tasks, including one three-objective example. We compare the captured Pareto fronts across different methods and demonstrate that our multi-objective graph heuristic search quantitatively and qualitatively outperforms other techniques.",
                "ieee_keywords": [
                    "Measurement",
                    "Network topology",
                    "Navigation",
                    "Morphology",
                    "Aerospace electronics",
                    "Search problems",
                    "Topology"
                ],
                "author_keywords": []
            },
            {
                "title": "FabSquare: Fabricating Photopolymer Objects by Mold 3D Printing and UV Curing",
                "link": "https://ieeexplore.ieee.org/document/7912271/",
                "date_of_publication": null,
                "doi": "10.1109/MCG.2017.37",
                "citations": "5",
                "abstract": "The FabSquare system is a personal fabrication method that lets users fabricate objects by molding photopolymers inside a 3D printed mold. The molds are printed with UV-transparent materials that allow for UV curing--the polymerization and solidification of the fluid content. The molds can be repeatedly reused to fabricate identical objects or create new objects with identical geometry, but different components. Because the necessary equipment is easily obtainable and affordable, the FabSquare approach is suitable for ordinary users in nonspecialized labs, allowing them to rapidly fabricate a range of objects. https://extras.computer.org/extra/mcg2017030034s1.mp4https://extras.computer.org/extra/mcg2017030034s2.pdf",
                "ieee_keywords": [
                    "Three-dimensional displays",
                    "Fabrication",
                    "Injection molding",
                    "Printers",
                    "Three-dimensional printing"
                ],
                "author_keywords": [
                    "computer graphics",
                    "3D printing",
                    "injection molding",
                    "rapid prototyping"
                ]
            }
        ]
    },
    {
        "name": "Tommi Jaakkola",
        "publications": [
            {
                "title": "High Dimensional Inference With Random Maximum A-Posteriori Perturbations",
                "link": "https://ieeexplore.ieee.org/document/8715392/",
                "date_of_publication": null,
                "doi": "10.1109/TIT.2019.2916805",
                "citations": "2",
                "abstract": "This paper presents a new approach, called perturb-max, for high-dimensional statistical inference in graphical models that is based on applying random perturbations followed by optimization. This framework injects randomness into maximum a-posteriori (MAP) predictors by randomly perturbing the potential function for the input. A classic result from extreme value statistics asserts that perturb-max operations generate unbiased samples from the Gibbs distribution using high-dimensional perturbations. Unfortunately, the computational cost of generating so many high-dimensional random variables can be prohibitive. However, when the perturbations are of low dimension, sampling the perturb-max prediction is as efficient as MAP optimization. This paper shows that the expected value of perturb-max inference with low dimensional perturbations can be used sequentially to generate unbiased samples from the Gibbs distribution. Furthermore the expected value of the maximal perturbations is a natural bound on the entropy of such perturb-max models. A measure concentration result for perturb-max values shows that the deviation of their sampled average from its expectation decays exponentially in the number of samples, allowing effective approximation of the expectation.",
                "ieee_keywords": [
                    "Perturbation methods",
                    "Random variables",
                    "Optimization",
                    "Distribution functions",
                    "Maximum a posteriori estimation",
                    "Image segmentation",
                    "Entropy"
                ],
                "author_keywords": [
                    "Graphical models",
                    "MAP inference",
                    "Measure concentration",
                    "Markov Chain Monte Carlo"
                ]
            }
        ]
    },
    {
        "name": "Stefanie Jegelka",
        "publications": [
            {
                "title": "Focused model-learning and planning for non-Gaussian continuous state-action systems",
                "link": "https://ieeexplore.ieee.org/document/7989433/",
                "date_of_publication": "24 July 2017",
                "doi": "10.1109/ICRA.2017.7989433",
                "citations": "5",
                "abstract": "We introduce a framework for model learning and planning in stochastic domains with continuous state and action spaces and non-Gaussian transition models. It is efficient because (1) local models are estimated only when the planner requires them; (2) the planner focuses on the most relevant states to the current planning problem; and (3) the planner focuses on the most informative and/or high-value actions. Our theoretical analysis shows the validity and asymptotic optimality of the proposed approach. Empirically, we demonstrate the effectiveness of our algorithm on a simulated multi-modal pushing problem.",
                "ieee_keywords": [
                    "Planning",
                    "Computational modeling",
                    "Stochastic processes",
                    "Aerospace electronics",
                    "Robots",
                    "Dynamic programming",
                    "Heuristic algorithms"
                ],
                "author_keywords": []
            },
            {
                "title": "Sensor Array Design Through Submodular Optimization",
                "link": "https://ieeexplore.ieee.org/document/8482297/",
                "date_of_publication": null,
                "doi": "10.1109/TIT.2018.2873795",
                "citations": "6",
                "abstract": "We consider the problem of far-field sensing by means of a sensor array. Traditional array geometry design techniques are agnostic to prior information about the far-field scene. However, in many applications such priors are available and may be utilized to design more efficient array topologies. We formulate the problem of array geometry design with scene prior as one of finding a sampling configuration that enables efficient inference, which turns out to be a combinatorial optimization problem. While generic combinatorial optimization problems are NP-hard and resist efficient solvers, we show how for array design problems the theory of submodular optimization may be utilized to obtain efficient algorithms that are guaranteed to achieve solutions within a constant approximation factor from the optimum. We leverage the connection between array design problems and submodular optimization and port several results of interest. We demonstrate efficient methods for designing arrays with constraints on the sensing aperture, as well as arrays respecting combinatorial placement constraints. This novel connection between array design and submodularity suggests the possibility for utilizing other insights and techniques from the growing body of literature on submodular optimization in the field of array design.",
                "ieee_keywords": [
                    "Optimization",
                    "Geometry",
                    "Sensor arrays",
                    "Bayes methods",
                    "Array signal processing",
                    "Fourier transforms"
                ],
                "author_keywords": [
                    "Array design",
                    "submodular optimization",
                    "far field sensing"
                ]
            },
            {
                "title": "Multiple wavelength sensing array design",
                "link": "https://ieeexplore.ieee.org/document/7952792/",
                "date_of_publication": "19 June 2017",
                "doi": "10.1109/ICASSP.2017.7952792",
                "citations": "5",
                "abstract": "We design finite antenna arrays for far-field sensing at multiple wavelengths, under two design paradigms. The first design paradigm is optimized for collection of measurements at multiple wavelengths, fusing these together for joint inference over an underlying scene. The second design paradigm is robust, in a sense that it is guaranteed to allow good inference over the scene at any one single wavelength at a time. We quantify inference quality via the D-Bayes optimality criterion and limit the design space by restricting the number of allowed sensors and the positions where these can be placed. We show that the resulting combinatorial optimization problems are instances of problems in a class known to have efficient guaranteed approximation algorithms, namely submodular optimization problems, and showcase the design of arrays under both paradigms utilizing simple greedy selection algorithms, and state-of-the-art robust submodular maximization algorithms.",
                "ieee_keywords": [
                    "Optimization",
                    "Approximation algorithms",
                    "Robustness",
                    "Sensor arrays",
                    "Wavelength measurement",
                    "Geometry"
                ],
                "author_keywords": [
                    "Multiple Wavelength",
                    "Far Field",
                    "Array Design",
                    "Submodular Optimization",
                    "Robust Optimization"
                ]
            }
        ]
    },
    {
        "name": "Samuel Madden",
        "publications": [
            {
                "title": "Database Abstractions for Managing Sensor Network Data",
                "link": "https://ieeexplore.ieee.org/document/5575355/",
                "date_of_publication": null,
                "doi": "10.1109/JPROC.2010.2063691",
                "citations": "5",
                "abstract": "Sensor networking hardware, networking, and operating system software has matured to the point that the major challenges facing the field now have to do with storing, cleaning, and querying the data such networks produce. In this paper, we survey several research systems designed for managing sensor data using declarative database-like abstractions from the database community and specifically the Massachusetts Institute of Technology (MIT, Cambridge) database group. The systems we discuss are designed to help prioritize data collection in the face of intermittent bandwidth, clean and smooth data using statistical models stored inside the database, and run declarative queries over probabilistic data.",
                "ieee_keywords": [
                    "Databases",
                    "Data models",
                    "Portals",
                    "Bandwidth",
                    "Mathematical model",
                    "Temperature sensors",
                    "Hardware"
                ],
                "author_keywords": [
                    "Database management systems",
                    "database query processing",
                    "intelligent sensors"
                ]
            },
            {
                "title": "Nonintrusive Measurements for Detecting Progressive Equipment Faults",
                "link": "https://ieeexplore.ieee.org/document/9837093/",
                "date_of_publication": null,
                "doi": "10.1109/TIM.2022.3193178",
                "citations": "3",
                "abstract": "The gradual environmental degradation of materials can lead to corrosion and mechanical failures of electrical loads. Vibration and corrosion both contribute to the failure of electrical connections and conductors, and electrical arcs destroy materials in ways that enhance the opportunities for further damage. These failures lead to loss of system capability and the possibility of electrical fires. The signatures of these impending electrical failures, which may occur over weeks or months, can be detected in many systems through nonintrusive electrical monitoring. This article demonstrates techniques for decoding the type of electrical signatures that lead to progressive failure. Example applications of these techniques are illustrated with observations of the degradation and failure of copper-sheathed jacket water (JW) heaters aboard a United States Coast Guard (USCG) vessel. Electrical, material, and vibration analysis are presented to demonstrate the basis for diagnostic electrical signatures.",
                "ieee_keywords": [
                    "Resistance heating",
                    "Monitoring",
                    "Degradation",
                    "Heat engines",
                    "Corrosion",
                    "Water heating",
                    "Voltage measurement"
                ],
                "author_keywords": [
                    "Condition-based maintenance",
                    "corrosion",
                    "fault detection",
                    "nonintrusive load monitoring",
                    "power monitoring",
                    "root-cause analysis"
                ]
            }
        ]
    },
    {
        "name": "Dina Katabi",
        "publications": [
            {
                "title": "27.4 A 0.75-million-point fourier-transform chip for frequency-sparse signals",
                "link": "https://ieeexplore.ieee.org/document/6757512/",
                "date_of_publication": "06 March 2014",
                "doi": "10.1109/ISSCC.2014.6757512",
                "citations": "22",
                "abstract": "Applications like spectrum sensing, radar signal processing, and pattern matching by convolving a signal with a long code, as in GPS, require large FFT sizes. ASIC implementations of such FFTs are challenging due to their large silicon area and high power consumption. However, the signals in these applications are sparse, i.e., the energy at the output of the FFT/IFFT is concentrated at a limited number of frequencies and with zero/negligible energy at most frequencies. Recent advances in signal processing have shown that, for such sparse signals, a new algorithm called the sparse FFT (sFFT) can compute the Fourier transform more efficiently than traditional FFTs [1].",
                "ieee_keywords": [
                    "Estimation",
                    "Fourier transforms",
                    "OFDM",
                    "Frequency measurement",
                    "Solid state circuits",
                    "Energy efficiency",
                    "Signal processing algorithms"
                ],
                "author_keywords": []
            },
            {
                "title": "Sample-optimal average-case sparse Fourier Transform in two dimensions",
                "link": "https://ieeexplore.ieee.org/document/6736670/",
                "date_of_publication": "13 February 2014",
                "doi": "10.1109/Allerton.2013.6736670",
                "citations": "46",
                "abstract": "We present the first sample-optimal sublinear time algorithms for the sparse Discrete Fourier Transform over a two-dimensional √n × √n grid. Our algorithms are analyzed for the average case signals. For signals whose spectrum is exactly sparse, we present algorithms that use O(k) samples and run in O(k log k) time, where k is the expected sparsity of the signal. For signals whose spectrum is approximately sparse, we have an algorithm that uses O(k log n) samples and runs in O(k log 2 n) time, for k = Θ(√n). All presented algorithms match the lower bounds on sample complexity for their respective signal models.",
                "ieee_keywords": [
                    "Approximation algorithms",
                    "Discrete Fourier transforms",
                    "OFDM",
                    "Complexity theory",
                    "Algorithm design and analysis",
                    "Vectors"
                ],
                "author_keywords": []
            },
            {
                "title": "High-throughput implementation of a million-point sparse Fourier Transform",
                "link": "https://ieeexplore.ieee.org/document/6927450/",
                "date_of_publication": "20 October 2014",
                "doi": "10.1109/FPL.2014.6927450",
                "citations": "21",
                "abstract": "The emergence of data-intensive problems in areas like computational biology, astronomy, medical imaging, etc. has emphasized the need for fast and efficient very large Fourier Transforms. Recent work has shown that we can compute million-point transforms efficiently provided the data is sparse in the frequency domain. Processing input samples at rates approaching 1 GHz would allow real-time processing in several such applications. In this paper, we present a high-throughput FPGA implementation that performs a million-point sparse Fourier Transform on frequency-sparse input data, generating the largest 500 frequency component locations and values every 1.16 milliseconds. This design can process streamed input data at 0.86 Giga samples per second, and does not make any assumptions of the distribution of the frequency components beyond sparsity.",
                "ieee_keywords": [
                    "Field programmable gate arrays",
                    "Computer architecture",
                    "Vectors",
                    "Indexes",
                    "Fourier transforms",
                    "Noise",
                    "Algorithm design and analysis"
                ],
                "author_keywords": []
            },
            {
                "title": "GHz-wide sensing and decoding using the sparse Fourier transform",
                "link": "https://ieeexplore.ieee.org/document/6848169/",
                "date_of_publication": "08 July 2014",
                "doi": "10.1109/INFOCOM.2014.6848169",
                "citations": "68",
                "abstract": "We present BigBand, a technology that can capture GHz of spectrum in realtime without sampling the signal at GS/s - i.e., without high speed ADCs. Further, it is simple and can be implemented on commodity low-power radios. Our approach builds on recent advances in the area of sparse Fourier transforms, which show that it is possible to reconstruct a sparse signal without sampling it at the Nyquist rate. To demonstrate our design, we implement it using 3 software radios, each sampling the spectrum at 50 MS/s, producing a device that captures 0.9 GHz - i.e., 6× larger digital bandwidth than the three software radios combined. Finally, an extension of BigBand can perform GHz spectrum sensing even in scenarios where the spectrum is not sparse.",
                "ieee_keywords": [
                    "Frequency estimation",
                    "Sensors",
                    "Bandwidth",
                    "Time-frequency analysis",
                    "Fourier transforms",
                    "Hardware",
                    "Wireless communication"
                ],
                "author_keywords": []
            },
            {
                "title": "1.3 Working at the Intersection of Machine Learning, Signal Processing, Sensors, and Circuits",
                "link": "https://ieeexplore.ieee.org/document/9365938/",
                "date_of_publication": "03 March 2021",
                "doi": "10.1109/ISSCC42613.2021.9365938",
                "citations": "2",
                "abstract": "Wireless technologies have propelled major advances in various industries including telecommunications, entertainment, and smart environments. Today, radios are embedded in cellphones, wearable devices, sensors, and control systems. Yet, in all of these diverse domains, radios are used for communications. Even when embedded in a sensor, the radio signals are used for communicating the sensed data, rather than performing the sensing function. We argue that future innovations should leverage radio signals for sensing. Radio signals are fascinating creatures: they travel in space, traverse walls and occlusions, and reflect off people and objects. The reflected signals are modulated by people's movements, actions, respiration, and heart beats. By analyzing such radio reflections, we can learn much information about people and their physiological signals, and leverage such knowledge to enable new capabilities and novel applications.",
                "ieee_keywords": [
                    "Wireless communication",
                    "Wireless sensor networks",
                    "Technological innovation",
                    "Wearable computers",
                    "Telecommunications",
                    "Intelligent sensors",
                    "Wearable sensors"
                ],
                "author_keywords": []
            },
            {
                "title": "Performance regimes of uncoded linear communications over AWGN channels",
                "link": "https://ieeexplore.ieee.org/document/5766155/",
                "date_of_publication": "12 May 2011",
                "doi": "10.1109/CISS.2011.5766155",
                "citations": "16",
                "abstract": "Streaming degradable content (such as video or audio) over a wireless channel presents new challenges to modern digital design, which was founded on the separation theorem of Shannon theory. Joint source-channel coding (JSCC) has recently received increasing interest to address the varying nature of the wireless channel conditions (under mobility or multicast). The conventional approach to JSCC which combines successive refinement with superposition coding is still digital and separable. However, for a white Gaussian source on a white Gaussian channel, it is outperformed by uncoded, linear scaling, which achieves Shannon's distortion limit. Practical degradable content sources are not white, but better approximated by multivariate / non-white Gaussian models. We investigate the performance of a linear, uncoded communication scheme for such sources. We find that there exist regimes where the uncoded scheme is near-optimal for point-to-point communication and provides significant gains over the conventional digital design for broadcast.",
                "ieee_keywords": [
                    "Signal to noise ratio",
                    "Encoding",
                    "Bandwidth",
                    "Decoding",
                    "Correlation",
                    "Wireless communication",
                    "Robustness"
                ],
                "author_keywords": []
            },
            {
                "title": "AirShare: Distributed coherent transmission made seamless",
                "link": "https://ieeexplore.ieee.org/document/7218555/",
                "date_of_publication": "24 August 2015",
                "doi": "10.1109/INFOCOM.2015.7218555",
                "citations": "73",
                "abstract": "Distributed coherent transmission is necessary for a variety of high-gain communication protocols such as distributed MIMO and creating codes over the air. Unfortunately, however, distributed coherent transmission is intrinsically difficult because different nodes are driven by independent clocks, which do not have the exact same frequency. This causes the nodes to have frequency offsets relative to each other, and hence their transmissions fail to combine coherently over the air. This paper presents AirShare, a primitive that makes distributed coherent transmission seamless. AirShare transmits a shared clock on the air and feeds it to the wireless nodes as a reference clock, hence eliminating the root cause for incoherent transmissions. The paper addresses the challenges in designing and delivering such a shared clock. It also implements AirShare in a network of USRP software radios, and demonstrates that it achieves tight phase coherence. Further, to illustrate AirShare's versatility, the paper uses it to deliver a coherent-radio abstraction on top of which it demonstrates two cooperative protocols: distributed MIMO, and distributed rate adaptation.",
                "ieee_keywords": [
                    "Clocks",
                    "Wireless communication",
                    "Phase locked loops",
                    "Wireless sensor networks",
                    "Protocols",
                    "MIMO",
                    "Radio transmitters"
                ],
                "author_keywords": []
            },
            {
                "title": "Physical layer wireless security made fast and channel independent",
                "link": "https://ieeexplore.ieee.org/document/5934889/",
                "date_of_publication": "30 June 2011",
                "doi": "10.1109/INFCOM.2011.5934889",
                "citations": "101",
                "abstract": "There is a growing interest in physical layer security. Recent work has demonstrated that wireless devices can generate a shared secret key by exploiting variations in their channel. The rate at which the secret bits are generated, however, depends heavily on how fast the channel changes. As a result, existing schemes have a low secrecy rate and are mainly applicable to mobile environments. In contrast, this paper presents a new physical-layer approach to secret key generation that is both fast and independent of channel variations. Our approach makes a receiver jam the signal in a manner that still allows it to decode the data, yet prevents other nodes from decoding. Results from a testbed implementation show that our method is significantly faster and more accurate than state of the art physical-layer secret key generation protocols. Specifically, while past work generates up to 44 secret bits/s with a 4% bit disagreement between the two devices, our design has a secrecy rate of 3-18 Kb/s with 0% bit disagreement.",
                "ieee_keywords": [
                    "Jamming",
                    "OFDM",
                    "Receivers",
                    "Binary phase shift keying",
                    "Bit error rate",
                    "Communication system security",
                    "Wireless communication"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Tomas Lozano-Perez",
        "publications": [
            {
                "title": "Omnipush: accurate, diverse, real-world dataset of pushing dynamics with RGB-D video",
                "link": "https://ieeexplore.ieee.org/document/8967920/",
                "date_of_publication": "28 January 2020",
                "doi": "10.1109/IROS40897.2019.8967920",
                "citations": "9",
                "abstract": "Pushing is a fundamental robotic skill. Existing work has shown how to exploit models of pushing to achieve a variety of tasks, including grasping under uncertainty, in-hand manipulation and clearing clutter. Such models, however, are approximate, which limits their applicability.Learning-based methods can reason directly from raw sensory data with accuracy, and have the potential to generalize to a wider diversity of scenarios. However, developing and testing such methods requires rich-enough datasets. In this paper we introduce Omnipush, a dataset with high variety of planar pushing behavior.In particular, we provide 250 pushes for each of 250 objects, all recorded with RGB-D and a high precision tracking system. The objects are constructed so as to systematically explore key factors that affect pushing-the shape of the object and its mass distribution-which have not been broadly explored in previous datasets, and allow to study generalization in model learning.Omnipush includes a benchmark for meta-learning dynamic models, which requires algorithms that make good predictions and estimate their own uncertainty. We also provide an RGB video prediction benchmark and propose other relevant tasks that can be suited with this dataset. Data and code are available at https://web.mit.edu/mcube/omnipush-dataset/.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "LQR-RRT*: Optimal sampling-based motion planning with automatically derived extension heuristics",
                "link": "https://ieeexplore.ieee.org/document/6225177/",
                "date_of_publication": "28 June 2012",
                "doi": "10.1109/ICRA.2012.6225177",
                "citations": "125",
                "abstract": "The RRT* algorithm has recently been proposed as an optimal extension to the standard RRT algorithm [1]. However, like RRT, RRT* is difficult to apply in problems with complicated or underactuated dynamics because it requires the design of a two domain-specific extension heuristics: a distance metric and node extension method. We propose automatically deriving these two heuristics for RRT* by locally linearizing the domain dynamics and applying linear quadratic regulation (LQR). The resulting algorithm, LQR-RRT*, finds optimal plans in domains with complex or underactuated dynamics without requiring domain-specific design choices. We demonstrate its application in domains that are successively torque-limited, underactuated, and in belief space.",
                "ieee_keywords": [
                    "Trajectory",
                    "Heuristic algorithms",
                    "Planning",
                    "Measurement",
                    "Standards",
                    "Cost function",
                    "Convergence"
                ],
                "author_keywords": []
            },
            {
                "title": "Visual Prediction of Priors for Articulated Object Interaction",
                "link": "https://ieeexplore.ieee.org/document/9196541/",
                "date_of_publication": "15 September 2020",
                "doi": "10.1109/ICRA40945.2020.9196541",
                "citations": "1",
                "abstract": "Exploration in novel settings can be challenging without prior experience in similar domains. However, humans are able to build on prior experience quickly and efficiently. Children exhibit this behavior when playing with toys. For example, given a toy with a yellow and blue door, a child will explore with no clear objective, but once they have discovered how to open the yellow door, they will most likely be able to open the blue door much faster. Adults also exhibit this behaviour when entering new spaces such as kitchens. We develop a method, Contextual Prior Prediction, which provides a means of transferring knowledge between interactions in similar domains through vision. We develop agents that exhibit exploratory behavior with increasing efficiency, by learning visual features that are shared across environments, and how they correlate to actions. Our problem is formulated as a Contextual Multi-Armed Bandit where the contexts are images, and the robot has access to a parameterized action space. Given a novel object, the objective is to maximize reward with few interactions. A domain which strongly exhibits correlations between visual features and motion is kinemetically constrained mechanisms. We evaluate our method on simulated prismatic and revolute joints 1 .",
                "ieee_keywords": [
                    "Robots",
                    "Visualization",
                    "Training",
                    "Kinematics",
                    "Gaussian processes",
                    "Optimization",
                    "Kernel"
                ],
                "author_keywords": []
            },
            {
                "title": "Object placement as inverse motion planning",
                "link": "https://ieeexplore.ieee.org/document/6631099/",
                "date_of_publication": "17 October 2013",
                "doi": "10.1109/ICRA.2013.6631099",
                "citations": "9",
                "abstract": "We present an approach to robust placing that uses movable surfaces in the environment to guide a poorly grasped object into a goal pose. This problem is an instance of the inverse motion planning problem, in which we solve for a configuration of the environment that makes desired trajectories likely. To calculate the probability that an object will take a particular trajectory, we model the physics of placing as a mixture model of simple object motions. Our algorithm searches over the possible configurations of the object and environment and uses this model to choose the configuration most likely to lead to a successful place. We show that this algorithm allows the PR2 robot to execute placements that fail with traditional placing implementations.",
                "ieee_keywords": [
                    "Robots",
                    "Grippers",
                    "Planning",
                    "Trajectory",
                    "Mathematical model",
                    "Robustness",
                    "Poles and towers"
                ],
                "author_keywords": []
            },
            {
                "title": "Focused model-learning and planning for non-Gaussian continuous state-action systems",
                "link": "https://ieeexplore.ieee.org/document/7989433/",
                "date_of_publication": "24 July 2017",
                "doi": "10.1109/ICRA.2017.7989433",
                "citations": "5",
                "abstract": "We introduce a framework for model learning and planning in stochastic domains with continuous state and action spaces and non-Gaussian transition models. It is efficient because (1) local models are estimated only when the planner requires them; (2) the planner focuses on the most relevant states to the current planning problem; and (3) the planner focuses on the most informative and/or high-value actions. Our theoretical analysis shows the validity and asymptotic optimality of the proposed approach. Empirically, we demonstrate the effectiveness of our algorithm on a simulated multi-modal pushing problem.",
                "ieee_keywords": [
                    "Planning",
                    "Computational modeling",
                    "Stochastic processes",
                    "Aerospace electronics",
                    "Robots",
                    "Dynamic programming",
                    "Heuristic algorithms"
                ],
                "author_keywords": []
            },
            {
                "title": "Fully Persistent Spatial Data Structures for Efficient Queries in Path-Dependent Motion Planning Applications",
                "link": "https://ieeexplore.ieee.org/document/9812173/",
                "date_of_publication": "12 July 2022",
                "doi": "10.1109/ICRA46639.2022.9812173",
                "citations": "173",
                "abstract": "Motion planning is a ubiquitous problem that is often a bottleneck in robotic applications. We demonstrate that motion planning problems such as minimum constraint removal, belief-space planning, and visibility-aware motion planning (VAMP) benefit from a path-dependent formulation, in which the state at a search node is represented implicitly by the path to that node. A naïve approach to computing the feasibility of a successor node in such a path-dependent formulation takes time linear in the path length to the node, in contrast to a (possibly very large) constant time for a more typical search formulation. For long-horizon plans, performing this linear-time computation, which we call the lookback, for each node becomes prohibitive. To improve upon this, we introduce the use of a fully persistent spatial data structure (FPSDS), which bounds the size of the lookback. We then focus on the application of the FPSDS in VAMP, which involves incremental geometric computations that can be accelerated by filtering configurations with bounding volumes using nearest-neighbor data structures. We demonstrate an asymptotic and practical improvement in the runtime of finding VAMP solutions in several illustrative domains. To the best of our knowledge, this is the first use of a fully persistent data structure for accelerating motion planning.",
                "ieee_keywords": [
                    "Runtime",
                    "Automation",
                    "Filtering",
                    "Data structures",
                    "Search problems",
                    "Spatial databases",
                    "Planning"
                ],
                "author_keywords": []
            },
            {
                "title": "A hierarchical approach to manipulation with diverse actions",
                "link": "https://ieeexplore.ieee.org/document/6630814/",
                "date_of_publication": "17 October 2013",
                "doi": "10.1109/ICRA.2013.6630814",
                "citations": "33",
                "abstract": "We define the Diverse Action Manipulation (DAMA) problem in which we are given a mobile robot, a set of movable objects, and a set of diverse, possibly non-prehensile manipulation actions, and the goal is to find a sequence of actions that moves each of the objects to a goal configuration. We show that the DAMA problem can be framed as a multi-modal planning problem and describe a hierarchical algorithm that takes advantage of this multi-modal nature. We also extend our earlier forward search sampling algorithm to a bi-directional version. We give results on a complicated manipulation domain and demonstrate that both new algorithms are significantly more efficient than the original, and that the hierarchical algorithm is usually much more efficient than the forward or bi-directional searches.",
                "ieee_keywords": [
                    "Planning",
                    "Collision avoidance",
                    "Bidirectional control",
                    "Grasping",
                    "Legged locomotion",
                    "Clutter"
                ],
                "author_keywords": []
            },
            {
                "title": "Backward-forward search for manipulation planning",
                "link": "https://ieeexplore.ieee.org/document/7354287/",
                "date_of_publication": "17 December 2015",
                "doi": "10.1109/IROS.2015.7354287",
                "citations": "28",
                "abstract": "In this paper we address planning problems in high-dimensional hybrid configuration spaces, with a particular focus on manipulation planning problems involving many objects. We present the hybrid backward-forward (HBF) planning algorithm that uses a backward identification of constraints to direct the sampling of the infinite action space in a forward search from the initial state towards a goal configuration. The resulting planner is probabilistically complete and can effectively construct long manipulation plans requiring both prehensile and nonprehensile actions in cluttered environments.",
                "ieee_keywords": [
                    "Planning",
                    "Search problems",
                    "Probabilistic logic",
                    "Heuristic algorithms",
                    "Intelligent robots",
                    "Ovens"
                ],
                "author_keywords": []
            },
            {
                "title": "Manipulation-based active search for occluded objects",
                "link": "https://ieeexplore.ieee.org/document/6630966/",
                "date_of_publication": "17 October 2013",
                "doi": "10.1109/ICRA.2013.6630966",
                "citations": "26",
                "abstract": "Object search is an integral part of daily life, and in the quest for competent mobile manipulation robots it is an unavoidable problem. Previous approaches focus on cases where objects are in unknown rooms but lying out in the open, which transforms object search into active visual search. However, in real life, objects may be in the back of cupboards occluded by other objects, instead of conveniently on a table by themselves. Extending search to occluded objects requires a more precise model and tighter integration with manipulation. We present a novel generative model for representing container contents by using object co-occurrence information and spatial constraints. Given a target object, a planner uses the model to guide an agent to explore containers where the target is likely, potentially needing to move occluding objects to enable further perception. We demonstrate the model on simulated domains and a detailed simulation involving a PR2 robot.",
                "ieee_keywords": [
                    "Systematics",
                    "Containers"
                ],
                "author_keywords": []
            },
            {
                "title": "Interactive Bayesian identification of kinematic mechanisms",
                "link": "https://ieeexplore.ieee.org/document/6907126/",
                "date_of_publication": "29 September 2014",
                "doi": "10.1109/ICRA.2014.6907126",
                "citations": "19",
                "abstract": "This paper addresses the problem of identifying mechanisms based on data gathered while interacting with them. We present a decision-theoretic formulation of this problem, using Bayesian filtering techniques to maintain a distributional estimate of the mechanism type and parameters. In order to reduce the amount of interaction required to arrive at a confident identification, we select actions explicitly to reduce entropy in the current estimate. We demonstrate the approach on a domain with four primitive and two composite mechanisms. The results show that this approach can correctly identify complex mechanisms including mechanisms which are difficult to model analytically. The results also show that entropy-based action selection can significantly decrease the number of actions required to gather the same information.",
                "ieee_keywords": [
                    "Joints",
                    "Latches",
                    "Analytical models",
                    "Kinematics",
                    "Entropy",
                    "Robot sensing systems"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Constantinos Daskalakis",
        "publications": [
            {
                "title": "On the Structure, Covering, and Learning of Poisson Multinomial Distributions",
                "link": "https://ieeexplore.ieee.org/document/7354451/",
                "date_of_publication": "17 December 2015",
                "doi": "10.1109/FOCS.2015.77",
                "citations": "10",
                "abstract": "An (n, k)-Poisson Multinomial Distribution (PMD) is the distribution of the sum of n independent random vectors supported on the set Bk={e1,, ek} of standard basis vectors in Rk. We prove a structural characterization of these distributions, showing that, for all ε > 0, any (n, k)-Poisson multinomial random vector is ε-close, in total variation distance, to the sum of a discretized multidimensional Gaussian and an independent (poly(k/ε), k)-Poisson multinomial random vector. Our structural characterization extends the multi-dimensional CLT of Valiant and Valiant, by simultaneously applying to all approximation requirements ε. In particular, it overcomes factors depending on log n and, importantly, the minimum Eigen value of the PMD's covariance matrix. We use our structural characterization to obtain an ε-cover, in total variation distance, of the set of all (n, k)-PMDs, significantly improving the cover size of Daskalakis and Papadimitriou, and obtaining the same qualitative dependence of the cover size on n and ε as the k=2 cover of Daskalakis and Papadimitriou. We further exploit this structure to show that (n, k)-PMDs can be learned to within ε in total variation distance from Õk(1/ε2) samples, which is near-optimal in terms of dependence on ε and independent of n. In particular, our result generalizes the single-dimensional result of Daskalakis, Diakonikolas and Servedio for Poisson binomials to arbitrary dimension. Finally, as a corollary of our results on PMDs, we give a Õk(1/ε2) sample algorithm for learning (n, k)-sums of independent integer random variables (SIIRVs), which is near-optimal for constant k.",
                "ieee_keywords": [
                    "Approximation methods",
                    "Covariance matrices",
                    "Polynomials",
                    "Standards",
                    "Eigenvalues and eigenfunctions",
                    "Games",
                    "Yttrium"
                ],
                "author_keywords": [
                    "Structure",
                    "Learning",
                    "Applied probability",
                    "Gaussian distribution",
                    "Multivariate statistics",
                    "Limit theorem"
                ]
            },
            {
                "title": "Testing Ising Models",
                "link": "https://ieeexplore.ieee.org/document/8782628/",
                "date_of_publication": null,
                "doi": "10.1109/TIT.2019.2932255",
                "citations": "11",
                "abstract": "Given samples from an unknown multivariate distribution p, is it possible to distinguish whether p is the product of its marginals versus p being far from every product distribution? Similarly, is it possible to distinguish whether p equals a given distribution q versus p and q being far from each other? These problems of testing independence and goodnessof-fit have received enormous attention in statistics, information theory, and theoretical computer science, with sample-optimal algorithms known in several interesting regimes of parameters. Unfortunately, it has also been understood that these problems become intractable in large dimensions, necessitating exponential sample complexity. Motivated by the exponential lower bounds for general distributions as well as the ubiquity of Markov random fields (MRFs) in the modeling of high-dimensional distributions, we initiate the study of distribution testing on structured multivariate distributions, and in particular, the prototypical example of MRFs: the Ising Model. We demonstrate that, in this structured setting, we can avoid the curse of dimensionality, obtaining sample, and time efficient testers for independence and goodness-of-fit. One of the key technical challenges we face along the way is bounding the variance of functions of the Ising model.",
                "ieee_keywords": [
                    "Testing",
                    "Complexity theory",
                    "Computational modeling",
                    "Analytical models",
                    "Upper bound",
                    "Computer science",
                    "Markov random fields"
                ],
                "author_keywords": [
                    "Ising models",
                    "graphical models",
                    "Markov random fields",
                    "hypothesis testing",
                    "statistics"
                ]
            }
        ]
    },
    {
        "name": "Phillip Isola",
        "publications": [
            {
                "title": "Omnipush: accurate, diverse, real-world dataset of pushing dynamics with RGB-D video",
                "link": "https://ieeexplore.ieee.org/document/8967920/",
                "date_of_publication": "28 January 2020",
                "doi": "10.1109/IROS40897.2019.8967920",
                "citations": "9",
                "abstract": "Pushing is a fundamental robotic skill. Existing work has shown how to exploit models of pushing to achieve a variety of tasks, including grasping under uncertainty, in-hand manipulation and clearing clutter. Such models, however, are approximate, which limits their applicability.Learning-based methods can reason directly from raw sensory data with accuracy, and have the potential to generalize to a wider diversity of scenarios. However, developing and testing such methods requires rich-enough datasets. In this paper we introduce Omnipush, a dataset with high variety of planar pushing behavior.In particular, we provide 250 pushes for each of 250 objects, all recorded with RGB-D and a high precision tracking system. The objects are constructed so as to systematically explore key factors that affect pushing-the shape of the object and its mass distribution-which have not been broadly explored in previous datasets, and allow to study generalization in model learning.Omnipush includes a benchmark for meta-learning dynamic models, which requires algorithms that make good predictions and estimate their own uncertainty. We also provide an RGB video prediction benchmark and propose other relevant tasks that can be suited with this dataset. Data and code are available at https://web.mit.edu/mcube/omnipush-dataset/.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "What Makes a Visualization Memorable?",
                "link": "https://ieeexplore.ieee.org/document/6634103/",
                "date_of_publication": null,
                "doi": "10.1109/TVCG.2013.234",
                "citations": "322",
                "abstract": "An ongoing debate in the Visualization community concerns the role that visualization types play in data understanding. In human cognition, understanding and memorability are intertwined. As a first step towards being able to ask questions about impact and effectiveness, here we ask: 'What makes a visualization memorable?' We ran the largest scale visualization study to date using 2,070 single-panel visualizations, categorized with visualization type (e.g., bar chart, line graph, etc.), collected from news media sites, government reports, scientific journals, and infographic sources. Each visualization was annotated with additional attributes, including ratings for data-ink ratios and visual densities. Using Amazon's Mechanical Turk, we collected memorability scores for hundreds of these visualizations, and discovered that observers are consistent in which visualizations they find memorable and forgettable. We find intuitive results (e.g., attributes like color and the inclusion of a human recognizable object enhance memorability) and less intuitive results (e.g., common graphs are less memorable than unique visualization types). Altogether our findings suggest that quantifying memorability is a general metric of the utility of information, an essential step towards determining how to design effective visualizations.",
                "ieee_keywords": [
                    "Data visualization",
                    "Taxonomy",
                    "Information technology",
                    "Encoding"
                ],
                "author_keywords": [
                    "Data visualization",
                    "Taxonomy",
                    "Information technology",
                    "Encoding",
                    "memorability",
                    "Visualization taxonomy",
                    "information visualization",
                    "MeSH Terms",
                    "Artificial Intelligence",
                    "Cues",
                    "Humans",
                    "Image Interpretation, Computer-Assisted",
                    "Memory",
                    "Pattern Recognition, Visual",
                    "Task Performance and Analysis",
                    "User-Computer Interface"
                ]
            },
            {
                "title": "What Makes a Photograph Memorable?",
                "link": "https://ieeexplore.ieee.org/document/6629991/",
                "date_of_publication": null,
                "doi": "10.1109/TPAMI.2013.200",
                "citations": "165",
                "abstract": "When glancing at a magazine, or browsing the Internet, we are continuously exposed to photographs. Despite this overflow of visual information, humans are extremely good at remembering thousands of pictures along with some of their visual details. But not all images are equal in memory. Some stick in our minds while others are quickly forgotten. In this paper, we focus on the problem of predicting how memorable an image will be. We show that memorability is an intrinsic and stable property of an image that is shared across different viewers, and remains stable across delays. We introduce a database for which we have measured the probability that each picture will be recognized after a single view. We analyze a collection of image features, labels, and attributes that contribute to making an image memorable, and we train a predictor based on global image descriptors. We find that predicting image memorability is a task that can be addressed with current computer vision techniques. While making memorable images is a challenging task in visualization, photography, and education, this work is a first attempt to quantify this useful property of images.",
                "ieee_keywords": [
                    "Delays",
                    "Games",
                    "Atmospheric measurements",
                    "Particle measurements",
                    "Visualization",
                    "Observers",
                    "Correlation"
                ],
                "author_keywords": [
                    "Vision and Scene Understanding",
                    "Scene Analysis",
                    "MeSH Terms",
                    "Computer Simulation",
                    "Cues",
                    "Humans",
                    "Mental Recall",
                    "Models, Biological",
                    "Photography",
                    "Recognition (Psychology)",
                    "Task Performance and Analysis",
                    "Visual Perception",
                    "Scene understanding",
                    "image memorability",
                    "global image features",
                    "attributes"
                ]
            },
            {
                "title": "What makes an image memorable?",
                "link": "https://ieeexplore.ieee.org/document/5995721/",
                "date_of_publication": "22 August 2011",
                "doi": "10.1109/CVPR.2011.5995721",
                "citations": "216",
                "abstract": "When glancing at a magazine, or browsing the Internet, we are continuously being exposed to photographs. Despite of this overflow of visual information, humans are extremely good at remembering thousands of pictures along with some of their visual details. But not all images are equal in memory. Some stitch to our minds, and other are forgotten. In this paper we focus on the problem of predicting how memorable an image will be. We show that memorability is a stable property of an image that is shared across different viewers. We introduce a database for which we have measured the probability that each picture will be remembered after a single view. We analyze image features and labels that contribute to making an image memorable, and we train a predictor based on global image descriptors. We find that predicting image memorability is a task that can be addressed with current computer vision techniques. Whereas making memorable images is a challenging task in visualization and photography, this work is a first attempt to quantify this useful quality of images.",
                "ieee_keywords": [
                    "Particle measurements",
                    "Atmospheric measurements",
                    "Games",
                    "Humans",
                    "Correlation",
                    "Semantics",
                    "Visualization"
                ],
                "author_keywords": []
            },
            {
                "title": "Noisy Agents: Self-supervised Exploration by Predicting Auditory Events",
                "link": "https://ieeexplore.ieee.org/document/9981614/",
                "date_of_publication": "26 December 2022",
                "doi": "10.1109/IROS47612.2022.9981614",
                "citations": "64",
                "abstract": "Humans integrate multiple sensory modalities (e.g., visual and audio) to build a causal understanding of the physical world. In this work, we propose a novel type of intrinsic motivation for Reinforcement Learning (RL) that encourages the agent to understand the causal effect of its actions through auditory event prediction. First, we allow the agent to collect a small amount of acoustic data and use K-means to discover underlying auditory event clusters. We then train a neural network to predict the auditory events and use the prediction errors as intrinsic rewards to guide RL exploration. We first conduct proof-of-concept experiments using a set of Atari games for an in-depth analysis of our module. We then apply our model to embodied audio-visual exploration using the Habitat simulator and active exploration with a rolling robot using the ThreeDWorld (TDW) simulator. Experimental results demonstrate the advantages of using audio signals over vision-based models as intrinsic rewards to guide RL explorations.",
                "ieee_keywords": [
                    "Visualization",
                    "Neural networks",
                    "Reinforcement learning",
                    "Games",
                    "Robot sensing systems",
                    "Acoustics",
                    "Noise measurement"
                ],
                "author_keywords": []
            },
            {
                "title": "OPEn: An Open-ended Physics Environment for Learning Without a Task",
                "link": "https://ieeexplore.ieee.org/document/9636830/",
                "date_of_publication": "16 December 2021",
                "doi": "10.1109/IROS51168.2021.9636830",
                "citations": "129",
                "abstract": "Humans have mental models that allow them to plan, experiment, and reason in the physical world. How should an intelligent agent go about learning such models? In this paper, we will study if models of the world learned in an open-ended physics environment, without any specific tasks, can be reused for downstream physics reasoning tasks. To this end, we build a benchmark Open-ended Physics Environment (OPEn) and also design several tasks to test learning representations in this environment explicitly. This setting reflects the conditions in which real agents (i.e. rolling robots) find themselves, where they may be placed in a new kind of environment and must adapt without any teacher to tell them how this environment works. This setting is challenging because it requires solving an exploration problem in addition to a model building and representation learning problem. We test several existing RL-based exploration methods on this benchmark and find that an agent using unsupervised contrastive learning for representation learning, and impact-driven learning for exploration, achieved the best results. However, all models still fall short in sample efficiency when transferring to the downstream tasks. We expect that OPEn will encourage the development of novel rolling robot agents that can build reusable mental models of the world that facilitate many tasks.",
                "ieee_keywords": [
                    "Representation learning",
                    "Adaptation models",
                    "Buildings",
                    "Benchmark testing",
                    "Cognition",
                    "Cognitive science",
                    "Intelligent agents"
                ],
                "author_keywords": []
            },
            {
                "title": "Discovering states and transformations in image collections",
                "link": "https://ieeexplore.ieee.org/document/7298744/",
                "date_of_publication": "15 October 2015",
                "doi": "10.1109/CVPR.2015.7298744",
                "citations": "69",
                "abstract": "Objects in visual scenes come in a rich variety of transformed states. A few classes of transformation have been heavily studied in computer vision: mostly simple, parametric changes in color and geometry. However, transformations in the physical world occur in many more flavors, and they come with semantic meaning: e.g., bending, folding, aging, etc. The transformations an object can undergo tell us about its physical and functional properties. In this paper, we introduce a dataset of objects, scenes, and materials, each of which is found in a variety of transformed states. Given a novel collection of images, we show how to explain the collection in terms of the states and transformations it depicts. Our system works by generalizing across object classes: states and transformations learned on one set of objects are used to interpret the image collection for an entirely new object class.",
                "ieee_keywords": [
                    "Visualization",
                    "Training",
                    "Computer vision",
                    "Sugar",
                    "Dairy products",
                    "Mathematical model",
                    "Semantics"
                ],
                "author_keywords": []
            },
            {
                "title": "iNeRF: Inverting Neural Radiance Fields for Pose Estimation",
                "link": "https://ieeexplore.ieee.org/document/9636708/",
                "date_of_publication": "16 December 2021",
                "doi": "10.1109/IROS51168.2021.9636708",
                "citations": "64",
                "abstract": "We present iNeRF, a framework that performs mesh-free pose estimation by \"inverting\" a Neural Radiance Field (NeRF). NeRFs have been shown to be remarkably effective for the task of view synthesis — synthesizing photorealistic novel views of real-world scenes or objects. In this work, we investigate whether we can apply analysis-by-synthesis via NeRF for mesh-free, RGB-only 6DoF pose estimation – given an image, find the translation and rotation of a camera relative to a 3D object or scene. Our method assumes that no object mesh models are available during either training or test time. Starting from an initial pose estimate, we use gradient descent to minimize the residual between pixels rendered from a NeRF and pixels in an observed image. In our experiments, we first study 1) how to sample rays during pose refinement for iNeRF to collect informative gradients and 2) how different batch sizes of rays affect iNeRF on a synthetic dataset. We then show that for complex real-world scenes from the LLFF dataset [21], iNeRF can improve NeRF by estimating the camera poses of novel images and using these images as additional training data for NeRF. Finally, we show iNeRF can perform categorylevel object pose estimation, including object instances not seen during training, with RGB images by inverting a NeRF model inferred from a single view.",
                "ieee_keywords": [
                    "Training",
                    "Three-dimensional displays",
                    "Pose estimation",
                    "Training data",
                    "Cameras",
                    "Task analysis",
                    "Optimization"
                ],
                "author_keywords": []
            },
            {
                "title": "SparkleVision: Seeing the world through random specular microfacets",
                "link": "https://ieeexplore.ieee.org/document/7301369/",
                "date_of_publication": "26 October 2015",
                "doi": "10.1109/CVPRW.2015.7301369",
                "citations": "1",
                "abstract": "In this paper, we study the problem of reproducing the light from a single image of an object covered with random specular microfacets on the surface. We show that such reflectors can be interpreted as a randomized mapping from the lighting to the image. Such specular objects have very different optical properties from both diffuse surfaces and smooth specular objects like metals, so we design a special imaging system to robustly and effectively photograph them. We present simple yet reliable algorithms to calibrate the proposed system and do the inference. We conduct experiments to verify the correctness of our model assumptions and prove the effectiveness of our pipeline.",
                "ieee_keywords": [
                    "Cameras",
                    "Noise",
                    "Mirrors",
                    "Calibration",
                    "Lighting",
                    "Optical imaging",
                    "Image reconstruction"
                ],
                "author_keywords": []
            },
            {
                "title": "InGAN: Capturing and Retargeting the “DNA” of a Natural Image",
                "link": "https://ieeexplore.ieee.org/document/9009560/",
                "date_of_publication": "27 February 2020",
                "doi": "10.1109/ICCV.2019.00459",
                "citations": "57",
                "abstract": "Generative Adversarial Networks (GANs) typically learn a distribution of images in a large image dataset, and are then able to generate new images from this distribution. However, each natural image has its own internal statistics, captured by its unique distribution of patches. In this paper we propose an \"Internal GAN'' (InGAN) - an image-specific GAN - which trains on a single input image and learns its internal distribution of patches. It is then able to synthesize a plethora of new natural images of significantly different sizes, shapes and aspect-ratios - all with the same internal patch-distribution (same \"DNA'') as the input image. In particular, despite large changes in global size/shape of the image, all elements inside the image maintain their local size/shape. InGAN is fully unsupervised, requiring no additional data other than the input image itself. Once trained on the input image, it can remap the input to any size or shape in a single feedforward pass, while preserving the same internal patch distribution. InGAN provides a unified framework for a variety of tasks, bridging the gap between textures and natural images.",
                "ieee_keywords": [
                    "Gallium nitride",
                    "Shape",
                    "Generators",
                    "Task analysis",
                    "DNA",
                    "Visualization",
                    "Image reconstruction"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Jacob Andreas",
        "publications": [
            {
                "title": "Subgraph Matching on Multiplex Networks",
                "link": "https://ieeexplore.ieee.org/document/9346051/",
                "date_of_publication": null,
                "doi": "10.1109/TNSE.2021.3056329",
                "citations": "7",
                "abstract": "An active area of research in computational science is the design of algorithms for solving the subgraph matching problem to find copies of a given template graph in a larger world graph. Prior works have largely addressed single-channel networks using a variety of approaches. We present a suite of filtering methods for subgraph isomorphisms for multiplex networks (with different types of edges between nodes and more than one edge within each channel type). We aim to understand the entire solution space rather than focusing on finding one isomorphism. Results are shown on several classes of datasets: (a) Sudoku puzzles mapped to the subgraph isomorphism problem, (b) Erdös-Rényi multigraphs, (c) real-world datasets from Twitter and transportation networks, (d) synthetic data created for the DARPA MAA program.",
                "ieee_keywords": [
                    "Silicon",
                    "Switched mode power supplies",
                    "Multiplexing",
                    "Image edge detection",
                    "Social networking (online)",
                    "Databases",
                    "Space exploration"
                ],
                "author_keywords": [
                    "Graph isomorphism",
                    "graph matching",
                    "multiplex network",
                    "subgraph isomorphism",
                    "subgraph matching."
                ]
            }
        ]
    },
    {
        "name": "James M. Rehg",
        "publications": [
            {
                "title": "AF classification from ECG recording using feature ensemble and sparse coding",
                "link": "https://ieeexplore.ieee.org/document/8331583/",
                "date_of_publication": "05 April 2018",
                "doi": "10.22489/CinC.2017.174-192",
                "citations": "187",
                "abstract": "Introduction: The aim of the Physionet/CinC Challenge 2017 is to automatically classify atrial fibrillation (AF) from a short single lead ECG recording. The Challenge provides 8,528 labeled ECG recordings; each recording was labeled as normal, AF, other, or noisy. In addition, the Challenge provides sample code which includes an R-peak detector and a simple classifier. Algorithm: We use an ensemble of features extracted from the ECG signals to create a four-class support vector machine (SVM) classifier. Included in the feature set are statistics obtained from the ECG signal, its spectrum, and the RR-intervals. In addition, we learn a 32-element sparse coding dictionary on the sorted RR-intervals of the ECG signals. Using the dictionary, we calculate a sparse coefficient vector for each training sample and put these through a soft-margin linear SVM. The soft-margin scores are used as additional features in the final classifier. Results: Our algorithm achieves cross-validated F1 scores of 0.874, 0.756, and 0.689 (for normal, AF, and other files, respectively), resulting in afinal cross-validated challenge score of0.773. The score when tested on a subset of the unknown data is 0.78 (with F1 scores of 0.88, 0.80, 0.65). The official challenge score was 0.77. Conclusions: We developed an algorithm to classify ECG recordings as normal, AF, other, or noisy. Our results show that sparse coding is an effective way to define discriminating features from a list of sorted RR-intervals. In addition, these sparse codes complement more commonly used features in the classification task. Further work will attempt to increase the accuracy of the algorithm by exploring other features and classifiers while still using sparse coding as an unsupervised feature extractor.",
                "ieee_keywords": [
                    "Electrocardiography",
                    "Feature extraction",
                    "Encoding",
                    "Dictionaries",
                    "Sparse matrices",
                    "Support vector machines",
                    "Entropy"
                ],
                "author_keywords": []
            },
            {
                "title": "Decoding Children's Social Behavior",
                "link": "https://ieeexplore.ieee.org/document/6619282/",
                "date_of_publication": "03 October 2013",
                "doi": "10.1109/CVPR.2013.438",
                "citations": "83",
                "abstract": "We introduce a new problem domain for activity recognition: the analysis of children's social and communicative behaviors based on video and audio data. We specifically target interactions between children aged 1-2 years and an adult. Such interactions arise naturally in the diagnosis and treatment of developmental disorders such as autism. We introduce a new publicly-available dataset containing over 160 sessions of a 3-5 minute child-adult interaction. In each session, the adult examiner followed a semi-structured play interaction protocol which was designed to elicit a broad range of social behaviors. We identify the key technical challenges in analyzing these behaviors, and describe methods for decoding the interactions. We present experimental results that demonstrate the potential of the dataset to drive interesting research questions, and show preliminary results for multi-modal activity recognition.",
                "ieee_keywords": [
                    "Face",
                    "Cameras",
                    "Protocols",
                    "Decoding",
                    "Pediatrics",
                    "Training",
                    "Testing"
                ],
                "author_keywords": []
            },
            {
                "title": "Vision-Based High-Speed Driving With a Deep Dynamic Observer",
                "link": "https://ieeexplore.ieee.org/document/8630018/",
                "date_of_publication": null,
                "doi": "10.1109/LRA.2019.2896449",
                "citations": "28",
                "abstract": "In this letter, we present a framework for combining deep learning-based road detection, particle filters, and model predictive control (MPC) to drive aggressively using only a monocular camera, IMU, and wheel speed sensors. This framework uses deep convolutional neural networks combined with LSTMs to learn a local cost map representation of the track in front of the vehicle. A particle filter uses this dynamic observation model to localize in a schematic map, and MPC is used to drive aggressively using this particle filter based state estimate. We show extensive real world testing results and demonstrate reliable operation of the vehicle at the friction limits on a complex dirt track. We reach speeds above 27 m/h (12 m/s) on a dirt track with a 105 ft (32 m) long straight using our 1:5 scale test vehicle.",
                "ieee_keywords": [
                    "Cameras",
                    "Vehicle dynamics",
                    "Wheels",
                    "Neural networks",
                    "Computer architecture",
                    "Predictive control",
                    "Sensors"
                ],
                "author_keywords": [
                    "Deep learning in robotics and automation",
                    "autonomous vehicle navigation",
                    "localization",
                    "computer vision for transportation"
                ]
            },
            {
                "title": "Behavioral Imaging and Autism",
                "link": "https://ieeexplore.ieee.org/document/6818509/",
                "date_of_publication": null,
                "doi": "10.1109/MPRV.2014.23",
                "citations": "40",
                "abstract": "Behavioral imaging encompasses the use of computational sensing and modeling techniques to measure and analyze human behavior. This article discusses a research program focused on the study of dyadic social interactions between children and their caregivers and peers. The study has resulted in a dataset containing semi-structured play interactions between children and adults. Behavioral imaging could broadly affect the quality of care for individuals with a developmental or behavioral disorder.",
                "ieee_keywords": [
                    "Pediatrics",
                    "Autism",
                    "Biomedical monitoring",
                    "Patient monitoring",
                    "Behavioral science",
                    "Sensors",
                    "Image sensing"
                ],
                "author_keywords": [
                    "behavioral imaging",
                    "autism",
                    "ASD",
                    "computer vision",
                    "assistive technologies",
                    "pervasive computing"
                ]
            },
            {
                "title": "Classification of Decompensated Heart Failure From Clinical and Home Ballistocardiography",
                "link": "https://ieeexplore.ieee.org/document/8801932/",
                "date_of_publication": null,
                "doi": "10.1109/TBME.2019.2935619",
                "citations": "26",
                "abstract": "Objective: To improve home monitoring of heart failure patients so as to reduce emergency room visits and hospital readmissions. We aim to do this by analyzing the ballistocardiogram (BCG) to evaluate the clinical state of the patient. Methods: 1) High quality BCG signals were collected at home from HF patients after discharge. 2) The BCG recordings were preprocessed to exclude outliers and artifacts. 3) Parameters of the BCG that contain information about the cardiovascular system were extracted. These features were used for the task of classification of the BCG recording based on the status of HF. Results: The best AUC score for the task of classification obtained was 0.78 using slight variant of the leave one subject out validation method. Conclusion: This work demonstrates that high quality BCG signals can be collected in a home environment and used to detect the clinical state of HF patients. Significance: In future work, a clinician/caregiver can be introduced into the system so that appropriate interventions can be performed based on the clinical state monitored at home.",
                "ieee_keywords": [
                    "Monitoring",
                    "Heart",
                    "Hafnium",
                    "Biomedical monitoring",
                    "Sensors",
                    "Indexes"
                ],
                "author_keywords": [
                    "Ballistocardiography",
                    "machine learning",
                    "heart failure",
                    "MeSH Terms",
                    "Artifacts",
                    "Ballistocardiography",
                    "Heart Failure",
                    "Humans",
                    "Monitoring, Physiologic"
                ]
            },
            {
                "title": "BioGlass: Physiological parameter estimation using a head-mounted wearable device",
                "link": "https://ieeexplore.ieee.org/document/7015908/",
                "date_of_publication": "22 January 2015",
                "doi": "10.1109/MOBIHEALTH.2014.7015908",
                "citations": "15",
                "abstract": "This work explores the feasibility of using sensors embedded in Google Glass, a head-mounted wearable device, to measure physiological signals of the wearer. In particular, we develop new methods to use Glass's accelerometer, gyroscope, and camera to extract pulse and respiratory rates of 12 participants during a controlled experiment. We show it is possible to achieve a mean absolute error of 0.83 beats per minute (STD: 2.02) for heart rate and 1.18 breaths per minute (STD: 2.04) for respiration rate when considering different combinations of sensors. These results included testing across sitting, supine, and standing still postures before and after physical exercise.",
                "ieee_keywords": [
                    "Decision support systems",
                    "DVD"
                ],
                "author_keywords": [
                    "Ballistocardiogram (BCG)",
                    "blood volume pulse (BVP)",
                    "heart rate",
                    "respiration rate",
                    "head-mounted wearable device",
                    "gyroscope",
                    "accelererometer",
                    "camera",
                    "daily life monitoring"
                ]
            },
            {
                "title": "Center of Excellence for Mobile Sensor Data-to-Knowledge (MD2K)",
                "link": "https://ieeexplore.ieee.org/document/7891193/",
                "date_of_publication": null,
                "doi": "10.1109/MPRV.2017.29",
                "citations": "18",
                "abstract": "The Center of Excellence for Mobile Sensor Data-to-Knowledge (MD2K) is enabling the collection of high-frequency mobile sensor data for the development and validation of novel multisensory biomarkers and sensor-triggered interventions.",
                "ieee_keywords": [
                    "Biomarkers",
                    "Mobile communication",
                    "Biological system modeling",
                    "Computational modeling",
                    "Medical services",
                    "Bioinformatics",
                    "Data models"
                ],
                "author_keywords": [
                    "pervasive computing",
                    "mobile",
                    "healthcare",
                    "Internet of Things",
                    "bioinformatics"
                ]
            },
            {
                "title": "Information theoretic MPC for model-based reinforcement learning",
                "link": "https://ieeexplore.ieee.org/document/7989202/",
                "date_of_publication": "24 July 2017",
                "doi": "10.1109/ICRA.2017.7989202",
                "citations": "187",
                "abstract": "We introduce an information theoretic model predictive control (MPC) algorithm capable of handling complex cost criteria and general nonlinear dynamics. The generality of the approach makes it possible to use multi-layer neural networks as dynamics models, which we incorporate into our MPC algorithm in order to solve model-based reinforcement learning tasks. We test the algorithm in simulation on a cart-pole swing up and quadrotor navigation task, as well as on actual hardware in an aggressive driving task. Empirical results demonstrate that the algorithm is capable of achieving a high level of performance and does so only utilizing data collected from the system.",
                "ieee_keywords": [
                    "Robots",
                    "Heuristic algorithms",
                    "Trajectory",
                    "Learning (artificial intelligence)",
                    "Cost function",
                    "Optimal control"
                ],
                "author_keywords": []
            },
            {
                "title": "Ego4D: Around the World in 3,000 Hours of Egocentric Video",
                "link": "https://ieeexplore.ieee.org/document/9879279/",
                "date_of_publication": "27 September 2022",
                "doi": "10.1109/CVPR52688.2022.01842",
                "citations": "47",
                "abstract": "We introduce Ego4D, a massive-scale egocentric video dataset and benchmark suite. It offers 3,670 hours of dailylife activity video spanning hundreds of scenarios (household, outdoor, workplace, leisure, etc.) captured by 931 unique camera wearers from 74 worldwide locations and 9 different countries. The approach to collection is designed to uphold rigorous privacy and ethics standards, with consenting participants and robust de-identification procedures where relevant. Ego4D dramatically expands the volume of diverse egocentric video footage publicly available to the research community. Portions of the video are accompanied by audio, 3D meshes of the environment, eye gaze, stereo, and/or synchronized videos from multiple egocentric cameras at the same event. Furthermore, we present a host of new benchmark challenges centered around understanding the first-person visual experience in the past (querying an episodic memory), present (analyzing hand-object manipulation, audio-visual conversation, and social interactions), and future (forecasting activities). By publicly sharing this massive annotated dataset and benchmark suite, we aim to push the frontier of first-person perception. Project page: https://ego4d-data.org/",
                "ieee_keywords": [
                    "Visualization",
                    "Technological innovation",
                    "Privacy",
                    "Three-dimensional displays",
                    "Benchmark testing",
                    "Cameras",
                    "Solids"
                ],
                "author_keywords": [
                    "Datasets and evaluation; Video analysis and understanding"
                ]
            },
            {
                "title": "AutoRally: An Open Platform for Aggressive Autonomous Driving",
                "link": "https://ieeexplore.ieee.org/document/8616931/",
                "date_of_publication": null,
                "doi": "10.1109/MCS.2018.2876958",
                "citations": "47",
                "abstract": "The technical challenge of creating a self-driving vehicle remains an open problem despite significant advancements from universities, car manufacturers, and technology companies. Full autonomy, known as level 5 (see \"Society of Automotive Engineers Levels of Driving Automation\"), is defined as full-time performance by an automated driving system of all aspects of the dynamic driving task under all roadway and environmental conditions that can be managed by a human driver. It is estimated that level 5 autonomous vehicles on public roads will help eliminate more than 90% [1] of the 35,000 annual traffic fatalities caused by human error in the United States [2]; reduce commute time, road congestion, and pollution; and increase driving resource utilization [3].",
                "ieee_keywords": [
                    "Autonomous vehicles",
                    "Autonomous automobiles",
                    "Task analysis",
                    "Vehicle dynamics",
                    "Wireless sensor networks",
                    "Vehicular ad hoc networks"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Bo Dai",
        "publications": [
            {
                "title": "Decoupled Networks",
                "link": "https://ieeexplore.ieee.org/document/8578391/",
                "date_of_publication": "16 December 2018",
                "doi": "10.1109/CVPR.2018.00293",
                "citations": "20",
                "abstract": "Inner product-based convolution has been a central component of convolutional neural networks (CNNs) and the key to learning visual representations. Inspired by the observation that CNN-learned features are naturally decoupled with the norm of features corresponding to the intra-class variation and the angle corresponding to the semantic difference, we propose a generic decoupled learning framework which models the intra-class variation and semantic difference independently. Specifically, we first reparametrize the inner product to a decoupled form and then generalize it to the decoupled convolution operator which serves as the building block of our decoupled networks. We present several effective instances of the decoupled convolution operator. Each decoupled operator is well motivated and has an intuitive geometric interpretation. Based on these decoupled operators, we further propose to directly learn the operator from data. Extensive experiments show that such decoupled reparameterization renders significant performance gain with easier convergence and stronger robustness.",
                "ieee_keywords": [
                    "Convolution",
                    "Semantics",
                    "Kernel",
                    "Convergence",
                    "Task analysis",
                    "Robustness",
                    "Convolutional neural networks"
                ],
                "author_keywords": []
            },
            {
                "title": "Learning from semantically dependent multi-tasks",
                "link": "https://ieeexplore.ieee.org/document/7966296/",
                "date_of_publication": "03 July 2017",
                "doi": "10.1109/IJCNN.2017.7966296",
                "citations": "4",
                "abstract": "We consider a different setting from regular multitask learning, where data from different tasks share no common instances and no common feature dictionary, while the features can be semantically correlated and all tasks share the same class space. For example, in the two tasks of identifying terrorism information from English news and Arabic news respectively, one associated dataset could be news from Cable News Network (CNN), and the other be news crawled from websites of Arabic countries. Intuitively, these two tasks could help each other, although they share no common feature space. This new setting has brought obstacles to traditional multi-task learning algorithms and multi-view learning algorithms. We argue that these different data sources can be co-trained together by exploring the latent semantics among them. To this end, we propose a new graphical model based on sparse Gaussian Conditional Random Fields (GCRF) and Hilbert-Schmidt Independence Criterion (HSIC). In additional to output the prediction accuracy for each single task, it can also model (1) the dependency between the latent feature spaces of different tasks, (2) the dependency of the category spaces, and (3) the dependency between the latent feature space and the category space in each task. To make the model inference effective, we have provided an efficient variational EM algorithm. Experiments on both synthetic data sets and real-world data sets have indicated the feasibility and effectiveness of the proposed framework.",
                "ieee_keywords": [
                    "Dictionaries",
                    "Covariance matrices",
                    "Gaussian distribution",
                    "Electronic mail",
                    "Graphical models",
                    "Standards",
                    "Data models"
                ],
                "author_keywords": []
            },
            {
                "title": "A Context-Aware Framework for Reducing Bandwidth Usage of Mobile Video Chats",
                "link": "https://ieeexplore.ieee.org/document/7478066/",
                "date_of_publication": null,
                "doi": "10.1109/TMM.2016.2572001",
                "citations": "8",
                "abstract": "Mobile video chat apps offer users an approachable way to communicate with others. As high-speed 4G networks are being deployed worldwide, the number of mobile video chat app users increases. However, video chatting on mobile devices brings users financial concerns, since streaming video demands high bandwidth and can use up a large amount of data in dozens of minutes. Lowering the bandwidth usage of mobile video chats is challenging since video quality may be compromised. In this paper, we attempt to tame this challenge. Technically, we propose a context-aware frame rate adaption framework, named low-bandwidth video chat (LBVC). It follows a sender-receiver cooperative principle that smartly handles the tradeoff between lowering bandwidth usage and maintaining video quality. We implement LBVC by modifying an open-source app-Linphone- and evaluate it with both objective experiments and subjective studies.",
                "ieee_keywords": [
                    "Streaming media",
                    "Bandwidth",
                    "Context",
                    "Mobile communication",
                    "Video recording",
                    "Quality assessment",
                    "Vibrations"
                ],
                "author_keywords": [
                    "Bandwidth reduction",
                    "context awareness",
                    "frame interpolation",
                    "frame rate adaption",
                    "mobile video chats"
                ]
            }
        ]
    },
    {
        "name": "Tuo Zhao",
        "publications": [
            {
                "title": "Residual Network Based Direct Synthesis of EM Structures: A Study on One-to-One Transformers",
                "link": "https://ieeexplore.ieee.org/document/9218278/",
                "date_of_publication": "09 October 2020",
                "doi": "10.1109/RFIC49505.2020.9218278",
                "citations": "4",
                "abstract": "We propose using machine learning models for the direct synthesis of on-chip electromagnetic (EM) passive structures to enable rapid or even automated designs and optimizations of RF/mm-Wave circuits. As a proof of concept, we demonstrate the direct synthesis of a 1:1 transformer on a 45nm SOI process using our proposed neural network model. Using pre-existing transformer s-parameter files and their geometric design training samples, the model predicts target geometric designs.",
                "ieee_keywords": [
                    "Integrated circuit modeling",
                    "Geometry",
                    "Computational modeling",
                    "Machine learning",
                    "Circuit optimization",
                    "Software"
                ],
                "author_keywords": [
                    "direct synthesis",
                    "machine learning",
                    "rapid design"
                ]
            },
            {
                "title": "Deep Learning Assisted End-to-End Synthesis of mm-Wave Passive Networks with 3D EM Structures: A Study on A Transformer-Based Matching Network",
                "link": "https://ieeexplore.ieee.org/document/9575030/",
                "date_of_publication": "27 October 2021",
                "doi": "10.1109/IMS19712.2021.9575030",
                "citations": "2",
                "abstract": "This paper presents a deep learning assisted synthesis approach for direct end-to-end generation of RF/mm-wave passive matching network with 3D EM structures. Different from prior approaches that synthesize EM structures from target circuit component values and target topologies, our proposed approach achieves the direct synthesis of the passive network given the network topology from desired performance values as input. We showcase the proposed synthesis Neural Network (NN) model on an on-chip 1:1 transformer-based impedance matching network. By leveraging parameter sharing, the synthesis NN model successfully extracts relevant features from the input impedance and load capacitors, and predict the transformer 3D EM geometry in a 45nm SOI process that will match the standard 50Ω load to the target input impedance while absorbing the two loading capacitors. As a proof-of-concept, several example transformer geometries were synthesized, and verified in Ansys HFSS to provide the desired input impedance.",
                "ieee_keywords": [
                    "Geometry",
                    "Passive networks",
                    "Deep learning",
                    "Solid modeling",
                    "Three-dimensional displays",
                    "Network topology",
                    "Impedance matching"
                ],
                "author_keywords": [
                    "direct synthesis",
                    "deep learning",
                    "millimeter wave",
                    "impedance matching",
                    "transformer"
                ]
            },
            {
                "title": "A Task Performance-guided Model of Functional Networks Identification",
                "link": "https://ieeexplore.ieee.org/document/8759254/",
                "date_of_publication": "11 July 2019",
                "doi": "10.1109/ISBI.2019.8759254",
                "citations": "2",
                "abstract": "Understanding the organization of brain cortical functions has long been an intriguing research domain. Since the popularity of whole-brain in vivo imaging techniques, such as functional magnetic resonance imaging (fMRI), researchers have developed various brain network analysis methods for functional network identification, including principal component analysis (PCA), independent component analysis (ICA), and the methods based on sparse representation. However, all these aforementioned methods were either data-driven or hypothesis-driven, while the individual behavioral or task performance interpretation of the identified networks remains to be examined. To this end, we proposed a framework that incorporates the behavioral measures of in-scanner task performance to a hybrid temporo-spatial dictionary learning and sparse representation pipeline to identify group-wise basic networks from task fMRI data. The identified holistic functional networks were intrinsically guided by behavioral measures that encode across-individual functional variations. This framework was applied to working memory task fMRI data and the results demonstrate the effectiveness of the proposed framework.",
                "ieee_keywords": [
                    "Task analysis",
                    "Functional magnetic resonance imaging",
                    "Sparse matrices",
                    "Dictionaries",
                    "Matrix converters",
                    "Brain modeling"
                ],
                "author_keywords": [
                    "functional networks",
                    "in-scanner task performance",
                    "hybrid sparse representation"
                ]
            },
            {
                "title": "Eye-gaze-guided Vision Transformer for Rectifying Shortcut Learning",
                "link": "https://ieeexplore.ieee.org/document/10155473/",
                "date_of_publication": null,
                "doi": "10.1109/TMI.2023.3287572",
                "citations": "134",
                "abstract": "Learning harmful shortcuts such as spurious correlations and biases prevents deep neural networks from learning meaningful and useful representations, thus jeopardizing the generalizability and interpretability of the learned representation. The situation becomes even more serious in medical image analysis, where the clinical data are limited and scarce while the reliability, generalizability and transparency of the learned model are highly required. To rectify the harmful shortcuts in medical imaging applications, in this paper, we propose a novel eye-gaze-guided vision transformer (EG-ViT) model which infuses the visual attention from radiologists to proactively guide the vision transformer (ViT) model to focus on regions with potential pathology rather than spurious correlations. To do so, the EG-ViT model takes the masked image patches that are within the radiologists’ interest as input while has an additional residual connection to the last encoder layer to maintain the interactions of all patches. The experiments on two medical imaging datasets demonstrate that the proposed EG-ViT model can effectively rectify the harmful shortcut learning and improve the interpretability of the model. Meanwhile, infusing the experts’ domain knowledge can also improve the large-scale ViT model’s performance over all compared baseline methods with limited samples available. In general, EG-ViT takes the advantages of powerful deep neural networks while rectifies the harmful shortcut learning with human expert’s prior knowledge. This work also opens new avenues for advancing current artificial intelligence paradigms by infusing human intelligence.",
                "ieee_keywords": [
                    "Transformers",
                    "Analytical models",
                    "Medical diagnostic imaging",
                    "Deep learning",
                    "Task analysis",
                    "Solid modeling",
                    "Pathology"
                ],
                "author_keywords": [
                    "Eye Tracking",
                    "Generalizability",
                    "Interpretability",
                    "Shortcut Learning",
                    "Vision Transformer"
                ]
            },
            {
                "title": "Symmetry, Saddle Points, and Global Optimization Landscape of Nonconvex Matrix Factorization",
                "link": "https://ieeexplore.ieee.org/document/8675509/",
                "date_of_publication": null,
                "doi": "10.1109/TIT.2019.2898663",
                "citations": "8",
                "abstract": "We propose a general theory for studying the landscape of nonconvex optimization with underlying symmetric structures for a class of machine learning problems (e.g., low-rank matrix factorization, phase retrieval, and deep linear neural networks). In particular, we characterize the locations of stationary points and the null space of Hessian matrices of the objective function via the lens of invariant groups. As a major motivating example, we apply the proposed general theory to characterize the global landscape of the nonconvex optimization in low-rank matrix factorization problem. We illustrate how the rotational symmetry group gives rise to infinitely many nonisolated strict saddle points and equivalent global minima of the objective function. By explicitly identifying all stationary points, we divide the entire parameter space into three regions: (R 1 ) the region containing the neighborhoods of all strict saddle points where the objective has negative curvature; (R 2 ) the region containing neighborhoods of all global minima, where the objective enjoys strong convexity along certain directions; and (R 3 ) the complement of the above regions, where the gradient has sufficiently large magnitude. We further extend our result to the matrix sensing problem. Such global landscape implies that strong global convergence guarantees for popular iterative algorithms with arbitrary initial solutions.",
                "ieee_keywords": [
                    "Optimization",
                    "Sensors",
                    "Convergence",
                    "Symmetric matrices",
                    "Iterative methods",
                    "Sparse matrices",
                    "Machine learning"
                ],
                "author_keywords": [
                    "Strict saddle problem",
                    "global landscape",
                    "nonconvex matrix factorization",
                    "matrix sensing",
                    "invariant group"
                ]
            }
        ]
    },
    {
        "name": "Taesoo Kim",
        "publications": [
            {
                "title": "Persistent Memory Needs Lifetime Extension Techniques",
                "link": "https://ieeexplore.ieee.org/document/9289495/",
                "date_of_publication": "21 December 2020",
                "doi": "10.1109/ICTC49870.2020.9289495",
                "citations": "133",
                "abstract": "Recently, some commercial systems start to use persistent memory(PMem). To take full advantage of this new memory, you need to develop PMem-aware applications through persistent memory programming. Unfortunately, PMem developers don't understand yet that they should consider life span issues as important as performance and consistency. So this paper was written to introduce the lifespan problem of PMem, which is not widely known.",
                "ieee_keywords": [
                    "Memory management",
                    "Programming",
                    "Information and communication technology",
                    "Convergence"
                ],
                "author_keywords": [
                    "persistent memory",
                    "endurance",
                    "weal-leveling",
                    "lifetime management"
                ]
            },
            {
                "title": "Scalable and Secure Virtualization of HSM With ScaleTrust",
                "link": "https://ieeexplore.ieee.org/document/9954229/",
                "date_of_publication": null,
                "doi": "10.1109/TNET.2022.3220427",
                "citations": "188",
                "abstract": "Hardware security modules (HSMs) have been utilized as a trustworthy foundation for cloud services. Unfortunately, existing systems using HSMs fail to meet multi-tenant scalability arising from the emerging trends such as microservices, which utilize frequent cryptographic operations. As an alternative, cloud vendors provide HSMs as a service. However, such cloud-managed HSM usage models raise security concerns due to their untrusted and shared operating environment. We propose ScaleTrust, a scalable and secure system for key management. ScaleTrust allows us to scale the number of virtual HSM partitions, each of which is isolated with respect to each other and is robust against cloud insider attacks, while preserving physical isolation of the root of trust. To enable this, ScaleTrust uses Intel SGX and multiple HSM features, such as restricting key usage by controlling key attributes of in-HSM keys and establishing a secure channel using only HSM commands. Finally, we apply ScaleTrust to four real-world systems: Keyless SSL for TLS private key offloading, JSON Web Token authentication for microservices, key provisioning, and encryption in database systems. Our evaluation shows that ScaleTrust achieves multi-tenancy in a scalable way by providing multiple virtual HSMs with legacy HSM devices that are designed to support a single tenant. ScaleTrust provides security against insider threats while incurring 11.9% and 39.0% of end-to-end throughput and latency overhead for Keyless SSL compared to stand-alone HSMs.",
                "ieee_keywords": [
                    "Security",
                    "Cloud computing",
                    "Cryptography",
                    "Hardware",
                    "Scalability",
                    "Side-channel attacks",
                    "Encryption"
                ],
                "author_keywords": [
                    "Hardware security module (HSM)",
                    "trusted execution environment (TEE)",
                    "key management service (KMS)",
                    "scalability",
                    "cloud computing security"
                ]
            },
            {
                "title": "Fast, Scalable and Secure Onloading of Edge Functions Using AirBox",
                "link": "https://ieeexplore.ieee.org/document/7774350/",
                "date_of_publication": "08 December 2016",
                "doi": "10.1109/SEC.2016.15",
                "citations": "42",
                "abstract": "This paper argues for the utility of back-end driven onloading to the edge as a way to address bandwidth use and latency challenges for future device-cloud interactions. Supporting such edge functions (EFs) requires solutions that can provide (i) fast and scalable EF provisioning and (ii) strong guarantees for the integrity of the EF execution and confidentiality of the state stored at the edge. In response to these goals, we (i) present a detailed design space exploration of the current technologies that can be leveraged in the design of edge function platforms (EFPs), (ii) develop a solution to address security concerns of EFs that leverages emerging hardware support for OS agnostic trusted execution environments such as Intel SGX enclaves, and (iii) propose and evaluate AirBox, a platform for fast, scalable and secure onloading of edge functions.",
                "ieee_keywords": [
                    "Cloud computing",
                    "Security",
                    "Servers",
                    "Libraries",
                    "Containers",
                    "Bandwidth"
                ],
                "author_keywords": [
                    "Edge Computing",
                    "Edge Cloud",
                    "Security",
                    "Intel SGX"
                ]
            },
            {
                "title": "HDFI: Hardware-Assisted Data-Flow Isolation",
                "link": "https://ieeexplore.ieee.org/document/7546472/",
                "date_of_publication": "18 August 2016",
                "doi": "10.1109/SP.2016.9",
                "citations": "59",
                "abstract": "Memory corruption vulnerabilities are the root cause of many modern attacks. Existing defense mechanisms are inadequate; in general, the software-based approaches are not efficient and the hardware-based approaches are not flexible. In this paper, we present hardware-assisted data-flow isolation, or, HDFI, a new fine-grained data isolation mechanism that is broadly applicable and very efficient. HDFI enforces isolation at the machine word granularity by virtually extending each memory unit with an additional tag that is defined by dataflow. This capability allows HDFI to enforce a variety of security models such as the Biba Integrity Model and the Bell -- LaPadula Model. We implemented HDFI by extending the RISC-V instruction set architecture (ISA) and instantiating it on the Xilinx Zynq ZC706 evaluation board. We ran several benchmarks including the SPEC CINT 2000 benchmark suite. Evaluation results show that the performance overhead caused by our modification to the hardware is low (<; 2%). We also developed or ported several security mechanisms to leverage HDFI, including stack protection, standard library enhancement, virtual function table protection, code pointer protection, kernel data protection, and information leak prevention. Our results show that HDFI is easy to use, imposes low performance overhead, and allows us to create more elegant and more secure solutions.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Amnesiac DRAM: A Proactive Defense Mechanism Against Cold Boot Attacks",
                "link": "https://ieeexplore.ieee.org/document/8862904/",
                "date_of_publication": null,
                "doi": "10.1109/TC.2019.2946365",
                "citations": "5",
                "abstract": "DRAMs in modern computers or hand-held devices store private or often security-sensitive data. Unfortunately, one known attack vector, called a cold boot attack, remains threatening and easy-to-exploit, especially when attackers have physical access to the device. It exploits the fundamental property of current DRAMs: remanence effects that retain the stored contents for a certain period of time even after powering off. To magnify the remanence effect, cold boot attacks typically freeze the victim DRAM, thereby providing a chance to detach, move, and reattach it to an attacker's computer. Once power is on, attackers can steal all the security-critical information from the victim's DRAM, such as a master decryption key for an encrypted disk storage. Two types of defenses were proposed in the past: 1) CPU-bound cryptography, where keys are stored in CPU registers and caches instead of in DRAMs, and 2) full or partial memory encryption, where sensitive data are stored encrypted. However, both methods impose non-negligible performance or energy overheads to the running systems, and worse, significantly increase the hardware and software manufacturing costs. We found that these proposed solutions attempted to address the cold boot attacks passively: either by avoiding or by indirectly addressing the root cause of the problem, the remanence effect. In this article, we propose and evaluate a proactive defense mechanism, Amnesiac DRAM, that comprehensively prevents the cold boot attacks. The key idea is to discard the contents in the DRAM when attackers attempt to retrieve (i.e., power on) them from the stolen DRAM. When Amnesiac DRAM senses a physical separation, it locks itself and deletes all the remaining contents, making it amnesiac. The Amnesiac DRAM causes neither performance nor energy overhead in ordinary operations (e.g., load and store) and can be easily implemented with negligible area overhead in commodity DRAM architectures.",
                "ieee_keywords": [
                    "Remanence",
                    "Pins",
                    "Encryption",
                    "DRAM chips",
                    "Capacitors"
                ],
                "author_keywords": [
                    "Cold boot attack",
                    "DRAM",
                    "hardware defense",
                    "self erasing memory"
                ]
            },
            {
                "title": "SGX-Tor: A Secure and Practical Tor Anonymity Network With SGX Enclaves",
                "link": "https://ieeexplore.ieee.org/document/8464097/",
                "date_of_publication": null,
                "doi": "10.1109/TNET.2018.2868054",
                "citations": "20",
                "abstract": "With Tor being a popular anonymity network, many attacks have been proposed to break its anonymity or leak information of a private communication on Tor. However, guaranteeing complete privacy in the face of an adversary on Tor is especially difficult, because Tor relays are under complete control of world-wide volunteers. Currently, one can gain private information, such as circuit identifiers and hidden service identifiers, by running Tor relays and can even modify their behaviors with malicious intent. This paper presents a practical approach to effectively enhancing the security and privacy of Tor by utilizing Intel SGX, a commodity trusted execution environment. We present a design and implementation of Tor, called SGX-Tor, that prevents code modification and limits the information exposed to untrusted parties. We demonstrate that our approach is practical and effectively reduces the power of an adversary to a traditional network-level adversary. Finally, SGX-Tor incurs moderate performance overhead; the end-to-end latency and throughput overheads for HTTP connections are 3.9% and 11.9%, respectively.",
                "ieee_keywords": [
                    "Relays",
                    "Servers",
                    "Hardware",
                    "Privacy",
                    "Security",
                    "IP networks",
                    "Software"
                ],
                "author_keywords": [
                    "Tor network",
                    "Intel SGX",
                    "anonymous network",
                    "trusted execution environment (TEE)"
                ]
            },
            {
                "title": "Fuzzing File Systems via Two-Dimensional Input Space Exploration",
                "link": "https://ieeexplore.ieee.org/document/8835267/",
                "date_of_publication": "16 September 2019",
                "doi": "10.1109/SP.2019.00035",
                "citations": "31",
                "abstract": "File systems, a basic building block of an OS, are too big and too complex to be bug free. Nevertheless, file systems rely on regular stress-testing tools and formal checkers to find bugs, which are limited due to the ever-increasing complexity of both file systems and OSes. Thus, fuzzing, proven to be an effective and a practical approach, becomes a preferable choice, as it does not need much knowledge about a target. However, three main challenges exist in fuzzing file systems: mutating a large image blob that degrades overall performance, generating image-dependent file operations, and reproducing found bugs, which is difficult for existing OS fuzzers. Hence, we present JANUS, the first feedback-driven fuzzer that explores the two-dimensional input space of a file system, i.e., mutating metadata on a large image, while emitting image-directed file operations. In addition, JANUS relies on a library OS rather than on traditional VMs for fuzzing, which enables JANUS to load a fresh copy of the OS, thereby leading to better reproducibility of bugs. We evaluate JANUS on eight file systems and found 90 bugs in the upstream Linux kernel, 62 of which have been acknowledged. Forty-three bugs have been fixed with 32 CVEs assigned. In addition, JANUS achieves higher code coverage on all the file systems after fuzzing 12 hours, when compared with the state-of-the-art fuzzer Syzkaller for fuzzing file systems. JANUS visits 4.19x and 2.01x more code paths in Btrfs and ext4, respectively. Moreover, JANUS is able to reproduce 88-100% of the crashes, while Syzkaller fails on all of them.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Krace: Data Race Fuzzing for Kernel File Systems",
                "link": "https://ieeexplore.ieee.org/document/9152693/",
                "date_of_publication": "30 July 2020",
                "doi": "10.1109/SP40000.2020.00078",
                "citations": "31",
                "abstract": "Data races occur when two threads fail to use proper synchronization when accessing shared data. In kernel file systems, which are highly concurrent by design, data races are common mistakes and often wreak havoc on the users, causing inconsistent states or data losses. Prior fuzzing practices on file systems have been effective in uncovering hundreds of bugs, but they mostly focus on the sequential aspect of file system execution and do not comprehensively explore the concurrency dimension and hence, forgo the opportunity to catch data races.In this paper, we bring coverage-guided fuzzing to the concurrency dimension with three new constructs: 1) a new coverage tracking metric, alias coverage, specially designed to capture the exploration progress in the concurrency dimension; 2) an evolution algorithm for generating, mutating, and merging multi-threaded syscall sequences as inputs for concurrency fuzzing; and 3) a comprehensive lockset and happens-before modeling for kernel synchronization primitives for precise data race detection. These components are integrated into Krace, an end-to-end fuzzing framework that has discovered 23 data races in ext4, btrfs, and the VFS layer so far, and 9 are confirmed to be harmful.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Fuzzing JavaScript Engines with Aspect-preserving Mutation",
                "link": "https://ieeexplore.ieee.org/document/9152648/",
                "date_of_publication": "30 July 2020",
                "doi": "10.1109/SP40000.2020.00067",
                "citations": "27",
                "abstract": "Fuzzing is a practical, widely-deployed technique to find bugs in complex, real-world programs like JavaScript engines. We observed, however, that existing fuzzing approaches, either generative or mutational, fall short in fully harvesting high-quality input corpora such as known proof of concept (PoC) exploits or unit tests. Existing fuzzers tend to destruct subtle semantics or conditions encoded in the input corpus in order to generate new test cases because this approach helps in discovering new code paths of the program. Nevertheless, for JavaScript-like complex programs, such a conventional design leads to test cases that tackle only shallow parts of the complex codebase and fails to reach deep bugs effectively due to the huge input space.In this paper, we advocate a new technique, called an aspect-preserving mutation, that stochastically preserves the desirable properties, called aspects, that we prefer to be maintained across mutation. We demonstrate the aspect preservation with two mutation strategies, namely, structure and type preservation, in our fully-fledged JavaScript fuzzer, called Die. Our evaluation shows that Die’s aspect-preserving mutation is more effective in discovering new bugs (5.7× more unique crashes) and producing valid test cases (2.4× fewer runtime errors) than the state-of-the-art JavaScript fuzzers. Die newly discovered 48 high-impact bugs in ChakraCore, JavaScriptCore, and V8 (38 fixed with 12 CVEs assigned as of today). The source code of Die is publicly available as an open-source project. 1",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Pyfet: Forensically Equivalent Transformation for Python Binary Decompilation",
                "link": "https://ieeexplore.ieee.org/document/10179370/",
                "date_of_publication": "21 July 2023",
                "doi": "10.1109/SP46215.2023.10179370",
                "citations": "40",
                "abstract": "Decompilation is a crucial capability in forensic analysis, facilitating analysis of unknown binaries. The recent rise of Python malware has brought attention to Python decompilers that aim to obtain source code representation from a Python binary. However, Python decompilers fail to handle various binaries, limiting their capabilities in forensic analysis.This paper proposes a novel solution that transforms a decompilation error-inducing Python binary into a decompilable binary. Our key intuition is that we can resolve the decompilation errors by transforming error-inducing code blocks in the input binary into another form. The core of our approach is the concept of Forensically Equivalent Transformation (FET) which allows non-semantic preserving transformation in the context of forensic analysis. We carefully define the FETs to minimize their undesirable consequences while fixing various error-inducing instructions that are difficult to solve when preserving the exact semantics. We evaluate the prototype of our approach with 17,117 real-world Python malware samples causing decompilation errors in five popular decompilers. It successfully identifies and fixes 77,022 errors. Our approach also handles anti-analysis techniques, including opcode remapping, and helps migrate Python 3.9 binaries to 3.8 binaries.",
                "ieee_keywords": [
                    "Privacy",
                    "Limiting",
                    "Forensics",
                    "Source coding",
                    "Semantics",
                    "Field effect transistors",
                    "Prototypes"
                ],
                "author_keywords": [
                    "Reverse-Engineering",
                    "Decompilation",
                    "Binary-Transformation",
                    "Python-Malware"
                ]
            }
        ]
    },
    {
        "name": "Wenke Lee",
        "publications": [
            {
                "title": "HDFI: Hardware-Assisted Data-Flow Isolation",
                "link": "https://ieeexplore.ieee.org/document/7546472/",
                "date_of_publication": "18 August 2016",
                "doi": "10.1109/SP.2016.9",
                "citations": "59",
                "abstract": "Memory corruption vulnerabilities are the root cause of many modern attacks. Existing defense mechanisms are inadequate; in general, the software-based approaches are not efficient and the hardware-based approaches are not flexible. In this paper, we present hardware-assisted data-flow isolation, or, HDFI, a new fine-grained data isolation mechanism that is broadly applicable and very efficient. HDFI enforces isolation at the machine word granularity by virtually extending each memory unit with an additional tag that is defined by dataflow. This capability allows HDFI to enforce a variety of security models such as the Biba Integrity Model and the Bell -- LaPadula Model. We implemented HDFI by extending the RISC-V instruction set architecture (ISA) and instantiating it on the Xilinx Zynq ZC706 evaluation board. We ran several benchmarks including the SPEC CINT 2000 benchmark suite. Evaluation results show that the performance overhead caused by our modification to the hardware is low (<; 2%). We also developed or ported several security mechanisms to leverage HDFI, including stack protection, standard library enhancement, virtual function table protection, code pointer protection, kernel data protection, and information leak prevention. Our results show that HDFI is easy to use, imposes low performance overhead, and allows us to create more elegant and more secure solutions.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Modeling Large-Scale Manipulation in Open Stock Markets",
                "link": "https://ieeexplore.ieee.org/document/9436109/",
                "date_of_publication": null,
                "doi": "10.1109/MSEC.2021.3076717",
                "citations": "513",
                "abstract": "This article studies the feasibility of using a botnet to automate stock market manipulation, incorporating data from U.S. Securities and Exchange Commission case files, security surveys of online retail brokerage accounts, and dark web marketplace listings.",
                "ieee_keywords": [
                    "Computer security",
                    "Electronic mail",
                    "Browsers",
                    "Botnet",
                    "Stock markets",
                    "Data models",
                    "Trojan horses",
                    "Modeling",
                    "Large-scale systems",
                    "Open systems"
                ],
                "author_keywords": []
            },
            {
                "title": "DNS Noise: Measuring the Pervasiveness of Disposable Domains in Modern DNS Traffic",
                "link": "https://ieeexplore.ieee.org/document/6903614/",
                "date_of_publication": "22 September 2014",
                "doi": "10.1109/DSN.2014.61",
                "citations": "29",
                "abstract": "In this paper, we present an analysis of a new class of domain names: disposable domains. We observe that popular web applications, along with other Internet services, systematically use this new class of domain names. Disposable domains are likely generated automatically, characterized by a \"one-time use\" pattern, and appear to be used as a way of \"signaling\" via DNS queries. To shed light on the pervasiveness of disposable domains, we study 24 days of live DNS traffic spanning a year observed at a large Internet Service Provider. We find that disposable domains increased from 23.1% to 27.6% of all queried domains, and from 27.6% to 37.2% of all resolved domains observed daily. While this creative use of DNS may enable new applications, it may also have unanticipated negative consequences on the DNS caching infrastructure, DNSSEC validating resolvers, and passive DNS data collection systems.",
                "ieee_keywords": [
                    "Servers",
                    "Google",
                    "Monitoring",
                    "Internet",
                    "Web and internet services",
                    "Data collection",
                    "Educational institutions"
                ],
                "author_keywords": [
                    "Disposable Domain Name",
                    "Internet Measurement"
                ]
            },
            {
                "title": "One Engine to Fuzz ’em All: Generic Language Processor Testing with Semantic Validation",
                "link": "https://ieeexplore.ieee.org/document/9519403/",
                "date_of_publication": "26 August 2021",
                "doi": "10.1109/SP40001.2021.00071",
                "citations": "7",
                "abstract": "Language processors, such as compilers and interpreters, are indispensable in building modern software. Errors in language processors can lead to severe consequences, like incorrect functionalities or even malicious attacks. However, it is not trivial to automatically test language processors to find bugs. Existing testing methods (or fuzzers) either fail to generate high-quality (i.e., semantically correct) test cases, or only support limited programming languages.In this paper, we propose POLYGLOT, a generic fuzzing framework that generates high-quality test cases for exploring processors of different programming languages. To achieve the generic applicability, POLYGLOT neutralizes the difference in syntax and semantics of programming languages with a uniform intermediate representation (IR). To improve the language validity, POLYGLOT performs constrained mutation and semantic validation to preserve syntactic correctness and fix semantic errors. We have applied POLYGLOT on 21 popular language processors of 9 programming languages, and identified 173 new bugs, 113 of which are fixed with 18 CVEs assigned. Our experiments show that POLYGLOT can support a wide range of programming languages, and outperforms existing fuzzers with up to 30× improvement in code coverage.",
                "ieee_keywords": [
                    "Computer languages",
                    "Program processors",
                    "Semantics",
                    "Computer bugs",
                    "Syntactics",
                    "Fuzzing",
                    "Programming"
                ],
                "author_keywords": [
                    "Software-security",
                    "Application-Security",
                    "Fuzzing"
                ]
            },
            {
                "title": "From Zygote to Morula: Fortifying Weakened ASLR on Android",
                "link": "https://ieeexplore.ieee.org/document/6956579/",
                "date_of_publication": "20 November 2014",
                "doi": "10.1109/SP.2014.34",
                "citations": "23",
                "abstract": "There have been many research efforts to secure Android applications and the high-level system mechanisms. The low-level operating system designs have been overlooked partially due to the belief that security issues at this level are similar to those on Linux, which are well-studied. However, we identify that certain Android modifications are at odds with security and result in serious vulnerabilities that need to be addressed immediately. In this paper, we analyze the Zygote process creation model, an Android operating system design for speeding up application launches. Zygote weakens Address Space Layout Randomization (ASLR) because all application processes are created with largely identical memory layouts. We design both remote and local attacks capable of bypassing the weakened ASLR and executing return-oriented programming on Android. We demonstrate the attacks using real applications, such as the Chrome Browser and VLC Media Player. Further, we design and implement Morula, a secure replacement for Zygote. Morula introduces a small amount of code to the Android operating system and can be easily adopted by device vendors. Our evaluation shows that, compared to Zygote, Morula incurs a 13 MB memory increase for each running application but allows each Android process to have an individually randomized memory layout and even a slightly shorter average launch time.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "DIFT Games: Dynamic Information Flow Tracking Games for Advanced Persistent Threats",
                "link": "https://ieeexplore.ieee.org/document/8619416/",
                "date_of_publication": "20 January 2019",
                "doi": "10.1109/CDC.2018.8619416",
                "citations": "15",
                "abstract": "Dynamic Information Flow Tracking (DIFT) has been proposed to detect stealthy and persistent cyber attacks that evade existing defenses such as firewalls and signature-based antivirus systems. A DIFT defense taints and tracks suspicious information flows across the network in order to identify possible attacks, at the cost of additional memory overhead for tracking non-adversarial information flows. In this paper, we present the first analytical model that describes the interaction between DIFT and adversarial information flows, including the probability that the adversary evades detection and the performance overhead of the defense. Our analytical model consists of a multi-stage game, in which each stage represents a system process through which the information flow passes. We characterize the optimal strategies for both the defense and adversary, and derive efficient algorithms for computing the strategies. Our results are evaluated on a realworld attack dataset obtained using the Refinable Attack Investigation (RAIN) framework, enabling us to draw conclusions on the optimal adversary and defense strategies, as well as the effect of valid information flows on the interaction between adversary and defense.",
                "ieee_keywords": [
                    "Games",
                    "Security",
                    "Tagging",
                    "Analytical models",
                    "Computational modeling",
                    "Stochastic processes",
                    "Monitoring"
                ],
                "author_keywords": []
            },
            {
                "title": "Detecting stealthy P2P botnets using statistical traffic fingerprints",
                "link": "https://ieeexplore.ieee.org/document/5958212/",
                "date_of_publication": "18 July 2011",
                "doi": "10.1109/DSN.2011.5958212",
                "citations": "57",
                "abstract": "Peer-to-peer (P2P) botnets have recently been adopted by botmasters for their resiliency to take-down efforts. Besides being harder to take down, modern botnets tend to be stealthier in the way they perform malicious activities, making current detection approaches, including, ineffective. In this paper, we propose a novel botnet detection system that is able to identify stealthy P2P botnets, even when malicious activities may not be observable. First, our system identifies all hosts that are likely engaged in P2P communications. Then, we derive statistical fingerprints to profile different types of P2P traffic, and we leverage these fingerprints to distinguish between P2P botnet traffic and other legitimate P2P traffic. Unlike previous work, our system is able to detect stealthy P2P botnets even when the underlying compromised hosts are running legitimate P2P applications (e.g., Skype) and the P2P bot software at the same time. Our experimental evaluation based on real-world data shows that the proposed system can achieve high detection accuracy with a low false positive rate.",
                "ieee_keywords": [
                    "Peer to peer computing",
                    "Monitoring",
                    "Protocols",
                    "Clustering algorithms",
                    "IP networks",
                    "Storms",
                    "Electronic mail"
                ],
                "author_keywords": [
                    "Botnet",
                    "P2P",
                    "Intrusion Detection",
                    "Security"
                ]
            },
            {
                "title": "Virtuoso: Narrowing the Semantic Gap in Virtual Machine Introspection",
                "link": "https://ieeexplore.ieee.org/document/5958036/",
                "date_of_publication": "18 July 2011",
                "doi": "10.1109/SP.2011.11",
                "citations": "170",
                "abstract": "Introspection has featured prominently in many recent security solutions, such as virtual machine-based intrusion detection, forensic memory analysis, and low-artifact malware analysis. Widespread adoption of these approaches, however, has been hampered by the semantic gap: in order to extract meaningful information about the current state of a virtual machine, detailed knowledge of the guest operating system's inner workings is required. In this paper, we present a novel approach for automatically creating introspection tools for security applications with minimal human effort. By analyzing dynamic traces of small, in-guest programs that compute the desired introspection information, we can produce new programs that retrieve the same information from outside the guest virtual machine. We demonstrate the efficacy of our techniques by automatically generating 17 programs that retrieve security information across 3 different operating systems, and show that their functionality is unaffected by the compromise of the guest system. Our technique allows introspection tools to be effortlessly generated for multiple platforms, and enables the development of rich introspection-based security applications.",
                "ieee_keywords": [
                    "Virtual machining",
                    "Training",
                    "Kernel",
                    "Malware",
                    "Data mining"
                ],
                "author_keywords": [
                    "virtual machine introspection",
                    "security",
                    "dynamic analysis",
                    "virtualization"
                ]
            },
            {
                "title": "Stopping Memory Disclosures via Diversification and Replicated Execution",
                "link": "https://ieeexplore.ieee.org/document/8510822/",
                "date_of_publication": null,
                "doi": "10.1109/TDSC.2018.2878234",
                "citations": "10",
                "abstract": "With the wide deployment of security mechanisms such as Address Space Layout Randomization (ASLR), memory disclosures have become a prerequisite for critical memory-corruption attacks (e.g., code-reuse attack)-adversaries are forced to exploit memory disclosures to circumvent ASLR as the first step. As a result, the security threats of memory disclosures are now significantly aggravated-they break not only data confidentiality but also the effectiveness of security mechanisms. In this paper, we propose a general detection methodology and develop a system to stop memory disclosures. We observe that memory disclosures are not root causes but rather consequences of a variety of hard-to-detect program errors such as memory corruption and uninitialized read. We thus propose a replicated execution-based methodology to generally detect memory disclosures, regardless of their causes. We realize this methodology with Buddy: By seamlessly maintaining two identical running instances of a target program and diversifying only its target data, Buddy can accurately detects memory disclosures of the data, as doing so will result in the two instances outputting different values. Extensive evaluation results show that Buddy is reliable and efficient while stopping real memory disclosures such as the Heartbleed leak.",
                "ieee_keywords": [
                    "Synchronization",
                    "Security",
                    "Kernel",
                    "Benchmark testing",
                    "Web servers",
                    "Memory management",
                    "Layout"
                ],
                "author_keywords": [
                    "Memory disclosure",
                    "diversification",
                    "replicated execution",
                    "N-version system",
                    "code-reuse attack"
                ]
            },
            {
                "title": "Building a Scalable System for Stealthy P2P-Botnet Detection",
                "link": "https://ieeexplore.ieee.org/document/6661360/",
                "date_of_publication": null,
                "doi": "10.1109/TIFS.2013.2290197",
                "citations": "68",
                "abstract": "Peer-to-peer (P2P) botnets have recently been adopted by botmasters for their resiliency against take-down efforts. Besides being harder to take down, modern botnets tend to be stealthier in the way they perform malicious activities, making current detection approaches ineffective. In addition, the rapidly growing volume of network traffic calls for high scalability of detection systems. In this paper, we propose a novel scalable botnet detection system capable of detecting stealthy P2P botnets. Our system first identifies all hosts that are likely engaged in P2P communications. It then derives statistical fingerprints to profile P2P traffic and further distinguish between P2P botnet traffic and legitimate P2P traffic. The parallelized computation with bounded complexity makes scalability a built-in feature of our system. Extensive evaluation has demonstrated both high detection accuracy and great scalability of the proposed system.",
                "ieee_keywords": [
                    "Peer-to-peer computing",
                    "Scalability",
                    "Electronic mail",
                    "Educational institutions",
                    "Overlay networks",
                    "Monitoring",
                    "Feature extraction"
                ],
                "author_keywords": [
                    "Botnet",
                    "P2P",
                    "intrusion detection",
                    "network security"
                ]
            }
        ]
    },
    {
        "name": "Yingyan (Celine) Lin",
        "publications": [
            {
                "title": "EyeCoD: Eye Tracking System Acceleration via FlatCam-Based Algorithm and Hardware Co-Design",
                "link": "https://ieeexplore.ieee.org/document/10123119/",
                "date_of_publication": null,
                "doi": "10.1109/MM.2023.3274736",
                "citations": "213",
                "abstract": "Eye tracking has become an essential human–machine interaction modality in virtual reality (VR) and augmented reality (AR) applications requiring high throughput (e.g., more than 240 frames per second), small form factor, and enhanced visual privacy. Existing eye tracking systems have adopted bulky, lens-based cameras, and thus suffer from both a large form factor and high communication cost between the camera and back-end processor. This work presents a camera, algorithm, and accelerator co-designed lensless eye tracking system dubbed EyeCoD, which, to the best of our knowledge, is the first to provide a general, front-end eye tracking solution for AR/VR while satisfying the requirements for both high throughput and smaller form factor. Specifically, EyeCoD integrates system-, algorithm-, and accelerator-level techniques to boost system efficiency without sacrificing eye tracking accuracy. We believe that our EyeCoD system will pave the way for next-generation eye tracking solutions in VR/AR and shed light on future innovations for intelligent imaging systems.",
                "ieee_keywords": [
                    "Gaze tracking",
                    "Cameras",
                    "Estimation",
                    "Predictive models",
                    "Prediction algorithms",
                    "Pipelines",
                    "Computational modeling"
                ],
                "author_keywords": []
            },
            {
                "title": "ViTCoD: Vision Transformer Acceleration via Dedicated Algorithm and Accelerator Co-Design",
                "link": "https://ieeexplore.ieee.org/document/10071027/",
                "date_of_publication": "24 March 2023",
                "doi": "10.1109/HPCA56546.2023.10071027",
                "citations": "1",
                "abstract": "Vision Transformers (ViTs) have achieved state-of-the-art performance on various vision tasks. However, ViTs’ self-attention module is still arguably a major bottleneck, limiting their achievable hardware efficiency and more extensive applications to resource constrained platforms. Meanwhile, existing accelerators dedicated to NLP Transformers are not optimal for ViTs. This is because there is a large difference between ViTs and Transformers for natural language processing (NLP) tasks: ViTs have a relatively fixed number of input tokens, whose attention maps can be pruned by up to 90% even with fixed sparse patterns, without severely hurting the model accuracy (e.g., <=1.5% under 90% pruning ratio); while NLP Transformers need to handle input sequences of varying numbers of tokens and rely on on-the-fly predictions of dynamic sparse attention patterns for each input to achieve a decent sparsity (e.g., >=50%). To this end, we propose a dedicated algorithm and accelerator co-design framework dubbed ViTCoD for accelerating ViTs. Specifically, on the algorithm level, ViTCoD prunes and polarizes the attention maps to have either denser or sparser fixed patterns for regularizing two levels of workloads without hurting the accuracy, largely reducing the attention computations while leaving room for alleviating the remaining dominant data movements; on top of that, we further integrate a lightweight and learnable auto-encoder module to enable trading the dominant high-cost data movements for lower-cost computations. On the hardware level, we develop a dedicated accelerator to simultaneously coordinate the aforementioned enforced denser and sparser workloads for boosted hardware utilization, while integrating on-chip encoder and decoder engines to leverage ViTCoD’s algorithm pipeline for much reduced data movements. Extensive experiments and ablation studies validate that ViTCoD largely reduces the dominant data movement costs, achieving speedups of up to 235.3×, 142.9×, 86.... (Show More)",
                "ieee_keywords": [
                    "Heuristic algorithms",
                    "Predictive models",
                    "Transformers",
                    "Prediction algorithms",
                    "Natural language processing",
                    "Hardware",
                    "Decoding"
                ],
                "author_keywords": []
            },
            {
                "title": "ERSAM: Neural Architecture Search for Energy-Efficient and Real-Time Social Ambiance Measurement",
                "link": "https://ieeexplore.ieee.org/document/10095360/",
                "date_of_publication": "05 May 2023",
                "doi": "10.1109/ICASSP49357.2023.10095360",
                "citations": "64",
                "abstract": "Social ambiance describes the context in which social interactions happen, and can be measured using speech audio by counting the number of concurrent speakers. This measurement has enabled various mental health tracking and human-centric IoT applications. While on-device Socal Ambiance Measure (SAM) is highly desirable to ensure user privacy and thus facilitate wide adoption of the aforementioned applications, the required computational complexity of state-of-the-art deep neural networks (DNNs) powered SAM solutions stands at odds with the often constrained resources on mobile devices. Furthermore, only limited labeled data is available or practical when it comes to SAM under clinical settings due to various privacy constraints and the required human effort, further challenging the achievable accuracy of on-device SAM solutions. To this end, we propose a dedicated neural architecture search framework for Energy-efficient and Real-time SAM (ERSAM). Specifically, our ERSAM framework can automatically search for DNNs that push forward the achievable accuracy vs. hardware efficiency frontier of mobile SAM solutions. For example, ERSAM-delivered DNNs only consume 40 mW • 12 h energy and 0.05 seconds processing latency for a 5 seconds audio segment on a Pixel 3 phone, while only achieving an error rate of 14.3% on a social ambiance dataset generated by LibriSpeech. We can expect that our ERSAM framework can pave the way for ubiquitous on-device SAM solutions which are in growing demand.",
                "ieee_keywords": [
                    "Training",
                    "Privacy",
                    "Training data",
                    "Computer architecture",
                    "Signal processing",
                    "Size measurement",
                    "Real-time systems"
                ],
                "author_keywords": [
                    "social ambiance",
                    "neural arch search"
                ]
            },
            {
                "title": "ViTALiTy: Unifying Low-rank and Sparse Approximation for Vision Transformer Acceleration with a Linear Taylor Attention",
                "link": "https://ieeexplore.ieee.org/document/10071081/",
                "date_of_publication": "24 March 2023",
                "doi": "10.1109/HPCA56546.2023.10071081",
                "citations": "1",
                "abstract": "Vision Transformer (ViT) has emerged as a competitive alternative to convolutional neural networks for various computer vision applications. Specifically, ViTs’ multi-head attention layers make it possible to embed information globally across the overall image. Nevertheless, computing and storing such attention matrices incurs a quadratic cost dependency on the number of patches, limiting its achievable efficiency and scalability and prohibiting more extensive real-world ViT applications on resource-constrained devices. Sparse attention has been shown to be a promising direction for improving hardware acceleration efficiency for NLP models. However, a systematic counterpart approach is still missing for accelerating ViT models. To close the above gap, we propose a first-of-its-kind algorithm-hardware codesigned framework, dubbed VITALITY, for boosting the inference efficiency of ViTs. Unlike sparsity-based Transformer accelerators for NLP, VITALITY unifies both low-rank and sparse components of the attention in ViTs. At the algorithm level, we approximate the dot-product softmax operation via first-order Taylor attention with row-mean centering as the low-rank component to linearize the cost of attention blocks and further boost the accuracy by incorporating a sparsity-based regularization. At the hardware level, we develop a dedicated accelerator to better leverage the resulting workload and pipeline from VITALITY’s linear Taylor attention which requires the execution of only the low-rank component, to further boost the hardware efficiency. Extensive experiments and ablation studies validate that VITALITY offers boosted end-to-end efficiency (e.g., 3× faster and 3× energy-efficient) under comparable accuracy, with respect to the state-of-the-art solution. We make the codes available on https://github.com/GATECH-EIC/ViTaLiTy",
                "ieee_keywords": [
                    "Training",
                    "Costs",
                    "Systematics",
                    "Approximation algorithms",
                    "Transformers",
                    "Boosting",
                    "Sparse representation"
                ],
                "author_keywords": []
            },
            {
                "title": "NASA+: Neural Architecture Search and Acceleration for Multiplication-Reduced Hybrid Networks",
                "link": "https://ieeexplore.ieee.org/document/10078392/",
                "date_of_publication": null,
                "doi": "10.1109/TCSI.2023.3256700",
                "citations": "1",
                "abstract": "Multiplication is arguably the most computation-intensive operation in modern deep neural networks (DNNs), limiting their extensive deployment on resource-constrained devices. Thereby, pioneering works have handcrafted multiplication-free DNNs, which are hardware-efficient but generally inferior to their multiplication-based counterparts in task accuracy, calling for multiplication-reduced hybrid DNNs to marry the best of both worlds. To this end, we propose a Neural Architecture Search and Acceleration (NASA) framework for the above hybrid models, dubbed NASA+, to boost both task accuracy and hardware efficiency. Specifically, NASA+ augments the state-of-the-art (SOTA) search space with multiplication-free operators to construct hybrid ones, and then adopts a novel progressive pretraining strategy to enable the effective search. Furthermore, NASA+ develops a chunk-based accelerator with novel reconfigurable processing elements to better support searched hybrid models, and integrates an auto-mapper to search for optimal dataflows. Experimental results and ablation studies consistently validate the effectiveness of our NASA+ algorithm-hardware co-design framework, e.g., we can achieve up to 65.1% lower energy-delay-product with comparable accuracy over the SOTA multiplication-based system on CIFAR100. Codes are available at https://github.com/GATECH-EIC/NASA .",
                "ieee_keywords": [
                    "Hardware",
                    "Adders",
                    "Task analysis",
                    "Engines",
                    "Computer architecture",
                    "Convolutional codes",
                    "Semiconductor device modeling"
                ],
                "author_keywords": [
                    "Multiplication-reduced hybrid networks",
                    "neural architecture search",
                    "chunk-based accelerator",
                    "reconfigurable PE",
                    "algorithm-hardware co-design"
                ]
            },
            {
                "title": "Hint-Aug: Drawing Hints from Foundation Vision Transformers towards Boosted Few-shot Parameter-Efficient Tuning",
                "link": "https://ieeexplore.ieee.org/document/10205109/",
                "date_of_publication": "22 August 2023",
                "doi": "10.1109/CVPR52729.2023.01068",
                "citations": "2",
                "abstract": "Despite the growing demand for tuningfoundation vision transformers (FViTs) on downstream tasks, fully unleashing FViTs' potential under data-limited scenarios (e.g., few-shot tuning) remains a challenge due to FViTs' data-hungry nature. Common data augmentation techniques fall short in this context due to the limited features contained in the few-shot tuning data. To tackle this challenge, we first identify an opportunity for FViTs in few-shot tuning: pretrained FViTs themselves have already learned highly representative features from large-scale pretraining data, which are fully preserved during widely used parameter-efficient tuning. We thus hypothesize that leveraging those learned features to augment the tuning data can boost the effectiveness of few-shot FViT tuning. To this end, we propose a framework called Hint-based Data Augmentation (Hint-Aug), which aims to boost FViT in few-shot tuning byaugmenting the over-fitted parts of tuning samples with the learned features of pretrained FViTs. Specifically, Hint-Aug integrates two key enablers: (1) an Attentive Over-fitting Detector (AOD) to detect over-confident patches offoundation ViTs for potentially alleviating their over-fitting on the few-shot tuning data and (2) a Confusion-based Feature Infusion (CFI) module to infuse easy-to-confuse features from the pretrained FViTs with the over-confident patches detected by the above AOD in order to enhance the feature diversity during tuning. Extensive experiments and ablation studies on five datasets and three parameter-efficient tuning techniques consistently validate Hint-Aug's effectiveness: 0.04% ~ 32.91 % higher accuracy over the state-of-the-art (SOTA) data augmentation method under various low-shot settings. For example, on the Pet dataset, Hint-Aug achieves a 2.22% higher accuracy with 50% less training data over SOTA data augmentation methods.",
                "ieee_keywords": [
                    "Computer vision",
                    "Training data",
                    "Detectors",
                    "Data augmentation",
                    "Feature extraction",
                    "Transformers",
                    "Boosting"
                ],
                "author_keywords": [
                    "Transfer",
                    "meta",
                    "low-shot",
                    "continual",
                    "or long-tail learning"
                ]
            },
            {
                "title": "Auto-CARD: Efficient and Robust Codec Avatar Driving for Real-time Mobile Telepresence",
                "link": "https://ieeexplore.ieee.org/document/10203405/",
                "date_of_publication": "22 August 2023",
                "doi": "10.1109/CVPR52729.2023.02015",
                "citations": "1",
                "abstract": "Real-time and robust photorealistic avatars for telepresence in AR/VR have been highly desired for enabling im-mersive photorealistic telepresence. However, there still exists one key bottleneck: the considerable computational expense needed to accurately infer facial expressions captured from headset-mounted cameras with a quality level that can match the realism of the avatar's human appearance. To this end, we propose a framework called Auto-CARD, which for the first time enables realtime and robust driving of Codec Avatars when exclusively using merely on-device computing resources. This is achieved by minimizing two sources of redundancy. First, we develop a dedicated neural architecture search technique called AVE-NAS for avatar encoding in AR/VR, which explicitly boosts both the searched architectures' robustness in the presence of extreme facial ex-pressions and hardware friendliness on fast evolving AR/VR headsets. Second, we leverage the temporal redundancy in consecutively captured images during continuous rendering and develop a mechanism dubbed LATEX to skip the computation of redundant frames. Specifically, we first identify an opportunity from the linearity of the latent space derived by the avatar decoder and then propose to perform adaptive latent extrapolation for redundant frames. For evaluation, we demonstrate the efficacy of our Auto-CARD framework in realtime Codec Avatar driving settings, where we achieve a $5.05\\times$ speedup on Meta Quest 2 while maintaining a compa-rable or even better animation quality than state-of-the-art avatar encoder designs.",
                "ieee_keywords": [
                    "Headphones",
                    "Computer vision",
                    "Codecs",
                    "Telepresence",
                    "Avatars",
                    "Redundancy",
                    "Computer architecture"
                ],
                "author_keywords": [
                    "Vision applications and systems"
                ]
            },
            {
                "title": "Castling-ViT: Compressing Self-Attention via Switching Towards Linear-Angular Attention at Vision Transformer Inference",
                "link": "https://ieeexplore.ieee.org/document/10203309/",
                "date_of_publication": "22 August 2023",
                "doi": "10.1109/CVPR52729.2023.01387",
                "citations": "5",
                "abstract": "Vision Transformers (ViTs) have shown impressive per-formance but still require a high computation cost as compared to convolutional neural networks (CNNs), one rea-son is that ViTs' attention measures global similarities and thus has a quadratic complexity with the number of in-put tokens. Existing efficient ViTs adopt local attention or linear attention, which sacrifice ViTs' capabilities of capturing either global or local context. In this work, we ask an important research question: Can ViTs learn both global and local context while being more efficient during inference? To this end, we propose a framework called Castling- ViT, which trains ViTs using both linear-angular attention and masked softmax-based quadratic attention, but then switches to having only linear-angular attention during inference. Our Castling- ViT leverages angular ker-nels to measure the similarities between queries and keys via spectral angles. And we further simplify it with two techniques: (1) a novel linear-angular attention mechanism: we decompose the angular kernels into linear terms and high-order residuals, and only keep the linear terms; and (2) we adopt two parameterized modules to approximate high-order residuals: a depthwise convolution and an aux-iliary masked softmax attention to help learn global and lo-cal information, where the masks for softmax attention are regularized to gradually become zeros and thus incur no overhead during inference. Extensive experiments validate the effectiveness of our Castling- ViT, e.g., achieving up to a 1.8% higher accuracy or 40% MACs reduction on classification and 1.2 higher mAP on detection under comparable FLOPs, as compared to ViTs with vanilla softmax-based at-tentions. Project page is available at here.",
                "ieee_keywords": [
                    "Training",
                    "Computer vision",
                    "Costs",
                    "Convolution",
                    "Deep architecture",
                    "Switches",
                    "Transformers"
                ],
                "author_keywords": [
                    "Deep learning architectures and techniques"
                ]
            }
        ]
    },
    {
        "name": "James Hays",
        "publications": [
            {
                "title": "TextureGAN: Controlling Deep Image Synthesis with Texture Patches",
                "link": "https://ieeexplore.ieee.org/document/8578980/",
                "date_of_publication": "16 December 2018",
                "doi": "10.1109/CVPR.2018.00882",
                "citations": "135",
                "abstract": "In this paper, we investigate deep image synthesis guided by sketch, color, and texture. Previous image synthesis methods can be controlled by sketch and color strokes but we are the first to examine texture control. We allow a user to place a texture patch on a sketch at arbitrary locations and scales to control the desired output texture. Our generative network learns to synthesize objects consistent with these texture suggestions. To achieve this, we develop a local texture loss in addition to adversarial and content loss to train the generative network. We conduct experiments using sketches generated from real images and textures sampled from a separate texture database and results show that our proposed algorithm is able to generate plausible images that are faithful to user controls. Ablation studies show that our proposed pipeline can generate more realistic images than adapting existing methods directly.",
                "ieee_keywords": [
                    "Image generation",
                    "Image color analysis",
                    "Gallium nitride",
                    "Pipelines",
                    "Training",
                    "Task analysis",
                    "Generators"
                ],
                "author_keywords": []
            },
            {
                "title": "Visual Pressure Estimation and Control for Soft Robotic Grippers",
                "link": "https://ieeexplore.ieee.org/document/9982073/",
                "date_of_publication": "26 December 2022",
                "doi": "10.1109/IROS47612.2022.9982073",
                "citations": "149",
                "abstract": "Soft robotic grippers facilitate contact-rich manipulation, including robust grasping of varied objects. Yet the beneficial compliance of a soft gripper also results in significant deformation that can make precision manipulation challenging. We present visual pressure estimation & control (VPEC), a method that infers pressure applied by a soft gripper using an RGB image from an external camera. We provide results for visual pressure inference when a pneumatic gripper and a tendon-actuated gripper make contact with a flat surface. We also show that VPEC enables precision manipulation via closed-loop control of inferred pressure images. In our evaluation, a mobile manipulator (Stretch RE1 from Hello Robot) uses visual servoing to make contact at a desired pressure; follow a spatial pressure trajectory; and grasp small low-profile objects, including a microSD card, a penny, and a pill. Overall, our results show that visual estimates of applied pressure can enable a soft gripper to perform precision manipulation.",
                "ieee_keywords": [
                    "Visualization",
                    "Estimation",
                    "Soft robotics",
                    "Pneumatic systems",
                    "Manipulators",
                    "Visual servoing",
                    "Trajectory"
                ],
                "author_keywords": []
            },
            {
                "title": "Complex Event Recognition from Images with Few Training Examples",
                "link": "https://ieeexplore.ieee.org/document/7926663/",
                "date_of_publication": "15 May 2017",
                "doi": "10.1109/WACV.2017.80",
                "citations": "12",
                "abstract": "We propose to leverage concept-level representations for complex event recognition in photographs given limited training examples. We introduce a novel framework to discover event concept attributes from the web and use that to extract semantic features from images and classify them into social event categories with few training examples. Discovered concepts include a variety of objects, scenes, actions and event sub-types, leading to a discriminative and compact representation for event images. Web images are obtained for each discovered event concept and we use (pretrained) CNN features to train concept classifiers. Extensive experiments on challenging event datasets demonstrate that our proposed method outperforms several baselines using deep CNN features directly in classifying images into events with limited training examples. We also demonstrate that our method achieves the best overall accuracy on a dataset with unseen event categories using a single training example.",
                "ieee_keywords": [
                    "Training",
                    "Flickr",
                    "Image segmentation",
                    "Visualization",
                    "Image recognition",
                    "Feature extraction",
                    "Encyclopedias"
                ],
                "author_keywords": []
            },
            {
                "title": "Argoverse: 3D Tracking and Forecasting With Rich Maps",
                "link": "https://ieeexplore.ieee.org/document/8953693/",
                "date_of_publication": "09 January 2020",
                "doi": "10.1109/CVPR.2019.00895",
                "citations": "493",
                "abstract": "We present Argoverse, a dataset designed to support autonomous vehicle perception tasks including 3D tracking and motion forecasting. Argoverse includes sensor data collected by a fleet of autonomous vehicles in Pittsburgh and Miami as well as 3D tracking annotations, 300k extracted interesting vehicle trajectories, and rich semantic maps. The sensor data consists of 360 degree images from 7 cameras with overlapping fields of view, forward-facing stereo imagery, 3D point clouds from long range LiDAR, and 6-DOF pose. Our 290km of mapped lanes contain rich geometric and semantic metadata which are not currently available in any public dataset. All data is released under a Creative Commons license at Argoverse.org. In baseline experiments, we use map information such as lane direction, driveable area, and ground height to improve the accuracy of 3D object tracking. We use 3D object tracking to mine for more than 300k interesting vehicle trajectories to create a trajectory forecasting benchmark. Motion forecasting experiments ranging in complexity from classical methods (k-NN) to LSTMs demonstrate that using detailed vector maps with lane-level information substantially reduces prediction error. Our tracking and forecasting experiments represent only a superficial exploration of the potential of rich maps in robotic perception. We hope that Argoverse will enable the research community to explore these problems in greater depth.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Geometry-Aware Learning of Maps for Camera Localization",
                "link": "https://ieeexplore.ieee.org/document/8578375/",
                "date_of_publication": "16 December 2018",
                "doi": "10.1109/CVPR.2018.00277",
                "citations": "187",
                "abstract": "Maps are a key component in image-based camera localization and visual SLAM systems: they are used to establish geometric constraints between images, correct drift in relative pose estimation, and relocalize cameras after lost tracking. The exact definitions of maps, however, are often application-specific and hand-crafted for different scenarios (e.g. 3D landmarks, lines, planes, bags of visual words). We propose to represent maps as a deep neural net called MapNet, which enables learning a data-driven map representation. Unlike prior work on learning maps, MapNet exploits cheap and ubiquitous sensory inputs like visual odometry and GPS in addition to images and fuses them together for camera localization. Geometric constraints expressed by these inputs, which have traditionally been used in bundle adjustment or pose-graph optimization, are formulated as loss terms in MapNet training and also used during inference. In addition to directly improving localization accuracy, this allows us to update the MapNet (i.e., maps) in a self-supervised manner using additional unlabeled video sequences from the scene. We also propose a novel parameterization for camera rotation which is better suited for deep-learning based camera pose regression. Experimental results on both the indoor 7-Scenes dataset and the outdoor Oxford RobotCar dataset show significant performance improvement over prior work. The MapNet project webpage is https://goo.gl/mRB3Au.",
                "ieee_keywords": [
                    "Cameras",
                    "Visualization",
                    "Robot vision systems",
                    "Simultaneous localization and mapping",
                    "Three-dimensional displays",
                    "Visual odometry",
                    "Training"
                ],
                "author_keywords": []
            },
            {
                "title": "SketchyGAN: Towards Diverse and Realistic Sketch to Image Synthesis",
                "link": "https://ieeexplore.ieee.org/document/8579079/",
                "date_of_publication": "16 December 2018",
                "doi": "10.1109/CVPR.2018.00981",
                "citations": "173",
                "abstract": "Synthesizing realistic images from human drawn sketches is a challenging problem in computer graphics and vision. Existing approaches either need exact edge maps, or rely on retrieval of existing photographs. In this work, we propose a novel Generative Adversarial Network (GAN) approach that synthesizes plausible images from 50 categories including motorcycles, horses and couches. We demonstrate a data augmentation technique for sketches which is fully automatic, and we show that the augmented data is helpful to our task. We introduce a new network building block suitable for both the generator and discriminator which improves the information flow by injecting the input image at multiple scales. Compared to state-of-the-art image translation methods, our approach generates more realistic images and achieves significantly higher Inception Scores.",
                "ieee_keywords": [
                    "Image edge detection",
                    "Image generation",
                    "Gallium nitride",
                    "Training",
                    "Databases",
                    "Task analysis",
                    "Generative adversarial networks"
                ],
                "author_keywords": []
            },
            {
                "title": "MSeg: A Composite Dataset for Multi-Domain Semantic Segmentation",
                "link": "https://ieeexplore.ieee.org/document/9157628/",
                "date_of_publication": "05 August 2020",
                "doi": "10.1109/CVPR42600.2020.00295",
                "citations": "53",
                "abstract": "We present MSeg, a composite dataset that unifies se- mantic segmentation datasets from different domains. A naive merge of the constituent datasets yields poor performance due to inconsistent taxonomies and annotation practices. We reconcile the taxonomies and bring the pixel-level annotations into alignment by relabeling more than 220,000 object masks in more than 80,000 images. The resulting composite dataset enables training a single semantic segmentation model that functions effectively across domains and generalizes to datasets that were not seen during training. We adopt zero-shot cross-dataset transfer as a benchmark to systematically evaluate a model's robustness and show that MSeg training yields substantially more robust models in comparison to training on individual datasets or naive mixing of datasets without the presented contributions. A model trained on MSeg ranks first on the WildDash leaderboard for robust semantic segmentation, with no exposure to WildDash data during training.",
                "ieee_keywords": [
                    "Training",
                    "Taxonomy",
                    "Semantics",
                    "Computational modeling",
                    "Visualization",
                    "Image segmentation",
                    "Robustness"
                ],
                "author_keywords": []
            },
            {
                "title": "DeepNav: Learning to Navigate Large Cities",
                "link": "https://ieeexplore.ieee.org/document/8099812/",
                "date_of_publication": "09 November 2017",
                "doi": "10.1109/CVPR.2017.329",
                "citations": "28",
                "abstract": "We present DeepNav, a Convolutional Neural Network (CNN) based algorithm for navigating large cities using locally visible street-view images. The DeepNav agent learns to reach its destination quickly by making the correct navigation decisions at intersections. We collect a large-scale dataset of street-view images organized in a graph where nodes are connected by roads. This dataset contains 10 city graphs and a total of more than 1 million street-view images. We propose 3 supervised learning approaches for the navigation task, and show how A* search in the city graph can be used to generate labels for the images. Our annotation process is fully automated using publicly available mapping services, and requires no human input. We evaluate the proposed DeepNav models on 4 held-out cities for navigating to 5 different types of destinations and show that our algorithms outperform previous work that uses hand-crafted features and Support Vector Regression (SVR) [19].",
                "ieee_keywords": [
                    "Urban areas",
                    "Navigation",
                    "Roads",
                    "Training",
                    "Robots",
                    "Buildings"
                ],
                "author_keywords": []
            },
            {
                "title": "Pixel-aligned Volumetric Avatars",
                "link": "https://ieeexplore.ieee.org/document/9577448/",
                "date_of_publication": "02 November 2021",
                "doi": "10.1109/CVPR46437.2021.01156",
                "citations": "16",
                "abstract": "Acquisition and rendering of photo-realistic human heads is a highly challenging research problem of particular importance for virtual telepresence. Currently, the highest quality is achieved by volumetric approaches trained in a person-specific manner on multi-view data. These models better represent fine structure, such as hair, compared to simpler mesh-based models. Volumetric models typically employ a global code to represent facial expressions, such that they can be driven by a small set of animation parameters. While such architectures achieve impressive rendering quality, they can not easily be extended to the multi-identity setting. In this paper, we devise a novel approach for predicting volumetric avatars of the human head given just a small number of inputs. We enable generalization across identities by a novel parameterization that combines neural radiance fields with local, pixel-aligned features extracted directly from the inputs, thus side-stepping the need for very deep or complex networks. Our approach is trained in an end-to-end manner solely based on a photometric re-rendering loss without requiring explicit 3D supervision. We demonstrate that our approach outperforms the existing state of the art in terms of quality and is able to generate faithful facial expressions in a multi-identity setting.",
                "ieee_keywords": [
                    "Hair",
                    "Computer vision",
                    "Head",
                    "Three-dimensional displays",
                    "Telepresence",
                    "Avatars",
                    "Neural networks"
                ],
                "author_keywords": []
            },
            {
                "title": "StuffNet: Using ‘Stuff’ to Improve Object Detection",
                "link": "https://ieeexplore.ieee.org/document/7926692/",
                "date_of_publication": "15 May 2017",
                "doi": "10.1109/WACV.2017.109",
                "citations": "11",
                "abstract": "We propose a Convolutional Neural Network (CNN) based algorithm - StuffNet - for object detection. In addition to the standard convolutional features trained for region proposal and object detection [33], StuffNet uses convolutional features trained for segmentation of objects and 'stuff' (amorphous categories such as ground and water). Through experiments on Pascal VOC 2010, we show the importance of features learnt from stuff segmentation for improving object detection performance. StuffNet improves performance from 18.8% mAP to 23.9% mAP for small objects. We also devise a method to train StuffNet on datasets that do not have stuff segmentation labels. Through experiments on Pascal VOC 2007 and 2012, we demonstrate the effectiveness of this method and show that StuffNet also significantly improves object detection performance on such datasets.",
                "ieee_keywords": [
                    "Object detection",
                    "Proposals",
                    "Feature extraction",
                    "Training",
                    "Image segmentation",
                    "Semantics",
                    "Context"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "seth hutchinson",
        "publications": [
            {
                "title": "Integrated Task and Motion Planning for Safe Legged Navigation in Partially Observable Environments",
                "link": "https://ieeexplore.ieee.org/document/10215076/",
                "date_of_publication": null,
                "doi": "10.1109/TRO.2023.3299524",
                "citations": "94",
                "abstract": "This study proposes a hierarchically integrated framework for safe task and motion planning (TAMP) of bipedal locomotion in a partially observable environment with dynamic obstacles and uneven terrain. The high-level task planner employs linear temporal logic for a reactive game synthesis between the robot and its environment and provides a formal guarantee on navigation safety and task completion. To address environmental partial observability, a belief abstraction model is designed by partitioning the environment into multiple belief regions and employed at the high-level navigation planner to estimate the dynamic obstacles' location. This additional location information of dynamic obstacles offered by belief abstraction enables less conservative long-horizon navigation actions beyond guaranteeing immediate collision avoidance. Accordingly, a synthesized action planner sends a set of locomotion actions to the middle-level motion planner while incorporating safe locomotion specifications extracted from safety theorems based on a reduced-order model (ROM) of the locomotion process. The motion planner employs the ROM to design safety criteria and a sampling algorithm to generate nonperiodic motion plans that accurately track high-level actions. At the low level, a foot placement controller based on an angular-momentum linear inverted pendulum model is implemented and integrated with an ankle-actuated passivity-based controller for full-body trajectory tracking. To address external perturbations, this study also investigates the safe sequential composition of the keyframe locomotion state and achieves robust transitions against external perturbations through reachability analysis. The overall TAMP framework is validated with extensive simulations and hardware experiments on bipedal walking robots Cassie and Digit designed by Agility Robotics.",
                "ieee_keywords": [
                    "Navigation",
                    "Planning",
                    "Robots",
                    "Task analysis",
                    "Safety",
                    "Dynamics",
                    "Collision avoidance"
                ],
                "author_keywords": [
                    "Formal methods in robotics and automation",
                    "humanoid and bipedal locomotion",
                    "motion and path planning",
                    "task planning"
                ]
            },
            {
                "title": "GTGraffiti: Spray Painting Graffiti Art from Human Painting Motions with a Cable Driven Parallel Robot",
                "link": "https://ieeexplore.ieee.org/document/9812008/",
                "date_of_publication": "12 July 2022",
                "doi": "10.1109/ICRA46639.2022.9812008",
                "citations": "3",
                "abstract": "We present GTGraffiti, a graffiti painting system from Georgia Tech that tackles challenges in art, hardware, and human-robot collaboration. The problem of painting graffiti in a human style is particularly challenging and requires a system-level approach because the robotics and art must be designed around each other. The robot must be highly dynamic over a large workspace while the artist must work within the robot's limitations. Our approach consists of three stages: artwork capture, robot hardware, and planning & control. We use motion capture to capture collaborator painting motions which are then composed and processed into a time-varying linear feedback controller for a cable-driven parallel robot (CDPR) to execute. In this work, we will describe the capturing process, the design and construction of a purpose-built CDPR, and the software for turning an artist's vision into control commands. Our work represents an important step towards faithfully recreating human graffiti artwork by demonstrating that we can reproduce artist motions up to 2m/s and 20m/s 2 within 9.3mm RMSE to paint artworks.",
                "ieee_keywords": [
                    "Parallel robots",
                    "Art",
                    "Process control",
                    "Turning",
                    "Hardware",
                    "Software",
                    "Motion capture"
                ],
                "author_keywords": []
            },
            {
                "title": "Momentum-Aware Trajectory Optimization and Control for Agile Quadrupedal Locomotion",
                "link": "https://ieeexplore.ieee.org/document/9804764/",
                "date_of_publication": null,
                "doi": "10.1109/LRA.2022.3185374",
                "citations": "1",
                "abstract": "In this letter, we present a versatile hierarchical offline planning algorithm, along with an online control pipeline for agile quadrupedal locomotion. Our offline planner alternates between optimizing centroidal dynamics for a reduced-order model and whole-body trajectory optimization, with the aim of achieving dynamics consensus. Our novel momentum-inertia-aware centroidal optimization, which uses an equimomental ellipsoid parameterization, is able to generate highly acrobatic motions via “inertia shaping”. Our whole-body optimization approach significantly improves upon the quality of standard DDP-based approaches by iteratively exploiting feedback from the centroidal level. For online control, we have developed a novel convex model predictive control scheme through a linear transformation of the full centroidal dynamics. Our controller can efficiently optimize for both contact forces and joint accelerations in single optimization, enabling more straightforward tracking for momentum-rich motions compared to existing quadrupedal MPC controllers. We demonstrate the capability and generality of our trajectory planner on four different dynamic maneuvers. We then present one hardware experiment on the MIT Mini Cheetah platform to demonstrate the performance of the entire planning and control pipeline on a twisting jump maneuver.",
                "ieee_keywords": [
                    "Dynamics",
                    "Tracking",
                    "Ellipsoids",
                    "Trajectory optimization",
                    "Standards",
                    "Planning",
                    "Tensors"
                ],
                "author_keywords": [
                    "Whole-body motion planning and control",
                    "legged robots",
                    "optimization and optimal control"
                ]
            },
            {
                "title": "Controlling Collision-Induced Aggregations in a Swarm of Micro Bristle Robots",
                "link": "https://ieeexplore.ieee.org/document/9835143/",
                "date_of_publication": null,
                "doi": "10.1109/TRO.2022.3189846",
                "citations": "1",
                "abstract": "Systematically designing local interaction rules to achieve collective behaviors in robot swarms is a challenging endeavor, especially in micro robots, where size restrictions imply severe sensing, communication, and computation limitations. In such robot swarms, performing useful functions is often preconditioned on the formation of high-density aggregations which can facilitate collective signaling and information sharing. In this article, we present a systematic approach to control aggregation behaviors by leveraging the physical interactions in a swarm of 300 3-mm vibration-driven micro bristle robots that we designed and fabricated. We demonstrate the ability to control the degree of aggregation by varying the motility characteristics of the robots through global vibration frequency and amplitude inputs, after comprehensive characterization, modeling, and simulation of the locomotion dynamics and robot interactions. To quantify the degree of aggregation, we also introduce a new metric, the motility-induced phase separation index index, which unlike many existing methods does not require a scenario-specific tuning of parameters. Our investigations reveal how physics-driven interaction mechanisms can be exploited to achieve desired behaviors in minimally equipped robot swarms and highlight the specific ways in which hardware and software developments aid in the achievement of collision-induced aggregations.",
                "ieee_keywords": [
                    "Robots",
                    "Collision avoidance",
                    "Robot kinematics",
                    "Substrates",
                    "Vibrations",
                    "Behavioral sciences",
                    "Robot sensing systems"
                ],
                "author_keywords": [
                    "Aggregation control",
                    "micro/nano robots",
                    "multirobot systems",
                    "swarms"
                ]
            },
            {
                "title": "A Study of a Class of Vibration-Driven Robots: Modeling, Analysis, Control and Design of the Brushbot",
                "link": "https://ieeexplore.ieee.org/document/8968490/",
                "date_of_publication": "28 January 2020",
                "doi": "10.1109/IROS40897.2019.8968490",
                "citations": "8",
                "abstract": "In this paper we present a study of a specific class of vibration-driven robots: the brushbots. In a bottom-up fashion, we start by deriving dynamic models of the brushes and we discuss the conditions under which these models can be employed to describe the motion of brushbots. Then, we present two designs of brushbots: a fully-actuated platform and a differential-drive-like one. The former is employed to experimentally validate both the developed theoretical models and the devised motion control algorithms. Finally, a coordinated-control algorithm is implemented on a swarm of differential-drive-like brushbots in order to demonstrate the design simplicity and robustness that can be achieved by employing a vibration-based locomotion strategy.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Extending Riemmanian Motion Policies to a Class of Underactuated Wheeled-Inverted-Pendulum Robots",
                "link": "https://ieeexplore.ieee.org/document/9196866/",
                "date_of_publication": "15 September 2020",
                "doi": "10.1109/ICRA40945.2020.9196866",
                "citations": "6",
                "abstract": "Riemannian Motion Policies (RMPs) have recently been introduced as a way to specify second-order motion policies defined on robot task spaces. RMP-based approaches have the advantage of being more general than traditional approaches based on operational space control; for example, the generalized task inertia in an RMP can be fully state-dependent, which is particularly effective in designing collision avoidance bahaviors. But until now RMPs have been applied only to fully actuated systems, i.e. systems for which each degree of freedom (DoF) can be directly actuated by a control input. In this paper, we present a method that extends the RMP formalism to a class of underacutated systems whose dynamics are amenable to a decomposition into a fully-actuated subsystem and a residual dynamics. We show the efficacy of the approach by constructing a suitable decomposition for a Wheeled-Inverted-Pendulum (WIP) humanoid robot and applying our method to derive motion policies for combined locomotion and manipulation tasks. Simulation results are presented for a 7-DoF system with one degree of underactuation.",
                "ieee_keywords": [
                    "Task analysis",
                    "Manifolds",
                    "Manipulator dynamics",
                    "Aerospace electronics",
                    "Humanoid robots",
                    "Measurement"
                ],
                "author_keywords": []
            },
            {
                "title": "TIE: Time-Informed Exploration for Robot Motion Planning",
                "link": "https://ieeexplore.ieee.org/document/9372896/",
                "date_of_publication": null,
                "doi": "10.1109/LRA.2021.3064255",
                "citations": "4",
                "abstract": "Anytime sampling-based methods are an attractive technique for solving kino-dynamic motion planning problems. These algorithms scale well to higher dimensions and can efficiently handle state and control constraints. However, an intelligent exploration strategy is required to accelerate their convergence and avoid redundant computations. Using ideas from reachability analysis, this work defines a “Time-Informed Set (TIS),” that focuses the search for time-optimal kino-dynamic planning after an initial solution is found. Such a Time-Informed Set includes all trajectories that can potentially improve the current best solution and hence exploration outside this set is redundant. Benchmarking experiments show that an exploration strategy based on the TIS can accelerate the convergence of sampling-based kino-dynamic motion planners.",
                "ieee_keywords": [
                    "Planning",
                    "Trajectory",
                    "Heuristic algorithms",
                    "Aerospace electronics",
                    "Reachability analysis",
                    "Convergence",
                    "Two dimensional displays"
                ],
                "author_keywords": [
                    "Autonomous agents",
                    "collision avoidance",
                    "motion and path planning"
                ]
            },
            {
                "title": "Robot Calligraphy using Pseudospectral Optimal Control in Conjunction with a Novel Dynamic Brush Model",
                "link": "https://ieeexplore.ieee.org/document/9341787/",
                "date_of_publication": "10 February 2021",
                "doi": "10.1109/IROS45743.2020.9341787",
                "citations": "6",
                "abstract": "Chinese calligraphy is a unique art form with great artistic value but difficult to master. In this paper, we formulate the calligraphy writing problem as a trajectory optimization problem, and propose an improved virtual brush model for simulating the real writing process. Our approach is inspired by pseudospectral optimal control in that we parameterize the actuator trajectory for each stroke as a Chebyshev polynomial. The proposed dynamic virtual brush model plays a key role in formulating the objective function to be optimized. Our approach shows excellent performance in drawing aesthetically pleasing characters, and does so much more efficiently than previous work, opening up the possibility to achieve real-time closed-loop control.",
                "ieee_keywords": [
                    "Brushes",
                    "Optimal control",
                    "Writing",
                    "Soft robotics",
                    "Linear programming",
                    "Real-time systems",
                    "Trajectory optimization"
                ],
                "author_keywords": [
                    "Motion and Path Planning",
                    "Optimization and Optimal Control",
                    "Modeling",
                    "Control",
                    "Learning for Soft Robots"
                ]
            },
            {
                "title": "An Interleaved Approach to Trait-Based Task Allocation and Scheduling",
                "link": "https://ieeexplore.ieee.org/document/9636569/",
                "date_of_publication": "16 December 2021",
                "doi": "10.1109/IROS51168.2021.9636569",
                "citations": "4",
                "abstract": "To realize effective heterogeneous multi-robot teams, researchers must leverage individual robots’ relative strengths and coordinate their individual behaviors. Specifically, heterogeneous multi-robot systems must answer three important questions: who (task allocation), when (scheduling), and how (motion planning). While specific variants of each of these problems are known to be NP-Hard, their interdependence only exacerbates the challenges involved in solving them together. In this paper, we present a novel framework that interleaves task allocation, scheduling, and motion planning. We introduce a search-based approach for trait-based time-extended task allocation named Incremental Task Allocation Graph Search (ITAGS). In contrast to approaches that solve the three problems in sequence, ITAGS’s interleaved approach enables efficient search for allocations while simultaneously satisfying scheduling constraints and accounting for the time taken to execute motion plans. To enable effective interleaving, we develop a convex combination of two search heuristics that optimizes the satisfaction of task requirements as well as the makespan of the associated schedule. We demonstrate the efficacy of ITAGS using detailed ablation studies and comparisons against two state-of-the-art algorithms in a simulated emergency response domain.",
                "ieee_keywords": [
                    "Schedules",
                    "Robot kinematics",
                    "Emergency services",
                    "Search problems",
                    "Resource management",
                    "Multi-robot systems",
                    "Iterative methods"
                ],
                "author_keywords": []
            },
            {
                "title": "Non-Uniform Robot Densities in Vibration Driven Swarms Using Phase Separation Theory",
                "link": "https://ieeexplore.ieee.org/document/8967985/",
                "date_of_publication": "28 January 2020",
                "doi": "10.1109/IROS40897.2019.8967985",
                "citations": "7",
                "abstract": "In robot swarms operating under highly restrictive sensing and communication constraints, individuals may need to use direct physical proximity to facilitate information exchange. However, in certain task-related scenarios, this requirement might conflict with the need for robots to spread out in the environment, e.g., for distributed sensing or surveillance applications. This paper demonstrates how a swarm of minimally-equipped robots can form high-density robot aggregates that coexist with lower robot densities in space. We envision a scenario where a swarm of vibration-driven robots-which sit atop bristles and achieve directed motion by vibrating them-move randomly in an environment while colliding with each other. Theoretical techniques from the study of far-from-equilibrium collectives and statistical mechanics clarify the mechanisms underlying the formation of these high and low density regions. Specifically, we capitalize on a transformation that connects the collective properties of a system of self-propelled particles with that of a well-studied molecular fluid system, thereby inheriting the rich theory of equilibrium thermodynamics. Real robot experiments as well as simulations illustrate how inter-robot collisions can precipitate the formation of non-uniform robot densities in a closed and bounded region.",
                "ieee_keywords": [],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Evangelos Theodorou",
        "publications": [
            {
                "title": "Information theoretic MPC for model-based reinforcement learning",
                "link": "https://ieeexplore.ieee.org/document/7989202/",
                "date_of_publication": "24 July 2017",
                "doi": "10.1109/ICRA.2017.7989202",
                "citations": "187",
                "abstract": "We introduce an information theoretic model predictive control (MPC) algorithm capable of handling complex cost criteria and general nonlinear dynamics. The generality of the approach makes it possible to use multi-layer neural networks as dynamics models, which we incorporate into our MPC algorithm in order to solve model-based reinforcement learning tasks. We test the algorithm in simulation on a cart-pole swing up and quadrotor navigation task, as well as on actual hardware in an aggressive driving task. Empirical results demonstrate that the algorithm is capable of achieving a high level of performance and does so only utilizing data collected from the system.",
                "ieee_keywords": [
                    "Robots",
                    "Heuristic algorithms",
                    "Trajectory",
                    "Learning (artificial intelligence)",
                    "Cost function",
                    "Optimal control"
                ],
                "author_keywords": []
            },
            {
                "title": "Vision-Based High-Speed Driving With a Deep Dynamic Observer",
                "link": "https://ieeexplore.ieee.org/document/8630018/",
                "date_of_publication": null,
                "doi": "10.1109/LRA.2019.2896449",
                "citations": "28",
                "abstract": "In this letter, we present a framework for combining deep learning-based road detection, particle filters, and model predictive control (MPC) to drive aggressively using only a monocular camera, IMU, and wheel speed sensors. This framework uses deep convolutional neural networks combined with LSTMs to learn a local cost map representation of the track in front of the vehicle. A particle filter uses this dynamic observation model to localize in a schematic map, and MPC is used to drive aggressively using this particle filter based state estimate. We show extensive real world testing results and demonstrate reliable operation of the vehicle at the friction limits on a complex dirt track. We reach speeds above 27 m/h (12 m/s) on a dirt track with a 105 ft (32 m) long straight using our 1:5 scale test vehicle.",
                "ieee_keywords": [
                    "Cameras",
                    "Vehicle dynamics",
                    "Wheels",
                    "Neural networks",
                    "Computer architecture",
                    "Predictive control",
                    "Sensors"
                ],
                "author_keywords": [
                    "Deep learning in robotics and automation",
                    "autonomous vehicle navigation",
                    "localization",
                    "computer vision for transportation"
                ]
            },
            {
                "title": "Cross-entropy optimization for neuromodulation",
                "link": "https://ieeexplore.ieee.org/document/7592182/",
                "date_of_publication": "18 October 2016",
                "doi": "10.1109/EMBC.2016.7592182",
                "citations": "1",
                "abstract": "This study presents a reinforcement learning approach for the optimization of the proportional-integral gains of the feedback controller represented in a computational model of epilepsy. The chaotic oscillator model provides a feedback control systems view of the dynamics of an epileptic brain with an internal feedback controller representative of the natural seizure suppression mechanism within the brain circuitry. Normal and pathological brain activity is simulated in this model by adjusting the feedback gain values of the internal controller. With insufficient gains, the internal controller cannot provide enough feedback to the brain dynamics causing an increase in correlation between different brain sites. This increase in synchronization results in the destabilization of the brain dynamics, which is representative of an epileptic seizure. To provide compensation for an insufficient internal controller an external controller is designed using proportional-integral feedback control strategy. A cross-entropy optimization algorithm is applied to the chaotic oscillator network model to learn the optimal feedback gains for the external controller instead of hand-tuning the gains to provide sufficient control to the pathological brain and prevent seizure generation. The correlation between the dynamics of neural activity within different brain sites is calculated for experimental data to show similar dynamics of epileptic neural activity as simulated by the network of chaotic oscillators.",
                "ieee_keywords": [
                    "Oscillators",
                    "Brain modeling",
                    "Correlation",
                    "Couplings",
                    "Pathology",
                    "Computational modeling",
                    "Epilepsy"
                ],
                "author_keywords": []
            },
            {
                "title": "Game Theoretic continuous time Differential Dynamic Programming",
                "link": "https://ieeexplore.ieee.org/document/7172215/",
                "date_of_publication": "30 July 2015",
                "doi": "10.1109/ACC.2015.7172215",
                "citations": "12",
                "abstract": "In this work, we derive a Game Theoretic Differential Dynamic Programming (GT-DDP) algorithm in continuous time. We provide a set of backward differential equations for the value function expansion without assuming closeness of the initial nominal control to the optimal control solution, and derive the update law for the controls. We introduce the GT-DDP algorithm and analyze the effect of the game theoretic formulation in the feed-forward and feedback parts of the control policies. Furthermore, we investigate the performance of GT-DDP through simulations on the inverted pendulum with conflicting controls and we apply the control gains on a stochastic system to demonstrate the effect of the design of the cost function to the feed-forward and feedback parts of the control policies. Finally, we conclude with some possible future directions.",
                "ieee_keywords": [
                    "Trajectory",
                    "Heuristic algorithms",
                    "Differential equations",
                    "Optimal control",
                    "Cost function",
                    "Games",
                    "Dynamic programming"
                ],
                "author_keywords": []
            },
            {
                "title": "Aggressive driving with model predictive path integral control",
                "link": "https://ieeexplore.ieee.org/document/7487277/",
                "date_of_publication": "09 June 2016",
                "doi": "10.1109/ICRA.2016.7487277",
                "citations": "141",
                "abstract": "In this paper we present a model predictive control algorithm designed for optimizing non-linear systems subject to complex cost criteria. The algorithm is based on a stochastic optimal control framework using a fundamental relationship between the information theoretic notions of free energy and relative entropy. The optimal controls in this setting take the form of a path integral, which we approximate using an efficient importance sampling scheme. We experimentally verify the algorithm by implementing it on a Graphics Processing Unit (GPU) and apply it to the problem of controlling a fifth-scale Auto-Rally vehicle in an aggressive driving task.",
                "ieee_keywords": [
                    "Trajectory",
                    "Optimal control",
                    "Entropy",
                    "Vehicles",
                    "Prediction algorithms",
                    "Q measurement",
                    "Stochastic processes"
                ],
                "author_keywords": []
            },
            {
                "title": "A Multi-step Dynamics Modeling Framework For Autonomous Driving In Multiple Environments",
                "link": "https://ieeexplore.ieee.org/document/10161330/",
                "date_of_publication": "04 July 2023",
                "doi": "10.1109/ICRA48891.2023.10161330",
                "citations": "70",
                "abstract": "Modeling dynamics is often the first step to making a vehicle autonomous. While on-road autonomous vehicles have been extensively studied, off-road vehicles pose many challenging modeling problems. An off-road vehicle encounters highly complex and difficult-to-model terrain/vehicle interactions, as well as having complex vehicle dynamics of its own. These complexities can create challenges for effective high-speed control and planning. In this paper, we introduce a framework for multistep dynamics prediction that explicitly handles the accumulation of modeling error and remains scalable for sampling-based controllers. Our method uses a specially-initialized Long Short-Term Memory (LSTM) over a limited time horizon as the learned component in a hybrid model to predict the dynamics of a 4-person seating all-terrain vehicle (Polaris S4 1000 RZR) in two distinct environments. By only having the LSTM predict over a fixed time horizon, we negate the need for long term stability that is often a challenge when training recurrent neural networks. Our framework is flexible as it only requires odometry information for labels. Through extensive experimentation, we show that our method is able to predict millions of possible trajectories in real-time, with a time horizon of five seconds in challenging off road driving scenarios.",
                "ieee_keywords": [
                    "Training",
                    "Recurrent neural networks",
                    "Roads",
                    "Predictive models",
                    "Real-time systems",
                    "Stability analysis",
                    "Trajectory"
                ],
                "author_keywords": []
            },
            {
                "title": "Robust Model Predictive Path Integral Control: Analysis and Performance Guarantees",
                "link": "https://ieeexplore.ieee.org/document/9349120/",
                "date_of_publication": null,
                "doi": "10.1109/LRA.2021.3057563",
                "citations": "16",
                "abstract": "In this letter we propose a novel decision making architecture for Robust Model-Predictive Path Integral Control (RMPPI) and investigate its performance guarantees and applicability to off-road navigation. Key building blocks of the proposed architecture are an augmented state space representation of the system consisting of nominal and actual dynamics, a placeholder for different types of tracking controllers, a safety logic for nominal state propagation, and an importance sampling scheme that takes into account the capabilities of the underlying tracking control. Using these ingredients, we derive a bound on the free energy growth of the dynamical system which is a function of task constraint satisfaction level, the performance of the underlying tracking controller, and the sampling error of the stochastic optimization used within RMPPI. To validate the bound on free energy growth, we perform experiments in simulation using two types of tracking controllers, namely the iterative Linear Quadratic Gaussian and Contraction-Metric based control. We further demonstrate the applicability of RMPPI in real hardware using the GT AutoRally vehicle. Our experiments demonstrate that RMPPI outperforms MPPI and Tube-MPPI by alleviating issues of the aforementioned model predictive controllers related to either lack of robustness or excessive conservatism. RMPPI provides the best of the two worlds in terms of agility and robustness to disturbances.",
                "ieee_keywords": [
                    "Robustness",
                    "Predictive models",
                    "Trajectory",
                    "Aerospace electronics",
                    "Adaptive control",
                    "Task analysis",
                    "Stochastic processes"
                ],
                "author_keywords": [
                    "Optimization and optimal control",
                    "field robots",
                    "model predictive control",
                    "stochastic optimization"
                ]
            },
            {
                "title": "Best Response Model Predictive Control for Agile Interactions Between Autonomous Ground Vehicles",
                "link": "https://ieeexplore.ieee.org/document/8462831/",
                "date_of_publication": "13 September 2018",
                "doi": "10.1109/ICRA.2018.8462831",
                "citations": "22",
                "abstract": "We introduce an algorithm for autonomous control of multiple fast ground vehicles operating in close proximity to each other. The algorithm is based on a combination of the game theoretic notion of iterated best response, and an information theoretic model predictive control algorithm designed for non-linear stochastic systems. We test the algorithm on two one-fifth scale AutoRally platforms traveling at speeds upwards of 8 meters per second, while maintaining a following distance of under two meters from bumper-to-bumper.",
                "ieee_keywords": [
                    "Games",
                    "Stochastic processes",
                    "Predictive control",
                    "Nash equilibrium",
                    "Optimization",
                    "Vehicle dynamics",
                    "Prediction algorithms"
                ],
                "author_keywords": []
            },
            {
                "title": "Deep Forward-Backward SDEs for Min-max Control",
                "link": "https://ieeexplore.ieee.org/document/9028871/",
                "date_of_publication": "12 March 2020",
                "doi": "10.1109/CDC40024.2019.9028871",
                "citations": "4",
                "abstract": "This paper presents a novel approach to numerically solve stochastic differential games for nonlinear systems. The proposed approach relies on the nonlinear Feynman-Kac theorem that establishes a connection between parabolic deterministic partial differential equations and forward-backward stochastic differential equations. Using this theorem the Hamilton-Jacobi-Isaacs partial differential equation associated with differential games is represented by a system of forward-backward stochastic differential equations. Numerical solution of the aforementioned system of stochastic differential equations is performed using importance sampling and a neural network with Long Short-Term Memory and Fully Connected layers. The resulting algorithm is tested on two example systems in simulation and compared against the standard risk neutral stochastic optimal control formulations.",
                "ieee_keywords": [
                    "Optimal control",
                    "Games",
                    "Heuristic algorithms",
                    "Monte Carlo methods",
                    "Mathematical model",
                    "Differential equations",
                    "Standards"
                ],
                "author_keywords": []
            },
            {
                "title": "Data-driven differential dynamic programming using Gaussian processes",
                "link": "https://ieeexplore.ieee.org/document/7172032/",
                "date_of_publication": "30 July 2015",
                "doi": "10.1109/ACC.2015.7172032",
                "citations": "6",
                "abstract": "We present a Bayesian nonparametric trajectory optimization framework for systems with unknown dynamics using Gaussian Processes (GPs), called Gaussian Process Differential Dynamic Programming (GPDDP). Rooted in the Dynamic Programming principle and second-order local approximations of the value function, GPDDP learns time-varying optimal control policies from sampled data. Based on this framework, we propose two algorithms for implementations. We demonstrate the effectiveness and efficiency of the proposed framework using three numerical examples.",
                "ieee_keywords": [
                    "Trajectory",
                    "Optimal control",
                    "Heuristic algorithms",
                    "Helicopters",
                    "Approximation methods",
                    "Dynamic programming",
                    "Approximation algorithms"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Sonia Chernova",
        "publications": [
            {
                "title": "Anticipatory Human-Robot Collaboration via Multi-Objective Trajectory Optimization",
                "link": "https://ieeexplore.ieee.org/document/9341058/",
                "date_of_publication": "10 February 2021",
                "doi": "10.1109/IROS45743.2020.9341058",
                "citations": "2",
                "abstract": "We address the problem of adapting robot trajectories to improve safety, comfort, and efficiency in humanrobot collaborative tasks. To this end, we propose CoMOTO, a trajectory optimization framework that utilizes stochastic motion prediction to anticipate the human's motion and adapt the robot's joint trajectory accordingly. We design a multiobjective cost function that simultaneously optimizes for i) separation distance, ii) visibility of the end-effector, iii) legibility, iv) efficiency, and v) smoothness. We evaluate CoMOTO against three existing methods for robot trajectory generation when in close proximity to humans. Our experimental results indicate that our approach consistently outperforms existing methods over a combined set of safety, comfort, and efficiency metrics.",
                "ieee_keywords": [
                    "Measurement",
                    "Collaboration",
                    "Real-time systems",
                    "Safety",
                    "Task analysis",
                    "Trajectory optimization",
                    "Intelligent robots"
                ],
                "author_keywords": []
            },
            {
                "title": "Human Trust After Robot Mistakes: Study of the Effects of Different Forms of Robot Communication",
                "link": "https://ieeexplore.ieee.org/document/8956424/",
                "date_of_publication": "13 January 2020",
                "doi": "10.1109/RO-MAN46459.2019.8956424",
                "citations": "8",
                "abstract": "Collaborative robots that work alongside humans will experience service breakdowns and make mistakes. These robotic failures can cause a degradation of trust between the robot and the community being served. A loss of trust may impact whether a user continues to rely on the robot for assistance. In order to improve the teaming capabilities between humans and robots, forms of communication that aid in developing and maintaining trust need to be investigated. In our study, we identify four forms of communication which dictate the timing of information given and type of initiation used by a robot. We investigate the effect that these forms of communication have on trust with and without robot mistakes during a cooperative task. Participants played a memory task game with the help of a humanoid robot that was designed to make mistakes after a certain amount of time passed. The results showed that participants' trust in the robot was better preserved when that robot offered advice only upon request as opposed to when the robot took initiative to give advice.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Learning Generalizable Robot Skills from Demonstrations in Cluttered Environments",
                "link": "https://ieeexplore.ieee.org/document/8593624/",
                "date_of_publication": "06 January 2019",
                "doi": "10.1109/IROS.2018.8593624",
                "citations": "3",
                "abstract": "Learning from Demonstration (LfD) is a popular approach to endowing robots with skills without having to program them by hand. Typically, LfD relies on human demonstrations in clutter-free environments. This prevents the demonstrations from being affected by irrelevant objects, whose influence can obfuscate the true intention of the human or the constraints of the desired skill. However, it is unrealistic to assume that the robot's environment can always be restructured to remove clutter when capturing human demonstrations. To contend with this problem, we develop an importance weighted batch and incremental skill learning approach, building on a recent inference-based technique for skill representation and reproduction. Our approach reduces unwanted environmental influences on the learned skill, while still capturing the salient human behavior. We provide both batch and incremental versions of our approach and validate our algorithms on a 7-DOF JACO2 manipulator with reaching and placing skills.",
                "ieee_keywords": [
                    "Trajectory",
                    "Robots",
                    "Clamps",
                    "Covariance matrices",
                    "Collision avoidance",
                    "Stochastic processes",
                    "Clutter"
                ],
                "author_keywords": []
            },
            {
                "title": "Multimodal Material Classification for Robots using Spectroscopy and High Resolution Texture Imaging",
                "link": "https://ieeexplore.ieee.org/document/9341165/",
                "date_of_publication": "10 February 2021",
                "doi": "10.1109/IROS45743.2020.9341165",
                "citations": "20",
                "abstract": "Material recognition can help inform robots about how to properly interact with and manipulate real-world objects. In this paper, we present a multimodal sensing technique, leveraging near-infrared spectroscopy and close-range high resolution texture imaging, that enables robots to estimate the materials of household objects. We release a dataset of high resolution texture images and spectral measurements collected from a mobile manipulator that interacted with 144 house-hold objects. We then present a neural network architecture that learns a compact multimodal representation of spectral measurements and texture images. When generalizing material classification to new objects, we show that this multimodal representation enables a robot to recognize materials with greater performance as compared to prior state-of-the-art approaches. Finally, we present how a robot can combine this high resolution local sensing with images from the robot's head-mounted camera to achieve accurate material classification over a scene of objects on a table.",
                "ieee_keywords": [
                    "Spectroscopy",
                    "Image resolution",
                    "Multimodal sensors",
                    "Robot vision systems",
                    "Neural networks",
                    "Robot sensing systems",
                    "Robots"
                ],
                "author_keywords": []
            },
            {
                "title": "An Interleaved Approach to Trait-Based Task Allocation and Scheduling",
                "link": "https://ieeexplore.ieee.org/document/9636569/",
                "date_of_publication": "16 December 2021",
                "doi": "10.1109/IROS51168.2021.9636569",
                "citations": "4",
                "abstract": "To realize effective heterogeneous multi-robot teams, researchers must leverage individual robots’ relative strengths and coordinate their individual behaviors. Specifically, heterogeneous multi-robot systems must answer three important questions: who (task allocation), when (scheduling), and how (motion planning). While specific variants of each of these problems are known to be NP-Hard, their interdependence only exacerbates the challenges involved in solving them together. In this paper, we present a novel framework that interleaves task allocation, scheduling, and motion planning. We introduce a search-based approach for trait-based time-extended task allocation named Incremental Task Allocation Graph Search (ITAGS). In contrast to approaches that solve the three problems in sequence, ITAGS’s interleaved approach enables efficient search for allocations while simultaneously satisfying scheduling constraints and accounting for the time taken to execute motion plans. To enable effective interleaving, we develop a convex combination of two search heuristics that optimizes the satisfaction of task requirements as well as the makespan of the associated schedule. We demonstrate the efficacy of ITAGS using detailed ablation studies and comparisons against two state-of-the-art algorithms in a simulated emergency response domain.",
                "ieee_keywords": [
                    "Schedules",
                    "Robot kinematics",
                    "Emergency services",
                    "Search problems",
                    "Resource management",
                    "Multi-robot systems",
                    "Iterative methods"
                ],
                "author_keywords": []
            },
            {
                "title": "Approximated Dynamic Trait Models for Heterogeneous Multi-Robot Teams",
                "link": "https://ieeexplore.ieee.org/document/9341107/",
                "date_of_publication": "10 February 2021",
                "doi": "10.1109/IROS45743.2020.9341107",
                "citations": "1",
                "abstract": "To realize effective heterogeneous multi-agent teams, we must be able to leverage individual agents' relative strengths. Recent work has addressed this challenge by introducing trait-based task assignment approaches that exploit the agents' relative advantages. These approaches, however, assume that the agents' traits remain static. Indeed, in real-world scenarios, traits are likely to vary as agents execute tasks. In this paper, we present a transformation-based modeling framework to bridge the gap between state-of-the-art task assignment algorithms and the reality of dynamic traits. We define a transformation as a function that approximates dynamic traits with static traits based on a specific statistical measure. We define different candidate transformations, investigate their effects on different dynamic trait models, and the resulting task performance. Further, we propose a variance-based transformation as a general solution that approximates a variety of dynamic models, eliminating the need for hand specification. Finally, we demonstrate the benefits of reasoning about dynamic traits both in simulation and in a physical experiment involving the game of capture-the-flag.",
                "ieee_keywords": [
                    "Heuristic algorithms",
                    "Games",
                    "Approximation algorithms",
                    "Cognition",
                    "Task analysis",
                    "Intelligent robots"
                ],
                "author_keywords": []
            },
            {
                "title": "Extraction of Important Temporal Order for eXplainable AI on Time-series data",
                "link": "https://ieeexplore.ieee.org/document/10150408/",
                "date_of_publication": "21 June 2023",
                "doi": "10.1109/PerComWorkshops56833.2023.10150408",
                "citations": "66",
                "abstract": "We propose a new eXplainable AI method on time-series data in which multiple events are arranged in temporal order, to extract important temporal order between events for opaque model decision. Toward this, our proposed method analyzes the model behavior when inputting perturbated data generated by changing the order of the events. We evaluate our method via an exemplary classification task on a simulated dataset to confirm how accurate our method is in extracting important temporal order of events. In addition, we evaluate our method on a smart home sensor dataset to demonstrate what kind of important order is actually extracted.",
                "ieee_keywords": [
                    "Pervasive computing",
                    "Analytical models",
                    "Conferences",
                    "Smart homes",
                    "Activity recognition",
                    "Data models",
                    "Behavioral sciences"
                ],
                "author_keywords": [
                    "eXplainable AI",
                    "time-series data",
                    "human activity recognition"
                ]
            },
            {
                "title": "Classification of Household Materials via Spectroscopy",
                "link": "https://ieeexplore.ieee.org/document/8610196/",
                "date_of_publication": null,
                "doi": "10.1109/LRA.2019.2892593",
                "citations": "22",
                "abstract": "Recognizing an object's material can inform a robot on the object's fragility or appropriate use. To estimate an object's material during manipulation, many prior works have explored the use of haptic sensing. In this letter, we explore a technique for robots to estimate the materials of objects using spectroscopy. We demonstrate that spectrometers provide several benefits for material recognition, including fast response times and accurate measurements with low noise. Furthermore, spectrometers do not require direct contact with an object. To explore this, we collected a dataset of spectral measurements from two commercially available spectrometers during which a robotic platform interacted with 50 flat material objects, and we show that a neural network model can accurately analyze these measurements. Due to the similarity between consecutive spectral measurements, our model achieved a material classification accuracy of 94.6% when given only one spectral sample per object. Similar to prior works with haptic sensors, we found that generalizing material recognition to new objects posed a greater challenge, for which we achieved an accuracy of 79.1% via leave-one-object-out cross validation. Finally, we demonstrate how a PR2 robot can leverage spectrometers to estimate the materials of everyday objects found in the home. From this letter, we find that spectroscopy poses a promising approach for material classification during robotic manipulation.",
                "ieee_keywords": [
                    "Robot sensing systems",
                    "Spectroscopy",
                    "Haptic interfaces",
                    "Wavelength measurement",
                    "Temperature measurement"
                ],
                "author_keywords": [
                    "Perception for grasping and manipulation",
                    "haptics and haptic interfaces",
                    "mobile manipulation"
                ]
            },
            {
                "title": "CAGE: Context-Aware Grasping Engine",
                "link": "https://ieeexplore.ieee.org/document/9197289/",
                "date_of_publication": "15 September 2020",
                "doi": "10.1109/ICRA40945.2020.9197289",
                "citations": "11",
                "abstract": "Semantic grasping is the problem of selecting stable grasps that are functionally suitable for specific object manipulation tasks. In order for robots to effectively perform object manipulation, a broad sense of contexts, including object and task constraints, needs to be accounted for. We introduce the Context-Aware Grasping Engine, which combines a novel semantic representation of grasp contexts with a neural network structure based on the Wide & Deep model, capable of capturing complex reasoning patterns. We quantitatively validate our approach against three prior methods on a novel dataset consisting of 14,000 semantic grasps for 44 objects, 7 tasks, and 6 different object states. Our approach outperformed all baselines by statistically significant margins, producing new insights into the importance of balancing memorization and generalization of contexts for semantic grasping. We further demonstrate the effectiveness of our approach on robot experiments in which the presented model successfully achieved 31 of 32 suitable grasps. The code and data are available at: https://github.com/wliu88/railsemanticgrasping.",
                "ieee_keywords": [
                    "Semantics",
                    "Task analysis",
                    "Grasping",
                    "Feature extraction",
                    "Robots",
                    "Cognition",
                    "Context modeling"
                ],
                "author_keywords": []
            },
            {
                "title": "Identifying reusable primitives in narrated demonstrations",
                "link": "https://ieeexplore.ieee.org/document/7451815/",
                "date_of_publication": "14 April 2016",
                "doi": "10.1109/HRI.2016.7451815",
                "citations": "1",
                "abstract": "The assumption that we can preprogram robots with all the information necessary for their function becomes impractical as the range of robotics applications grows. One widely proposed solution is the specification of reusable task plans, or recipes, consisting of a sequence of primitive actions. However, the primitive actions, such as unscrewing a nut in a car maintenance task, cannot always be predefined and thus must be learned for a particular robot platform and workspace. In this work, we present a novel algorithm that enables a robot to identify a reusable motion trajectory associated with each primitive action of a plan. Our approach segments the motion data captured during the demonstration of a human performing the given task, while also leveraging the human's verbal cues. We evaluated our algorithms in a pilot study with 6 users executing 90 primitive actions.",
                "ieee_keywords": [
                    "Robots",
                    "Fasteners",
                    "Trajectory",
                    "Context",
                    "Approximation algorithms",
                    "Heuristic algorithms",
                    "Maintenance engineering"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Stefano Ermon",
        "publications": [
            {
                "title": "Geography-Aware Self-Supervised Learning",
                "link": "https://ieeexplore.ieee.org/document/9711401/",
                "date_of_publication": "28 February 2022",
                "doi": "10.1109/ICCV48922.2021.01002",
                "citations": "40",
                "abstract": "Contrastive learning methods have significantly narrowed the gap between supervised and unsupervised learning on computer vision tasks. In this paper, we explore their application to geo-located datasets, e.g. remote sensing, where unlabeled data is often abundant but labeled data is scarce. We first show that due to their different characteristics, a non-trivial gap persists between contrastive and supervised learning on standard benchmarks. To close the gap, we propose novel training methods that exploit the spatio-temporal structure of remote sensing data. We leverage spatially aligned images over time to construct temporal positive pairs in contrastive learning and geo-location to design pre-text tasks. Our experiments show that our proposed method closes the gap between contrastive and supervised learning on image classification, object detection and semantic segmentation for remote sensing. Moreover, we demonstrate that the proposed method can also be applied to geo-tagged ImageNet images, improving downstream performance on various tasks.",
                "ieee_keywords": [
                    "Training",
                    "Image segmentation",
                    "Computer vision",
                    "Supervised learning",
                    "Semantics",
                    "Object detection",
                    "Task analysis"
                ],
                "author_keywords": [
                    "Representation learning",
                    "Datasets and evaluation",
                    "Recognition and classification",
                    "Transfer/Low-shot/Semi/Unsupervised Learning"
                ]
            },
            {
                "title": "Farm Parcel Delineation Using Spatio-temporal Convolutional Networks",
                "link": "https://ieeexplore.ieee.org/document/9150690/",
                "date_of_publication": "28 July 2020",
                "doi": "10.1109/CVPRW50498.2020.00046",
                "citations": "9",
                "abstract": "Farm parcel delineation (delineation of boundaries of farmland parcels/segmentation of farmland areas) provides cadastral data that is important in developing and managing climate change policies. Specifically, farm parcel delineation informs applications in downstream governmental policies of land allocation, irrigation, fertilization, green-house gases (GHG's), etc. This data can also be useful for the agricultural insurance sector for assessing compensations following damages associated with extreme weather events - a growing trend related to climate change [4]. Using satellite imaging can be a scalable and cost-effective manner to perform the task of farm parcel delineation to collect this valuable data. In this paper, we break down this task using satellite imaging into two approaches: 1) Segmentation of parcel boundaries, and 2) Segmentation of parcel areas. We implemented variations of U-Nets, one of which takes into account temporal information, which achieved the best results on our dataset on farm parcels in France in 2017.",
                "ieee_keywords": [
                    "Satellites",
                    "Climate change",
                    "Image segmentation",
                    "Meteorology",
                    "Machine learning",
                    "Computer vision",
                    "Computer science"
                ],
                "author_keywords": []
            },
            {
                "title": "Efficient Conditional Pre-training for Transfer Learning",
                "link": "https://ieeexplore.ieee.org/document/9857117/",
                "date_of_publication": "23 August 2022",
                "doi": "10.1109/CVPRW56347.2022.00469",
                "citations": "1",
                "abstract": "Almost all the state-of-the-art neural networks for computer vision tasks are trained by (1) pre-training on a large-scale dataset and (2) finetuning on the target dataset. This strategy helps reduce dependence on the target dataset and improves convergence rate and generalization on the target task. Although pre-training on large-scale datasets is very useful for new methods or models, its foremost disadvantage is high training cost. To address this, we propose efficient filtering methods to select relevant subsets from the pre-training dataset. Additionally, we discover that lowering image resolutions in the pre-training step offers a great trade-off between cost and performance. We validate our techniques by pre-training on ImageNet in both the unsupervised and supervised settings and finetuning on a diverse collection of target datasets and tasks. Our proposed methods drastically reduce pre-training cost and provide strong performance boosts. Finally, we improve the current standard of ImageNet pre-training by 1-3% by tuning available models on our subsets and pre-training on a dataset filtered from a larger scale dataset.",
                "ieee_keywords": [
                    "Training",
                    "Computer vision",
                    "Costs",
                    "Image resolution",
                    "Filtering",
                    "Computational modeling",
                    "Transfer learning"
                ],
                "author_keywords": []
            },
            {
                "title": "Cloud Removal in Satellite Images Using Spatiotemporal Generative Networks",
                "link": "https://ieeexplore.ieee.org/document/9093564/",
                "date_of_publication": "14 May 2020",
                "doi": "10.1109/WACV45572.2020.9093564",
                "citations": "26",
                "abstract": "Satellite images hold great promise for continuous environmental monitoring and earth observation. Occlusions cast by clouds, however, can severely limit coverage, making ground information extraction more difficult. Existing pipelines typically perform cloud removal with simple temporal composites and hand-crafted filters. In contrast, we cast the problem of cloud removal as a conditional image synthesis challenge, and we propose a trainable spatiotemporal generator network (STGAN) to remove clouds. We train our model on a new large-scale spatiotemporal dataset that we construct, containing 97640 image pairs covering all continents. We demonstrate experimentally that the proposed STGAN model outperforms standard models and can generate realistic cloud-free images with high PSNR and SSIM values across a variety of atmospheric conditions, leading to improved performance in downstream tasks such as land cover classification.",
                "ieee_keywords": [
                    "Clouds",
                    "Satellites",
                    "Agriculture",
                    "Spatiotemporal phenomena",
                    "Generators",
                    "Feature extraction",
                    "Gallium nitride"
                ],
                "author_keywords": []
            },
            {
                "title": "A Survey on Behavior Recognition Using WiFi Channel State Information",
                "link": "https://ieeexplore.ieee.org/document/8067693/",
                "date_of_publication": null,
                "doi": "10.1109/MCOM.2017.1700082",
                "citations": "244",
                "abstract": "In this article, we present a survey of recent advances in passive human behavior recognition in indoor areas using the channel state information (CSI) of commercial WiFi systems. The movement of the human body parts cause changes in the wireless signal reflections, which result in variations in the CSI. By analyzing the data streams of CSIs for different activities and comparing them against stored models, human behavior can be recognized. This is done by extracting features from CSI data streams and using machine learning techniques to build models and classifiers. The techniques from the literature that are presented herein have great performance; however, instead of the machine learning techniques employed in these works, we propose to use deep learning techniques such as long-short term memory (LSTM) recurrent neural networking (RNN) and show the improved performance. We also discuss different challenges such as environment change, frame rate selection, and the multi-user scenario; and finally suggest possible directions for future work.",
                "ieee_keywords": [
                    "Antennas",
                    "Wireless fidelity",
                    "OFDM",
                    "Behavioral sciences",
                    "Wireless communication",
                    "Doppler shift",
                    "Receivers"
                ],
                "author_keywords": []
            },
            {
                "title": "Efficient Object Detection in Large Images Using Deep Reinforcement Learning",
                "link": "https://ieeexplore.ieee.org/document/9093447/",
                "date_of_publication": "14 May 2020",
                "doi": "10.1109/WACV45572.2020.9093447",
                "citations": "43",
                "abstract": "Traditionally, an object detector is applied to every part of the scene of interest, and its accuracy and computational cost increases with higher resolution images. However, in some application domains such as remote sensing, purchasing high spatial resolution images is expensive. To reduce the large computational and monetary cost associated with using high spatial resolution images, we propose a reinforcement learning agent that adaptively selects the spatial resolution of each image that is provided to the detector. In particular, we train the agent in a dual reward setting to choose low spatial resolution images to be run through a coarse level detector when the image is dominated by large objects, and high spatial resolution images to be run through a fine level detector when it is dominated by small objects. This reduces the dependency on high spatial resolution images for building a robust detector and increases run-time efficiency. We perform experiments on the xView dataset, consisting of large images, where we increase runtime efficiency by 50% and use high resolution images only 30% of the time while maintaining similar accuracy as a detector that uses only high resolution images.",
                "ieee_keywords": [
                    "Detectors",
                    "Spatial resolution",
                    "Microsoft Windows",
                    "Object detection",
                    "Proposals",
                    "Satellites"
                ],
                "author_keywords": []
            },
            {
                "title": "Monitoring Ethiopian Wheat Fungus with Satellite Imagery and Deep Feature Learning",
                "link": "https://ieeexplore.ieee.org/document/8014930/",
                "date_of_publication": "24 August 2017",
                "doi": "10.1109/CVPRW.2017.196",
                "citations": "12",
                "abstract": "Wheat is the most important Ethiopian crop, and rust one of its greatest antagonists. There is a need for cheap and scalable rust monitoring in the developing world, but existing methods employ costly data collection techniques. We introduce a scalable, accurate, and inexpensive method for tracking outbreaks with publicly available remote sensing data. Our approach improves existing techniques in two ways. First, we forgo the spectral features employed by the remote sensing community in favor of automatically learned features generated by Convolutional and Long Short-Term Memory Networks. Second, we aggregate data into larger geospatial regions. We evaluate our approach on nine years of agricultural outcomes, show that it outperforms competing techniques, and demonstrate its predictive foresight. This is a promising new direction in crop disease monitoring, one that has the potential to grow more powerful with time.",
                "ieee_keywords": [
                    "Monitoring",
                    "Remote sensing",
                    "Diseases",
                    "Satellites",
                    "Agriculture",
                    "Spatial resolution"
                ],
                "author_keywords": []
            },
            {
                "title": "Conditional Imitation Learning for Multi-Agent Games",
                "link": "https://ieeexplore.ieee.org/document/9889671/",
                "date_of_publication": "29 September 2022",
                "doi": "10.1109/HRI53351.2022.9889671",
                "citations": "2",
                "abstract": "While advances in multi-agent learning have enabled the training of increasingly complex agents, most existing techniques produce a final policy that is not designed to adapt to a new partner's strategy. However, we would like our AI agents to adjust their strategy based on the strategies of those around them. In this work, we study the problem of conditional multi-agent imitation learning, where we have access to joint trajectory demonstrations at training time, and we must interact with and adapt to new partners at test time. This setting is challenging because we must infer a new partner's strategy and adapt our policy to that strategy, all without knowledge of the environment reward or dynamics. We formalize this problem of conditional multi-agent imitation learning, and propose a novel approach to address the difficulties of scalability and data scarcity. Our key insight is that variations across partners in multi-agent games are often highly structured, and can be represented via a low-rank subspace. Leveraging tools from tensor decomposition, our model learns a low-rank subspace over ego and partner agent strategies, then infers and adapts to a new partner strategy by interpolating in the subspace. We experiments with a mix of collaborative tasks, including bandits, particle, and Hanabi environments. Additionally, we test our conditional policies against real human partners in a user study on the Overcooked game. Our model adapts better to new partners compared to baselines, and robustly handles diverse settings ranging from discrete/continuous actions and static/online evaluation with AI/human partners.",
                "ieee_keywords": [
                    "Training",
                    "Adaptation models",
                    "Interpolation",
                    "Tensors",
                    "Scalability",
                    "Collaboration",
                    "Games"
                ],
                "author_keywords": [
                    "multi-agent",
                    "imitation learning",
                    "conditional policies",
                    "adaptive policies",
                    "low-rank",
                    "collaboration"
                ]
            },
            {
                "title": "Learning When and Where to Zoom With Deep Reinforcement Learning",
                "link": "https://ieeexplore.ieee.org/document/9156745/",
                "date_of_publication": "05 August 2020",
                "doi": "10.1109/CVPR42600.2020.01236",
                "citations": "21",
                "abstract": "While high resolution images contain semantically more useful information than their lower resolution counterparts, processing them is computationally more expensive, and in some applications, e.g. remote sensing, they can be much more expensive to acquire. For these reasons, it is desirable to develop an automatic method to selectively use high resolution data when necessary while maintaining accuracy and reducing acquisition/run-time cost. In this direction, we propose PatchDrop a reinforcement learning approach to dynamically identify when and where to use/acquire high resolution data conditioned on the paired, cheap, low resolution images. We conduct experiments on CIFAR10, CIFAR100, ImageNet and fMoW datasets where we use significantly less high resolution data while maintaining similar accuracy to models which use full high resolution images.",
                "ieee_keywords": [
                    "Task analysis",
                    "Spatial resolution",
                    "Training",
                    "Random variables",
                    "Computer vision",
                    "Satellites"
                ],
                "author_keywords": []
            },
            {
                "title": "On Distillation of Guided Diffusion Models",
                "link": "https://ieeexplore.ieee.org/document/10204696/",
                "date_of_publication": "22 August 2023",
                "doi": "10.1109/CVPR52729.2023.01374",
                "citations": "5",
                "abstract": "Classifier-free guided diffusion models have recently been shown to be highly effective at high-resolution image generation, and they have been widely used in large-scale diffusion frameworks including DALL.E 2, Stable Diffusion and Imagen. However, a downside of classifier-free guided diffusion models is that they are computationally expensive at inference time since they require evaluating two diffusion models, a class-conditional model and an unconditional model, tens to hundreds of times. To deal with this limitation, we propose an approach to distilling classifier-free guided diffusion models into models that are fast to sample from: Given a pre-trained classifier-free guided model, we first learn a single model to match the output of the combined conditional and unconditional models, and then we progressively distill that model to a diffusion model that requires much fewer sampling steps. For standard diffusion models trained on the pixel-space, our approach is able to generate images visually comparable to that of the original model using as few as 4 sampling steps on ImageNet $64\\times 64$ and CIFAR-10, achieving FID/IS scores comparable to that of the original model while being up to 256 times faster to sample from. For diffusion models trained on the latent-space (e.g., Stable Diffusion), our approach is able to generate high-fidelity images using as few as 1 to 4 denoising steps, accelerating inference by at least 10-fold compared to existing methods on ImageNet $256\\times 256$ and LAION datasets. We further demonstrate the effectiveness of our approach on text-guided image editing and inpainting, where our distilled model is able to generate high-quality results using as few as 2–4 denoising steps.",
                "ieee_keywords": [
                    "Computer vision",
                    "Costs",
                    "Image synthesis",
                    "Computational modeling",
                    "Impedance matching",
                    "Noise reduction",
                    "Pattern recognition"
                ],
                "author_keywords": [
                    "Image and video synthesis and generation"
                ]
            }
        ]
    },
    {
        "name": "Li Fei-Fei",
        "publications": [
            {
                "title": "iGibson 1.0: A Simulation Environment for Interactive Tasks in Large Realistic Scenes",
                "link": "https://ieeexplore.ieee.org/document/9636667/",
                "date_of_publication": "16 December 2021",
                "doi": "10.1109/IROS51168.2021.9636667",
                "citations": "37",
                "abstract": "We present iGibson 1.0, a novel simulation environment to develop robotic solutions for interactive tasks in large-scale realistic scenes. Our environment contains 15 fully interactive home-sized scenes with 108 rooms populated with rigid and articulated objects. The scenes are replicas of real-world homes, with distribution and the layout of objects aligned to those of the real world. iGibson 1.0 integrates several key features to facilitate the study of interactive tasks: i) generation of high-quality virtual sensor signals (RGB, depth, segmentation, LiDAR, flow and so on), ii) domain randomization to change the materials of the objects (both visual and physical) and/or their shapes, iii) integrated sampling-based motion planners to generate collision-free trajectories for robot bases and arms, and iv) intuitive human-iGibson interface that enables efficient collection of human demonstrations. Through experiments, we show that the full interactivity of the scenes enables agents to learn useful visual representations that accelerate the training of downstream manipulation tasks. We also show that iGibson features enable the generalization of navigation agents, and that the human-iGibson interface and integrated motion planners facilitate efficient imitation learning of human demonstrated (mobile) manipulation behaviors. iGibson 1.0 is open-source, equipped with comprehensive examples and documentation. For more information, visit our project website: http://svl.stanford.edu/igibson/.",
                "ieee_keywords": [
                    "Training",
                    "Visualization",
                    "Navigation",
                    "Shape",
                    "Soft sensors",
                    "Motion segmentation",
                    "Robot sensing systems"
                ],
                "author_keywords": []
            },
            {
                "title": "Human action recognition by learning bases of action attributes and parts",
                "link": "https://ieeexplore.ieee.org/document/6126386/",
                "date_of_publication": "12 January 2012",
                "doi": "10.1109/ICCV.2011.6126386",
                "citations": "340",
                "abstract": "In this work, we propose to use attributes and parts for recognizing human actions in still images. We define action attributes as the verbs that describe the properties of human actions, while the parts of actions are objects and poselets that are closely related to the actions. We jointly model the attributes and parts by learning a set of sparse bases that are shown to carry much semantic meaning. Then, the attributes and parts of an action image can be reconstructed from sparse coefficients with respect to the learned bases. This dual sparsity provides theoretical guarantee of our bases learning and feature reconstruction approach. On the PASCAL action dataset and a new “Stanford 40 Actions” dataset, we show that our method extracts meaningful high-order interactions between attributes and parts in human actions while achieving state-of-the-art classification performance.",
                "ieee_keywords": [
                    "Humans",
                    "Detectors",
                    "Image reconstruction",
                    "Vectors",
                    "Feature extraction",
                    "Noise",
                    "Image recognition"
                ],
                "author_keywords": []
            },
            {
                "title": "Sonicverse: A Multisensory Simulation Platform for Embodied Household Agents that See and Hear",
                "link": "https://ieeexplore.ieee.org/document/10160461/",
                "date_of_publication": "04 July 2023",
                "doi": "10.1109/ICRA48891.2023.10160461",
                "citations": "78",
                "abstract": "Developing embodied agents in simulation has been a key research topic in recent years. Exciting new tasks, algorithms, and benchmarks have been developed in various simulators. However, most of them assume deaf agents in silent environments, while we humans perceive the world with multiple senses. We introduce Sonicverse, a multisensory simulation platform with integrated audio-visual simulation for training household agents that can both see and hear. Sonicverse models realistic continuous audio rendering in 3D environments in real-time. Together with a new audio-visual VR interface that allows humans to interact with agents with audio, Sonicverse enables a series of embodied AI tasks that need audio-visual perception. For semantic audio-visual navigation in particular, we also propose a new multi-task learning model that achieves state-of-the-art performance. In addition, we demonstrate Sonicverse's realism via sim-to-real transfer, which has not been achieved by other simulators: an agent trained in Sonicverse can successfully perform audio-visual navigation in real-world environments. Sonicverse is available at: https://github.com/StanfordVL/Sonicverse.",
                "ieee_keywords": [
                    "Training",
                    "Solid modeling",
                    "Three-dimensional displays",
                    "Navigation",
                    "Semantics",
                    "Rendering (computer graphics)",
                    "Multitasking"
                ],
                "author_keywords": []
            },
            {
                "title": "Scaling Robot Supervision to Hundreds of Hours with RoboTurk: Robotic Manipulation Dataset through Human Reasoning and Dexterity",
                "link": "https://ieeexplore.ieee.org/document/8968114/",
                "date_of_publication": "28 January 2020",
                "doi": "10.1109/IROS40897.2019.8968114",
                "citations": "13",
                "abstract": "Large, richly annotated datasets have accelerated progress in fields such as computer vision and natural language processing, but replicating these successes in robotics has been challenging. While prior data collection methodologies such as self-supervision have resulted in large datasets, the data can have poor signal-to-noise ratio. By contrast, previous efforts to collect task demonstrations with humans provide better quality data, but they cannot reach the same data magnitude. Furthermore, neither approach places guarantees on the diversity of the data collected, in terms of solution strategies. In this work, we leverage and extend the RoboTurk platform to scale up data collection for robotic manipulation using remote teleoperation. The primary motivation for our platform is two-fold: (1) to address the shortcomings of prior work and increase the total quantity of manipulation data collected through human supervision by an order of magnitude without sacrificing the quality of the data and (2) to collect data on challenging manipulation tasks across several operators and observe a diverse set of emergent behaviors and solutions. We collected over 111 hours of robot manipulation data across 54 users and 3 challenging manipulation tasks in 1 week, resulting in the largest robot dataset collected via remote teleoperation. We evaluate the quality of our platform, the diversity of demonstrations in our dataset, and the utility of our dataset via quantitative and qualitative analysis. For additional results, supplementary videos, and to download our dataset, visit http;//roboturk.stanford.edu/realrobotdataset.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Task-Driven Graph Attention for Hierarchical Relational Object Navigation",
                "link": "https://ieeexplore.ieee.org/document/10161157/",
                "date_of_publication": "04 July 2023",
                "doi": "10.1109/ICRA48891.2023.10161157",
                "citations": "118",
                "abstract": "Embodied AI agents in large scenes often need to navigate to find objects. In this work, we study a naturally emerging variant of the object navigation task, hierarchical relational object navigation (HRON), where the goal is to find objects specified by logical predicates organized in a hierarchical structure-objects related to furniture and then to rooms-such as finding an apple on top of a table in the kitchen. Solving such a task requires an efficient representation to reason about object relations and correlate the relations in the environment and in the task goal. HRON in large scenes (e.g. homes) is particularly challenging due to its partial observability and long horizon, which invites solutions that can compactly store the past information while effectively exploring the scene. We demonstrate experimentally that scene graphs are the best-suited representation compared to conventional representations such as images or 2D maps. We propose a solution that uses scene graphs as part of its input and integrates graph neural networks as its backbone, with an integrated task-driven attention mechanism, and demonstrate its better scalability and learning efficiency than state-of-the-art baselines.",
                "ieee_keywords": [
                    "Automation",
                    "Navigation",
                    "Scalability",
                    "Graph neural networks",
                    "Task analysis",
                    "Observability",
                    "Artificial intelligence"
                ],
                "author_keywords": []
            },
            {
                "title": "Tool Detection and Operative Skill Assessment in Surgical Videos Using Region-Based Convolutional Neural Networks",
                "link": "https://ieeexplore.ieee.org/document/8354185/",
                "date_of_publication": "07 May 2018",
                "doi": "10.1109/WACV.2018.00081",
                "citations": "150",
                "abstract": "Five billion people in the world lack access to quality surgical care. Surgeon skill varies dramatically, and many surgical patients suffer complications and avoidable harm. Improving surgical training and feedback would help to reduce the rate of complications-half of which have been shown to be preventable. To do this, it is essential to assess operative skill, a process that currently requires experts and is manual, time consuming, and subjective. In this work, we introduce an approach to automatically assess surgeon performance by tracking and analyzing tool movements in surgical videos, leveraging region-based convolutional neural networks. In order to study this problem, we also introduce a new dataset, m2cai16-tool-locations, which extends the m2cai16-tool dataset with spatial bounds of tools. While previous methods have addressed tool presence detection, ours is the first to not only detect presence but also spatially localize surgical tools in real-world laparoscopic surgical videos. We show that our method both effectively detects the spatial bounds of tools as well as significantly outperforms existing methods on tool presence detection. We further demonstrate the ability of our method to assess surgical quality through analysis of tool usage patterns, movement range, and economy of motion.",
                "ieee_keywords": [
                    "Tools",
                    "Videos",
                    "Task analysis",
                    "Training",
                    "Minimally invasive surgery",
                    "Convolutional neural networks"
                ],
                "author_keywords": []
            },
            {
                "title": "Making Sense of Vision and Touch: Learning Multimodal Representations for Contact-Rich Tasks",
                "link": "https://ieeexplore.ieee.org/document/9043710/",
                "date_of_publication": null,
                "doi": "10.1109/TRO.2019.2959445",
                "citations": "60",
                "abstract": "Contact-rich manipulation tasks in unstructured environments often require both haptic and visual feedback. It is nontrivial to manually design a robot controller that combines these modalities, which have very different characteristics. While deep reinforcement learning has shown success in learning control policies for high-dimensional inputs, these algorithms are generally intractable to train directly on real robots due to sample complexity. In this article, we use self-supervision to learn a compact and multimodal representation of our sensory inputs, which can then be used to improve the sample efficiency of our policy learning. Evaluating our method on a peg insertion task, we show that it generalizes over varying geometries, configurations, and clearances, while being robust to external perturbations. We also systematically study different self-supervised learning objectives and representation learning architectures. Results are presented in simulation and on a physical robot.",
                "ieee_keywords": [
                    "Task analysis",
                    "Haptic interfaces",
                    "Visualization",
                    "Robot sensing systems",
                    "Solid modeling",
                    "Reinforcement learning"
                ],
                "author_keywords": [
                    "Deep learning in robotics and automation",
                    "perception for grasping and manipulation",
                    "sensor fusion",
                    "sensor-based control"
                ]
            },
            {
                "title": "Continuous Relaxation of Symbolic Planner for One-Shot Imitation Learning",
                "link": "https://ieeexplore.ieee.org/document/8967761/",
                "date_of_publication": "28 January 2020",
                "doi": "10.1109/IROS40897.2019.8967761",
                "citations": "14",
                "abstract": "We address one-shot imitation learning, where the goal is to execute a previously unseen task based on a single demonstration. While there has been exciting progress in this direction, most of the approaches still require a few hundred tasks for meta-training, which limits the scalability of the approaches. Our main contribution is to formulate one-shot imitation learning as a symbolic planning problem along with the symbol grounding problem. This formulation disentangles the policy execution from the inter-task generalization and leads to better data efficiency. The key technical challenge is that the symbol grounding is prone to error with limited training data and leads to subsequent symbolic planning failures. We address this challenge by proposing a continuous relaxation of the discrete symbolic planner that directly plans on the probabilistic outputs of the symbol grounding model. Our continuous relaxation of the planner can still leverage the information contained in the probabilistic symbol grounding and significantly improve over the baseline planner for the one-shot imitation learning tasks without using large training data.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Building and using a semantivisual image hierarchy",
                "link": "https://ieeexplore.ieee.org/document/5540027/",
                "date_of_publication": "05 August 2010",
                "doi": "10.1109/CVPR.2010.5540027",
                "citations": "59",
                "abstract": "A semantically meaningful image hierarchy can ease the human effort in organizing thousands and millions of pictures (e.g., personal albums), and help to improve performance of end tasks such as image annotation and classification. Previous work has focused on using either low-level image features or textual tags to build image hierarchies, resulting in limited success in their general usage. In this paper, we propose a method to automatically discover the “semantivisual” image hierarchy by incorporating both image and tag information. This hierarchy encodes a general-to-specific image relationship. We pay particular attention to quantifying the effectiveness of the learned hierarchy, as well as comparing our method with others in the end-task applications. Our experiments show that humans find our semantivisual image hierarchy more effective than those solely based on texts or low-level visual features. And using the constructed image hierarchy as a knowledge ontology, our algorithm can perform challenging image classification and annotation tasks more accurately.",
                "ieee_keywords": [
                    "Humans",
                    "Organizing",
                    "Ontologies",
                    "Image classification"
                ],
                "author_keywords": []
            },
            {
                "title": "The Object Folder Benchmark : Multisensory Learning with Neural and Real Objects",
                "link": "https://ieeexplore.ieee.org/document/10203718/",
                "date_of_publication": "22 August 2023",
                "doi": "10.1109/CVPR52729.2023.01657",
                "citations": "1",
                "abstract": "We introduce the ObjectFolder Benchmark, a benchmark suite of 10 tasks for multisensory object-centric learning, centered around object recognition, reconstruction, and manipulation with sight, sound, and touch. We also introduce the Objectfolder Real dataset, including the multisensory measurements for 100 real-world household objects, building upon a newly designed pipeline for collecting the 3D meshes, videos, impact sounds, and tactile readings of real-world objects. We conduct systematic benchmarking on both the 1,000 multisensory neural objects from Objectfolder, and the real multisensory data from Objectfolder Real. Our results demonstrate the importance of multisensory perception and reveal the respective roles of vision, audio, and touch for different object-centric learning tasks. By publicly releasing our dataset and benchmark suite, we hope to catalyze and enable new research in multisensory object-centric learning in computer vision, robotics, and beyond. Project page: https://objectfolder.stanford.edu",
                "ieee_keywords": [
                    "Computer vision",
                    "Visualization",
                    "Technological innovation",
                    "Three-dimensional displays",
                    "Systematics",
                    "Benchmark testing",
                    "Solids"
                ],
                "author_keywords": [
                    "Multi-modal learning"
                ]
            }
        ]
    },
    {
        "name": "Silvio Savarese",
        "publications": [
            {
                "title": "Time-Varying Interaction Estimation Using Ensemble Methods",
                "link": "https://ieeexplore.ieee.org/document/8755565/",
                "date_of_publication": "08 July 2019",
                "doi": "10.1109/DSW.2019.8755565",
                "citations": "1",
                "abstract": "Directed information (DI) is a useful tool to explore time-directed interactions in multivariate data. However, as originally formulated DI is not well suited to interactions that change over time. In previous work, adaptive directed information was introduced to accommodate non-stationarity, while still preserving the utility of DI to discover complex dependencies between entities. There are many design decisions and parameters that are crucial to the effectiveness of ADI. Here, we apply ideas from ensemble learning in order to alleviate this issue, allowing for a more robust estimator for exploratory data analysis. We apply these techniques to interaction estimation in a crowded scene, utilizing the Stanford drone dataset as an example.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "iGibson 1.0: A Simulation Environment for Interactive Tasks in Large Realistic Scenes",
                "link": "https://ieeexplore.ieee.org/document/9636667/",
                "date_of_publication": "16 December 2021",
                "doi": "10.1109/IROS51168.2021.9636667",
                "citations": "37",
                "abstract": "We present iGibson 1.0, a novel simulation environment to develop robotic solutions for interactive tasks in large-scale realistic scenes. Our environment contains 15 fully interactive home-sized scenes with 108 rooms populated with rigid and articulated objects. The scenes are replicas of real-world homes, with distribution and the layout of objects aligned to those of the real world. iGibson 1.0 integrates several key features to facilitate the study of interactive tasks: i) generation of high-quality virtual sensor signals (RGB, depth, segmentation, LiDAR, flow and so on), ii) domain randomization to change the materials of the objects (both visual and physical) and/or their shapes, iii) integrated sampling-based motion planners to generate collision-free trajectories for robot bases and arms, and iv) intuitive human-iGibson interface that enables efficient collection of human demonstrations. Through experiments, we show that the full interactivity of the scenes enables agents to learn useful visual representations that accelerate the training of downstream manipulation tasks. We also show that iGibson features enable the generalization of navigation agents, and that the human-iGibson interface and integrated motion planners facilitate efficient imitation learning of human demonstrated (mobile) manipulation behaviors. iGibson 1.0 is open-source, equipped with comprehensive examples and documentation. For more information, visit our project website: http://svl.stanford.edu/igibson/.",
                "ieee_keywords": [
                    "Training",
                    "Visualization",
                    "Navigation",
                    "Shape",
                    "Soft sensors",
                    "Motion segmentation",
                    "Robot sensing systems"
                ],
                "author_keywords": []
            },
            {
                "title": "JRDB: A Dataset and Benchmark of Egocentric Robot Visual Perception of Humans in Built Environments",
                "link": "https://ieeexplore.ieee.org/document/9394786/",
                "date_of_publication": null,
                "doi": "10.1109/TPAMI.2021.3070543",
                "citations": "21",
                "abstract": "We present JRDB, a novel egocentric dataset collected from our social mobile manipulator JackRabbot. The dataset includes 64 minutes of annotated multimodal sensor data including stereo cylindrical 360 ${}^\\circ$ RGB video at 15 fps, 3D point clouds from two 16 planar rays Velodyne LiDARs, line 3D point clouds from two Sick Lidars, audio signal, RGB-D video at 30 fps, 360 ${}^\\circ$ spherical image from a fisheye camera and encoder values from the robot's wheels. Our dataset incorporates data from traditionally underrepresented scenes such as indoor environments and pedestrian areas, all from the ego-perspective of the robot, both stationary and navigating. The dataset has been annotated with over 2.4 million bounding boxes spread over five individual cameras and 1.8 million associated 3D cuboids around all people in the scenes totaling over 3500 time consistent trajectories. Together with our dataset and the annotations, we launch a benchmark and metrics for 2D and 3D person detection and tracking. With this dataset, which we plan on extending with further types of annotation in the future, we hope to provide a new source of data and a test-bench for research in the areas of egocentric robot vision, autonomous navigation, and all perceptual tasks around social robotics in human environments.",
                "ieee_keywords": [
                    "Three-dimensional displays",
                    "Robots",
                    "Sensors",
                    "Annotations",
                    "Two dimensional displays",
                    "Cameras",
                    "Benchmark testing"
                ],
                "author_keywords": [
                    "Robot navigation",
                    "social robotics",
                    "person detection",
                    "person tracking",
                    "MeSH Terms",
                    "Humans",
                    "Robotics",
                    "Benchmarking",
                    "Algorithms",
                    "Visual Perception",
                    "Built Environment"
                ]
            },
            {
                "title": "VUNet: Dynamic Scene View Synthesis for Traversability Estimation Using an RGB Camera",
                "link": "https://ieeexplore.ieee.org/document/8624332/",
                "date_of_publication": null,
                "doi": "10.1109/LRA.2019.2894869",
                "citations": "19",
                "abstract": "We present VUNet, a novel view(VU) synthesis method for mobile robots in dynamic environments, and its application to the estimation of future traversability. Our method predicts future images for given virtual robot velocity commands using only RGB images at previous and current time steps. The future images result from applying two types of image changes to the previous and current images: first, changes caused by different camera pose. Second, changes due to the motion of the dynamic obstacles. We learn to predict these two types of changes disjointly using two novel network architectures, SNet and DNet. We combine SNet and DNet to synthesize future images that we pass to our previously presented method GONet [N. Hirose, A. Sadeghian, M. Vazquez, P. Goebel, and S. Savarese, “Gonet: A semi-supervised deep learning approach for traversability estimation,” in Proc. IEEE International Conference on Intelligent Robots and Systems, 2018, pp. 3044-3051] to estimate the traversable areas around the robot. Our quantitative and qualitative evaluation indicate that our approach for view synthesis predicts accurate future images in both static and dynamic environments. We also show that these virtual images can be used to estimate future traversability correctly. We apply our view synthesis-based traversability estimation method to two applications for assisted teleoperation.",
                "ieee_keywords": [
                    "Cameras",
                    "Dynamics",
                    "Robot vision systems",
                    "Estimation",
                    "Navigation"
                ],
                "author_keywords": [
                    "Robot safety",
                    "computer vision for other robotic applications",
                    "collision avoidance"
                ]
            },
            {
                "title": "Interactive Gibson Benchmark: A Benchmark for Interactive Navigation in Cluttered Environments",
                "link": "https://ieeexplore.ieee.org/document/8954627/",
                "date_of_publication": null,
                "doi": "10.1109/LRA.2020.2965078",
                "citations": "66",
                "abstract": "We present Interactive Gibson Benchmark, the first comprehensive benchmark for training and evaluating Interactive Navigation solutions. Interactive Navigation tasks are robot navigation problems where physical interaction with objects (e.g., pushing) is allowed and even encouraged to reach the goal. Our benchmark comprises two novel elements: 1) a new experimental simulated environment, the Interactive Gibson Environment, that generate photo-realistic images of indoor scenes and simulates realistic physical interactions of robots and common objects found in these scenes; 2) the Interactive Navigation Score, a novel metric to study the interplay between navigation and physical interaction of Interactive Navigation solutions. We present and evaluate multiple learning-based baselines in Interactive Gibson Benchmark, and provide insights into regimes of navigation with different trade-offs between navigation, path efficiency and disturbance of surrounding objects. We make our benchmark publicly available1 and encourage researchers from related robotics disciplines (e.g., planning, learning, control) to propose, evaluate, and compare their Interactive Navigation solutions in Interactive Gibson Benchmark.",
                "ieee_keywords": [
                    "Navigation",
                    "Benchmark testing",
                    "Robots",
                    "Measurement",
                    "Engines",
                    "Solid modeling",
                    "Task analysis"
                ],
                "author_keywords": [
                    "Visual-based navigation",
                    "deep learning in robotics and automation",
                    "mobile manipulation"
                ]
            },
            {
                "title": "Scaling Robot Supervision to Hundreds of Hours with RoboTurk: Robotic Manipulation Dataset through Human Reasoning and Dexterity",
                "link": "https://ieeexplore.ieee.org/document/8968114/",
                "date_of_publication": "28 January 2020",
                "doi": "10.1109/IROS40897.2019.8968114",
                "citations": "13",
                "abstract": "Large, richly annotated datasets have accelerated progress in fields such as computer vision and natural language processing, but replicating these successes in robotics has been challenging. While prior data collection methodologies such as self-supervision have resulted in large datasets, the data can have poor signal-to-noise ratio. By contrast, previous efforts to collect task demonstrations with humans provide better quality data, but they cannot reach the same data magnitude. Furthermore, neither approach places guarantees on the diversity of the data collected, in terms of solution strategies. In this work, we leverage and extend the RoboTurk platform to scale up data collection for robotic manipulation using remote teleoperation. The primary motivation for our platform is two-fold: (1) to address the shortcomings of prior work and increase the total quantity of manipulation data collected through human supervision by an order of magnitude without sacrificing the quality of the data and (2) to collect data on challenging manipulation tasks across several operators and observe a diverse set of emergent behaviors and solutions. We collected over 111 hours of robot manipulation data across 54 users and 3 challenging manipulation tasks in 1 week, resulting in the largest robot dataset collected via remote teleoperation. We evaluate the quality of our platform, the diversity of demonstrations in our dataset, and the utility of our dataset via quantitative and qualitative analysis. For additional results, supplementary videos, and to download our dataset, visit http;//roboturk.stanford.edu/realrobotdataset.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Making Sense of Vision and Touch: Learning Multimodal Representations for Contact-Rich Tasks",
                "link": "https://ieeexplore.ieee.org/document/9043710/",
                "date_of_publication": null,
                "doi": "10.1109/TRO.2019.2959445",
                "citations": "60",
                "abstract": "Contact-rich manipulation tasks in unstructured environments often require both haptic and visual feedback. It is nontrivial to manually design a robot controller that combines these modalities, which have very different characteristics. While deep reinforcement learning has shown success in learning control policies for high-dimensional inputs, these algorithms are generally intractable to train directly on real robots due to sample complexity. In this article, we use self-supervision to learn a compact and multimodal representation of our sensory inputs, which can then be used to improve the sample efficiency of our policy learning. Evaluating our method on a peg insertion task, we show that it generalizes over varying geometries, configurations, and clearances, while being robust to external perturbations. We also systematically study different self-supervised learning objectives and representation learning architectures. Results are presented in simulation and on a physical robot.",
                "ieee_keywords": [
                    "Task analysis",
                    "Haptic interfaces",
                    "Visualization",
                    "Robot sensing systems",
                    "Solid modeling",
                    "Reinforcement learning"
                ],
                "author_keywords": [
                    "Deep learning in robotics and automation",
                    "perception for grasping and manipulation",
                    "sensor fusion",
                    "sensor-based control"
                ]
            },
            {
                "title": "JRMOT: A Real-Time 3D Multi-Object Tracker and a New Large-Scale Dataset",
                "link": "https://ieeexplore.ieee.org/document/9341635/",
                "date_of_publication": "10 February 2021",
                "doi": "10.1109/IROS45743.2020.9341635",
                "citations": "39",
                "abstract": "Robots navigating autonomously need to perceive and track the motion of objects and other agents in its surroundings. This information enables planning and executing robust and safe trajectories. To facilitate these processes, the motion should be perceived in 3D Cartesian space. However, most recent multi-object tracking (MOT) research has focused on tracking people and moving objects in 2D RGB video sequences. In this work we present JRMOT, a novel 3D MOT system that integrates information from RGB images and 3D point clouds to achieve real-time, state-of-the-art tracking performance. Our system is built with recent neural networks for re-identification, 2D and 3D detection and track description, combined into a joint probabilistic data-association framework within a multi-modal recursive Kalman architecture. As part of our work, we release the JRDB dataset, a novel large scale 2D+3D dataset and benchmark, annotated with over 2 million boxes and 3500 time consistent 2D+3D trajectories across 54 indoor and outdoor scenes. JRDB contains over 60 minutes of data including 360 o cylindrical RGB video and 3D pointclouds in social settings that we use to develop, train and evaluate JRMOT. The presented 3D MOT system demonstrates state-of-the-art performance against competing methods on the popular 2D tracking KITTI benchmark and serves as first 3D tracking solution for our benchmark. Real-robot tests on our social robot JackRabbot indicate that the system is capable of tracking multiple pedestrians fast and reliably. We provide the ROS code of our tracker at https://sites.google.com/view/jrmot.",
                "ieee_keywords": [
                    "Three-dimensional displays",
                    "Tracking",
                    "Two dimensional displays",
                    "Video sequences",
                    "Benchmark testing",
                    "Real-time systems",
                    "Trajectory"
                ],
                "author_keywords": []
            },
            {
                "title": "Continuous Relaxation of Symbolic Planner for One-Shot Imitation Learning",
                "link": "https://ieeexplore.ieee.org/document/8967761/",
                "date_of_publication": "28 January 2020",
                "doi": "10.1109/IROS40897.2019.8967761",
                "citations": "14",
                "abstract": "We address one-shot imitation learning, where the goal is to execute a previously unseen task based on a single demonstration. While there has been exciting progress in this direction, most of the approaches still require a few hundred tasks for meta-training, which limits the scalability of the approaches. Our main contribution is to formulate one-shot imitation learning as a symbolic planning problem along with the symbol grounding problem. This formulation disentangles the policy execution from the inter-task generalization and leads to better data efficiency. The key technical challenge is that the symbol grounding is prone to error with limited training data and leads to subsequent symbolic planning failures. We address this challenge by proposing a continuous relaxation of the discrete symbolic planner that directly plans on the probabilistic outputs of the symbol grounding model. Our continuous relaxation of the planner can still leverage the information contained in the probabilistic symbol grounding and significantly improve over the baseline planner for the one-shot imitation learning tasks without using large training data.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Variable Impedance Control in End-Effector Space: An Action Space for Reinforcement Learning in Contact-Rich Tasks",
                "link": "https://ieeexplore.ieee.org/document/8968201/",
                "date_of_publication": "28 January 2020",
                "doi": "10.1109/IROS40897.2019.8968201",
                "citations": "74",
                "abstract": "Reinforcement Learning (RL) of contact-rich manipulation tasks has yielded impressive results in recent years. While many studies in RL focus on varying the observation space or reward model, few efforts focused on the choice of action space (e.g. joint or end-effector space, position, velocity, etc.). However, studies in robot motion control indicate that choosing an action space that conforms to the characteristics of the task can simplify exploration and improve robustness to disturbances. This paper studies the effect of different action spaces in deep RL and advocates for variable impedance control in end-effector space (VICES) as an advantageous action space for constrained and contact-rich tasks. We evaluate multiple action spaces on three prototypical manipulation tasks: Path Following (task with no contact), Door Opening (task with kinematic constraints), and Surface Wiping (task with continuous contact). We show that VICES improves sample efficiency, maintains low energy consumption, and ensures safety across all three experimental setups. Further, RL policies learned with VICES can transfer across different robot models in simulation, and from simulation to real for the same robot. Further information is available at https://stanfordvl.github.io/vices.",
                "ieee_keywords": [],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Chelsea Finn",
        "publications": [
            {
                "title": "Greedy Hierarchical Variational Autoencoders for Large-Scale Video Prediction",
                "link": "https://ieeexplore.ieee.org/document/9578715/",
                "date_of_publication": "02 November 2021",
                "doi": "10.1109/CVPR46437.2021.00235",
                "citations": "23",
                "abstract": "A video prediction model that generalizes to diverse scenes would enable intelligent agents such as robots to perform a variety of tasks via planning with the model. However, while existing video prediction models have produced promising results on small datasets, they suffer from severe underfitting when trained on large and diverse datasets. To address this underfitting challenge, we first observe that the ability to train larger video prediction models is often bottlenecked by the memory constraints of GPUs or TPUs. In parallel, deep hierarchical latent variable models can produce higher quality predictions by capturing the multi-level stochasticity of future observations, but end-to-end optimization of such models is notably difficult. Our key insight is that greedy and modular optimization of hierarchical autoencoders can simultaneously address both the memory constraints and the optimization challenges of large-scale video prediction. We introduce Greedy Hierarchical Variational Autoencoders (GHVAEs), a method that learns highfidelity video predictions by greedily training each level of a hierarchical autoencoder. In comparison to state- of-the-art models, GHVAEs provide 17-55% gains in prediction performance on four video datasets, a 35–40% higher success rate on real robot tasks, and can improve performance monotonically by simply adding more modules. Visualization and more details are at https://sites.google.com/view/ghvae.",
                "ieee_keywords": [
                    "Training",
                    "Visualization",
                    "Memory management",
                    "Stacking",
                    "Predictive models",
                    "Planning",
                    "Pattern recognition"
                ],
                "author_keywords": []
            },
            {
                "title": "Recovery RL: Safe Reinforcement Learning With Learned Recovery Zones",
                "link": "https://ieeexplore.ieee.org/document/9392290/",
                "date_of_publication": null,
                "doi": "10.1109/LRA.2021.3070252",
                "citations": "49",
                "abstract": "Safety remains a central obstacle preventing widespread use of RL in the real world: learning new tasks in uncertain environments requires extensive exploration, but safety requires limiting exploration. We propose Recovery RL, an algorithm which navigates this tradeoff by (1) leveraging offline data to learn about constraint violating zones before policy learning and (2) separating the goals of improving task performance and constraint satisfaction across two policies: a task policy that only optimizes the task reward and a recovery policy that guides the agent to safety when constraint violation is likely. We evaluate Recovery RL on 6 simulation domains, including two contact-rich manipulation tasks and an image-based navigation task, and an image-based obstacle avoidance task on a physical robot. We compare Recovery RL to 5 prior safe RL methods which jointly optimize for task performance and safety via constrained optimization or reward shaping and find that Recovery RL outperforms the next best prior method across all domains. Results suggest that Recovery RL trades off constraint violations and task successes 2–20 times more efficiently in simulation domains and 3 times more efficiently in physical experiments. See https://tinyurl.com/rl-recovery for videos and supplementary material.",
                "ieee_keywords": [
                    "Safety",
                    "Navigation",
                    "Optimization",
                    "Reinforcement learning"
                ],
                "author_keywords": [
                    "Reinforcement learning",
                    "safety"
                ]
            },
            {
                "title": "Bayesian Embeddings for Few-Shot Open World Recognition",
                "link": "https://ieeexplore.ieee.org/document/9875990/",
                "date_of_publication": null,
                "doi": "10.1109/TPAMI.2022.3201541",
                "citations": "2",
                "abstract": "As autonomous decision-making agents move from narrow operating environments to unstructured worlds, learning systems must move from a closed-world formulation to an open-world and few-shot setting in which agents continuously learn new classes from small amounts of information. This stands in stark contrast to modern machine learning systems that are typically designed with a known set of classes and a large number of examples for each class. In this work we extend embedding-based few-shot learning algorithms to the open-world recognition setting. We combine Bayesian non-parametric class priors with an embedding-based pre-training scheme to yield a highly flexible framework which we refer to as few-shot learning for open world recognition (FLOWR). We benchmark our framework on open-world extensions of the common MiniImageNet and TieredImageNet few-shot learning datasets. Our results show, compared to prior methods, strong classification accuracy performance and up to a 12% improvement in H-measure (a measure of novel class detection) from our non-parametric open-world few-shot learning scheme.",
                "ieee_keywords": [
                    "Training",
                    "Bayes methods",
                    "Measurement",
                    "Taxonomy",
                    "Predictive models",
                    "Pattern recognition",
                    "Optimization"
                ],
                "author_keywords": [
                    "Machine Learning",
                    "Few-Shot Learning",
                    "Meta-Learning",
                    "Open-World Learning"
                ]
            },
            {
                "title": "Train Offline, Test Online: A Real Robot Learning Benchmark",
                "link": "https://ieeexplore.ieee.org/document/10160594/",
                "date_of_publication": "04 July 2023",
                "doi": "10.1109/ICRA48891.2023.10160594",
                "citations": "51",
                "abstract": "Three challenges limit the progress of robot learning research: robots are expensive (few labs can participate), everyone uses different robots (findings do not generalize across labs), and we lack internet-scale robotics data. We take on these challenges via a new benchmark: Train Offline, Test Online (TOTO). TOTO provides remote users with access to shared robots for evaluating methods on common tasks and an open-source dataset of these tasks for offline training. Its manipulation task suite requires challenging generalization to unseen objects, positions, and lighting. We present initial results on TOTO comparing five pretrained visual representations and four offline policy learning baselines, remotely contributed by five institutions. The real promise of TOTO, however, lies in the future: we release the benchmark for additional submissions from any user, enabling easy, direct comparison to several methods without the need to obtain hardware or collect data.",
                "ieee_keywords": [
                    "Training",
                    "Visualization",
                    "Automation",
                    "Lighting",
                    "Benchmark testing",
                    "Robot learning",
                    "Hardware"
                ],
                "author_keywords": []
            },
            {
                "title": "Batch Exploration With Examples for Scalable Robotic Reinforcement Learning",
                "link": "https://ieeexplore.ieee.org/document/9385945/",
                "date_of_publication": null,
                "doi": "10.1109/LRA.2021.3068655",
                "citations": "3",
                "abstract": "Learning from diverse offline datasets is a promising path towards learning general purpose robotic agents. However, a core challenge in this paradigm lies in collecting large amounts of meaningful data, while not depending on a human in the loop for data collection. One way to address this challenge is through task-agnostic exploration , where an agent attempts to explore without a task-specific reward function, and collect data that can be useful for any subsequent task. While these approaches have shown some promise in simple domains, they often struggle to explore the relevant regions of the state space in more challenging settings, such as vision-based robotic manipulation. This challenge stems from an objective that encourages exploring everything in a potentially vast state space. To mitigate this challenge, we propose to focus exploration on the important parts of the state space using weak human supervision . Concretely, we propose an exploration technique, Batch Exploration with Examples (BEE), that explores relevant regions of the state-space, guided by a modest number of human-provided images of important states. These human-provided images only need to be provided once at the beginning of data collection and can be acquired in a matter of minutes, allowing us to scalably collect diverse datasets, which can then be combined with any batch RL algorithm. We find that BEE is able to tackle challenging vision-based manipulation tasks both in simulation and on a real Franka Emika Panda robot, and observe that compared to task-agnostic and weakly-supervised exploration techniques, it (1) interacts more than twice as often with relevant objects, and (2) improves subsequent task performance when used in conjunction with offline RL.",
                "ieee_keywords": [
                    "Deep learning",
                    "Reinforcement learning",
                    "Data collection"
                ],
                "author_keywords": [
                    "Deep learning methods",
                    "reinforcement learning"
                ]
            },
            {
                "title": "Bayesian Meta-Learning for Few-Shot Policy Adaptation Across Robotic Platforms",
                "link": "https://ieeexplore.ieee.org/document/9636628/",
                "date_of_publication": "16 December 2021",
                "doi": "10.1109/IROS51168.2021.9636628",
                "citations": "8",
                "abstract": "Reinforcement learning methods can achieve significant performance but require a large amount of training data collected on the same robotic platform. A policy trained with expensive data is rendered useless after making even a minor change to the robot hardware. In this paper, we address the challenging problem of adapting a policy, trained to perform a task, to a novel robotic hardware platform given only few demonstrations of robot motion trajectories on the target robot. We formulate it as a few-shot meta-learning problem where the goal is to find a meta-model that captures the common structure shared across different robotic platforms such that data-efficient adaptation can be performed. We achieve such adaptation by introducing a learning framework consisting of a probabilistic gradient-based meta-learning algorithm that models the uncertainty arising from the few-shot setting with a low-dimensional latent variable. We experimentally evaluate our framework on a simulated reaching and a real-robot picking task using 400 simulated robots generated by varying the physical parameters of an existing set of robotic platforms. Our results show that the proposed method can successfully adapt a trained policy to different robotic platforms with novel physical parameters and the superiority of our meta-learning algorithm compared to state-of-the-art methods for the introduced few-shot policy adaptation problem.",
                "ieee_keywords": [
                    "Robot motion",
                    "Adaptation models",
                    "Uncertainty",
                    "Training data",
                    "Reinforcement learning",
                    "Probabilistic logic",
                    "Hardware"
                ],
                "author_keywords": []
            },
            {
                "title": "One-Shot Composition of Vision-Based Skills from Demonstration",
                "link": "https://ieeexplore.ieee.org/document/8967745/",
                "date_of_publication": "28 January 2020",
                "doi": "10.1109/IROS40897.2019.8967745",
                "citations": "3",
                "abstract": "We consider the problem of learning multi-stage vision-based tasks on a real robot from a single video of a human performing the task, while leveraging demonstration data of subtasks with other objects. This problem presents a number of major challenges. Video demonstrations without teleoperation are easy for humans to provide, but do not provide any direct supervision. Learning policies from raw pixels enables full generality but calls for large function approximators with many parameters to be learned. Finally, compound tasks can require impractical amounts of demonstration data, when treated as a monolithic skill. To address these challenges, we propose a method that learns both how to learn primitive behaviors from video demonstrations and how to dynamically compose these behaviors to perform multi-stage tasks by “watching” a human demonstrator. Our results on a simulated Sawyer robot and real PR2 robot illustrate our method for learning a variety of order fulfillment and kitchen serving tasks with novel objects and raw pixel inputs. Video results are linked at https://sites.google.com/view/one-shot-hil.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "OmniTact: A Multi-Directional High-Resolution Touch Sensor",
                "link": "https://ieeexplore.ieee.org/document/9196712/",
                "date_of_publication": "15 September 2020",
                "doi": "10.1109/ICRA40945.2020.9196712",
                "citations": "37",
                "abstract": "Incorporating touch as a sensing modality for robots can enable finer and more robust manipulation skills. Existing tactile sensors are either flat, have small sensitive fields or only provide low-resolution signals. In this paper, we introduce OmniTact, a multi-directional high-resolution tactile sensor. OmniTact is designed to be used as a fingertip for robotic manipulation with robotic hands, and uses multiple micro-cameras to detect multi-directional deformations of a gel-based skin. This provides a rich signal from which a variety of different contact state variables can be inferred using modern image processing and computer vision methods. We evaluate the capabilities of OmniTact on a challenging robotic control task that requires inserting an electrical connector into an outlet, as well as a state estimation problem that is representative of those typically encountered in dexterous robotic manipulation, where the goal is to infer the angle of contact of a curved finger pressing against an object. Both tasks are performed using only touch sensing and deep convolutional neural networks to process images from the sensor's cameras. We compare with a state-of-the-art tactile sensor that is only sensitive on one side, as well as a state-of-the-art multi-directional tactile sensor, and find that OmniTact's combination of high-resolution and multi-directional sensing is crucial for reliably inserting the electrical connector and allows for higher accuracy in the state estimation task. Videos and supplementary material can be found here 4 .",
                "ieee_keywords": [
                    "Cameras",
                    "Tactile sensors",
                    "Sensitivity",
                    "Task analysis"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Christopher D Manning",
        "publications": [
            {
                "title": "Veridicality and Utterance Understanding",
                "link": "https://ieeexplore.ieee.org/document/6061472/",
                "date_of_publication": "27 October 2011",
                "doi": "10.1109/ICSC.2011.10",
                "citations": "3",
                "abstract": "Natural language understanding depends heavily on assessing veridicality -- whether the speaker intends to convey that events mentioned are actual, non-actual, or uncertain. However, this property is little used in relation and event extraction systems, and the work that has been done has generally assumed that it can be captured by lexical semantic properties. Here, we show that context and world knowledge play a significant role in shaping veridicality. We extend the Fact Bank corpus, which contains semantically driven veridicality annotations, with pragmatically informed ones. Our annotations are more complex than the lexical assumption predicts but systematic enough to be included in computational work on textual understanding. They also indicate that veridicality judgments are not always categorical, and should therefore be modeled as distributions. We build a classifier to automatically assign event veridicality distributions based on our new annotations. The classifier relies not only on lexical features like hedges or negations, but also structural features and approximations of world knowledge, thereby providing a nuanced picture of the diverse factors that shape veridicality.",
                "ieee_keywords": [
                    "Pragmatics",
                    "Training",
                    "Gold",
                    "Uncertainty",
                    "Training data",
                    "Semantics",
                    "Computational modeling"
                ],
                "author_keywords": []
            },
            {
                "title": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering",
                "link": "https://ieeexplore.ieee.org/document/8953451/",
                "date_of_publication": "09 January 2020",
                "doi": "10.1109/CVPR.2019.00686",
                "citations": "255",
                "abstract": "We introduce GQA, a new dataset for real-world visual reasoning and compositional question answering, seeking to address key shortcomings of previous VQA datasets. We have developed a strong and robust question engine that leverages Visual Genome scene graph structures to create 22M diverse reasoning questions, which all come with functional programs that represent their semantics. We use the programs to gain tight control over the answer distribution and present a new tunable smoothing technique to mitigate question biases. Accompanying the dataset is a suite of new metrics that evaluate essential qualities such as consistency, grounding and plausibility. A careful analysis is performed for baselines as well as state-of-the-art models, providing fine-grained results for different question types and topologies. Whereas a blind LSTM obtains a mere 42.1%, and strong VQA models achieve 54.1%, human performance tops at 89.3%, offering ample opportunity for new research to explore. We hope GQA will provide an enabling resource for the next generation of models with enhanced robustness, improved consistency, and deeper semantic understanding of vision and language.",
                "ieee_keywords": [],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Jiajun Wu",
        "publications": [
            {
                "title": "Differentiable Physics Simulation of Dynamics-Augmented Neural Objects",
                "link": "https://ieeexplore.ieee.org/document/10070844/",
                "date_of_publication": null,
                "doi": "10.1109/LRA.2023.3257707",
                "citations": "537",
                "abstract": "We present a differentiable pipeline for simulating the motion of objects that represent their geometry as a continuous density field parameterized as a deep network. This includes Neural Radiance Fields (NeRFs), and other related models. From the density field, we estimate the dynamical properties of the object, including its mass, center of mass, and inertia matrix. We then introduce a differentiable contact model based on the density field for computing normal and friction forces resulting from collisions. This allows a robot to autonomously build object models that are visually and dynamically accurate from still images and videos of objects in motion. The resulting Dynamics-Augmented Neural Objects (DANOs) are simulated with an existing differentiable simulation engine, Dojo, interacting with other standard simulation objects, such as spheres, planes, and robots specified as URDFs. A robot can use this simulation to optimize grasps and manipulation trajectories of neural objects, or to improve the neural object models through gradient-based real-to-simulation transfer. We demonstrate the pipeline to learn the coefficient of friction of a bar of soap from a real video of the soap sliding on a table. We also learn the coefficient of friction and mass of a Stanford bunny through interactions with a Panda robot arm from synthetic data, and we optimize trajectories in simulation for the Panda arm to push the bunny to a goal location.",
                "ieee_keywords": [
                    "Computational modeling",
                    "Friction",
                    "Trajectory",
                    "Physics",
                    "Geometry",
                    "Dynamics",
                    "Three-dimensional displays"
                ],
                "author_keywords": [
                    "Simulation and animation",
                    "contact modeling",
                    "neural object representations",
                    "differentiable contact simulation",
                    "real-to-sim transfer"
                ]
            },
            {
                "title": "Sonicverse: A Multisensory Simulation Platform for Embodied Household Agents that See and Hear",
                "link": "https://ieeexplore.ieee.org/document/10160461/",
                "date_of_publication": "04 July 2023",
                "doi": "10.1109/ICRA48891.2023.10160461",
                "citations": "78",
                "abstract": "Developing embodied agents in simulation has been a key research topic in recent years. Exciting new tasks, algorithms, and benchmarks have been developed in various simulators. However, most of them assume deaf agents in silent environments, while we humans perceive the world with multiple senses. We introduce Sonicverse, a multisensory simulation platform with integrated audio-visual simulation for training household agents that can both see and hear. Sonicverse models realistic continuous audio rendering in 3D environments in real-time. Together with a new audio-visual VR interface that allows humans to interact with agents with audio, Sonicverse enables a series of embodied AI tasks that need audio-visual perception. For semantic audio-visual navigation in particular, we also propose a new multi-task learning model that achieves state-of-the-art performance. In addition, we demonstrate Sonicverse's realism via sim-to-real transfer, which has not been achieved by other simulators: an agent trained in Sonicverse can successfully perform audio-visual navigation in real-world environments. Sonicverse is available at: https://github.com/StanfordVL/Sonicverse.",
                "ieee_keywords": [
                    "Training",
                    "Solid modeling",
                    "Three-dimensional displays",
                    "Navigation",
                    "Semantics",
                    "Rendering (computer graphics)",
                    "Multitasking"
                ],
                "author_keywords": []
            },
            {
                "title": "Task-Driven Graph Attention for Hierarchical Relational Object Navigation",
                "link": "https://ieeexplore.ieee.org/document/10161157/",
                "date_of_publication": "04 July 2023",
                "doi": "10.1109/ICRA48891.2023.10161157",
                "citations": "118",
                "abstract": "Embodied AI agents in large scenes often need to navigate to find objects. In this work, we study a naturally emerging variant of the object navigation task, hierarchical relational object navigation (HRON), where the goal is to find objects specified by logical predicates organized in a hierarchical structure-objects related to furniture and then to rooms-such as finding an apple on top of a table in the kitchen. Solving such a task requires an efficient representation to reason about object relations and correlate the relations in the environment and in the task goal. HRON in large scenes (e.g. homes) is particularly challenging due to its partial observability and long horizon, which invites solutions that can compactly store the past information while effectively exploring the scene. We demonstrate experimentally that scene graphs are the best-suited representation compared to conventional representations such as images or 2D maps. We propose a solution that uses scene graphs as part of its input and integrates graph neural networks as its backbone, with an integrated task-driven attention mechanism, and demonstrate its better scalability and learning efficiency than state-of-the-art baselines.",
                "ieee_keywords": [
                    "Automation",
                    "Navigation",
                    "Scalability",
                    "Graph neural networks",
                    "Task analysis",
                    "Observability",
                    "Artificial intelligence"
                ],
                "author_keywords": []
            },
            {
                "title": "PyPose: A Library for Robot Learning with Physics-based Optimization",
                "link": "https://ieeexplore.ieee.org/document/10204505/",
                "date_of_publication": "22 August 2023",
                "doi": "10.1109/CVPR52729.2023.02109",
                "citations": "8",
                "abstract": "Deep learning has had remarkable success in robotic perception, but its data-centric nature suffers when it comes to generalizing to ever-changing environments. By contrast, physics-based optimization generalizes better, but it does not perform as well in complicated tasks due to the lack of high-level semantic information and reliance on manual parametric tuning. To take advantage of these two complementary worlds, we present PyPose: a robotics-oriented, PyTorch-based library that combines deep perceptual models with physics-based optimization. PyPose's architecture is tidy and well-organized, it has an imperative style interface and is efficient and user-friendly, making it easy to integrate into real-world robotic applications. Besides, it supports parallel computing of any order gradients of Lie groups and Lie algebras and 2 nd -order optimizers, such as trust region methods. Experiments show that PyPose achieves more than 10× speedup in computation compared to the state-of-the-art libraries. To boost future research, we provide concrete examples for several fields of robot learning, including SLAM, planning, control, and inertial navigation.",
                "ieee_keywords": [
                    "Simultaneous localization and mapping",
                    "Semantics",
                    "Parallel processing",
                    "Libraries",
                    "Robot learning",
                    "Planning",
                    "Pattern recognition"
                ],
                "author_keywords": [
                    "Robotics"
                ]
            },
            {
                "title": "STAP: Sequencing Task-Agnostic Policies",
                "link": "https://ieeexplore.ieee.org/document/10160220/",
                "date_of_publication": "04 July 2023",
                "doi": "10.1109/ICRA48891.2023.10160220",
                "citations": "63",
                "abstract": "Advances in robotic skill acquisition have made it possible to build general-purpose libraries of learned skills for downstream manipulation tasks. However, naively executing these skills one after the other is unlikely to succeed without accounting for dependencies between actions prevalent in longhorizon plans. We present Sequencing Task-Agnostic Policies (STAP), a scalable framework for training manipulation skills and coordinating their geometric dependencies at planning time to solve long-horizon tasks never seen by any skill during training. Given that Q-functions encode a measure of skill feasibility, we formulate an optimization problem to maximize the joint success of all skills sequenced in a plan, which we estimate by the product of their Q-values. Our experiments indicate that this objective function approximates ground truth plan feasibility and, when used as a planning objective, reduces myopic behavior and thereby promotes long-horizon task success. We further demonstrate how STAP can be used for task and motion planning by estimating the geometric feasibility of skill sequences provided by a task planner. We evaluate our approach in simulation and on a real robot. Qualitative results and code are made available at sites.google.com/stanford.edu/stap.",
                "ieee_keywords": [
                    "Training",
                    "Sequential analysis",
                    "Robot kinematics",
                    "Predictive models",
                    "Linear programming",
                    "Libraries",
                    "Planning"
                ],
                "author_keywords": []
            },
            {
                "title": "REALIMPACT: A Dataset of Impact Sound Fields for Real Objects",
                "link": "https://ieeexplore.ieee.org/document/10203308/",
                "date_of_publication": "22 August 2023",
                "doi": "10.1109/CVPR52729.2023.00152",
                "citations": "1",
                "abstract": "Objects make unique sounds under different perturbations, environment conditions, and poses relative to the listener. While prior works have modeled impact sounds and sound propagation in simulation, we lack a standard dataset of impact sound fields of real objects for audio-visual learning and calibration of the sim-to-real gap. We present Realimpact,a large-scale dataset of real object impact sounds recorded under controlled conditions. Re-alimpactcontains 150,000 recordings of impact sounds of 50 everyday objects with detailed annotations, including their impact locations, microphone locations, contact force profiles, material labels, and RGBD images.**The project page and dataset are available at https://samuelpclarke.com/realimpact/ We make preliminary attempts to use our dataset as a reference to current simulation methods for estimating object impact sounds that match the real world. Moreover, we demon-strate the usefulness of our dataset as a testbed for acoustic and audio-visual learning via the evaluation of two bench-mark tasks, including listener location classification and vi-sual acoustic matching.",
                "ieee_keywords": [
                    "Computer vision",
                    "Perturbation methods",
                    "Force",
                    "Acoustics",
                    "Recording",
                    "Pattern recognition",
                    "Calibration"
                ],
                "author_keywords": [
                    "Datasets and evaluation"
                ]
            },
            {
                "title": "The Object Folder Benchmark : Multisensory Learning with Neural and Real Objects",
                "link": "https://ieeexplore.ieee.org/document/10203718/",
                "date_of_publication": "22 August 2023",
                "doi": "10.1109/CVPR52729.2023.01657",
                "citations": "1",
                "abstract": "We introduce the ObjectFolder Benchmark, a benchmark suite of 10 tasks for multisensory object-centric learning, centered around object recognition, reconstruction, and manipulation with sight, sound, and touch. We also introduce the Objectfolder Real dataset, including the multisensory measurements for 100 real-world household objects, building upon a newly designed pipeline for collecting the 3D meshes, videos, impact sounds, and tactile readings of real-world objects. We conduct systematic benchmarking on both the 1,000 multisensory neural objects from Objectfolder, and the real multisensory data from Objectfolder Real. Our results demonstrate the importance of multisensory perception and reveal the respective roles of vision, audio, and touch for different object-centric learning tasks. By publicly releasing our dataset and benchmark suite, we hope to catalyze and enable new research in multisensory object-centric learning in computer vision, robotics, and beyond. Project page: https://objectfolder.stanford.edu",
                "ieee_keywords": [
                    "Computer vision",
                    "Visualization",
                    "Technological innovation",
                    "Three-dimensional displays",
                    "Systematics",
                    "Benchmark testing",
                    "Solids"
                ],
                "author_keywords": [
                    "Multi-modal learning"
                ]
            },
            {
                "title": "pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis",
                "link": "https://ieeexplore.ieee.org/document/9577547/",
                "date_of_publication": "02 November 2021",
                "doi": "10.1109/CVPR46437.2021.00574",
                "citations": "187",
                "abstract": "We have witnessed rapid progress on 3D-aware image synthesis, leveraging recent advances in generative visual models and neural rendering. Existing approaches how-ever fall short in two ways: first, they may lack an under-lying 3D representation or rely on view-inconsistent rendering, hence synthesizing images that are not multi-view consistent; second, they often depend upon representation network architectures that are not expressive enough, and their results thus lack in image quality. We propose a novel generative model, named Periodic Implicit Generative Adversarial Networks (π-GAN or pi-GAN), for high-quality 3D-aware image synthesis. π-GAN leverages neural representations with periodic activation functions and volumetric rendering to represent scenes as view-consistent radiance fields. The proposed approach obtains state-of-the-art results for 3D-aware image synthesis with multiple real and synthetic datasets.",
                "ieee_keywords": [
                    "Image quality",
                    "Visualization",
                    "Computer vision",
                    "Three-dimensional displays",
                    "Image synthesis",
                    "Network architecture",
                    "Rendering (computer graphics)"
                ],
                "author_keywords": []
            },
            {
                "title": "Revisiting the “Video” in Video-Language Understanding",
                "link": "https://ieeexplore.ieee.org/document/9879080/",
                "date_of_publication": "27 September 2022",
                "doi": "10.1109/CVPR52688.2022.00293",
                "citations": "11",
                "abstract": "What makes a video task uniquely suited for videos, beyond what can be understood from a single image? Building on recent progress in self-supervised image-language models, we revisit this question in the context of video and language tasks. We propose the atemporal probe (ATP), a new model for video-language analysis which provides a stronger bound on the baseline accuracy of multimodal models constrained by image-level understanding. By applying this model to standard discriminative video and language tasks, such as video question answering and text-to-video retrieval, we characterize the limitations and potential of current video-language benchmarks. We find that understanding of event temporality is often not necessary to achieve strong or state-of-the-art performance, even compared with recent large-scale video-language models and in contexts intended to benchmark deeper video-level understanding. We also demonstrate how ATP can improve both video-language dataset and model design. We describe a technique for leveraging ATP to better disentangle dataset subsets with a higher concentration of temporally challenging data, improving benchmarking efficacy for causal and temporal understanding. Further, we show that effectively integrating ATP into full video-level temporal models can improve efficiency and state-of-the-art accuracy. 1 1 Project website: https://stanfordvl.github.io/atp-revisit-video-lang/",
                "ieee_keywords": [
                    "Analytical models",
                    "Image recognition",
                    "Buildings",
                    "Benchmark testing",
                    "Question answering (information retrieval)",
                    "Pattern recognition",
                    "Task analysis"
                ],
                "author_keywords": [
                    "Video analysis and understanding; Vision + language"
                ]
            },
            {
                "title": "Multi-Object Manipulation via Object-Centric Neural Scattering Functions",
                "link": "https://ieeexplore.ieee.org/document/10204441/",
                "date_of_publication": "22 August 2023",
                "doi": "10.1109/CVPR52729.2023.00871",
                "citations": "4",
                "abstract": "Learned visual dynamics models have proven effective for robotic manipulation tasks. Yet, it remains unclear how best to represent scenes involving multi-object interactions. Current methods decompose a scene into discrete objects, but they struggle with precise modeling and manipulation amid challenging lighting conditions as they only encode appearance tied with specific illuminations. In this work, we propose using object-centric neural scattering functions (OSFs) as object representations in a model-predictive control framework. OSFs model per-object light transport, enabling compositional scene re-rendering under object rearrangement and varying lighting conditions. By combining this approach with inverse parameter estimation and graph-based neural dynamics models, we demonstrate improved model-predictive control performance and generalization in compositional multi-object environments, even in previously unseen scenarios and harsh lighting conditions.",
                "ieee_keywords": [
                    "Adaptation models",
                    "Visualization",
                    "Computational modeling",
                    "Lighting",
                    "Scattering",
                    "Predictive models",
                    "Rendering (computer graphics)"
                ],
                "author_keywords": [
                    "Robotics"
                ]
            }
        ]
    },
    {
        "name": "Christopher Ré",
        "publications": [
            {
                "title": "Energy-Efficient Abundant-Data Computing: The N3XT 1,000x",
                "link": "https://ieeexplore.ieee.org/document/7368008/",
                "date_of_publication": null,
                "doi": "10.1109/MC.2015.376",
                "citations": "136",
                "abstract": "Next-generation information technologies will process unprecedented amounts of loosely structured data that overwhelm existing computing systems. N3XT improves the energy efficiency of abundant-data applications 1,000-fold by using new logic and memory technologies, 3D integration with fine-grained connectivity, and new architectures for computation immersed in memory.",
                "ieee_keywords": [
                    "Three-dimensional displays",
                    "CNTFETs",
                    "Random access memory",
                    "Memory management",
                    "Logic gates",
                    "Energy efficiency"
                ],
                "author_keywords": [
                    "green computing",
                    "high-performance computing",
                    "big data",
                    "hardware",
                    "emerging technologies",
                    "systems architectures",
                    "integration and modeling",
                    "nanotechnology"
                ]
            },
            {
                "title": "GRIP: A Graph Neural Network Accelerator Architecture",
                "link": "https://ieeexplore.ieee.org/document/9858684/",
                "date_of_publication": null,
                "doi": "10.1109/TC.2022.3197083",
                "citations": "4",
                "abstract": "We present GRIP, a graph neural network accelerator architecture designed for low-latency inference. Accelerating GNNs is challenging because they combine two distinct types of computation: arithmetic-intensive vertex-centric operations and memory-intensive edge-centric operations. GRIP splits GNN inference into a three edge- and vertex-centric execution phases that can be implemented in hardware. GRIP specializes each unit for the unique computational structure found in each phase. For vertex-centric phases, GRIP uses a high performance matrix multiply engine coupled with a dedicated memory subsystem for weights to improve reuse. For edge-centric phases, GRIP use multiple parallel prefetch and reduction engines to alleviate the irregularity in memory accesses. Finally, GRIP supports several GNN optimizations, including an optimization called vertex-tiling that increases the reuse of weight data. We evaluate GRIP by performing synthesis and place and route for a $28 \\;\\mathrm{n}\\mathrm{m}$ implementation capable of executing inference for several widely-used GNN models (GCN, GraphSAGE, G-GCN, and GIN). Across several benchmark graphs, it reduces 99th percentile latency by a geometric mean of $17\\times$ and $23\\times$ compared to a CPU and GPU baseline, respectively, while drawing only $5 \\;\\mathrm{W}$ .",
                "ieee_keywords": [
                    "Aggregates",
                    "Memory management",
                    "Bandwidth",
                    "Sparse matrices",
                    "Programming",
                    "Graph neural networks",
                    "Computational modeling"
                ],
                "author_keywords": [
                    "Accelerator architectures",
                    "neural networks",
                    "hardware",
                    "system-on-chip",
                    "graph neural networks"
                ]
            },
            {
                "title": "Understanding and optimizing asynchronous low-precision stochastic gradient descent",
                "link": "https://ieeexplore.ieee.org/document/8192502/",
                "date_of_publication": "14 December 2017",
                "doi": "10.1145/3079856.3080248",
                "citations": "12",
                "abstract": "Stochastic gradient descent (SGD) is one of the most popular numerical algorithms used in machine learning and other domains. Since this is likely to continue for the foreseeable future, it is important to study techniques that can make it run fast on parallel hardware. In this paper, we provide the first analysis of a technique called BUCKWILD! that uses both asynchronous execution and low-precision computation. We introduce the DMGC model, the first conceptualization of the parameter space that exists when implementing low-precision SGD, and show that it provides a way to both classify these algorithms and model their performance. We leverage this insight to propose and analyze techniques to improve the speed of low-precision SGD. First, we propose software optimizations that can increase throughput on existing CPUs by up to 11×. Second, we propose architectural changes, including a new cache technique we call an obstinate cache, that increase throughput beyond the limits of current-generation hardware. We also implement and analyze low-precision SGD on the FPGA, which is a promising alternative to the CPU for future SGD systems.",
                "ieee_keywords": [
                    "Hardware",
                    "Algorithm design and analysis",
                    "Computational modeling",
                    "Stochastic processes",
                    "Logistics",
                    "Optimization",
                    "Throughput"
                ],
                "author_keywords": [
                    "Stochastic gradient descent",
                    "low precision",
                    "asynchrony",
                    "multicore",
                    "FPGA"
                ]
            },
            {
                "title": "Old techniques for new join algorithms: A case study in RDF processing",
                "link": "https://ieeexplore.ieee.org/document/7495625/",
                "date_of_publication": "23 June 2016",
                "doi": "10.1109/ICDEW.2016.7495625",
                "citations": "12",
                "abstract": "Recently there has been significant interest around designing specialized RDF engines, as traditional query processing mechanisms incur orders of magnitude performance gaps on many RDF workloads. At the same time researchers have released new worst-case optimal join algorithms which can be asymptotically better than the join algorithms in traditional engines. In this paper we apply worst-case optimal join algorithms to a standard RDF workload, the LUBM benchmark, for the first time. We do so using two worst-case optimal engines: (1) LogicBlox, a commercial database engine, and (2) EmptyHeaded, our prototype research engine with enhanced worst-case optimal join algorithms. We show that without any added optimizations both LogicBlox and EmptyHeaded outperform two state-of-the-art specialized RDF engines, RDF-3X and TripleBit, by up to 6× on cyclic join queries-the queries where traditional optimizers are suboptimal. On the remaining, less complex queries in the LUBM benchmark, we show that three classic query optimization techniques enable EmptyHeaded to compete with RDF engines, even when there is no asymptotic advantage to the worst-case optimal approach. We validate that our design has merit as EmptyHeaded outperforms MonetDB by three orders of magnitude and LogicBlox by two orders of magnitude, while remaining within an order of magnitude of RDF-3X and TripleBit.",
                "ieee_keywords": [
                    "Engines",
                    "Resource description framework",
                    "Layout",
                    "Benchmark testing",
                    "Optimization",
                    "Algorithm design and analysis",
                    "Indexes"
                ],
                "author_keywords": []
            },
            {
                "title": "LevelHeaded: A Unified Engine for Business Intelligence and Linear Algebra Querying",
                "link": "https://ieeexplore.ieee.org/document/8509269/",
                "date_of_publication": "25 October 2018",
                "doi": "10.1109/ICDE.2018.00048",
                "citations": "8",
                "abstract": "Pipelines combining SQL-style business intelligence (BI) queries and linear algebra (LA) are becoming increasingly common in industry. As a result, there is a growing need to unify these workloads in a single framework. Unfortunately, existing solutions either sacrifice the inherent benefits of ex-clusively using a relational database (e.g. logical and physical independence) or incur orders of magnitude performance gaps compared to specialized engines (or both). In this work, we study applying a new type of query processing architecture to standard BI and LA benchmarks. To do this, we present a new in-memory query processing engine called LevelHeaded. LevelHeaded uses worst-case optimal joins as its core execution mechanism for both BI and LA queries. With LevelHeaded, we show how crucial optimizations for BI and LA queries can be captured in a worst-case optimal query architecture. Using these optimizations, LevelHeaded outperforms other relational database engines (LogicBlox, MonetDB, and HyPer) by orders of magnitude on standard LA benchmarks, while performing on average within 31% of the best-of-breed BI (HyPer) and LA (Intel MKL) solutions on their own benchmarks. Our results show that such a single query processing architecture can be efficient on both BI and LA queries.",
                "ieee_keywords": [
                    "Engines",
                    "Benchmark testing",
                    "Query processing",
                    "Linear algebra",
                    "Optimization",
                    "Standards",
                    "Business intelligence"
                ],
                "author_keywords": [
                    "join processing",
                    "worst case optimal join",
                    "business intelligence querying",
                    "linear algebra querying"
                ]
            },
            {
                "title": "Arterial travel time estimation based on vehicle re-identification using magnetic sensors: Performance analysis",
                "link": "https://ieeexplore.ieee.org/document/6083003/",
                "date_of_publication": "17 November 2011",
                "doi": "10.1109/ITSC.2011.6083003",
                "citations": "5",
                "abstract": "Two versions of an arterial travel time estimation method based on vehicle re-identification using wireless magnetic sensors were studied across an arterial segment with multiple intersections. Both methods are based on the same travel time estimation system, but one of them uses the so called original signal processing algorithm while the other one uses a recently modified version of it. Both methods were tested on a 0.51 km (0.32 mile)-long segment of West 34th Street in New York, NY, under harsh driving conditions (i.e. right after a winter storm). The original and modified system results were compared against ground truth data obtained from video. Based on the ground truth data it was possible to determine the travel time distribution and the percentage of vehicles that each of the different methods was able to re-identify. During an analysis period of 45 minutes, 318 vehicles were registered to go across the arterial segment. The original method has a 62% re-identification rate, while the modified method has a 69% rate. Based on comparisons of travel time distribution and empirical cumulative distribution functions, it was observed that the modified method travel time distribution is closely related to the ground truth distribution, while the original method significantly diverges from the ground truth at long travel times.",
                "ieee_keywords": [
                    "Vehicles",
                    "Estimation",
                    "Sensor arrays",
                    "Vehicle detection",
                    "Magnetic sensors"
                ],
                "author_keywords": [
                    "Vehicle Re-Identification",
                    "Real-Time Travel Time Estimation",
                    "Arterial Performance Measures",
                    "Magnetic Signature"
                ]
            },
            {
                "title": "Dark Data: Are we solving the right problems?",
                "link": "https://ieeexplore.ieee.org/document/7498366/",
                "date_of_publication": "23 June 2016",
                "doi": "10.1109/ICDE.2016.7498366",
                "citations": "7",
                "abstract": "With the increasing urge of the enterprises to ingest as much data as they can in what's commonly referred to as “Data Lakes”, the new environment presents serious challenges to traditional ETL models and to building analytic layers on top of well-understood global schema. With the recent development of multiple technologies to support this “load-first” paradigm, even traditional enterprises have fairly large HDFS-based data lakes now. They have even had them long enough that their first generation IT projects delivered on some, but not all, of the promise of integrating their enterprise's data assets. In short, we moved from no data to Dark data. Dark data is what enterprises might have in their possession, without the ability to access it or with limited awareness of what this data represents. In particular, business-critical information might still remain out of reach. This panel is about Dark Data and whether we have been focusing on the right data management challenges in dealing with it.",
                "ieee_keywords": [
                    "Big data",
                    "Data mining",
                    "Lakes",
                    "Cleaning",
                    "Databases",
                    "Computer science",
                    "Information retrieval"
                ],
                "author_keywords": []
            },
            {
                "title": "Scene Graph Prediction With Limited Labels",
                "link": "https://ieeexplore.ieee.org/document/9010622/",
                "date_of_publication": "27 February 2020",
                "doi": "10.1109/ICCV.2019.00267",
                "citations": "8",
                "abstract": "Visual knowledge bases such as Visual Genome power numerous applications in computer vision, including visual question answering and captioning, but suffer from sparse, incomplete relationships. All scene graph models to date are limited to training on a small set of visual relationships that have thousands of training labels each. Hiring human annotators is expensive, and using textual knowledge base completion methods are incompatible with visual data. In this paper, we introduce a semi-supervised method that assigns probabilistic relationship labels to a large number of unlabeled images using few labeled examples. We analyze visual relationships to suggest two types of image-agnostic features that are used to generate noisy heuristics, whose outputs are aggregated using a factor graph-based generative model. With as few as 10 labeled examples per relationship, the generative model creates enough training data to train any existing state-of-the-art scene graph model. We demonstrate that our method outperforms all baseline approaches on scene graph prediction by 5.16 recall@100 for PREDCLS. In our limited label setting, we define a complexity metric for relationships that serves as an indicator (R 2 = 0.778) for conditions under which our method succeeds over transfer learning, the de-facto approach for training with limited labels.",
                "ieee_keywords": [
                    "Visualization",
                    "Complexity theory",
                    "Feature extraction",
                    "Genomics",
                    "Bioinformatics",
                    "Training",
                    "Data models"
                ],
                "author_keywords": []
            },
            {
                "title": "Vehicle re-identification using wireless magnetic sensors: Algorithm revision, modifications and performance analysis",
                "link": "https://ieeexplore.ieee.org/document/5983819/",
                "date_of_publication": "18 August 2011",
                "doi": "10.1109/ICVES.2011.5983819",
                "citations": "12",
                "abstract": "A vehicle re-identification method based on matching vehicle signatures obtained from wireless magnetic sensors was studied on a single lane loop on-ramp. Different modifications were implemented in the algorithm in order to address limitations of the system when vehicles stop/move slowly over the detectors. The original and modified vehicle re-identification algorithm results were compared against ground truth data obtained from video. Based on the ground truth data it was possible to determine the percentage of vehicles that are re-identified and the number of those vehicles that are misidentified as a function of different algorithm parameters. For this analysis, vehicles were divided into two subsets: i) uncongested and ii) congested. The original method mismatched percentage was around or below 15% for the uncongested vehicle subset and between 20% to 60% for the congested one. With the modified method it was possible to improve the matching rate as well as the accuracy of the matching algorithm. For the uncongested subset, the modified method showed a higher vehicle re-identification rate while maintaining the mismatched percentage around or below 8%. The main improvement over the original method was achieved on the congested vehicle subset, since the number of re-identified vehicles was increased over the original method while keeping the mismatched percentage around or below 14%.",
                "ieee_keywords": [
                    "Vehicles",
                    "Arrays",
                    "Signal processing algorithms",
                    "Vehicle detection",
                    "Signal processing",
                    "Probability density function",
                    "Cameras"
                ],
                "author_keywords": [
                    "Vehicle Re-Identification",
                    "Magnetic Sensors",
                    "Signal Matching"
                ]
            },
            {
                "title": "Candidate massive galaxies at z ∼ 4 in the Dark Energy Survey",
                "link": "https://ieeexplore.ieee.org/document/8668869/",
                "date_of_publication": null,
                "doi": "10.1093/mnras/sty3305",
                "citations": "Abstract",
                "abstract": "Using stellar population models, we predicted that the Dark Energy Survey (DES) – due to its special combination of area (5000 deg 2 ) and depth (i = 24.3) – would be in the position to detect massive (≳10 11 M ⊙ ) galaxies at $z$  ∼ 4. We confront those theoretical calculations with the first ∼150 deg 2 of DES data reaching nominal depth. From a catalogue containing ∼5 million sources, ∼26 000 were found to have observed-frame g − r versus r − i colours within the locus predicted for $z$  ∼ 4 massive galaxies. We further removed contamination by stars and artefacts, obtaining 606 galaxies lining up by the model selection box. We obtained their photometric redshifts and physical properties by fitting model templates spanning a wide range of star formation histories, reddening and redshift. Key to constrain the models is the addition, to the optical DES bands g, r, i, $z$ , and Y, of near-IR J, H, K s data from the Vista Hemisphere Survey. We further applied several quality cuts to the fitting results, including goodness of fit and a unimodal redshift probability distribution. We finally select 233 candidates whose photometric redshift probability distribution function peaks around $z$  ∼ 4, have high stellar masses [log (M*/M ⊙ ) ∼ 11.7 for a Salpeter IMF] and ages around 0.1 Gyr, i.e. formation redshift around 5. These properties match those of the progenitors of the most massive galaxies in the local Universe. This is an ideal sample for spectroscopic follow-up to select the fraction of galaxies which are truly at high redshift. These initial results and those at the survey completion, which we shall push to higher redshifts, will set unprecedented constraints on galaxy formation, evolution, and the re-ionization epoch.",
                "ieee_keywords": [],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Aaron Sidford",
        "publications": [
            {
                "title": "Coordinate Methods for Matrix Games",
                "link": "https://ieeexplore.ieee.org/document/9317908/",
                "date_of_publication": "19 January 2021",
                "doi": "10.1109/FOCS46700.2020.00035",
                "citations": "5",
                "abstract": "We develop primal-dual coordinate methods for solving bilinear saddle-point problems of the form minx ∈ Xmaxy ∈ Yy T Ax which contain linear programming, classification, and regression as special cases. Our methods push existing fully stochastic sublinear methods and variance-reduced methods towards their limits in terms of per-iteration complexity and sample complexity. We obtain nearly-constant per-iteration complexity by designing efficient data structures leveraging Taylor approximations to the exponential and a binomial heap. We improve sample complexity via low-variance gradient estimators using dynamic sampling distributions that depend on both the iterates and the magnitude of the matrix entries. Our runtime bounds improve upon those of existing primal-dual methods by a factor depending on sparsity measures of the m by n matrix A. For example, when rows and columns have constant l1/l2 norm ratios, we offer improvements by a factor of m+n in the fully stochastic setting and √{m+n} in the variance-reduced setting. We apply our methods to computational geometry problems, i.e. minimum enclosing ball, maximum inscribed ball, and linear regression, and obtain improved complexity bounds. For linear regression with an elementwise nonnegative matrix, our guarantees improve on exact gradient methods by a factor of √{nnz(A)/(m+n)}.",
                "ieee_keywords": [
                    "Runtime",
                    "Stochastic processes",
                    "Complexity theory",
                    "Gradient methods",
                    "Linear regression",
                    "Data structures",
                    "Sparse matrices"
                ],
                "author_keywords": [
                    "minimax optimization",
                    "stochastic gradient methods",
                    "matrix games",
                    "linear regression"
                ]
            },
            {
                "title": "Derandomization Beyond Connectivity: Undirected Laplacian Systems in Nearly Logarithmic Space",
                "link": "https://ieeexplore.ieee.org/document/8104111/",
                "date_of_publication": "13 November 2017",
                "doi": "10.1109/FOCS.2017.79",
                "citations": "3",
                "abstract": "We give a deterministic Õ(log n)-space algorithm for approximately solving linear systems given by Laplacians of undirected graphs, and consequently also approximating hitting times, commute times, and escape probabilities for undirected graphs. Previously, such systems were known to be solvable by randomized algorithms using O(log n) space (Doron, Le Gall, and Ta-Shma, 2017) and hence by deterministic algorithms using O(log 3/2 n) space (Saks and Zhou, FOCS 1995 and JCSS 1999). Our algorithm combines ideas from time-efficient Laplacian solvers (Spielman and Teng, STOC `04; Peng and Spielman, STOC `14) with ideas used to show that UNDIRECTED S-T CONNECTIVITY is in deterministic logspace (Reingold, STOC `05 and JACM `08; Rozenman and Vadhan, RANDOM `05).",
                "ieee_keywords": [
                    "Laplace equations",
                    "Approximation algorithms",
                    "Complexity theory",
                    "Linear systems",
                    "Computer science",
                    "Electronic mail",
                    "Symmetric matrices"
                ],
                "author_keywords": [
                    "space complexity",
                    "derandomization",
                    "expander graphs",
                    "spectral sparsification",
                    "random walks",
                    "linear systems"
                ]
            },
            {
                "title": "High-precision Estimation of Random Walks in Small Space",
                "link": "https://ieeexplore.ieee.org/document/9317933/",
                "date_of_publication": "19 January 2021",
                "doi": "10.1109/FOCS46700.2020.00123",
                "citations": "3",
                "abstract": "In this paper, we provide a deterministic $\\tilde{O}(\\log N)$ -space algorithm for estimating random walk probabilities on undirected graphs, and more generally Eulerian directed graphs, to within inverse polynomial additive error $(\\epsilon=1/\\text{poly}(N))$ where $N$ is the length of the input. Previously, this problem was known to be solvable by a randomized algorithm using space $O(\\log N)$ (following Aleliunas et al., FOCS '79) and by a deterministic algorithm using space $O(\\log^{3/2}N)$ (Saks and Zhou, FOCS '95 and JCSS '99), both of which held for arbitrary directed graphs but had not been improved even for undirected graphs. We also give improvements on the space complexity of both of these previous algorithms for non-Eulerian directed graphs when the error is negligible $(\\epsilon=1/N^{\\omega(1)})$ , generalizing what Hoza and Zuckerman (FOCS '18) recently showed for the special case of distinguishing whether a random walk probability is 0 or greater than $\\epsilon$ . We achieve these results by giving new reductions between powering Eulerian random-walk matrices and inverting Eulerian Laplacian matrices, providing a new notion of spectral approximation for Eulerian graphs that is preserved under powering, and giving the first deterministic $\\tilde{O}(\\log N)$ -space algorithm for inverting Eulerian Laplacian matrices. The latter algorithm builds on the work of Murtagh et al. (FOCS '17) that gave a deterministic $\\tilde{O}(\\log N)$ -space algorithm for inverting undirected Laplacian matrices, and the work of Cohen et al. (FOCS '19) that gave a randomized $\\tilde{O}(N)$ -time algorithm for inverting Eulerian Laplacian matrices. A running theme throughout these contributions is an analysis of “cycle-lifted graphs,” where we take a graph and “lift” it to a new graph whose adjacency matrix is the tensor product of the original adjacency matrix and a directed cycle (or variants of one).",
                "ieee_keywords": [
                    "Laplace equations",
                    "Generators",
                    "Directed graphs",
                    "Complexity theory",
                    "Approximation algorithms",
                    "Additives",
                    "Power line communications"
                ],
                "author_keywords": [
                    "derandomization",
                    "space complexity",
                    "random walks",
                    "Markov chains",
                    "Laplacian systems",
                    "spectral sparsification",
                    "Eulerian graphs"
                ]
            },
            {
                "title": "Unit Capacity Maxflow in Almost",
                "link": "https://ieeexplore.ieee.org/document/9317871/",
                "date_of_publication": "19 January 2021",
                "doi": "10.1109/FOCS46700.2020.00020",
                "citations": "6",
                "abstract": "We present an algorithm, which given any m-edge n-vertex directed graph with positive integer capacities at most U computes a maximum s-t flow for any vertices s and t in O(m 4/3+o(1) U 1/3 ) time. This improves upon the previous best running times of O(m 11/8+o(1) U 1/4 ) [1], Õ(m√nlogU) [2] and O(mn) [3] when the graph is not too dense and doesn't have large capacities. We build upon advances for sparse maxflow based on interior point methods [1], [4], [5]. Whereas these methods increase the energy of local ℓ 2 -norm minimizing electrical flows, we instead increase the Bregman divergence value of flows which minimize the Bregman divergence with respect to a weighted log barrier. This allows us to trace the central path with progress depending only on ℓ ∞ norm bounds on the congestion vector as opposed to the ℓ 4 norm, which arises in these prior works. Further, we show that smoothed ℓ 2 -ℓ p flows [6], [7] which were used to maximize energy [1] can also be used to efficiently maximize divergence, thereby yielding our desired runtimes. We believe our approach towards Bregman divergences of barriers may be of further interest.",
                "ieee_keywords": [
                    "Runtime",
                    "Optimization",
                    "Laplace equations",
                    "Approximation algorithms",
                    "Standards",
                    "Perturbation methods",
                    "Linear systems"
                ],
                "author_keywords": [
                    "Maximum flow",
                    "interior point method",
                    "bipartite matching",
                    "optimization"
                ]
            },
            {
                "title": "Improved Lower Bounds for Submodular Function Minimization",
                "link": "https://ieeexplore.ieee.org/document/9996671/",
                "date_of_publication": "28 December 2022",
                "doi": "10.1109/FOCS54457.2022.00030",
                "citations": "93",
                "abstract": "We provide a generic technique for constructing families of submodular functions to obtain lower bounds for submodular function minimization (SFM). Applying this technique, we prove that any deterministic SFM algorithm on a ground set of n elements requires at least $\\Omega(n\\log n)$ queries to an evaluation oracle. This is the first super-linear query complexity lower bound for SFM and improves upon the previous best lower bound of 2n given by [Graur et al., ITCS 2020]. Using our construction, we also prove that any (possibly randomized) parallel SFM algorithm, which can make up to poly $(n)$ queries per round, requires at least $\\Omega(n/\\log n)$ rounds to minimize a submodular function. This improves upon the previous best lower bound of $\\tilde{\\Omega}(n^{1/3})$ rounds due to [Chakrabarty et al., FOCS 2021], and settles the parallel complexity of query-efficient SFM up to logarithmic factors due to a recent advance in [Jiang, SODA 2021].",
                "ieee_keywords": [
                    "Computer science",
                    "Minimization",
                    "Complexity theory"
                ],
                "author_keywords": [
                    "submodular function minimization",
                    "lower bound",
                    "query complexity",
                    "parallel complexity"
                ]
            },
            {
                "title": "Coordinate Methods for Accelerating ℓ∞ Regression and Faster Approximate Maximum Flow",
                "link": "https://ieeexplore.ieee.org/document/8555169/",
                "date_of_publication": "02 December 2018",
                "doi": "10.1109/FOCS.2018.00091",
                "citations": "7",
                "abstract": "In this paper we provide faster algorithms for approximately solving ℓ ∞ regression, a fundamental problem prevalent in both combinatorial and continuous optimization. In particular we provide an accelerated coordinate descent method which converges in k iterations at a O(1/k) rate independent of the dimension of the problem, and whose iterations can be implemented cheaply for many structured matrices. Our algorithm can be viewed as an alternative approach to the recent breakthrough result of Sherman [She17] which achieves a similar running time improvement over classic algorithmic approaches, i.e. smoothing and gradient descent, which either converge at a O(1/√k) rate or have running times with a worse dependence on problem parameters. Our running times match those of [She17] across a broad range of parameters and in certain cases, improves upon it. We demonstrate the efficacy of our result by providing faster algorithms for the well-studied maximum flow problem. We show how to leverage our algorithm to achieve a runtime of Õ(m + √ns/ε) to compute an ε-approximate maximum flow, for an undirected graph with m edges, n vertices, and where s is the squared ℓ 2 norm of the congestion of any optimal flow. As s = O(m) this yields a running time of Õ(m + √nm/ε), generically improving upon the previous best known runtime of Õ(m/ε) in [She17] whenever the graph is slightly dense. Moreover, we show how to leverage this result to achieve improved exact algorithms for maximum flow on a variety of unit capacity graphs. We achieve these results by providing an accelerated coordinate descent method capable of provably exploiting dynamic measures of coordinate smoothness for smoothed versions of ℓ ∞ regression. Our analysis leverages the structure of the Hessian of the smoothed problem via a simple bound on its trace, as well as techniques for exploiting column sparsity of the constraint matrix for faster sampling and improved smoothness estimates. We hope that the work of this... (Show More)",
                "ieee_keywords": [
                    "Approximation algorithms",
                    "Acceleration",
                    "Runtime",
                    "Optimization",
                    "Complexity theory",
                    "Computer science",
                    "Smoothing methods"
                ],
                "author_keywords": [
                    "ℓ_∞ regression",
                    "structured linear programming",
                    "accelerated coordinate descent",
                    "maximum flow"
                ]
            },
            {
                "title": "Bipartite Matching in Nearly-linear Time on Moderately Dense Graphs",
                "link": "https://ieeexplore.ieee.org/document/9317961/",
                "date_of_publication": "19 January 2021",
                "doi": "10.1109/FOCS46700.2020.00090",
                "citations": "18",
                "abstract": "We present an $\\tilde{O}(m+n^{1.5})$ -time randomized algorithm for maximum cardinality bipartite matching and related problems (e.g. transshipment, negative-weight shortest paths, and optimal transport) on m-edge, n-node graphs. For maximum cardinality bipartite matching on moderately dense graphs, i.e. $m=\\Omega(n^{1.5})$ , our algorithm runs in time nearly linear in the input size and constitutes the first improvement over the classic $O(m\\sqrt{n})$ -time [Dinic 1970; Hopcroft-Karp 1971; Karzanov 1973] and $\\widetilde{O}(n^{\\omega})$ -time algorithms [Ibarra-Moran 1981] (where currently $\\omega\\approx 2.373$ ). On sparser graphs, i.e. when $m=n^{9/8+\\delta}$ for any constant $\\delta > 0$ , our result improves upon the recent advances of [Madry 2013] and [Liu-Sidford 2020b, 2020a] which achieve an $\\widetilde{O}(m^{4/3+o(1)})$ runtime. We obtain these results by combining and advancing recent lines of research in interior point methods (IPMs) and dynamic graph algorithms. First, we simplify and improve the IPM of [v.d.Brand-Lee-Sidford-Song 2020], providing a general primal-dual IPM framework and new sampling-based techniques for handling infeasibility induced by approximate linear system solvers. Second, we provide a simple sublinear-time algorithm for detecting and sampling high-energy edges in electric flows on expanders and show that when combined with recent advances in dynamic expander decompositions, this yields efficient data structures for maintaining the iterates of both [v.d.Brand et al.] and our new IPMs. Combining this general machinery yields a simpler $\\widetilde{O}(n\\sqrt{m})$ time algorithm for matching based on the logarithmic barrier function, and our state-of-the-art $\\widetilde{O}(m+n^{1.5})$ time algorithm for matching based on the [Lee-Sidford 2014] barrier (as regularized in [v.d.Brand et al.]).",
                "ieee_keywords": [
                    "Runtime",
                    "Data structures",
                    "Heuristic algorithms",
                    "Approximation algorithms",
                    "Symmetric matrices",
                    "Sparse matrices",
                    "Machine learning algorithms"
                ],
                "author_keywords": [
                    "bipartite matching",
                    "shortest paths",
                    "transshipment",
                    "optimal transport",
                    "nearly linear time",
                    "interior point method",
                    "linear program"
                ]
            },
            {
                "title": "Parallel Reachability in Almost Linear Work and Square Root Depth",
                "link": "https://ieeexplore.ieee.org/document/8948616/",
                "date_of_publication": "06 January 2020",
                "doi": "10.1109/FOCS.2019.00098",
                "citations": "2",
                "abstract": "2008 11th IEEE High Assurance Systems Engineering Symposium Published: 2008 Communication-Efficient Distributed Algorithms for Solving Linear Algebraic Equations over Directed Graphs 2020 59th IEEE Conference on Decision and Control (CDC) Published: 2020 Show More References References is not available for this document.",
                "ieee_keywords": [
                    "Distributed algorithms",
                    "Computational modeling",
                    "Heuristic algorithms",
                    "Parallel algorithms",
                    "Directed graphs",
                    "Complexity theory",
                    "Optimization"
                ],
                "author_keywords": [
                    "Parallel Computing",
                    "Distributed Computing",
                    "Data Structures and Algorithms"
                ]
            },
            {
                "title": "Faster Matroid Intersection",
                "link": "https://ieeexplore.ieee.org/document/8948634/",
                "date_of_publication": "06 January 2020",
                "doi": "10.1109/FOCS.2019.00072",
                "citations": "4",
                "abstract": "In this paper we consider the classic matroid intersection problem: given two matroids M 1 = (V, I 1 ) and M 2 = (V, I 2 ) defined over a common ground set V , compute a set S ∈ I 1 ∩ I 2 of largest possible cardinality, denoted by r. We consider this problem both in the setting where each Mi is accessed through an independence oracle, i.e. a routine which returns whether or not a set S ∈ I i in T ind time, and the setting where each Mi is accessed through a rank oracle, i.e. a routine which returns the size of the largest independent subset of S in M i in T rank time. In each setting we provide faster exact and approximate algorithms. Given an independence oracle, we provide an exact O(nr log r · T ind ) time algorithm. This improves upon previous best known running times of O(nr 1.5 ·T ind ) due to Cunningham O(n 2 ·T ind in 1986 and + n 3 ) due to Lee, Sidford, and Wong in 2015. We also provide two algorithms which compute a (1- ε-approximate solution to matroid intersection running in times O(n 1.5 /ε 1.5 · Tind) and O((n 2 r -1 ε -2 + r 1.5 ε -4.5 ) · Tind), respectively. These results improve upon the O(nr/ε · T ind )time algorithm of Cunningham (noted recently by Chekuri and Quanrud). Given a rank oracle, we provide algorithms with even better dependence on n and r. We provide an O(n√r log n · T rank )time exact algorithm and an O(nε -1 log n · T rank )-time algorithm which obtains a (1 - 0)-approximation to the matroid intersection problem. The former result improves over the O(nr · T rank + n 3 )-time algorithm by Lee, Sidford, and Wong. The rank oracle is of particular interest as the matroid intersection problem with this oracle is a special case (via Edmond's minimax characterization of matroid intersection) of the submodular function minimization (SFM) problem with an evaluation oracle, and understanding SFM query complexity is an outstanding open question.",
                "ieee_keywords": [
                    "Approximation algorithms",
                    "Minimization",
                    "Complexity theory",
                    "Image edge detection",
                    "Computer science",
                    "Optimization",
                    "Distance measurement"
                ],
                "author_keywords": [
                    "Matroids",
                    "Combinatorial Optimization",
                    "Submodular Functions"
                ]
            },
            {
                "title": "Faster Algorithms for Computing the Stationary Distribution, Simulating Random Walks, and More",
                "link": "https://ieeexplore.ieee.org/document/7782973/",
                "date_of_publication": "15 December 2016",
                "doi": "10.1109/FOCS.2016.69",
                "citations": "20",
                "abstract": "In this paper, we provide faster algorithms for computing variousfundamental quantities associated with random walks on a directedgraph, including the stationary distribution, personalized PageRankvectors, hitting times, and escape probabilities. In particular, ona directed graph with n vertices and m edges, we show how tocompute each quantity in time Õ(m3/4n + mn2/3), wherethe Õ notation suppresses polylog factors in n, the desired accuracy, and the appropriate condition number (i.e. themixing time or restart probability). Our result improves upon the previous fastest running times for these problems, previous results either invoke a general purpose linearsystem solver on a n × n matrix with m non-zero entries, or depend polynomially on the desired error or natural condition numberassociated with the problem (i.e. the mixing time or restart probability). For sparse graphs, we obtain a running time of Õ(n7/4), breaking the O(n2) barrier of the best running time one couldhope to achieve using fast matrix multiplication. We achieve our result by providing a similar running time improvementfor solving directed Laplacian systems, a natural directedor asymmetric analog of the well studied symmetric or undirected Laplaciansystems. We show how to solve such systems in time Õ(m3/4n + mn2/3), and efficiently reduce a broad range of problems to solving Õ(1) directed Laplacian systems on Eulerian graphs. We hope these resultsand our analysis open the door for further study into directedspectral graph theory.",
                "ieee_keywords": [
                    "Laplace equations",
                    "Linear systems",
                    "Graph theory",
                    "Algorithm design and analysis",
                    "Clustering algorithms",
                    "Sparse matrices",
                    "Partitioning algorithms"
                ],
                "author_keywords": [
                    "PageRank",
                    "Markov chain",
                    "Laplacian",
                    "solver",
                    "diagonally dominant",
                    "stationary distribution"
                ]
            }
        ]
    },
    {
        "name": "Gordon Wetzstein",
        "publications": [
            {
                "title": "Deep Optics: Learning Cameras and Optical Computing Systems",
                "link": "https://ieeexplore.ieee.org/document/9443575/",
                "date_of_publication": "03 June 2021",
                "doi": "10.1109/IEEECONF51394.2020.9443575",
                "citations": "385",
                "abstract": "Neural networks and other advanced image processing algorithms excel in a wide variety of computer vision and imaging applications, but their high performance also comes at a high computational cost and their success is sometimes limited. Here, we review recent hybrid optical-digital strategies to computational imaging that outsource parts of the algorithm into the optical domain. Using such a co-design of optics and image processing, we can facilitate application-domain-specific cameras or compute parts of a convolutional neural network in optics. Optical computing happens at the speed of light and without any memory or power requirements, thereby opening new directions for intelligent imaging systems.",
                "ieee_keywords": [
                    "Computer vision",
                    "Image processing",
                    "Neural networks",
                    "Memory management",
                    "Optical computing",
                    "Optical fiber networks",
                    "Optics"
                ],
                "author_keywords": [
                    "computational optics",
                    "computational imaging",
                    "optical neural networks"
                ]
            },
            {
                "title": "An Easy-to-Use Pipeline for an RGBD Camera and an AR Headset",
                "link": "https://ieeexplore.ieee.org/document/9000808/",
                "date_of_publication": null,
                "doi": "10.1162/pres_a_00326",
                "citations": "1",
                "abstract": "Authors Citations Metrics",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Suremap: Predicting Uncertainty in Cnn-Based Image Reconstructions Using Stein’s Unbiased Risk Estimate",
                "link": "https://ieeexplore.ieee.org/document/9414306/",
                "date_of_publication": "13 May 2021",
                "doi": "10.1109/ICASSP39728.2021.9414306",
                "citations": "5",
                "abstract": "Convolutional neural networks (CNN) have emerged as a powerful tool for solving computational imaging reconstruction problems. However, CNNs are generally difficult-to-understand black-boxes. Accordingly, it is challenging to know when they will work and, more importantly, when they will fail. This limitation is a major barrier to their use in safety-critical applications like medical imaging: Is that blob in the reconstruction an artifact or a tumor?In this work we use Stein’s unbiased risk estimate (SURE) to develop per-pixel confidence intervals, in the form of heatmaps, for compressive sensing reconstruction using the approximate message passing (AMP) framework with CNN-based denoisers. These heatmaps tell end-users how much to trust an image formed by a CNN, which could greatly improve the utility of CNNs in various computational imaging applications.",
                "ieee_keywords": [
                    "Heating systems",
                    "Uncertainty",
                    "Image coding",
                    "Message passing",
                    "Tools",
                    "Signal processing",
                    "Sensors"
                ],
                "author_keywords": [
                    "Compressive Sensing",
                    "Approximate Message Passing",
                    "CNN",
                    "MRI"
                ]
            },
            {
                "title": "Keyhole Imaging: Non-Line-of-Sight Imaging and Tracking of Moving Objects Along a Single Optical Path",
                "link": "https://ieeexplore.ieee.org/document/9302876/",
                "date_of_publication": null,
                "doi": "10.1109/TCI.2020.3046472",
                "citations": "17",
                "abstract": "Non-line-of-sight (NLOS) imaging and tracking is an emerging technology that allows the shape or position of objects around corners or behind diffusers to be recovered from transient, time-of-flight measurements. However, existing NLOS approaches require the imaging system to scan a large area on a visible surface, where the indirect light paths of hidden objects are sampled. In many applications, such as robotic vision or autonomous driving, optical access to a large scanning area may not be available, which severely limits the practicality of existing NLOS techniques. Here, we propose a new approach, dubbed keyhole imaging, that captures a sequence of transient measurements along a single optical path, for example, through a keyhole. Assuming that the hidden object of interest moves during the acquisition time, we effectively capture a series of time-resolved projections of the object's shape from unknown viewpoints. We derive inverse methods based on expectation-maximization to recover the object's shape and location using these measurements. Then, with the help of long exposure times and retroreflective tape, we demonstrate successful experimental results with a prototype keyhole imaging system.",
                "ieee_keywords": [
                    "Imaging",
                    "Nonlinear optics",
                    "Image reconstruction",
                    "Optical imaging",
                    "Tomography",
                    "Shape",
                    "Optical variables measurement"
                ],
                "author_keywords": [
                    "Non-line-of-sight",
                    "time-of-flight",
                    "unknown-view tomography"
                ]
            },
            {
                "title": "Reconstructing Transient Images from Single-Photon Sensors",
                "link": "https://ieeexplore.ieee.org/document/8099729/",
                "date_of_publication": "09 November 2017",
                "doi": "10.1109/CVPR.2017.246",
                "citations": "60",
                "abstract": "Computer vision algorithms build on 2D images or 3D videos that capture dynamic events at the millisecond time scale. However, capturing and analyzing “transient images” at the picosecond scale-i.e., at one trillion frames per second-reveals unprecedented information about a scene and light transport within. This is not only crucial for time-of-flight range imaging, but it also helps further our understanding of light transport phenomena at a more fundamental level and potentially allows to revisit many assumptions made in different computer vision algorithms. In this work, we design and evaluate an imaging system that builds on single photon avalanche diode (SPAD) sensors to capture multi-path responses with picosecond-scale active illumination. We develop inverse methods that use modern approaches to deconvolve and denoise measurements in the presence of Poisson noise, and compute transient images at a higher quality than previously reported. The small form factor, fast acquisition rates, and relatively low cost of our system potentially makes transient imaging more practical for a range of applications.",
                "ieee_keywords": [
                    "Transient analysis",
                    "Photonics",
                    "Sensors",
                    "Image resolution",
                    "Holography",
                    "Histograms"
                ],
                "author_keywords": []
            },
            {
                "title": "Deep S3PR: Simultaneous Source Separation and Phase Retrieval Using Deep Generative Models",
                "link": "https://ieeexplore.ieee.org/document/9413714/",
                "date_of_publication": "13 May 2021",
                "doi": "10.1109/ICASSP39728.2021.9413714",
                "citations": "4",
                "abstract": "This paper introduces and solves the simultaneous source separation and phase retrieval (S 3 PR) problem. S 3 PR is an important but largely unsolved problem in a number application domains, including microscopy, wireless communication, and imaging through scattering media, where one has multiple independent coherent sources whose phase is difficult to measure. In general, S 3 PR is highly under-determined, non-convex, and difficult to solve. In this work, we demonstrate that by restricting the solutions to lie in the range of a deep generative model, we can constrain the search space sufficiently to solve S 3 PR.Code associated with this work is available at https://github.com/computational-imaging/DeepS3PR. An extended version of this work is available at https://arxiv.org/abs/2002.05856.",
                "ieee_keywords": [
                    "Wireless communication",
                    "Source separation",
                    "Phase measurement",
                    "Microscopy",
                    "Conferences",
                    "Scattering",
                    "Media"
                ],
                "author_keywords": [
                    "Phase Retrieval",
                    "Source Separation",
                    "Deep Generative Models"
                ]
            },
            {
                "title": "D-VDAMP: Denoising-Based Approximate Message Passing for Compressive MRI",
                "link": "https://ieeexplore.ieee.org/document/9414708/",
                "date_of_publication": "13 May 2021",
                "doi": "10.1109/ICASSP39728.2021.9414708",
                "citations": "3",
                "abstract": "Plug and play (P&P) algorithms iteratively apply highly optimized image denoisers to impose priors and solve computational image reconstruction problems, to great effect. However, in general the \"effective noise\", that is the difference between the true signal and the intermediate solution, within the iterations of P&P algorithms is neither Gaussian nor white. This fact makes existing denoising algorithms suboptimal.In this work, we propose a CNN architecture for removing colored Gaussian noise and combine it with the recently proposed VDAMP algorithm, whose effective noise follows a predictable colored Gaussian distribution. We apply the resulting denoising-based VDAMP (D-VDAMP) algorithm to variable density sampled compressive MRI where it substantially outperforms existing techniques.",
                "ieee_keywords": [
                    "Magnetic resonance imaging",
                    "Message passing",
                    "Noise reduction",
                    "Signal processing algorithms",
                    "Signal processing",
                    "Prediction algorithms",
                    "Approximation algorithms"
                ],
                "author_keywords": [
                    "Compressive Sensing",
                    "MRI",
                    "Approximate Message Passing",
                    "Plug and Play",
                    "Denoising"
                ]
            },
            {
                "title": "Aperture interference and the volumetric resolution of light field fluorescence microscopy",
                "link": "https://ieeexplore.ieee.org/document/7951486/",
                "date_of_publication": "19 June 2017",
                "doi": "10.1109/ICCPHOT.2017.7951486",
                "citations": "1",
                "abstract": "Light field microscopy (LFM) is an emerging technique for volumetric fluorescence imaging, but widespread use is hampered by its poor spatial resolution. Using diffraction-based analysis we show how this degraded resolution arises because conventional LFM aims to sample four dimensions of the light field. By instead prioritizing 3D volumetric information over 4D sampling, we can optically interfere certain redundant angular samples to allow higher spatial resolution while maintaining enough angular information for depth discrimination. With this in mind, we design a number of aperture plane sampling schemes, characterize their frequency support and invertibility, and describe how their relative performance depends on the operating signal-to-noise regime. With simulations and a prototype, we demonstrate a time-sequential amplitude mask-based acquisition approach that outperforms conventional LFM in terms of both spatial resolution and axial field of view.",
                "ieee_keywords": [
                    "Apertures",
                    "Spatial resolution",
                    "Microscopy",
                    "Three-dimensional displays",
                    "Interference"
                ],
                "author_keywords": []
            },
            {
                "title": "Depth from Defocus with Learned Optics for Imaging and Occlusion-aware Depth Estimation",
                "link": "https://ieeexplore.ieee.org/document/9466261/",
                "date_of_publication": "01 July 2021",
                "doi": "10.1109/ICCP51581.2021.9466261",
                "citations": "14",
                "abstract": "Monocular depth estimation remains a challenging problem, despite significant advances in neural network architectures that leverage pictorial depth cues alone. Inspired by depth from defocus and emerging point spread function engineering approaches that optimize programmable optics end-to-end with depth estimation networks, we propose a new and improved framework for depth estimation from a single RGB image using a learned phase-coded aperture. Our optimized aperture design uses rotational symmetry constraints for computational efficiency, and we jointly train the optics and the network using an occlusion-aware image formation model that provides more accurate defocus blur at depth discontinuities than previous techniques do. Using this framework and a custom prototype camera, we demonstrate state-of-the art image and depth estimation quality among end-to-end optimized computational cameras in simulation and experiment.",
                "ieee_keywords": [
                    "Photography",
                    "Computational modeling",
                    "Neural networks",
                    "Estimation",
                    "Prototypes",
                    "Apertures",
                    "Optics"
                ],
                "author_keywords": [
                    "Computational Photography",
                    "Computational Optics"
                ]
            },
            {
                "title": "Deep Adaptive LiDAR: End-to-end Optimization of Sampling and Depth Completion at Low Sampling Rates",
                "link": "https://ieeexplore.ieee.org/document/9105252/",
                "date_of_publication": "02 June 2020",
                "doi": "10.1109/ICCP48838.2020.9105252",
                "citations": "17",
                "abstract": "Current LiDAR systems are limited in their ability to capture dense 3D point clouds. To overcome this challenge, deep learning-based depth completion algorithms have been developed to inpaint missing depth guided by an RGB image. However, these methods fail for low sampling rates. Here, we propose an adaptive sampling scheme for LiDAR systems that demonstrates state-of-the-art performance for depth completion at low sampling rates. Our system is fully differentiable, allowing the sparse depth sampling and the depth inpainting components to be trained end-to-end with an upstream task.",
                "ieee_keywords": [],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Dan Boneh",
        "publications": [
            {
                "title": "Fidelius: Protecting User Secrets from Compromised Browsers",
                "link": "https://ieeexplore.ieee.org/document/8835331/",
                "date_of_publication": "16 September 2019",
                "doi": "10.1109/SP.2019.00036",
                "citations": "19",
                "abstract": "Users regularly enter sensitive data, such as passwords, credit card numbers, or tax information, into the browser window. While modern browsers provide powerful client-side privacy measures to protect this data, none of these defenses prevent a browser compromised by malware from stealing it. In this work, we present Fidelius, a new architecture that uses trusted hardware enclaves integrated into the browser to enable protection of user secrets during web browsing sessions, even if the entire underlying browser and OS are fully controlled by a malicious attacker. Fidelius solves many challenges involved in providing protection for browsers in a fully malicious environment, offering support for integrity and privacy for form data, JavaScript execution, XMLHttpRequests, and protected web storage, while minimizing the TCB. Moreover, interactions between the enclave and the browser, the keyboard, and the display all require new protocols, each with their own security considerations. Finally, Fidelius takes into account UI considerations to ensure a consistent and simple interface for both developers and users. As part of this project, we develop the first open source system that provides a trusted path from input and output peripherals to a hardware enclave with no reliance on additional hypervisor security assumptions. These components may be of independent interest and useful to future projects. We implement and evaluate Fidelius to measure its performance overhead, finding that Fidelius imposes acceptable overhead on page load and user interaction for secured pages and has no impact on pages and page components that do not use its enhanced security features.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Stickler: Defending against Malicious Content Distribution Networks in an Unmodified Browser",
                "link": "https://ieeexplore.ieee.org/document/7448352/",
                "date_of_publication": null,
                "doi": "10.1109/MSP.2016.32",
                "citations": "11",
                "abstract": "Website publishers can derive enormous performance benefits and cost savings by directing traffic to their sites through content distribution networks (CDNs). However, publishers who use CDNs must trust they won't modify the site's JavaScript, CSS, images, or other media en route to end users. A CDN that violates this trust could inject ads into websites, downsample media to save bandwidth, or, worse, inject malicious JavaScript code to steal user secrets it couldn't otherwise access. The authors present Stickler, a system for website publishers that guarantees the end-to-end authenticity of content served to users that simultaneously lets publishers reap the benefits of CDNs. Crucially, Stickler achieves these guarantees without requiring modifications to the browser.",
                "ieee_keywords": [
                    "Browsers",
                    "Servers",
                    "Content distribution networks",
                    "Cryptography",
                    "Privacy",
                    "Malware",
                    "Computer security"
                ],
                "author_keywords": [
                    "Web",
                    "Web cryptography",
                    "CDN",
                    "content distribution network",
                    "security"
                ]
            },
            {
                "title": "Falcon — A Flexible Architecture For Accelerating Cryptography",
                "link": "https://ieeexplore.ieee.org/document/9077379/",
                "date_of_publication": "27 April 2020",
                "doi": "10.1109/MASS.2019.00025",
                "citations": "4",
                "abstract": "Internet of Things (IoT) devices, once deployed, must remain secure for their entire lifetime, which can be as long as 20 years. Over this lifetime, devices must be able to update which ciphers they use to meet evolving security requirements. However, devices cannot rely on software updates for their cryptography because software implementations consume too much energy. At the same time, fixed function hardware accelerators such as an AES engine cannot support new ciphers. This paper presents Falcon, a hardware architecture for accelerating a broad range of cryptography on energy limited devices. Rather than accelerate a fixed set of current ciphers, Falcon provides a general execution engine that accelerates dominant and emerging ciphers, such as AES, Cha-Cha, SHA-256, RSA, ECC with Curve25519, as well as post-quantum ciphers such as R-LWE. For cryptography, Falcon provides the flexibility of software while reducing the energy consumption of cryptography by 5-60x compared to software. This reduction makes it feasible for IoT applications to upgrade the ciphers they use after deployment, allowing them to keep up to date with security best practices without reducing their deployment lifetime or reducing the application workload. In an application monitoring the temperature of sensitive medical supplies in hospitals, Falcon doubles the deployment lifetime (2.2x).",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Privacy and Cybersecurity: The Next 100 Years",
                "link": "https://ieeexplore.ieee.org/document/6182691/",
                "date_of_publication": null,
                "doi": "10.1109/JPROC.2012.2189794",
                "citations": "19",
                "abstract": "TLS: First, compute a checksum over the message, append it to the message, and encrypt the result. In symbols, computet:=MAC(k_{m},m)\\ {\\hbox {and output}}\\ c:=E(k_{e},m\\Vert t). View Source IPsec: First, encrypt the message and then output the resulting ciphertext followed by a checksum computed over the ciphertext. In symbols, computec_{0}:=E(k_{e},m)\\ {\\hbox {and output}}\\ c:=c_{0}\\Vert MAC(k_{m},c_{0}). View Source SSH: Send the concatenation of the separately computed encryption and checksum. In symbols, compute and output c:=E(k_{e},m)\\Vert MAC(k_{m},m). View Source During decryption, if the relevant integrity tag fails to verify, the decryption algorithm outputs a distinguished symbol (e.g., \\perp) to indicate error. Clearly the three approaches used in TLS, IPsec, and SSH combine integrity and encryption in three very different ways—the first encrypts the MAC, the second applies MAC to the encryption, and the third uses independent MAC and encryption. But which is right? Are they all secure? without a precise characterization of the desired form of security, we cannot compare these constructions or even evaluate whether they achieve our goals. 2. Definitions Long after these protocols were deployed, a number of papers [1], [2] defined the concept of authenticated encryption. In the threat model associated with authenticated encryption, the attacker is able to obtain the encryption of arbitrary messages of its choice and the attacker's goal is one of the following two: learn information about the decryption of a well-formed challenge ciphertext (thereby defeating confidentiality), or generate a new well-formed ciphertext different from all ciphertexts previously given to the attacker (thereby defeating integrity). If the attacker cannot do either then we say that the system provides authenticated encryption. Armed with a precise definition we can now compare the three constructions above. The IPsec construction can be shown to provide authenticated encryption for any MAC and CPA-secure encryption. The basic reason is that the MAC “locks” the ciphertext so that any modification of the ciphertext en-route will be detected by the decryptor. The TLS construction is not generically secure: there are specific examples of encryption and MAC such that the TLS combination does not provide authenticated encryption [3]. However, for specific encryption systems, such as randomized counter mode encryption, the TLS method provides authenticated encryption even if the MAC is only weakly secure (so called, one-time secure). The reason is that the MAC is protected by the encryption and therefore need not be a fully secure MAC; weak MAC security is sufficient. The SSH construction is known to be secure when a very specific MAC is used, but may not be secure for a general purpose MAC. To see why, recall that a MAC need not preserve confidentiality and therefore MAC(k_{m},m) may leak information about the encrypted plaintext. Based on these comparisons, a designer can choose the appropriate method for the application at hand. When countermode encryption is used, the TLS construction is adequate even if a simple MAC is used. Otherwise, one should use the IPsec construction. This clear understanding is only made possible thanks to the precise formulation of authenticated encryption. Using the definition of authenticated encryption, the National Institute of Standards and Technology (NIST) was able to publish precise encryption modes, called CCM and GCM, designed to meet the definition. 3. The Best Constructions The last chapter in the story is that once the goals of authenticated encryption were clearly spelled out, it turned out that authenticated encryption can be built far more efficiently than by combining encryption and MAC algorithms. The reason is that encryption and MAC systems are often built from an underlying primitive called a block cipher [for example, the Advanced Encryption Standard (AES) is a widely used block cipher]. Building authenticated encryption by combining an encryption and MAC means that every block of the message is processed twice by AES: once for encryption and once for computing the MAC. By using the block cipher directly, it is possible to construct authenticated encryption by processing every block of the message only once [4]. Initially this seemed counterintuitive: without a precise definition it seemed that defeating an active adversary must rely on a combination of encryption and integrity. The precise security definition enabled cryptographers to prove theorems showing that the goals can be achieved more efficiently using a block cipher directly. Had this been known earlier, protocols like TLS and IPsec could have been twice as fast without affecting security. D. Looking Forward 1. Composition One of the most vexing basic problems in computer security is the problem of secure composition. While almost all interesting contemporary systems are built up from smaller components, it is accepted folklore that security properties do not compose. Even if each component is secure in isolation, a system composed of secure components may not meet its security requirements. Attacks using properties of one component to subvert another have shown up in practice in many different settings, including network protocols and infrastructure, web browsers and infrastructure, and application and systems software and hardware. We can divide the composition problem into two separate forms: nondestructive composition and additive composition. Nondestructive composition is the problem of ensuring that if two system components are combined, then neither degrades the security properties of the other. This is particularly complicated when system components share state. For example, if an alternative mode of operation is added to a network protocol, then some party may initiate a session in one mode and simultaneously respond to another session in another mode, using the same public key (shared state) in both. Unless the modes are designed not to interfere, there may be an attack on the multimode protocol that would not arise if only one mode were possible. As another example, new attacks became possible when trusted computing systems were augmented with a new hardware instruction that could operate on protected registers (shared state) previously accessible only through a prescribed protocol. Additive composition supports a combination of system components in a way that accumulates security properties. A basic example is the problem of authenticated encryption described above. Given a CPA-secure encryption function providing confidentiality and a MAC providing integrity, we would like to compose these two to produce authenticated encryption, which has both integrity and confidentiality properties. As we saw with the examples of TLS, IPsec, and SSH, there are several ways to combine these two parts that look reasonable, but only one of them is secure on general grounds. We predict that, in the next decade, there will be significant progress on both additive and nondestructive composition. If we want a system with the positive security features of two components A and B, we need nondestructive composition conditions to be sure that we do not lose security features we want, and additive composition conditions to make sure we get the advantages of A and B combined. Since this is such an important core problem in computer security, we predict that secure composition will receive the increasing attention that it deserves. 2. Overcoming Limitations of Security Modeling By formulating a security model with a precise threat model, it is possible to prove that attackers cannot mount successful attacks. However, in reality, attackers may be successful by using more powerful attacks that are outside the threat model. Two good examples are side-channel and fault attacks. In a side-channel attack, the adversary obtains more information than the model allows by measuring the running time or power used by the running system. This may reveal information about the secret key used by the decryptor enabling the attacker to break the system [5], [6]. In a fault attack, the attacker causes the decryptor to malfunction and observes how the decryptor fails [7]. Again, this may reveal information not covered by the model. Attacks outside the threat model, however, are not a failure of the security modeling paradigm. Once side channels and faults are recognized as legitimate threats, they can be addressed within the paradigm. The challenge is to identify side channels appropriately, extend the threat model by modeling side channels abstractly, and then design systems that remain secure under this threat model [8], [9]. In extending security modeling beyond cryptography, there may be good reasons to consider several different threat models. In cybersecurity, for example, some mechanisms are designed only to protect against an adversary that operates malicious websites, but has no control over the network. We would like security against such attackers and network attackers, of course, but in practice the overhead of additional network protection may make simpler mechanisms more commonly used. 3. Future Computing Paradigms—Quantum Computing Another paradigm change may come from future success in building quantum computers. While at this time it is not clear that sufficiently powerful quantum computers will ever be built, success in this area would have a direct impact on deployed cryptographic systems. First, traditional public-key systems such as RSA, ElGamal, and Elliptic Curves used for key exchange in protocols like TLS would become insecure, no matter what key size is used. Fortunately, in recent years, researchers have come up with new public-key systems that remain secure against adversaries that possess a quantum computer. Currently, the best candidates are public-key systems based on hard problems on lattices [10]. We emphasize that these are classical public-key systems that operate on standard classical computers, and as far as we know, cannot be broken by a quantum computer. The second impact of quantum computers is a generic exhaustive search attack on all symmetric key ciphers that lets one find a k-bit secret key in time 2^{{k}/2}. On a classical computer exhaustive search takes time 2^{k}, but a quantum computer achieves a generic square-root speedup. For example, a 128-bit AES key can be found in time 2^{64}, which today is considered insecure. To defend against these attacks, designers will need to double the key length of all symmetric key systems. Concretely, this means moving to symmetric ciphers where the best classical exhaustive search attack takes time 2^{256}, so that the quantum attack will take time 2^{128}. Of course, there may be better quantum attacks on the cipher and there will likely be a need to design new classical symmetric ciphers that can resist clever quantum attacks. This is a fascinating area for future research. SECTION III. Technology (steven M. bellovin) A. The Past There is an oft-misunderstood quote from Santayana on the fate of those who ignore history. The full paragraph is more interesting [11]: Progress, far from consisting in change, depends on retentiveness. When change is absolute there remains no being to improve and no direction is set for possible improvement: and when experience is not retained, as among savages, infancy is perpetual. Those who cannot remember the past are condemned to repeat it. In the first stage of life the mind is frivolous and easily distracted; it misses progress by failing in consecutiveness and persistence. This is the condition of children and barbarians, in whom instinct has learned nothing from experience. In a second stage men are docile to events, plastic to new habits and suggestions, yet able to graft them on original instincts, which they thus bring to fuller satisfaction. This is the plane of manhood and true progress. Last comes a stage when retentiveness is exhausted and all that happens is at once forgotten; a vain, because unpractical, repetition of the past takes the place of plasticity and fertile readaptation. In a moving world readaptation is the price of longevity. The hard shell, far from protecting the vital principle, condemns it to die down slowly and be gradually chilled; immortality in such a case must have been secured earlier, by giving birth to a generation plastic to the contemporary world and able to retain its lessons. Thus old age is as forgetful as youth, and more incorrigible; it displays the same inattentiveness to conditions; its memory becomes self-repeating and degenerates into an instinctive reaction, like a bird's chirp. In other words, we need to know the deficiencies of the past in order to improve on them, but if we ever relax and get complacent, we will start making the same mistakes again. The history of cryptology, a crucial security and privacy technology, is no exception. Reading Kahn's magisterial work [12], [13], we see several trends that have persisted over the centuries. Security. Historically, the offense has generally had the upper hand. From the Black Chambers of Renaissance Europe to Bletchley Park, Arlington Hall, and beyond, cryptanalysts have been able to read messages in systems that were thought to be very strong. This is a priori quite reasonable: why strengthen a system that has not proved susceptible? Speed. Cryptosystems (and cryptanalysts) have continually had to adapt to communications speeds. Machine encryption was developed to deal with the speed of the telegraph; similarly, the Black Chamber of 18th century Vienna had a schedule dictated by the cycles of the Post Office [12, p. 261]. Usability. Over the years, usability has been a major driver in improvement, originally to aid in encryption speed and accuracy (and hence reduce the need to send some material in plaintext), and later to help avoid the mistakes that would aid enemy cryptanalysts. Amateurs. Many of the major advances in cryptology have come from people outside the cryptologic mainstream, including Jefferson's wheel cipher, public-key cryptography, and the one-time pad [14], [15]. Often, though, these discoveries were not appreciated until after later reinvention. When trying to predict the future, it is reasonable to use these four pillars for our analysis. That said, there seems to be little on which to base predictions about cryptography. Encryption speed, at least in dedicated hardware, is generally adequate today; increasing parallelism should let speeds continue to increase as needed. Similarly, though no one has ever deployed a system they knew to be weak, today's systems have proven remarkably strong; we do have a much better mathematical understanding of the problem today. In the 35 years that the Data Encryption Standard has been with us, only one attack significantly stronger than brute force has been found in the open literature [16]; the other known weakness, a brute force attack, was realized from the beginning and was arguably deliberate [17], [18]. It is impossible to foresee when a brilliant amateur will come up with a fundamentally new cryptologic idea. While the advance of the field makes it unlikely that such a person will conceive of a dramatic new improvement in today's technologies, someone who asks a very different sort of question—public-key cryptography is the obvious parallel—may indeed change the field. But genius occurs when it occurs; it is not predictable. There is less to say about the history of other security technologies, simply because they are so much younger. Fundamentally, all consist of a “trusted computing base” (TCB) (e.g., the operating system) that mediates access to protected resources, and a policy model that says who can have access to what. 1 By and large, this approach has failed for four reasons. The policies were not compatible with the kind of work people needed to do in the real world. The trusted software was buggy, allowing for policy violations. Many security violations now take place outside the TCB. For example, document viewers are never considered trusted, but many a system has been compromised when a document viewer attempted to render a booby-trapped file. Security systems are not usable; neither software developers, system administrators, nor end users can adequately carry out their intentions, even when the underlying system is sufficient. Again, these four points should underlie our prognostication. It is worth noting that usability is the only point in common. One more aspect of security and privacy should be touched upon: attacks are perpetrated by people. Any solution depends on understanding both what must be protected, and against whom. The latter may be random hackers, common criminals, national intelligence services, or corrupt insiders working for any or all of the above. B. Plus Ten Years In fields as mature as security and cryptology, radical changes are rare. It took NIST four years to select the AES [19]; selecting a new hash function will take five [20]. New operating systems take longer still; the design/code/test cycle, the hardware replacement interval, and the need for backwards compatibility all suggest that there will not be many radical changes in the next decade. Rather, the changes that we see will be driven more by technology (e.g., the rise of tablets) and the operational environment, rather than by new scientific or engineering insights. There is the chance of new cryptanalytic attacks on AES (given results such as [20], few would be shocked by a major success); if so, the attack is likely to be quite expensive and not a major threat to most users, but instead to be grounds for picking a newer standard. There is more room for usability improvements in the near term. Both security and cryptology technologies have a long history of user interface problems [13], [22]–[26]; these issues are finally getting significant attention in the mainstream research and development communities. The most likely significant change would be the advent of “encrypted by default”—encryption is always used, with little or no user input needed. A similar trend in privacy technologies seems unlikely, given the (commercially driven?) drive for people to share more of their lives. By this time, we should start to see a better division between the TCB and untrusted code. Rather than a simple binary choice or even a hierarchy, we will start to see more complex graphs of the trust relationships between different components. Trying to manage these graphs will not be easy; the increased complexity will likely more than balance the theoretically improved security. There is no reason whatsoever to think that either dishonesty or buggy software will be gone by then. C. Plus Twenty Years The last ten years have made two technological trends increasingly clear: more and more devices are being controlled by microprocessors, including objects as mundane as coffee makers; in addition, more and more of these controllers are being networked. In 20 years, everything will be connected. Even today, there are network-connected light bulbs for sale. The security implications of this are frightening. However, 20 years is long enough for fundamental new advances in system design to enter into commercial use. A number of new technologies that address today's limitations have been proposed, such as content-based access control [27]; perhaps these will help. Whether this will cope with the complexity problem is quite unclear, since there will be exponentially more user/device pairings to protect. In addition, many of these new devices will leak private information; while this can be beneficial, too often we see things deployed without adequate consideration of the question. It may be that we will see a societal shift away from privacy [28]. We are even told today that “all issues of privacy are old people issues” [29]. It is all but certain that there will still be dishonest people, and our software will still have bugs. D. Plus Fifty Years Fifty years hence is an interesting time scale in which to work, since one can easily look back at the history of the last 50 years of computing. Not only does that interval cover most of the commercial history of computers, most of the basics we rely on today were either present or about to appear, including remote access, large (by the standards of the day) mass storage devices, and video games on screens [30]. Packet switching [31] and protected mode (and hence the TCB) for operating systems had been invented [32]. Handheld mobile phones had been described, at least in science fiction [33]–[35]. What is new are the combinations and the unimaginable complexity of today's systems. It is plausible, then, that many of the basic concepts of the systems of a half-century from now exist today—but of course we do not know which they are or how they will be combined. One intriguing technology area to watch is the use of quantum phenomena. There are already encryptors based on the physical impossibility of undetectably measuring certain properties (such as polarization) of a photon while simultaneously not affecting it [36]. There have been a number of critiques of the concept on technical grounds, but one of the most vexing problems is that such secure communications are inherently point to point, since to a quantum state a legitimate router is indistinguishable from an eavesdropper. Will scientists and engineers find a way around this problem, perhaps by large-scale optical switching to provide direct, end-to-end paths for actual photons? Will there be some sort of quantum routing device, embedded in a tamper-proof chip, that will preserve the right properties while excluding eavesdroppers? A related notion is quantum computation. As noted, it is unknown at this time whether large, reliable quantum computers can be built, especially the massively parallel ones needed to attack symmetric ciphers via exhaustive search. Within 50 years, we should know, and—if they are indeed feasible—be able to build them and use them to attack real problems, with all the implications that we will have for cryptographic security [37]. One can hope that by 50 years from now, the complexity problem will have been tamed; alternatively, it is quite possible that complexity will continue to increase faster than our ability to cope with it. We do feel that there will still be dishonest people, and our software will still have some bugs. E. Plus Hundred Years In order to predict what security and privacy technologies will be like 100 years hence, it helps to envision the dialog one might have via a “chronophone” with Herman Hollerith, the inventor of the punch card, around 1910 or so. The most important basic concepts existed: there were telephones, radios, the ancestor of fax machines, voice recording and playback, cryptography, and of course the high-speed data processing devices that he invented. It would still be extremely challenging to explain things like Facebook, operating system access controls, Google, etc., let alone their challenges and failures. One can just imagine the response: “you're telling me I should ‘drag and drop’ a ‘file’—do you mean a metal tool or a bundle of papers from my desk, and by the way do you know what happens if someone drops a deck of my punch cards?—to an encrypted ‘flash drive’ (I can drive a car, but what does that have to do with a camera flash?); alternatively, I can wander down to the nearest Orthodox church and hope that the priest doesn't get angry with me for clicking at—‘on’?—his icons, all in order to protect something from a worm? A worm? One of those things I stick on my hooks when I go fishing? And why do you spell ‘fishing’ with a ‘ph’ and imply that it's a bad thing?” Privacy and authentication are not new issues. The use of a mother's maiden name as an authenticator goes back to at least 1882 [15] and though that particular scheme may be challenged by social changes [38], some form of secret-based authentication will likely persist. Usability concerns may be eased by the advent of very competent user agents. They may not implement the proverbial “Do What I Mean” (DWIM) instruction; they will, however, know individual's preferences well enough to make sophisticated judgments. Similarly, system file protections will be set automatically based on very high-level statements of how a system should act. [On the other hand, this may be seen as a form of artificial intelligence (AI). We note that AI has been just around the corner for about the last 50 years …] Cryptography is more problematic. It may not be needed (is it even possible to intercept modulated dark energy beams?). A more interesting question is the mathematical underpinning. Will it be possible to prove that some practical systems are unconditionally secure? Alternatively, will it be possible to prove that, say, public-key cryptography is not possible? If the latter, will the proof be of a form that leads to a real attack, or simply demonstrates that one must exist? Finally, there are two predictions we can be quite certain about: there will still be dishonest people, and our software will still have some bugs. SECTION IV. Policy: Never Quite Sure what it is Going to be from One Minute to the Next (susan landau) Fifty years ago phones stayed put, computers were the size of a room, and a call to Europe from the United States required booking in advance. Then, in 1970, one could dial the United Kingdom directly from the United States. Shortly afterwards, the same could be done for the European continent, and then the rest of the world. Now, calling someone no longer requires knowing where they are. But if you do not know where the person you are calling is, others do. And they see ways to use it that you never intended. In 2011, the Chinese government announced that it would track people's movements through their cell phones for better traffic control [39], while a recent study of the Haitian population after the 2010 earthquake showed that similar tracking is extremely useful in informing where people are—and where relief aid should go [40]. In some cases, shutting off a phone does not prevent one from being tracked: in France, in a location of interest—say where charges are being made on stolen credit cards—police note the appearance of shutoff cell phones in the vicinity [41]. The combination of the Internet, social networking sites, and data aggregators have enabled a perfect storm of capability for identifying individuals as they go about their daily lives: who you are [42], where you live and work [43], even whether you are catching the flu [44]. More than a half century ago, Isaac Asimov [45] premised that predicting the flow of human civilization is possible, but the author disavowed predicting actions of individuals. With the data we now have, the latter seems almost within reach. People's ubiquitous use of communications devices reveals their daily habits, their use of social networking sites makes public present activities and future plans, and sensors everywhere—in buildings, cars, even the great outdoors—shows what they are doing, where they are doing it, and with whom [46]. Meanwhile information about their genome reveals information not only about themselves, but also their relations: parents, siblings, even unborn offspring. Technology giveth, but technology also taketh away. Encrypted and peer-to-peer communications tools thwart wiretapping. Tor, an anonymizing overlay network that hides transactional information complicates investigations [46]. Anonymized SIM cards mean you cannot track when or to whom a target is calling. Criminals—and hackers who work in support of nation states—use the lack of IP packet-level attribution to hide who and where they actually are. For law enforcement, the world has become very complex. There are multiple legal requirements on providers—retain communications transactional data, do not retain search data—and multiple ways for the determined user to secure her information. There are multiple computer operating systems, multiple smartphone operating systems, multiple rich sources of information, complicating the investigatory landscape. But while digital forensics may have become more complex to untangle, the change in the way people live means that there are ready electronic trails everywhere for these investigators to mine. The only way to really secure anonymity is to disconnect. In fact, that alone may cause one to stand out. A prime example of this is Osama bin Laden, whose expensive compound in Abbottabad, Pakistan, was notable for its lack of telephone and Internet connectivity. We are at a time when individuals' privacy resides in an Alice-in-Wonderland state [48]. One moment Alice imbibes from the bottle labeled “DRINK ME” [48, p. 31] and she is in the world of cell phones and location tracking with a huge electronic profile. Her privacy almost disappears. Then Alice bites from a small cake [48, p. 63] of end-to-end encryption, Tor, and anonymizing cell phone cards, and her electronic profile shrinks. Her privacy becomes ENORMOUS. A fan appears. Hot and lonely, Alice waves her Facebook fan. Before she knows it, Alice—and 750 million others around the world—are sharing information on what they are doing for the weekend, what movie they are watching on Netflix (and whom they are watching it with)—and hundreds of thousands of other intimacies. Personal privacy has become as minute as the five-inch girl that Alice has become [48, pp. 37–39]. Police pull one way, other government agencies another. Europe, Canada, Australia, and New Zealand have long had activist government efforts that control both government and private enterprise use of individuals' data. The United States has become more activist as well: the U.S. Federal Trade Commission (FTC) has assumed a role as activist privacy regulator [49], while the government's effort for secure online identity management, the National Strategy for Trusted Identities in Cyberspace (NSTIC) [50], has explicitly pressed for privacy protections. And even while the FBI was pressing for stronger surveillance capabilities, the U.S. State Department was simultaneously lauding communication tools enabling secure communication by human rights activists. The United States is not the only place in which this occurs. At almost the same time that the European Union passed a data retention requirement for telecommunications providers, a European Union advisory group challenged the length of time that Google retained user search data [51]. Or as Alice puts it, “I'm never quite sure what I'm going to be, from one minute to the next!” [48, p. 77]. In the battle over privacy, one thing is sure: governments will follow self-interest. Consider the following example. In the late 1990s, European governments were concerned that the U.S. government was eavesdropping not only on diplomatic and military communications, but also on civilian ones, including those of industry. Having been on a trajectory to limit the use of strong cryptography (cryptography that cannot be broken using current technology), the European Union lifted export controls on systems containing strong cryptographic systems. Meanwhile, having spent from 1975 seeking to control civilian use of cryptography, the U.S. position made its own U-turn. The rationale was: partially competition (the government preferred that systems sold containing strong cryptography be domestic in origin), partially pressure from Congress, and partially a realization that it was time to focus elsewhere [52]. What really happened was a divergence of interests between the national security community, which supported the loosening of export controls, and law enforcement, which did not, enabling this shift. National security's dual role of securing information as well as conducting surveillance meant that by the late 1990s it saw the world differently, and that difference meant it supported the wider use of strong encryption. That split has implications for the future. Now predicting anything as capricious as the wild swings of privacy and government policy is hard, but certain drivers of government policy on privacy are clear. Everyone—businesses, governments, individuals—will be collecting and storing data. People will reveal and devices will store information that once was essentially inaccessible or entirely ephemeral: the number of steps they walk and foods they eat each day, the strand of music they listen to while waiting for the subway, the lanes they traverse as they drive to work. Data collected on individuals will continue to be an extraordinarily rich source of information. Its value to private enterprise means that such collection will continue. Capabilities once only available to governments—tools for tracking and tools for hiding aspects of data—will become increasingly available to private industry and even private citizens. In many cases, use of transactional data combined with other rich data sources will obviate the need for content. Remote access will confuse the ability to distinguish inside and outside of an organization (or a nation). Although the distinction is real, this blurring will have large implications for cybersecurity—and for privacy. In the absence of regulation by government, data collection will proliferate madly. Sophisticated users will have tools to hide their tracks—note that the use of complex encryption or anonymizing tools will mark a user as a “party of interest”—while the majority of the populace will have less privacy unless government intervenes. Because of the ready availability of open source data (information from publicly available sources), protection of individuals' privacy will become a national security issue. Democracies and authoritarian nations naturally have different self-interests when it comes to privacy. In democracies, the need to protect large swaths of industry as well as many who work in sensitive but unclassified parts of society—members of the judiciary, family members of the law enforcement and the military, etc.—may create an argument for the government to step in, as it has done in identity management and activity tracking, to protect privacy. Widespread revelations of personal information may not really be much to society's value. Authoritarian regimes function by controlling their population and are less likely to protect user privacy. 2 Many security breaches—and security breaches are often indistinguishable from privacy breaches—cross borders, making privacy an international issue. But international investigations, whether of criminal activity, spying, or hacking, are complicated, and in a world where some nations assiduously protect privacy and others do not, these investigations are often stymied. Many nations have signed the Council of Europe cybercrime treaty, which assures mutual law-enforcement aid in such investigations, but notably Russia and China have not.3 Given the wide disparity of views on whether privacy is worth protecting, global, or near-global, treaties enabling better privacy protections are unlikely. It is much more likely that instead there will be international standards, e.g., such as those emerging on identity management, and cross-national agreements, such as those promulgated by the European Union. Government action will also depend on perceptions of security. A greater ability to measure effectiveness of solutions, something proposed both in National Research Council studies [54], and in conferences such as the Workshops in Economics of Information Security [55], may lead to better governmental privacy protections. The real watchword, though, is volatility. “Are their heads off?,” shouted the Queen. “Their heads are gone, if it please your Majesty,” the soldiers shouted in reply [48, p. 104]. In Alice in Wonderland, the heads were still right where they belonged. In our world though, we will have to keep running as fast as we can to stay in place if we are to have any privacy at all. SECTION V. Economics (michael E. lesk) A. Background There is a traditional Hollywood summary of an actor's career. Who is Joe Blow? Get me Joe Blow. Get me a Joe Blow type. Get me a young Joe Blow. Who is Joe Blow? Right now, most people are still wondering “what is cybersecurity?” As time goes on, we predict people will start to demand it; then want it cheaper, and then eventually forget it. This section will focus not on how the technological problems of security will be addressed as much as who will wind up paying for it. Our lack of any general agreement on how we measure either the cost or value of security makes economic discussions particularly fuzzy and unsatisfying. Nevertheless, with only slightly better cost models, motorists pay for safer cars, with government regulation forcing enhancements such as antilock brakes; but taxes pay for safer roads. What model makes sense for cybersecurity over a long time? 1. Should you Believe Anything I Write about the Next Hundred Years? Either Yogi Berra, or Neils Bohr, or somebody, said something like “Prediction is difficult, especially about the future.” My favorite forecast is an 1888 book by Edward Bellamy entitled Looking Backward [56], which is a description of Boston as it will be in the year 2000. He imagined little technological progress, with everything still powered by steam or compressed air; but he foresaw great social progress, with crime and poverty eliminated. Thus, he had things pretty much backwards. This is often what happens with predictions. The IEEE has decided it wants a view of the future economics of privacy and cybersecurity, and I am replying perhaps in sympathy with Horatio Hornblower, who said “I'd rather be in trouble for having done something than for not having done anything.” And the IEEE has a history of publishing technology forecasts, such as its enthusiasm in 1999 for “wearable computers” and a 340-MB floppy drive [57], not to mention its explanation that year that energy price spikes were caused by bad weather [58]; Enron was mentioned only as an investor in photovoltaics. So, with your understanding that this may be even less reliable than the usual articles in this magazine, here is a guess at cybersecurity and economics. In discussing cybersecurity, there are two distinct problems: accidental losses and malicious behavior. Perhaps the most familiar noncriminal cybersecurity problem is “a disk crash ate my homework.” When there is nothing malicious about the problem, it is much easier to design a solution. General robustness should diminish as a problem. Technology is getting more reliable, and more importantly, cloud storage will provide the redundancy and general service. I am writing this on Google Docs, precisely to allow general access from many locations, and as a side effect freeing me from worry about backups. The primary issue with cloud storage is not robustness but privacy, which is some other author's problem. The economics of the “cloud” seem so attractive that even within ten years, I would expect that “lost files” will be a relatively solved problem at a relatively low cost. Some cloud services are so cheap that they are given away, or provided free in exchange for advertising. Just as nuclear power was going to make electricity “too cheap to meter,” gigabit bandwidths have made e-mail almost free (although this has not stopped text messaging companies from charging enormous amounts per byte for a short message). Even without the cloud, the ease of duplication has meant robust data storage. Surprisingly few important files have been lost. The 1960 census tapes, in the end, were mostly OK; any losses were small and unimportant [59]. Perhaps the most important example is that the National Aeronautics and Space Administration (NASA) lost the higher resolution video recording of the moon landing [60]. As more and more people get their computing from cloud resources with professional management, lost files should cease to be a problem. Yes, we will see the cloud companies outsourcing their operations to countries with inadequate infrastructure and poorly paid staff, but the steadily lower costs of computing will allow them to maintain robustness through redundancy. Criminal behavior will be more difficult to deal with. Cybercrime is of course a new activity. In the same way, during the 19th century we saw the new crime of train robbery, and it was eliminated through better policing. Car theft arose in the 20th century but has now declined as a result of better car locks. Similarly, I would expect that cybersecurity will improve over time. What I do not know is how. Unlike Bellamy, I do not think we will eliminate crime in general. Even if we were able to reduce some kinds of crime within one society, as with the general reduction in murder in the United States between the 1920s and the 1950s, and then, after a resurgence, another reduction during the 1990s and the 2000s, cybercrime is international. Much of the cyberfraud observed in developed countries is associated with foreign origins. Ask anyone what they think of when you say both “computer” and “Nigeria.” At the moment, cybercrime being new, we have not quite faced up to the need to do something about it, and we spend relatively little money on it. For example, a U.K. report suggested that the annual loss to cybercrime was £27 000 million, but that expenditures on coping with cybercrime were £650 million [61]. My expectation is that as we take cybercrime more seriously, we will need to substantially increase what we spend preventing it. But when will that happen? Based on past history, what we should expect is that the problem will get worse until we decide to spend the resources to deal with it. What I do not know is whether we are going to need an incident such as Pearl Harbor or 9/11, or whether we can begin to take steps in a reasonable way. As comparisons, consider things like sports scandals. We live through regular episodes of discovering that matches are being fixed. There was the “Black Sox” baseball scandal of 1919, the college basketball scandal of 1950, and as I write in 2011 there are allegations that world soccer matches are being fixed. Each produces some kind of reaction, involving new governance and investigative procedures. In each case (at least until now) something is done to make the matches more honest. There are also doping scandals, but in several of those cases it did not appear that the authorities showed much initial interest in ending doping (see baseball or cycling). So what is likely to happen? There will be some sufficiently distressing event to cause us to decide that we have to invest the effort needed to fix the problems. Again, look at history. We started air traffic control across the United States after a 1956 midair collision above the Grand Canyon [62]. Britain decided that railway signals should have a default position of red (stop) after an ice storm froze semaphore arms in the green position and caused an accident at Abbots Ripton in 1876 [63]. That particular kind of accident never happened in North America [64], and our signals remained default green (until it became common to turn them off to save on bulb lifetime). So our optimistic scenario is that within the next ten years, some relatively unimportant hacking scenario causes us to insist on more robust software, perhaps a more dramatic version of the episode in which somebody hacked the PBS website but only chose to announce that Tupac Shakur is alive and living in New Zealand [65]. My pessimistic scenario is a collapse of something like our electricity infrastructure or our banking infrastructure. Unfortunately, I suspect that it will take something dramatic to raise public awareness: the wiretapping of several Greek cabinet ministers, for example, has passed almost without notice in the rest of the world, and even wiretapping by Murdoch newspapers has not produced many calls for improvements to the security of voicemail systems. I fear we will need a disaster that affects far more people. Cybersecurity is an intricate mixture of actions taken by criminals, manufacturers, governments, and users. The economics of cybersecurity is about who should pay how much for what steps. Will cybersecurity economics be primarily money spent by the government on police efforts to put malefactors in jail? Or by manufacturers to build more robust software? Or by users to install virus checkers and password managers? Or by users in the form of losses to crime? Initially, the response is going to be from users. After 9/11, lots of people stocked up on canned food and bottled water. Similarly, after the trigger incident, whatever it is, people will start buying anything sold as virus protection or bank account insurance. Next, government action will come, with legal programs to investigate and prosecute the criminals. I am assuming that whatever the trigger event is, it will be worldwide, and so it will be possible to get international cooperation in chasing down the perpetrators. It may be expensive, but I do not see anyone quibbling about the cost of killing Osama bin Laden. Finally, it will become private industry—vendors will harden their products, and users will pay a bit more for better and more robust service. Stopping cybercrime should be easier than other law enforcement activities. Cybercrime is not as frightening as crimes of personal violence like robbery or kidnapping; nobody is afraid to leave their house at night because they might meet a spammer. It usually works through the financial system, meaning that its victims are sophisticated enough to have bank accounts and e-mail services. There is a problem with permanence; problems where you might in the past have said, “that was a mistake, but at least it is behind me” now live forever on the Internet. This offers new blackmail possibilities. Cybercrime is also not something that attracts public sympathy. IMDB reports that 17 movies have been made about John Dillinger and 80 about Jesse James; 115 have been made about Robin Hood. Even more recently, there have been four movies about the “Great Train Robbery” which took place near Aylesbury, U.K., in 1963. By contrast, I do not know the name of any cyberfraud hero. And I cannot see any cybercriminal featuring in a video game, a museum, or a festival, all of which exist for the James brothers. There are innumerable movies about geeks who use their hacking skills to defeat criminals and/or save the world (while finding true love at the same time), but they are the sheriff, not the bad guys. Assuming that somehow we do decide that we are going to pay for decent cybersecurity, who is likely to bear the cost? There are three possibilities: users, vendors, or the government. As always, the loudest voice is from the vendors, explaining why this is not their problem. For example, a Network World item from 2009 is headlined “User education key to IT security: Microsoft” [66]. Similarly, vendors of electronic medical records systems regularly attribute all problems to user errors, rather than bad product design. The right response is a line by Masys about aviation safety: “pilot error is not an explanation but rather is something to be explained” [67]. However, in practice, it has been relatively easy for software systems to place the blame on users, rather than themselves. This is not a new discussion. In 1909, the railway engineer Wilson wrote “the Machine is now perfected, and these pages will, without doubt, force the reader to the conclusion that present day accidents are due to failure of the Man” [68]. Then, in 1925, the same author said “the record of 40 to 50 years ago, which now appears to us so terrible a tale of destruction, was due less to the sins of omission and commission of the men than to the neglect of the companies to provide safeguards that were available” [69]. He even had the chutzpah to write in the preface to the 1925 book “in innumerable instances blame was cast on signalmen that should have been laid at the door of the officers and directors of the company, who failed to appreciate the benefits that concentration, the block, and uniform signaling would have provided,” not mentioning that he had been one of those casting the blame. Given that we decide we have to do something about cybersecurity, who will wind up paying for it? We can imagine several scenarios. Everybody is persuaded that they have to buy more robust systems or better virus checkers, and spend their own money on it. This would be in the same way, for example, that large numbers of people now sign up for intensively advertised credit reporting services. Vendors would have to be advertising that their systems were safe and there would need to be a way for purchasers to believe that the system was safe. The government might impose either liability or standards on the vendors. As an example, it is a result of government standards that every new car has seat belts and air bags. As an alternative, products such as asbestos insulation, IUD contraceptives, and climbing gyms have been driven out not by direct regulation but by liability lawsuits. Police agencies might get better funding to seek out cybercriminals, in the way that a rash of publicized bank robberies encouraged the creation of the modern FBI. The payment mechanism is likely to change over time, as the threat level rises and falls. Today, for most people, we are at the “what is cybersecurity?” level. Pretty soon people will want it, then they will want it but cheaper, then it will become routine, and then it will be forgotten. So right now, cybersecurity is a specialty for a few computer experts. We are in the “who is Joe Blow?” phase. What happens next? B. Plus Ten Years We are now in the stage of “Get me Joe Blow!” Some event causes society to decide that cybersecurity is essential. Unfortunately, it is probably going to have to take some form that affects a lot of people: a continent-wide blackout in the United States or Europe, a complete gridlock of road traffic when every traffic light stops functioning, or a freeze-up of all financial markets because no transaction can be believed in. After this, we see some steps taken. The first reaction will be user panic, widespread demand for security, much of which will be wasted money, but that is typical. Then, some regulatory and police responses will come. These are dictated by government action, since vendors will continue to explain why whatever happened was somebody else's problem. Vendors are about as likely to take responsibility as one's cat is to explain why the curtain is now on the floor. What actions might be taken? These might be dedicated networks for critical infrastructure (unlikely; we do not have a separate set of bridges for critical road traffic), or perhaps a combination of serious testing of software, lack of tolerance for continued buggy shipments, and more tracking and detection of malefactors (with some loss in personal privacy, as with the airlines). Perhaps the average browser no longer allows arbitrary downloads, there is no more anonymous e-mail, and some kind of tax on Internet Service Providers (ISPs) is used to pay for the police efforts. The cost is some combination of product costs, taxes, and hassle and delay of the users. C. Plus Twenty Years This is the stage of “Get me a Joe Blow type!” where people want a less annoying solution to cybercrime. The situation has improved to the point where we are complaining about the problems it makes for daily life, in the same way that we complain about air travel security. Fortunately, the increasing decline in system costs means that we do not complain about the price of cybersecurity. Major disasters no longer happen, and people think that avoiding them should be cheaper and easier. A few vendors and politicians continue to emphasize the threats and encourage people to spend more on cybersecurity products. A few nations try to capitalize on a reputation for honesty and reliability to encourage cloud computing vendors to set up on their territory, but they lose out to places that offer the cheapest cost and the least enforcement of tax laws (or any other laws). Costs shift from the public to the industry, and in fact start to arrive at the vendors, as they are producing decent software. D. Plus Fifty Years This is the stage of “Get me a young Joe Blow,” when the solutions have become routine. The system is now running fairly smoothly. To the extent that people can remember when you could send anonymous e-mail and there were no fingerprint or iris scanners, it has gone the way of other nostalgia. E. Plus Hundred Years Who is Joe Blow? Cybersecurity incidents are now rare and since the typical year goes by without anything on the news about it (when was the last time you heard about a U.S. train robbery?), nobody except a few specialists knows anything about cybersecurity. Perhaps the Smithsonian includes a “Museum of Spam” which proudly displays the “Green Card Lottery” information e-mail from 1994. This has been a basically optimistic view. You may be more pessimistic. I can only say if I could really predict the future, I would not be writing articles for the IEEE. I would be at the racetrack. ACKNOWLEDGMENT The authors would like to thank G. Cybenko for his expeditious review and helpful comments which led to improvements in the manuscript. Susan Landau would like to thank Dame G. Beer for her talk “Alice in Time” at the Radcliffe Institute for inspiring these ideas. Carl Landwehr would like to thank the Proceedings of the IEEE staff for their patience. Authors References Citations Keywords Metrics Footnotes More Like This Ensuring Security of Data and Information Flow in Emergency Response Decision Support 2016 11th International Conference on Availability, Reliability and Security (ARES) Published: 2016 The demographics of the do-not-call list [security of data] IEEE Security & Privacy Published: 2005 Show More References 1. M. Bellare and P. Rogaway, \"Encode-then-encipher encryption: How to exploit nonces or redundancy in plaintexts for efficient encryption\" in Advances in CryptologyASIACRYPT'00, Germany, Berlin:Springer-Verlag, vol. 1976, pp. 317-330, 2000. Show in Context Google Scholar 2. J. Katz and M. Yung, \"Unforgeable encryption and adaptively secure modes of operation\" in Fast Software Encryption 7th International Workshop FSE 2000, Germany, Berlin:Springer-Verlag, vol. 1978, pp. 284-299, 2000. Show in Context Google Scholar 3. H. Krawczyk, \"The order of encryption and authentication for protecting communications (or: How secure is SSL?)\" in Advances in CryptologyCRYPTO 2001, Germany, Berlin:Springer-Verlag, vol. 2139, pp. 310-331, 2001. Show in Context Google Scholar 4. P. Rogaway, M. Bellare, J. Black and T. Krovetz, \"OCB: A block-cipher mode of operation for efficient authenticated encryption\", Proc. ACM Conf. Comput. Commun. Security, pp. 196-205, 2001. Show in Context 5. P. C. Kocher, \"Timing Attacks on implementations of Diffie-Hellman RSA DSS and other systems\", Proc. CRYPTO, pp. 104-113, 1996. Show in Context Google Scholar 6. P. C. Kocher, J. Jaffe and B. Jun, \"Differential power analysis\", Proc. CRYPTO, pp. 388-397, 1999. Show in Context CrossRef Google Scholar 7. D. Boneh, R. A. DeMillo and R. J. Lipton, \"On the importance of checking cryptographic protocols for faults\", Proc. EUROCRYPT, pp. 37-51, 1997. Show in Context Google Scholar 8. A. Akavia, S. Goldwasser and V. Vaikuntanathan, \"Simultaneous hardcore bits and cryptography against memory attacks\", Proc. 6th Theory Cryptogr. Conf., pp. 474-495, 2009. Show in Context Google Scholar 9. M. Naor and G. Segev, \"Public-key cryptosystems resilient to key leakage\", Proc. CRYPTO, pp. 18-35, 2009. Show in Context Google Scholar 10. O. Regev, \"On lattices learning with errors random linear codes and cryptography\", J. ACM, vol. 56, no. 6, 2009. Show in Context CrossRef Google Scholar 11. G. Santayana, The Life of Reason, New York:C. Scribner's Sons, vol. 1, 1905, [online] Available: http://www.gutenberg.org/files/15000/15000-h/vol1.html. Show in Context Google Scholar 12. D. Kahn, The Codebreakers, New York:Macmillan, 1967. Show in Context Google Scholar 13. D. Kahn, Seizing the Enigma: The Race to Break the German U-Boat Codes 19391943, MA, Boston:Houghton-Mifflin, 1991. Show in Context Google Scholar 14. W. Diffie and M. E. Hellman, \"New directions in cryptography\", IEEE Trans. Inf. Theory, vol. IT-22, no. 6, pp. 644-654, Nov. 1976. Show in Context View Article Google Scholar 15. S. M. Bellovin, \"Frank Miller: Inventor of the one-time pad\", Cryptologia, vol. 35, no. 3, pp. 203-222, Jul. 2011, [online] Available: http://dx.doi.org/10.1080/01611194.2011.583711. Show in Context CrossRef Google Scholar 16. M. Matsui, \"The first experimental cryptanalysis of the data encryption standard\" in Advances in CryptologyCRYPTO'94, Germany, Berlin:Springer-Verlag, vol. 839, pp. 1-11, 1994. Show in Context CrossRef Google Scholar 17. Cracking DES: Secrets of Encryption Research Wiretap Politics & Chip Design, New York:O'Reilly, Jul. 1998. Show in Context Google Scholar 18. W. Diffie and M. E. Hellman, \"Exhaustive cryptanalysis of the NBS data encryption standard\", Computer, vol. 10, no. 6, pp. 74-84, Jun. 1977. Show in Context View Article Google Scholar 19. Advanced Encryption Standard, 2001. Show in Context Google Scholar 20. Cryptographic Hash Algorithm Competition. Show in Context Google Scholar 21. A. Bogdanov, D. Khovratovich and C. Rechberger, \"Biclique cryptanalysis of the full AES\", Proc. ASIACRYPT, 2011, [online] Available: https://lirias.kuleuven.be/handle/123456789/314284. CrossRef Google Scholar 22. A. Whitten and J. Tygar, \"Why Johnny can't encrypt: A usability evaluation of PGP 5.0\", Proc. Usenix Security Symp., 1999, [online] Available: http://db.usenix.org/publications/library/proceedings/sec99/whitten.html. Show in Context Google Scholar 23. S. Clark, T. Goodspeed, P. Metzger, Z. Wasserman, K. Xu and M. Blaze, \"Why (Special Agent) Johnny (still) can't encrypt: A security analysis of the APCO Project 25 two-way radio system\", Proc. Usenix Security Symp., 2011, [online] Available: http://www.usenix.org/events/sec11/tech/full_papers/Clark.pdf. Show in Context Google Scholar 24. R. W. Reeder and R. A. Maxion, \"User interface dependability through goal-error prevention\", Proc. Int. Conf. Dependable Syst. Netw., pp. 60-69, 2005. Show in Context View Article Google Scholar 25. R. W. Reeder, P. G. Kelley, A. M. McDonald and L. F. Cranor, \"A user study of the expandable grid applied to P3P privacy policy visualization\", Proc. 7th ACM Workshop Privacy Electron. Soc., pp. 45-54, 2008, [online] Available: http://doi.acm.org/10.1145/1456403.1456413. Show in Context CrossRef Google Scholar 26. H. R. Lipford, A. Besmer and J. Watson, \"Understanding privacy settings in Facebook with an audience view\", Proc. 1st Conf. Usability Psychol. Security, pp. 1-8, 2008. Show in Context Google Scholar 27. M. Hart, C. Castille, R. Johnson and A. Stent, \"Usable privacy controls for blogs\", Proc. Int. Conf. Comput. Sci. Eng., vol. 4, pp. 401-408, 2009-Aug., [online] Available: http://www.cs.sunysb.edu/~rob/papers/socialcom-final.pdf. Show in Context View Article Google Scholar 28. D. Brin, The Transparent Society: Will Technology Force Us to Choose Between Privacy and Freedom?, MA, Reading:Addison-Wesley, 1998. Show in Context Google Scholar 29. R. Hoffman, \"The growing influence of social networks\", Session of the World Economic Forum, 2010-Jan.-27. Show in Context Google Scholar 30. K. J. Nyitray, \"William Alfred Higinbotham: Scientist activist and computer game pioneer\", IEEE Ann. History Comput., vol. 33, no. 2, pp. 96-101, Feb. 2011, [online] Available: http://www.bnl.gov/bnlweb/history/higinbotham.asp. Show in Context View Article Google Scholar 31. P. Baran, Introduction to distributed communications networks, 1964, [online] Available: http://www.rand.org/about/history/baran-list.html. Show in Context Google Scholar 32. T. Kilburn, R. Payne and D. Howarth, \"The Atlas supervisor\", Proc. AFIPS Comput. Conf., pp. 279-294, 1961, [online] Available: http://dl.acm.org/citation.cfm?id=1460786. Show in Context CrossRef Google Scholar 33. R. A. Heinlein, \"Waldo\", Astounding Mag., Aug. 1942. Show in Context Google Scholar 34. R. A. Heinlein, \"Jerry was a man\", Thrilling Wonder Stories, Oct. 1947. Show in Context Google Scholar 35. R. A. Heinlein, The Star Beast, New York:Scribner, 1954. Show in Context Google Scholar 36. C. H. Bennett, F. Bessette, G. Brassard, L. Salvail and J. Smolin, \"Experimental quantum cryptography\", J. Cryptology, vol. 5, pp. 3-28, 1992, [online] Available: http://dx.doi.org/10.1007/BF00191318. Show in Context CrossRef Google Scholar 37. P. W. Schor, \"Algorithms for quantum computation: Discrete logarithms and factoring\", Proc. 35th Annu. Symp. Found. Comput. Sci., pp. 124-134, 1994. Show in Context View Article Google Scholar 38. L. Newman, Heather Has Two Mommies, MA, Boston:Alyson Wonderland, 1989. Show in Context Google Scholar 39. M. Kan, \"Beijing to track people's movements via their mobile phones\", Computer World, Mar. 2011. Show in Context Google Scholar 40. L. Bengsston, X. Lu, A. Thorson, R. Garfield and J. von Schreed, \"Improved response to disasters and outbreaks by tracking population movements with mobile phone network data: A post-earthquake geospatial study in Haiti\", PLoS Med., vol. 8, no. 8, 2011, [online] Available: http://dx.doi.org/10.1371\\%2Fjournal.pmed.1001083. Show in Context CrossRef Google Scholar 41. V. Gratzer and D. Naccache, \"Cryptography law enforcement and mobile communications\", IEEE Security Privacy, vol. 4, no. 6, pp. 67-70, Nov.-Dec. 2006. Show in Context View Article Google Scholar 42. M. Barbaro and T. Zeller, \"A face is exposed for AOL searches\", New York Times, Aug. 2006. Show in Context Google Scholar 43. P. Golle and K. Partridge, \"On the anonymity of home/work location pairs\", Proc. 7th Int. Conf. Pervasive Comput., 2009. Show in Context CrossRef Google Scholar 44. J. Ginsberg, M. Mohebbi, R. Patel, L. Brammer, M. Smolinski and L. Brilliant, \"Detecting influenza epidemics using search engine query data\", Nature, pp. 1012-1014, Feb. 2009. Show in Context CrossRef Google Scholar 45. I. Asimov, Foundation, New York:Gnome Press, 1951. Show in Context Google Scholar 46. N. Eagle and A. Pentland, \"Reality mining: Sensing complex social systems\", J. Pers. Ubiquitous Comput., vol. 10, no. 4, Jun. 2005. Show in Context CrossRef Google Scholar 47. Tor Project: Anonymity. Google Scholar 48. L. Carroll and M. Gardner, The Annotated Alice: Alice's Adventures in Wonderland and Through the Looking Glass, New York:Bramhall House, 1960. Show in Context Google Scholar 49. K. Bamberger and D. Mulligan, \"Privacy on the books and on the ground\", Stanford Law Rev., vol. 63, no. 2, pp. 247-316, Jan. 2011. Show in Context Google Scholar 50. National Strategy for Trusted Identities in Cyberspace. Show in Context Google Scholar 51. K. O'Brien and T. Crampton, \"E.U. probes Google over data retention\", New York Times, May 2007. Show in Context Google Scholar 52. S. Landau, Surveillance or Security? The Risks Posed by New Wiretapping Technologies, MA, Cambridge:MIT Press, 2011. Show in Context Google Scholar 53. J. Last, \"Israel blocks airborne protest questions dozens\", Forbes Mag., Jul. 2011, [online] Available: http://www.forbes.com/feeds/ap/2011/07/08/general-ml-israel-palestinians8555618.html. Show in Context Google Scholar 54. Protecting Individual Privacy in the Struggle Against Terrorists: A Framework for Program Assessment, 2008. Show in Context Google Scholar 55. Tenth Workshop on Economics of Information Security. Show in Context Google Scholar 56. E. Bellamy, Looking Backward, MA, Boston:Ticknor, 1888. Show in Context Google Scholar 57. A. Dutta-Roya, \"Computers [1999 technology forecast and analysis]\", IEEE Spectrum, vol. 36, no. 1, pp. 46-51, Jan. 1999. Show in Context View Article Google Scholar 58. W. Sweet, \"Power and energy [1999 technology analysis and forecast]\", IEEE Spectrum, vol. 36, no. 1, pp. 62-67, Jan. 1999. Show in Context View Article Google Scholar 59. M. A. Brown, \"Myths and Reality about the 1960 Census\", Prologue, vol. 32, no. 4, 2000. Show in Context Google Scholar 60. D. Kushner, \"One giant screwup for mankind\", Wired, vol. 15, no. 1, Jan. 2007. Show in Context Google Scholar 61. Cyber Criminals Should Get Tough Sentences Say Police, Nov. 2011. Show in Context Google Scholar 62. S. J. Levy, \"The expanding responsibility of the government air traffic controller\", Fordham Law Rev., vol. 36, no. 3, pp. 401, 1968. Show in Context Google Scholar 63. L. T. Rolt, Red for Danger, U.K., London:John Lane, 1955. Show in Context Google Scholar 64. R. Shaw, A History of Railroad Accidents Safety Precautions and Operating Practices, NY, Potsdam:Northern Press, 1978, [online] Available: . Show in Context Google Scholar 65. X. Jardin, PBS Hacked in Retribution for Frontline Wikileaks Episode, May 2011. Show in Context Google Scholar 66. M. Cheung, \"User education key to IT Security: Microsoft\", Network World, Apr. 2009, [online] Available: http://www.networkworld.com/news/2009/041309-user-education-key-to-it.html. Show in Context Google Scholar 67. A. J. Masys, \"Pilot error: Dispelling the hegemony of blamism—A case of de-centered causality and hardwired politics\", Disaster Prevention Manage., vol. 17, no. 2, pp. 221-231, 2008. Show in Context CrossRef Google Scholar 68. H. R. Wilson, The Safety of British Railways, U.K., London:King, 1909. Show in Context Google Scholar 69. H. R. Wilson, Railway Accidents, U.K., London:Self-published, 1925. Show in Context Google Scholar",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Hacking Blind",
                "link": "https://ieeexplore.ieee.org/document/6956567/",
                "date_of_publication": "20 November 2014",
                "doi": "10.1109/SP.2014.22",
                "citations": "179",
                "abstract": "We show that it is possible to write remote stack buffer overflow exploits without possessing a copy of the target binary or source code, against services that restart after a crash. This makes it possible to hack proprietary closed-binary services, or open-source servers manually compiled and installed from source where the binary remains unknown to the attacker. Traditional techniques are usually paired against a particular binary and distribution where the hacker knows the location of useful gadgets for Return Oriented Programming (ROP). Our Blind ROP (BROP) attack instead remotely finds enough ROP gadgets to perform a write system call and transfers the vulnerable binary over the network, after which an exploit can be completed using known techniques. This is accomplished by leaking a single bit of information based on whether a process crashed or not when given a particular input string. BROP requires a stack vulnerability and a service that restarts after a crash. We implemented Braille, a fully automated exploit that yielded a shell in under 4,000 requests (20 minutes) against a contemporary nginx vulnerability, yaSSL + MySQL, and a toy proprietary server written by a colleague. The attack works against modern 64-bit Linux with address space layout randomization (ASLR), no-execute page protection (NX) and stack canaries.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "OpenConflict: Preventing Real Time Map Hacks in Online Games",
                "link": "https://ieeexplore.ieee.org/document/5958049/",
                "date_of_publication": "18 July 2011",
                "doi": "10.1109/SP.2011.28",
                "citations": "23",
                "abstract": "We present a generic tool, Kartograph, that lifts the fog of war in online real-time strategy games by snooping on the memory used by the game. Kartograph is passive and cannot be detected remotely. Motivated by these passive attacks, we present secure protocols for distributing game state among players so that each client only has data it is allowed to see. Our system, Open Conflict, runs real-time games with distributed state. To support our claim that Open Conflict is sufficiently fast for real-time strategy games, we show the results of an extensive study of 1000 replays of Star craft II games between expert players. At the peak of a typical game, Open Conflict needs only 22 milliseconds on one CPU core each time state is synchronized.",
                "ieee_keywords": [
                    "Games",
                    "Computer crime",
                    "Real time systems",
                    "Computer hacking",
                    "Heating",
                    "Data visualization",
                    "Instruments"
                ],
                "author_keywords": [
                    "multi-player games",
                    "map hacks"
                ]
            },
            {
                "title": "How Relevant Is the Turing Test in the Age of Sophisbots?",
                "link": "https://ieeexplore.ieee.org/document/8886907/",
                "date_of_publication": null,
                "doi": "10.1109/MSEC.2019.2934193",
                "citations": "13",
                "abstract": "Popular culture has contemplated societies of intelligent machines for generations. Today, we find ourselves at the doorstep of technology that can at least simulate thinking, feeling, and other behaviors. The question is: Now what?",
                "ieee_keywords": [
                    "Videos",
                    "Generators",
                    "Detectors",
                    "Social networking (online)",
                    "Information integrity",
                    "Generative adversarial networks"
                ],
                "author_keywords": []
            },
            {
                "title": "Bulletproofs: Short Proofs for Confidential Transactions and More",
                "link": "https://ieeexplore.ieee.org/document/8418611/",
                "date_of_publication": "26 July 2018",
                "doi": "10.1109/SP.2018.00020",
                "citations": "408",
                "abstract": "We propose Bulletproofs, a new non-interactive zero-knowledge proof protocol with very short proofs and without a trusted setup; the proof size is only logarithmic in the witness size. Bulletproofs are especially well suited for efficient range proofs on committed values: they enable proving that a committed value is in a range using only 2 log_2(n)+9 group and field elements, where n is the bit length of the range. Proof generation and verification times are linear in n. Bulletproofs greatly improve on the linear (in n) sized range proofs in existing proposals for confidential transactions in Bitcoin and other cryptocurrencies. Moreover, Bulletproofs supports aggregation of range proofs, so that a party can prove that m commitments lie in a given range by providing only an additive O(log(m)) group elements over the length of a single proof. To aggregate proofs from multiple parties, we enable the parties to generate a single proof without revealing their inputs to each other via a simple multi-party computation (MPC) protocol for constructing Bulletproofs. This MPC protocol uses either a constant number of rounds and linear communication, or a logarithmic number of rounds and logarithmic communication. We show that verification time, while asymptotically linear, is very efficient in practice. The marginal cost of batch verifying 32 aggregated range proofs is less than the cost of verifying 32 ECDSA signatures. Bulletproofs build on the techniques of Bootle et al. (EUROCRYPT 2016). Beyond range proofs, Bulletproofs provide short zero-knowledge proofs for general arithmetic circuits while only relying on the discrete logarithm assumption and without requiring a trusted setup. We discuss many applications that would benefit from Bulletproofs, primarily in the area of cryptocurrencies. The efficiency of Bulletproofs is particularly well suited for the distributed and trustless nature of blockchains. The full version of this article is available on ePrint. (Show More)",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Riposte: An Anonymous Messaging System Handling Millions of Users",
                "link": "https://ieeexplore.ieee.org/document/7163034/",
                "date_of_publication": "20 July 2015",
                "doi": "10.1109/SP.2015.27",
                "citations": "71",
                "abstract": "This paper presents Riposte, a new system for anonymous broadcast messaging. Riposte is the first such system, to our knowledge, that simultaneously protects against traffic-analysis attacks, prevents anonymous denial-of-service by malicious clients, and scales to million-user anonymity sets. To achieve these properties, Riposte makes novel use of techniques used in systems for private information retrieval and secure multi-party computation. For latency-tolerant workloads with many more readers than writers (e.g. Twitter, Wikileaks), we demonstrate that a three-server Riposte cluster can build an anonymity set of 2,895,216 users in 32 hours.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "SoK: Hate, Harassment, and the Changing Landscape of Online Abuse",
                "link": "https://ieeexplore.ieee.org/document/9519435/",
                "date_of_publication": "26 August 2021",
                "doi": "10.1109/SP40001.2021.00028",
                "citations": "19",
                "abstract": "We argue that existing security, privacy, and antiabuse protections fail to address the growing threat of online hate and harassment. In order for our community to understand and address this gap, we propose a taxonomy for reasoning about online hate and harassment. Our taxonomy draws on over 150 interdisciplinary research papers that cover disparate threats ranging from intimate partner violence to coordinated mobs. In the process, we identify seven classes of attacks—such as toxic content and surveillance—that each stem from different attacker capabilities and intents. We also provide longitudinal evidence from a three-year survey that hate and harassment is a pervasive, growing experience for online users, particularly for at-risk communities like young adults and people who identify as LGBTQ+. Responding to each class of hate and harassment requires a unique strategy and we highlight five such potential research directions that ultimately empower individuals, communities, and platforms to do so.",
                "ieee_keywords": [
                    "Privacy",
                    "Social networking (online)",
                    "Taxonomy",
                    "Distance measurement",
                    "Cognition",
                    "Computer security"
                ],
                "author_keywords": [
                    "hate",
                    "harassment",
                    "emerging-threats",
                    "at-risk"
                ]
            }
        ]
    },
    {
        "name": "Kang G. Shin",
        "publications": [
            {
                "title": "Detection of botnets using combined host- and network-level information",
                "link": "https://ieeexplore.ieee.org/document/5544306/",
                "date_of_publication": "09 August 2010",
                "doi": "10.1109/DSN.2010.5544306",
                "citations": "25",
                "abstract": "Bots are coordinated by a command and control (C&C) infrastructure to launch attacks that seriously threaten the Internet services and users. Most botnet-detection approaches function at the network level and require the analysis of packets' payloads, raising privacy concerns and incurring large computational overheads. Moreover, network traffic analysis alone can seldom provide a complete picture of botnets' behavior. By contrast, in-host detection approaches are useful to identify each bot's host-wide behavior, but are susceptible to the host-resident malware if used alone. To address these limitations, we consider both the coordination within a botnet and the malicious behavior each bot exhibits at the host level, and propose a C&C protocol-independent detection framework that combines host- and network-level information for making detection decisions. The framework is shown to be effective in detecting various types of botnets with low false-alarm rates.",
                "ieee_keywords": [
                    "Web server",
                    "Storms",
                    "Protocols",
                    "Telecommunication traffic",
                    "Computer worms",
                    "Inspection",
                    "Command and control systems",
                    "Web and internet services",
                    "Counting circuits",
                    "Relays"
                ],
                "author_keywords": []
            },
            {
                "title": "Fast and accurate cardinality estimation in cellular-based wireless communications",
                "link": "https://ieeexplore.ieee.org/document/7127626/",
                "date_of_publication": "18 June 2015",
                "doi": "10.1109/WCNC.2015.7127626",
                "citations": "1",
                "abstract": "Cardinality estimation is of fundamental importance in various wireless communication applications. The performance of any adaptive medium access control in the uplink channel is in fact affected by its accuracy. This paper provides a fresh look at this fundamental problem, and proposes a novel scheme for fast and accurate cardinality estimation. This scheme utilizes the experienced outage probability in detecting IDs of the devices in the uplink. It is observed that for practically relevant values of Signal-to-Interference-plus-Noise Ratio (SINR) threshold, e.g., β ≤ 10 dB, and spreading sequence length, e.g., N c ≥ 7, the base station is able to estimate the true cardinality with vanishing error in only one shot, even if the network is so populated, e.g., the number of active devices N ≥ 100. Otherwise, one may apply the proposed method in couple of consecutive shots such that in each of which portion of the cardinality is estimated.",
                "ieee_keywords": [
                    "Interference",
                    "Estimation",
                    "Signal to noise ratio",
                    "Wireless communication",
                    "Sensors",
                    "Conferences",
                    "Fading"
                ],
                "author_keywords": []
            },
            {
                "title": "E-MiLi: Energy-Minimizing Idle Listening in Wireless Networks",
                "link": "https://ieeexplore.ieee.org/document/6197198/",
                "date_of_publication": null,
                "doi": "10.1109/TMC.2012.112",
                "citations": "104",
                "abstract": "WiFi interface is known to be a primary energy consumer in mobile devices, and idle listening (IL) is the dominant source of energy consumption in WiFi. Most existing protocols, such as the 802.11 power-saving mode (PSM), attempt to reduce the time spent in IL by sleep scheduling. However, through an extensive analysis of real-world traffic, we found more than 60 percent of energy is consumed in IL, even with PSM enabled. To remedy this problem, we propose Energy-Minimizing idle Listening (E-MiLi) that reduces the power consumption in IL, given that the time spent in IL has already been optimized by sleep scheduling. Observing that radio power consumption decreases proportionally to its clock rate, E-MiLi adaptively downclocks the radio during IL, and reverts to full clock rate when an incoming packet is detected or a packet has to be transmitted. E-MiLi incorporates sampling rate invariant detection, ensuring accurate packet detection and address filtering even when the receiver's sampling clock rate is much lower than the signal bandwidth. Further, it employs an opportunistic downclocking mechanism to optimize the efficiency of switching clock rate, based on a simple interface to existing MAC-layer scheduling protocols. We have implemented E-MiLi on the USRP software radio platform. Our experimental evaluation shows that E-MiLi can detect packets with close to 100 percent accuracy even with downclocking by a factor of 16. When integrated with 802.11, E-MiLi can reduce energy consumption by around 44 percent for 92 percent of users in real-world wireless networks.",
                "ieee_keywords": [
                    "Energy efficiency",
                    "IEEE 802.11 Standards",
                    "Power demand",
                    "Receivers",
                    "Software radio",
                    "Energy consumption",
                    "Switches",
                    "Synchronization"
                ],
                "author_keywords": [
                    "Energy efficiency",
                    "CSMA wireless networks",
                    "idle listening",
                    "packet detection",
                    "adapting clock rate",
                    "dynamic frequency scaling."
                ]
            },
            {
                "title": "DESA: Dependable, Efficient, Scalable Architecture for Management of Large-Scale Batteries",
                "link": "https://ieeexplore.ieee.org/document/6009192/",
                "date_of_publication": null,
                "doi": "10.1109/TII.2011.2166771",
                "citations": "64",
                "abstract": "Conventional battery management systems (BMSs) for electric vehicles (EVs) are designed in an ad hoc way, causing the supply of EVs to fall behind the market demand. A well-designed and combined hardware-software architecture is essential for the efficient management of a large-scale battery pack that may consists of thousands of battery cells as in Tesla Motors and GM Chevy Volt. We propose a Dependable, Efficient, Scalable Architecture (DESA) that effectively monitors a large number of battery cells, efficiently controls, and reconfigures, if needed, their connection arrangement. DESA supports hierarchical, autonomous management of battery cells, where a global BMS orchestrates a group of local BMSs. A local controller on each local BMS autonomously manages an array of battery cells, and the global controller reconfigures the connectivity of such battery-cell arrays in coordination with the local controllers. Also, DESA allows individual arrays and local BMSs to be selectively powered-off for energy savings. The performance of this energy-saving capability is modeled and evaluated using a Markov chain. Our evaluation results show that DESA effectively tolerates battery-cell failures by an order-of-magnitude—while achieving 7.4\\times service cost savings—better than a conventional BMS. This superior performance not only extends the battery life significantly, but also provides the flexibility in supporting diverse electric power demands from a growing number of on-board applications.",
                "ieee_keywords": [
                    "Batteries",
                    "Arrays",
                    "Control systems",
                    "Monitoring",
                    "Temperature sensors",
                    "Temperature measurement"
                ],
                "author_keywords": [
                    "Battery cells and packs",
                    "battery management system (BMS)",
                    "electric vehicles",
                    "reconfiguration of cell and pack connections",
                    "voltage and cell balancing"
                ]
            },
            {
                "title": "Efficient thermoelectric cooling for mobile devices",
                "link": "https://ieeexplore.ieee.org/document/8009199/",
                "date_of_publication": "14 August 2017",
                "doi": "10.1109/ISLPED.2017.8009199",
                "citations": "10",
                "abstract": "Mobile apps suffer large performance degradation when the underlying processors are throttled to cool down the devices. Fans or heat sinks are not a viable option for mobile devices, thus calling for a new portable cooling solution. Thermoelectric coolers are scalable and controllable cooling devices that can be embedded into mobile devices on the chip surface. This paper presents a thermoelectric cooling solution that enables efficient processor thermal management in mobile devices. Our goal is to minimize performance loss from thermal throttling by efficiently using thermoelectric cooling. Since mobile devices experience large variations in workloads and ambient temperature, our solution adaptively controls cooling power at runtime. Our evaluation on a smartphone using mobile benchmarks demonstrated that the performance loss from the maximum speed is only 1.8% with the TEC compared to 19.2% without the TEC.",
                "ieee_keywords": [
                    "Cooling",
                    "Program processors",
                    "Temperature measurement",
                    "Mobile handsets",
                    "Heating systems",
                    "Runtime"
                ],
                "author_keywords": []
            },
            {
                "title": "Poster Abstract: Neighbor-aware Queue-based Random Access",
                "link": "https://ieeexplore.ieee.org/document/9484625/",
                "date_of_publication": "19 July 2021",
                "doi": "10.1109/INFOCOMWKSHPS51825.2021.9484625",
                "citations": "137",
                "abstract": "The conventional queue-based random access algorithms are designed based on the local queue length in order to achieve the optimal throughput. This design may cause some long-queue nodes to starve when the channel is occupied by the neighbor nodes. To remedy this problem, we propose a queue-based random access algorithm for achieving low delay, by taking not only the local queue information but also the comparison with its neighbors’ queue lengths into consideration. The neighbor-aware weights are designed for max-weight scheduling which is asymptotically throughput-optimal. A distributed random access algorithm is proposed to sufficiently approximate the max-weight scheduling. Using simulation, we have shown the proposed neighbor-aware algorithm achieves a significantly lower delay than conventional local queue based algorithms.",
                "ieee_keywords": [
                    "Conferences",
                    "Approximation algorithms",
                    "Throughput",
                    "Scheduling",
                    "Delays"
                ],
                "author_keywords": []
            },
            {
                "title": "SpecHammer: Combining Spectre and Rowhammer for New Speculative Attacks",
                "link": "https://ieeexplore.ieee.org/document/9833802/",
                "date_of_publication": "27 July 2022",
                "doi": "10.1109/SP46214.2022.9833802",
                "citations": "7",
                "abstract": "The recent Spectre attacks have revealed how the performance gains from branch prediction come at the cost of weakened security. Spectre Variant 1 (v1) shows how an attacker-controlled variable passed to speculatively executed lines of code can leak secret information to an attacker. Numerous defenses have since been proposed to prevent Spectre attacks, each attempting to block all or some of the Spectre variants. In particular, defenses using taint-tracking are claimed to be the only way to protect against all forms of Spectre v1. However, we show that the defenses proposed thus far can be bypassed by combining Spectre with the well-known Rowhammer vulnerability. By using Rowhammer to modify victim values, we relax the requirement that the attacker needs to share a variable with the victim. Thus, defenses that rely on this requirement, such as taint-tracking, are no longer effective. Furthermore, without this crucial requirement, the number of gadgets that can potentially be used to launch a Spectre attack increases dramatically; those present in Linux kernel version 5.6 increases from about 100 to about 20,000 via Rowhammer bit-flips. Attackers can use these gadgets to steal sensitive information such as stack cookies or canaries, or use new triple gadgets to read any address in memory. We demonstrate two versions of the combined attack on example victims in both user and kernel spaces, showing the attack’s ability to leak sensitive data.",
                "ieee_keywords": [
                    "Privacy",
                    "Costs",
                    "Codes",
                    "Linux",
                    "Performance gain",
                    "Security",
                    "Kernel"
                ],
                "author_keywords": []
            },
            {
                "title": "Poster abstract: Routing meets caching in named data networks",
                "link": "https://ieeexplore.ieee.org/document/8406984/",
                "date_of_publication": "09 July 2018",
                "doi": "10.1109/INFCOMW.2018.8406984",
                "citations": "2",
                "abstract": "In-network caching is a distinct feature of named data networking (NDN). The current NDN routing protocols, however, still focus on the traditional problem of forwarding content requests to content producers, without explicit support of in-network caching, which will limit NDN's potential and benefits to applications. In this poster, as part of a new routing protocol, we equip the forwarding plane with a path which significantly improves the probability of meeting the cached contents before reaching the producers.",
                "ieee_keywords": [
                    "Routing protocols",
                    "Delays",
                    "Routing",
                    "Servers",
                    "Conferences",
                    "Computer science",
                    "Topology"
                ],
                "author_keywords": []
            },
            {
                "title": "Coverage Performance of MIMO-MRC in Heterogeneous Networks: A Stochastic Geometry Perspective",
                "link": "https://ieeexplore.ieee.org/document/7880947/",
                "date_of_publication": "20 March 2017",
                "doi": "10.1109/VTCFall.2016.7880947",
                "citations": "8",
                "abstract": "We study the coverage performance of multi-antenna (MIMO) communications with maximum ratio combining (MRC) at the receiver in heterogeneous networks (HetNets). Our main interest in on multi-stream communications when BSs do not have access to channel state information. Adopting stochastic geometry we evaluate the network-wise coverage performance of MIMO-MRC assuming maximum signal- to-interference ratio (SIR) cell association rule. Coverage analysis in MIMO-MRC HetNets is challenging due to inter-stream interference and statistical dependencies among streams' SIR values in each communication link. Using the results of stochastic geometry we then investigate this problem and obtain tractable analytical approximations for the coverage performance. We then show that our results are adequately accurate and easily computable. Our analysis sheds light on the impacts of important system parameters on the coverage performance, and provides quantitative insight on the densification in conjunction with high multiplexing gains in MIMO HetNets. We further observe that increasing multiplexing gain in high- power tier can cost a huge coverage reduction unless it is practiced with densification in femto-cell tier.",
                "ieee_keywords": [
                    "Multiplexing",
                    "Interference",
                    "MIMO",
                    "Geometry",
                    "Receivers",
                    "Heterogeneous networks",
                    "Measurement"
                ],
                "author_keywords": []
            },
            {
                "title": "Performance Evaluation of MISO-SDMA in Heterogeneous Networks with Practical Cell Association",
                "link": "https://ieeexplore.ieee.org/document/7881158/",
                "date_of_publication": "20 March 2017",
                "doi": "10.1109/VTCFall.2016.7881158",
                "citations": "1",
                "abstract": "In this paper adopting stochastic geometry we investigate the system performance in heterogeneous networks including multiple tiers of BSs with multiple-input single output spatial division multiple access (MISO-SDMA) technique. In the related literature on heterogeneous systems, ideal cell association (CA) rules are often considered for simplicity, where each user equipment (UE) examines a very large number of pilots across the tiers before choosing its associated base station (BS). Here we consider practical cases where UEs are restricted to examine K H ≥ 1 pilots across all tiers before choosing their associated BS. We then obtain closed-form expressions for the system performance measured by the coverage probability and UE's data rate. Our analytical results provide quantitative insights on the impact of different factors on the system performance including the BS's spatial density, their transmission powers, number of transmit antennas, SIR thresholds, number of UEs served by each BS, and K H . Interestingly, we observe that increasing K H always improves the coverage probability however, it only improves data rate up to a certain point. The data rate is then reduced by further increasing of K H . Given K H pilots in practical cases, the issue is how to allocate the pilots among different tiers. We address this issue by developing an algorithm and show that by careful allocation of available pilots, the network performance is significantly improved even in cases with small K H . Our results also indicate a fundamental tradeoff, as sharing strategies providing the best coverage performance yield very poor capacity and vice versa. Such trade-off provides a new degree of freedom in heterogeneous networks design.",
                "ieee_keywords": [
                    "Multiaccess communication",
                    "Fading channels",
                    "Geometry",
                    "System performance",
                    "MIMO",
                    "Antennas",
                    "Stochastic processes"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Rada Mihalcea",
        "publications": [
            {
                "title": "Learning From Personal Longitudinal Dialog Data",
                "link": "https://ieeexplore.ieee.org/document/8844687/",
                "date_of_publication": null,
                "doi": "10.1109/MIS.2019.2916965",
                "citations": "5",
                "abstract": "We explore the use of longitudinal dialog data for two dialog prediction tasks: next message prediction and response time prediction. We show that a neural model using personal data that leverages a combination of message content, style matching, time features, and speaker attributes leads to the best results for both tasks, with error rate reductions of up to 15% compared to a classifier that relies exclusively on message content and to a classifier that does not use personal data.",
                "ieee_keywords": [
                    "Time factors",
                    "Task analysis",
                    "Linguistics",
                    "Data models",
                    "Instant messaging",
                    "Microsoft Windows",
                    "Feature extraction"
                ],
                "author_keywords": []
            },
            {
                "title": "Evaluating Parameter-Efficient Transfer Learning Approaches on SURE Benchmark for Speech Understanding",
                "link": "https://ieeexplore.ieee.org/document/10095656/",
                "date_of_publication": "05 May 2023",
                "doi": "10.1109/ICASSP49357.2023.10095656",
                "citations": "186",
                "abstract": "Fine-tuning is widely used as the default algorithm for transfer learning from pre-trained models. Parameter inefficiency can however arise when, during transfer learning, all the parameters of a large pre-trained model need to be updated for individual downstream tasks. As the number of parameters grows, fine-tuning is prone to overfitting and catastrophic forgetting. In addition, full fine-tuning can become prohibitively expensive when the model is used for many tasks. To mitigate this issue, parameter-efficient transfer learning algorithms, such as adapters and prefix tuning, have been proposed as a way to introduce a few trainable parameters that can be plugged into large pre-trained language models such as BERT, HuBERT. In this paper, we introduce the Speech UndeRstanding Evaluation (SURE) benchmark for parameter-efficient learning for various speech processing tasks. Additionally, we introduce a new adapter, ConvAdapter, based on 1D convolution. We show that ConvAdapter outperforms the standard adapters while showing comparable performance against prefix tuning and Low-Rank Adaptation with only 0.94% of trainable parameters.",
                "ieee_keywords": [
                    "Adaptation models",
                    "Convolution",
                    "Transfer learning",
                    "Signal processing algorithms",
                    "Benchmark testing",
                    "Transformers",
                    "Task analysis"
                ],
                "author_keywords": [
                    "Transfer Learning",
                    "Adapters",
                    "Prefix tuning",
                    "LoRA",
                    "ConvAdapter",
                    "Speech Processing"
                ]
            },
            {
                "title": "Contact Versus Noncontact Detection of Driver’s Drowsiness",
                "link": "https://ieeexplore.ieee.org/document/9956336/",
                "date_of_publication": "29 November 2022",
                "doi": "10.1109/ICPR56361.2022.9956336",
                "citations": "62",
                "abstract": "With an estimated number of injuries in the millions, accidents caused due to drowsy driving remain a significant source of financial costs and loss of life. Accurate detection of driver’s drowsiness could provide a clear avenue towards eliminating a great majority of the associated accidents and losses. Existing research on the subject could be defined as either contact-based or noncontact-based alertness detection. This paper utilizes a novel multimodal driver’s alertness dataset consisting of 45 subjects via seven recorded channels, including four contact-based and three noncontact-based channels, to investigate the performance of said modalities in detecting driver’s drowsiness as well as provide a novel comparison between the results of multiple contact and noncontact methods. Our results highlight the viability of noncontact methods to detect driver’s drowsiness as an implementable technology in automobiles.",
                "ieee_keywords": [
                    "Visualization",
                    "Costs",
                    "Cameras",
                    "Hardware",
                    "Planning",
                    "Pattern recognition",
                    "Reliability"
                ],
                "author_keywords": []
            },
            {
                "title": "What Men Say, What Women Hear: Finding Gender-Specific Meaning Shades",
                "link": "https://ieeexplore.ieee.org/document/7515120/",
                "date_of_publication": null,
                "doi": "10.1109/MIS.2016.71",
                "citations": "28",
                "abstract": "The authors examine the problem of gender discrimination and attempt to move beyond the typical surface-level text classification approach by identifying differences between genders in the ways they use the same words. They present several experiments using data from a large collection of blogs authored by men and women, and they report results for a new task of \"gender-based word disambiguation\" for a set of over 350 words.",
                "ieee_keywords": [
                    "Blogs",
                    "Speech",
                    "Writing",
                    "Context awareness",
                    "Time-frequency analysis",
                    "Computational linguistics",
                    "Twitter",
                    "Gender issues"
                ],
                "author_keywords": [
                    "affective computing",
                    "sentiment analysis",
                    "gender discrimination",
                    "gender-based word disambiguation",
                    "intelligent systems"
                ]
            },
            {
                "title": "Gender Differences in Multimodal Contact-Free Deception Detection",
                "link": "https://ieeexplore.ieee.org/document/8543625/",
                "date_of_publication": null,
                "doi": "10.1109/MMUL.2018.2883128",
                "citations": "2",
                "abstract": "In this paper, we explore the hypothesis that multimodal features as well as demographic information can play an important role in increasing the performance of automatic lie detection. We introduce a large, multimodal deception detection dataset balanced across genders, and we analyze the patterns associated with the thermal, linguistic, and visual responses of liars and truth-tellers. We show that our multimodal noncontact deception detection approach can lead to a performance in the range of 60%–80%, with different modalities, different genders, and different domain settings playing a role in the accuracy of the system.",
                "ieee_keywords": [
                    "Feature extraction",
                    "Linguistics",
                    "Visualization",
                    "Cameras",
                    "Thermal sensors",
                    "Gender issues",
                    "Thermal analysis"
                ],
                "author_keywords": []
            },
            {
                "title": "Muse-ing on the Impact of Utterance Ordering on Crowdsourced Emotion Annotations",
                "link": "https://ieeexplore.ieee.org/document/8682793/",
                "date_of_publication": "17 April 2019",
                "doi": "10.1109/ICASSP.2019.8682793",
                "citations": "2",
                "abstract": "Emotion recognition algorithms rely on data annotated with high quality labels. However, emotion expression and perception are inherently subjective. There is generally not a single annotation that can be unambiguously declared \"correct.\" As a result, annotations are colored by the manner in which they were collected. In this paper, we conduct crowdsourcing experiments to investigate this impact on both the annotations themselves and on the performance of these algorithms. We focus on one critical question: the effect of context. We present a new emotion dataset, Multimodal Stressed Emotion (MuSE), and annotate the dataset using two conditions: randomized, in which annotators are presented with clips in random order, and contextualized, in which annotators are presented with clips in order. We find that contextual labeling schemes result in annotations that are more similar to a speaker's own self-reported labels and that labels generated from randomized schemes are most easily predictable by automated systems.",
                "ieee_keywords": [
                    "Labeling",
                    "Crowdsourcing",
                    "Stress",
                    "Emotion recognition",
                    "Reliability",
                    "Training",
                    "Task analysis"
                ],
                "author_keywords": [
                    "emotion",
                    "crowdsourcing",
                    "annotation",
                    "emotion perception",
                    "classifier performance"
                ]
            },
            {
                "title": "Towards Classifying Human Circadian Rhythm Using Multiple Modalities",
                "link": "https://ieeexplore.ieee.org/document/9597391/",
                "date_of_publication": "15 November 2021",
                "doi": "10.1109/ACII52823.2021.9597391",
                "citations": "2",
                "abstract": "Autonomous vehicles represent one of the most active technologies currently being developed, with research areas addressing, among others, the modeling of the states and behavioral elements of the occupants. This paper contributes to this line of research by studying the circadian rhythm of individuals using a novel multimodal dataset of 36 subjects consisting of five information channels. These channels include visual, thermal, physiological, linguistic, and background data. Moreover, we propose a framework to explore whether the circadian rhythm can be modeled without continuous monitoring and investigate the hypothesis that multimodal features have a greater propensity for improved performance using data points specific to certain times during the day. Our analysis shows that multimodal fusion can lead to an accuracy of up to 77% on identifying energized and enervated states of the participants. Our findings highlight the validity of our hypothesis and present a novel approach for future research.",
                "ieee_keywords": [
                    "Visualization",
                    "Image resolution",
                    "Sleep",
                    "Imaging",
                    "Machine learning",
                    "Linguistics",
                    "Data models"
                ],
                "author_keywords": [
                    "Multimodal dataset",
                    "circadian rhythm",
                    "action units",
                    "classification",
                    "machine learning",
                    "thermal imaging"
                ]
            },
            {
                "title": "Non-Contact Based Modeling of Enervation",
                "link": "https://ieeexplore.ieee.org/document/10042529/",
                "date_of_publication": "16 February 2023",
                "doi": "10.1109/FG57933.2023.10042529",
                "citations": "43",
                "abstract": "Significant research is currently carried out with a focus on autonomous vehicles; research is starting to focus on areas such as the modeling of occupant states and behavioral elements. This paper contributes to this line of research by developing a pipeline that extracts physiological signals from thermal imagery and modeling occupant enervation using a fully non-contact based approach. These signals are obtained via a multimodal dataset of 36 subjects across multiple channels, including the thermal and physiological modalities. Moreover, we provide a comparative analysis of non-contact and contact based channels to model the enervation state of individuals. Our analysis indicates that non-contact physiological signals extracted from thermal imagery can reach and exceed the performance of contact-based physiological signals. In addition, modeling of enervation is possible using said non-contact physiological signals and thermal features, with an accuracy of up to 70% in identifying energized and enervated occupant states. Our findings provide a novel approach for future research and opens the possibility for integration of unrestrictive sensors in future automobiles.",
                "ieee_keywords": [
                    "Temperature sensors",
                    "Heart rate",
                    "Pipelines",
                    "Physiology",
                    "Skin",
                    "Stability analysis",
                    "Thermal analysis"
                ],
                "author_keywords": []
            },
            {
                "title": "Multimodal Deception Detection Using Real-Life Trial Data",
                "link": "https://ieeexplore.ieee.org/document/9165161/",
                "date_of_publication": null,
                "doi": "10.1109/TAFFC.2020.3015684",
                "citations": "10",
                "abstract": "Hearings of witnesses and defendants play a crucial role when reaching court trial decisions. Given the high-stakes nature of trial outcomes, developing computational models that assist the decision-making process is an important research venue. In this article, we address the identification of deception in real-life trial data. We use a dataset consisting of videos collected from public court trials. We explore the use of verbal and non-verbal modalities to build a multimodal deception detection system that aims to discriminate between truthful and deceptive statements provided by defendants and witnesses. In particular, three complementary modalities (visual, acoustic and linguistic) are evaluated for the classification of deception at the subject level. The final classifier is obtained by combining the three modalities via score-level classification, achieving 83.05 percent accuracy in subject-level deceit detection. To place our results in perspective, we present a human deception detection study where we evaluate the human capability of detecting deception using different modalities and compare the results to the developed system. The results show that our system outperforms the average non-expert human capability of identifying deceit.",
                "ieee_keywords": [
                    "Videos",
                    "Visualization",
                    "Feature extraction",
                    "Eyebrows",
                    "Linguistics",
                    "Acoustics",
                    "Electronic mail"
                ],
                "author_keywords": [
                    "Real-life trial",
                    "deception detection",
                    "classification",
                    "multimodal",
                    "visual",
                    "acoustic",
                    "linguistic"
                ]
            },
            {
                "title": "Detecting Deceptive Behavior via Integration of Discriminative Features From Multiple Modalities",
                "link": "https://ieeexplore.ieee.org/document/7782429/",
                "date_of_publication": null,
                "doi": "10.1109/TIFS.2016.2639344",
                "citations": "52",
                "abstract": "Deception detection has received an increasing amount of attention in recent years, due to the significant growth of digital media, as well as increased ethical and security concerns. Earlier approaches to deception detection were mainly focused on law enforcement applications and relied on polygraph tests, which had proved to falsely accuse the innocent and free the guilty in multiple cases. In this paper, we explore a multimodal deception detection approach that relies on a novel data set of 149 multimodal recordings, and integrates multiple physiological, linguistic, and thermal features. We test the system on different domains, to measure its effectiveness and determine its limitations. We also perform feature analysis using a decision tree model, to gain insights into the features that are most effective in detecting deceit. Our experimental results indicate that our multimodal approach is a promising step toward creating a feasible, non-invasive, and fully automated deception detection system.",
                "ieee_keywords": [
                    "Feature extraction",
                    "Physiology",
                    "Face",
                    "Pragmatics",
                    "Sensors",
                    "Cameras",
                    "Visualization"
                ],
                "author_keywords": [
                    "Deception detection",
                    "multimodal processing",
                    "thermal features",
                    "linguistic features",
                    "physiological features"
                ]
            }
        ]
    },
    {
        "name": "Lu Wang",
        "publications": [
            {
                "title": "Detection of Low Cardiac Index Using a Polyvinylidene Fluoride-Based Wearable Ring and Convolutional Neural Networks",
                "link": "https://ieeexplore.ieee.org/document/9247148/",
                "date_of_publication": null,
                "doi": "10.1109/JSEN.2020.3022273",
                "citations": "2",
                "abstract": "This study investigated the use of a wearable ring made of polyvinylidene fluoride film to identify a low cardiac index (≤2 L/min). The waveform generated by the ring contains patterns that may be indicative of low blood pressure and/or high vascular resistance, both of which are markers of a low cardiac index. In particular, the waveform contains reflection waves whose timing and amplitude are correlated with pulse travel time and vascular resistance, respectively. Hence, the pattern of the waveform is expected to vary in response to changes in blood pressure and vascular resistance. By analyzing the morphology of the waveform, our aim was to create a tool to identify patients with low cardiac index. This was done using a convolutional neural network which was trained on data from animal models. The model was then tested on waveforms that were collected from patients undergoing pulmonary artery catheterization. The results indicate high accuracy in classifying patients with a low cardiac index, achieving an area under the receiver operating characteristics and precision-recall curves of 0.88 and 0.71, respectively.",
                "ieee_keywords": [
                    "Sensors",
                    "Monitoring",
                    "Biomedical monitoring",
                    "Indexes",
                    "Immune system",
                    "Blood pressure",
                    "Pressure measurement"
                ],
                "author_keywords": [
                    "Cardiac index",
                    "cardiac output",
                    "convolutional neural networks",
                    "polyvinylidene fluoride",
                    "wearable monitoring"
                ]
            },
            {
                "title": "Design, and Control of a SiC Isolated Bidirectional Power Converter for V2L Applications to both DC and AC Load",
                "link": "https://ieeexplore.ieee.org/document/8998951/",
                "date_of_publication": "17 February 2020",
                "doi": "10.1109/WiPDA46397.2019.8998951",
                "citations": "1",
                "abstract": "EV can help export its unused energy to the external load (such as home appliance) when the vehicle is stationary. Hence a bidirectional power converter with such function is of need. Due to the safety concern, a galvanic isolation transformer is required. This paper presents an isolated universal power converter which can perform vehicle to load (V2L) feature for both AC load and DC load in one single unit. Design details such as SiC MOSFET gate drive and high frequency isolation transformer are disclosed in this paper. Control algorithm and analysis for both operation modes are also performed. Experimental results are shown to validate the hardware design and control algorithm development.",
                "ieee_keywords": [
                    "Transformer cores",
                    "Circuit faults",
                    "Logic gates",
                    "Inductance",
                    "Power transformer insulation",
                    "Inverters"
                ],
                "author_keywords": [
                    "SiC MOSFET",
                    "Vehicle to Load (V2L)",
                    "On-board Charger",
                    "Dual Active Bridge",
                    "Bidirectional DC/DC",
                    "Electric Vehicle",
                    "High frequency Transformer",
                    "Nanocrystalline Core"
                ]
            },
            {
                "title": "A Deep Neural Network Accelerator Based on Tiled RRAM Architecture",
                "link": "https://ieeexplore.ieee.org/document/8993641/",
                "date_of_publication": "13 February 2020",
                "doi": "10.1109/IEDM19573.2019.8993641",
                "citations": "38",
                "abstract": "State-of-the-art deep neural networks (DNNs) have been successfully mapped on an RRAM-based tiled in-memory computing (IMC) architecture. Effects of moderate array size and quantized partial products (PPs) due to ADC precision constraints have been analyzed. Methods were developed to solve these challenges and preserve DNN accuracies and IMC performance gains in the tiled architecture. Popular models including VGG-16 and MobileNet have been successfully implemented and tested on ImageNet dataset.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Hardware Acceleration of Simulated Annealing of Spin Glass by RRAM Crossbar Array",
                "link": "https://ieeexplore.ieee.org/document/8614698/",
                "date_of_publication": "17 January 2019",
                "doi": "10.1109/IEDM.2018.8614698",
                "citations": "23",
                "abstract": "Simulated annealing (SA) was successfully implemented and accelerated by in-memory computing hardware/software package using RRAM crossbar arrays to solve a spin glass problem. Ta 2 O 5 -based RRAM array and stochastic Cu-based CBRAM devices were utilized for calculation of the Hamiltonian and decision of spin-flip events, respectively. A parallel spin-flip strategy was demonstrated to further accelerate the SA algorithm.",
                "ieee_keywords": [
                    "Glass",
                    "Simulated annealing",
                    "Acceleration",
                    "Switches",
                    "Stationary state",
                    "Two dimensional displays",
                    "Sparse matrices"
                ],
                "author_keywords": []
            },
            {
                "title": "Bidirectional Universal Converter Transformer Design for Electric Vehicle Onboard Charging",
                "link": "https://ieeexplore.ieee.org/document/9235905/",
                "date_of_publication": "30 October 2020",
                "doi": "10.1109/ECCE44975.2020.9235905",
                "citations": "284",
                "abstract": "This paper highlights the design aspects of the universal converter transformer as part of an effort done to use the universal converter for onboard charging applications for electric vehicles. Today's requirements of onboard charging ask for bidirectional follow capabilities and an ability to interface with various power grid types (single-phase, three-phase three- wires, and three-phase four-wire, etc.). Such requirements increase the difficulty of designing the transformer due to the high input to output voltage gain range of the DC-DC converter. The universal converter uses a dual active bridge that maintains consistent performance across all the operating regions. This paper provides the necessary steps of building the transformer and shows the experimental results to assess the proposed design method.",
                "ieee_keywords": [
                    "Inductance",
                    "Power transformer insulation",
                    "Bridge circuits",
                    "Batteries",
                    "Topology",
                    "Windings",
                    "Wires"
                ],
                "author_keywords": [
                    "dual active bridge",
                    "transformer design",
                    "onboard charging",
                    "electric vehicle"
                ]
            },
            {
                "title": "Hyperspectral Absorption of Semiconductor Monolayer Crystals",
                "link": "https://ieeexplore.ieee.org/document/9193646/",
                "date_of_publication": "10 September 2020",
                "doi": null,
                "citations": "11",
                "abstract": "One-dimensional semiconductor monolayer crystals are optimized to deliver a nearly perfect absorption over a broad spectral range by combining two complementary design schemes.",
                "ieee_keywords": [
                    "Absorption",
                    "Crystals",
                    "Couplings",
                    "Chirp",
                    "Gallium nitride",
                    "Color",
                    "Excitons"
                ],
                "author_keywords": []
            },
            {
                "title": "Very Low-Programming-Current RRAM With Self-Rectifying Characteristics",
                "link": "https://ieeexplore.ieee.org/document/7409979/",
                "date_of_publication": null,
                "doi": "10.1109/LED.2016.2530942",
                "citations": "90",
                "abstract": "To resolve the sneak leakage problem and reduce the power consumption in crossbar RRAM arrays, a Cu/Al 2 O 3 /aSi/Ta cell with self-rectifying characteristics is developed. The cell exhibits low operating current (~nA), high ON/OFF ratios (>100×), and pronounced nonlinearity. The use of low-programming-current RRAM elements avoids the current-driving capability bottleneck of selectors, while the integrated rectifying layer improves the RRAM operation reliability. Endurance of over 500 cycles with ~100 ON/OFF ratio was achieved without external current compliance. Even at such low programming levels, retention over 10 4 s at 100 °C can still be obtained.",
                "ieee_keywords": [
                    "Electrodes",
                    "Switches",
                    "Resistance",
                    "Temperature measurement",
                    "Reliability",
                    "Programming",
                    "Films"
                ],
                "author_keywords": [
                    "CBRAM",
                    "RRAM",
                    "low current",
                    "nonlinearity",
                    "self-rectifying",
                    "CBRAM",
                    "RRAM",
                    "low current",
                    "nonlinearity",
                    "self-rectifying"
                ]
            },
            {
                "title": "A 25kW SiC Universal Power Converter Building Block for G2V, V2G, and V2L Applications",
                "link": "https://ieeexplore.ieee.org/document/8590435/",
                "date_of_publication": "27 December 2018",
                "doi": "10.1109/PEAC.2018.8590435",
                "citations": "10",
                "abstract": "This paper presents a universal power converter building block (PCBB) with isolation transformer which can perform various functionalities such as charging (G2V), vehicle to grid (V2G) regenerative capability, and vehicle to load (V2L) feature. This converter can output different types of voltage, such as AC or DC, high voltage or low voltage, single-phase or three-phase. With different combinations of modularized PCBB, various voltage and power rating can also be achieved. The control algorithms for each operation mode are explained, including G2V, V2G, and V2L. Simulation results are accompanied to validate its capability. To achieve high power density, SiC MOSFET power module with high frequency switching is adopted along with the isolation transformer of nanocrystalline core. Overall, this paper extends a promising circuit topology of multiple practical and essential features for vehicle electrification.",
                "ieee_keywords": [
                    "Batteries",
                    "Vehicle-to-grid",
                    "High-voltage techniques",
                    "Voltage control",
                    "Bridge circuits",
                    "Control systems",
                    "Pulse width modulation"
                ],
                "author_keywords": [
                    "Vehicle to Grid (V2G)",
                    "Vehicle to Load (V2L)",
                    "On-board Charger",
                    "Dual Active Bridge",
                    "Bidirectional DC/DC",
                    "Electric Vehicle",
                    "Solid State Transformer"
                ]
            },
            {
                "title": "Development of a Screen-Printed Flexible Porous Graphite Electrode for Li-Ion Battery",
                "link": "https://ieeexplore.ieee.org/document/9469794/",
                "date_of_publication": "05 July 2021",
                "doi": "10.1109/FLEPS51544.2021.9469794",
                "citations": "13",
                "abstract": "A flexible screen-printed graphite electrode was developed for fabricating lithium-ion battery. A homogenous ink slurry was prepared by mixing graphite as active material along with carbon black (Super-P C45) as conductive additive and polyvinylidene fluoride (PVDF) as binder in N-Methyl-2- pyrrolidone (NMP) solvent. The ink was deposited via a screen with pattern of 1 mm pores on a flexible copper foil using screen printing process. Lithium-ion battery half-cell was assembled using lithium foil as anode, screen-printed graphite ink on copper substrate as cathode and lithium hexafluorophosphate (LiPF6) as electrolyte. Formation results were tested for the assembled half- cell with controlled porosity and an initial capacity loss (ICL) of 10.7% with 356.2 mAh/g first charge capacity was measured.",
                "ieee_keywords": [
                    "Printing",
                    "Lithium-ion batteries",
                    "Loading",
                    "Graphite",
                    "Ink",
                    "Lithium",
                    "Battery charge measurement"
                ],
                "author_keywords": [
                    "flexible",
                    "screen-printing",
                    "graphite",
                    "characterization"
                ]
            },
            {
                "title": "Deep Neural Network Mapping and Performance Analysis on Tiled RRAM Architecture",
                "link": "https://ieeexplore.ieee.org/document/9073942/",
                "date_of_publication": "23 April 2020",
                "doi": "10.1109/AICAS48895.2020.9073942",
                "citations": "2",
                "abstract": "Representative deep neural networks (DNNs) have been successfully mapped on an RRAM-based tiled in-memory computing (IMC) architecture. Effects of finite array size and quantized partial products (PPs) due to ADC precision constraints have been analyzed. Methods were developed to solve these challenges and preserve DNN accuracies and IMC performance gains in the tiled architecture. Popular models VGG-16, MobileNet, and RNN/LSTM have been successfully implemented and tested on ImageNet dataset and text classification tasks, respectively.",
                "ieee_keywords": [
                    "Quantization (signal)",
                    "Computer architecture",
                    "Integrated circuit modeling",
                    "Neural networks",
                    "Virtual machine monitors",
                    "Task analysis",
                    "Load modeling"
                ],
                "author_keywords": [
                    "RRAM",
                    "Deep Neural Network",
                    "In-Memory Computing",
                    "AI Accelerator"
                ]
            }
        ]
    },
    {
        "name": "Thatchaphol Saranurak",
        "publications": [
            {
                "title": "Near-Optimal Deterministic Vertex-Failure Connectivity Oracles",
                "link": "https://ieeexplore.ieee.org/document/9996628/",
                "date_of_publication": "28 December 2022",
                "doi": "10.1109/FOCS54457.2022.00098",
                "citations": "2",
                "abstract": "We revisit the vertex-failure connectivity oracle problem. This is one of the most basic graph data structure problems under vertex updates, yet its complexity is still not well-understood. We essentially settle the complexity of this problem by showing a new data structure whose space, preprocessing time, update time, and query time are simultaneously optimal up to sub-polynomial factors assuming popular conjectures. Moreover, the data structure is deterministic.More precisely, for any integer $d_{\\star}$, the data structure preprocesses a graph G with n vertices and m edges in $\\hat{O}\\left(m d_{\\star}\\right)$ time and uses $\\tilde{O}\\left(\\min \\left\\{m, n d_{\\star}\\right\\}\\right)$ space. Then, given the vertex set D to be deleted where $|D|=d \\leq d_{\\star}$, it takes $\\hat{O}\\left(d^{2}\\right)$ updates time. Finally, given any vertex pair $(u, v)$, it checks if u and v are connected in $G \\backslash D$ in $O(d)$ time. This improves the previously best deterministic algorithm by Duan and Pettie [SICOMP 2020] in both space and update time by a factor of d. It also significantly speeds up the $\\Omega\\left(\\min \\left\\{m n, n^{\\omega}\\right\\}\\right)$ preprocessing time of all known (even randomized) algorithms with update time at most $\\tilde{O}\\left(d^{5}\\right)$.",
                "ieee_keywords": [
                    "Computer science",
                    "Data structures",
                    "Complexity theory"
                ],
                "author_keywords": []
            },
            {
                "title": "Minimum Cuts in Directed Graphs via Partial Sparsification",
                "link": "https://ieeexplore.ieee.org/document/9719719/",
                "date_of_publication": "04 March 2022",
                "doi": "10.1109/FOCS52979.2021.00113",
                "citations": "169",
                "abstract": "We give an algorithm to find a minimum cut in an edge-weighted directed graph with $n$ vertices and $m$ edges in $\\tilde{O}(n\\cdot\\max\\{m^{2/3},\\ n\\})$ time. This improves on the 30 year old bound of $\\tilde{O}(nm)$ obtained by Hao and Orlin for this problem. Using similar techniques, we also obtain $\\tilde{O}(n^{2}/\\epsilon^{2})$ -time $(1+{\\epsilon})$ -approximation algorithms for both the minimum edge and minimum vertex cuts in directed graphs, for any fixed $\\epsilon$ . Before our work, no (1 + $\\epsilon)$ -approximation algorithm better than the exact runtime of $\\tilde{O}(nm)$ is known for either problem. Our algorithms follow a two-step template. In the first step, we employ a partial sparsification of the input graph to preserve a critical subset of cut values approximately. In the second step, we design algorithms to find the (edge/vertex) mincut among the preserved cuts from the first step. For edge mincut, we give a new reduction to $\\tilde{O}(\\min\\{{n}/m^{1/3}, \\sqrt{n}\\}){-}$ calls of any maxflow subroutine, via packing arborescences in the sparsifier. For vertex mincut, we develop new local flow algorithms to identify small unbalanced cuts in the sparsified graph.",
                "ieee_keywords": [
                    "Computer science",
                    "Runtime",
                    "Directed graphs",
                    "Approximation algorithms"
                ],
                "author_keywords": [
                    "Directed minimum cut",
                    "Sparsification"
                ]
            },
            {
                "title": "Breaking the Cubic Barrier for All-Pairs Max-Flow: Gomory-Hu Tree in Nearly Quadratic Time",
                "link": "https://ieeexplore.ieee.org/document/9996943/",
                "date_of_publication": "28 December 2022",
                "doi": "10.1109/FOCS54457.2022.00088",
                "citations": "1",
                "abstract": "In 1961, Gomory and Hu showed that the All-Pairs Max-Flow problem of computing the max-flow between all $\\begin{pmatrix}n\\\\2\\end{pmatrix}$ pairs of vertices in an undirected graph can be solved using only $n-1$ calls to any (single-pair) max-flow algorithm. Even assuming a linear-time max-flow algorithm, this yields a running time of $O(mn)$, which is $O(n^{3})$ when $m=\\Theta(n^{2})$. While subsequent work has improved this bound for various special graph classes, no subcubic-time algorithm has been obtained in the last 60 years for general graphs. We break this longstanding barrier by giving an $\\tilde{O}(n^{2})$-time algorithm on general, integer-weighted graphs. Combined with a popular complexity assumption, we establish a counter-intuitive separation: all-pairs max-flows are strictly easier to compute than all-pairs shortest-paths.Our algorithm produces a cut-equivalent tree, known as the Gomory-Hu tree, from which the max-flow value for any pair can be retrieved in near-constant time. For unweighted graphs, we refine our techniques further to produce a Gomory-Hu tree in the time of a poly-logarithmic number of calls to any maxflow algorithm. This shows an equivalence between the all-pairs and single-pair max-flow problems, and is optimal up to polylogarithmic factors. Using the recently announced $m^{1+o(1)}$-time max-flow algorithm (Chen et al., March 2022), our Gomory-Hu tree algorithm for unweighted graphs also runs in $m^{1+o(1)}$-time.",
                "ieee_keywords": [
                    "Steiner trees",
                    "Computer science",
                    "Heuristic algorithms",
                    "Complexity theory"
                ],
                "author_keywords": [
                    "Gomory-Hu tree",
                    "graph algorithms",
                    "minimum cut",
                    "maximum flow"
                ]
            },
            {
                "title": "Deterministic Decremental SSSP and Approximate Min-Cost Flow in Almost-Linear Time",
                "link": "https://ieeexplore.ieee.org/document/9719758/",
                "date_of_publication": "04 March 2022",
                "doi": "10.1109/FOCS52979.2021.00100",
                "citations": "2",
                "abstract": "In the decremental single-source shortest paths problem, the goal is to maintain distances from a fixed source $s$ to every vertex $v$ in an m-edge graph undergoing edge deletions. In this paper, we conclude a long line of research on this problem by showing a near-optimal deterministic data structure that maintains (1 + E) -approximate distance estimates and runs in m 1+o(1) total update time. Our result, in particular, removes the oblivious adversary assumption required by the previous breakthrough result by Henzinger et al. [FOCS'14], which leads to our second result: the first almost-linear time algorithm for (1 - E) -approximate min-cost flow in undirected graphs where capacities and costs can be taken over edges and vertices. Previously, algorithms for max flow with vertex capacities, or min-cost flow with any capacities required super-linear time. Our result essentially completes the picture for approximate flow in undirected graphs. The key technique of the first result is a novel framework that allows us to treat low-diameter graphs like expanders. This allows us to harness expander properties while bypassing shortcomings of expander decomposition, which almost all previous expander-based algorithms needed to deal with. For the second result, we break the notorious flow-decomposition barrier from the multiplicative-weight-update framework using randomization.",
                "ieee_keywords": [
                    "Shortest path problem",
                    "Computer science",
                    "Costs",
                    "Approximation algorithms",
                    "Data structures"
                ],
                "author_keywords": [
                    "component",
                    "formatting",
                    "style",
                    "styling"
                ]
            },
            {
                "title": "A Nearly Optimal All-Pairs Min-Cuts Algorithm in Simple Graphs",
                "link": "https://ieeexplore.ieee.org/document/9719859/",
                "date_of_publication": "04 March 2022",
                "doi": "10.1109/FOCS52979.2021.00111",
                "citations": "1",
                "abstract": "We give an $n^{2+o(1)}$ -time algorithm for finding $s-t$ min-cuts for all pairs of vertices $s$ and $t$ in a simple, undirected graph on $n$ vertices. We do so by constructing a Gomory-Hu tree (or cut equivalent tree) in the same running time, thereby improving on the recent bound of $\\tilde{O}(n^{2.5})$ by Abboud et al. (STOC 2021). Our running time is nearly optimal as a function of $n$ .",
                "ieee_keywords": [
                    "Computer science",
                    "Costs"
                ],
                "author_keywords": [
                    "Gomory Hu tree",
                    "Expander decomposition"
                ]
            },
            {
                "title": "Deterministic Small Vertex Connectivity in Almost Linear Time",
                "link": "https://ieeexplore.ieee.org/document/9996669/",
                "date_of_publication": "28 December 2022",
                "doi": "10.1109/FOCS54457.2022.00080",
                "citations": "74",
                "abstract": "In the vertex connectivity problem, given an undirected n-vertex m-edge graph G, we need to compute the minimum number of vertices that can disconnect G after removing them. This problem is one of the most well-studied graph problems. From 2019, a new line of work [Nanongkai et al. STOC’19;SODA’20;STOC’21] has used randomized techniques to break the quadratic-time barrier and, very recently, culminated in an almost-linear time algorithm via the recently announced maxflow algorithm by Chen et al. In contrast, all known deterministic algorithms are much slower. The fastest algorithm [Gabow FOCS’00] takes $O(m(n+min\\{c^{5/2}, cn^{3/4}\\}))$ time where c is the vertex connectivity. It remains open whether there exists a subquadratic-time deterministic algorithm for any constant c > 3. In this paper, we give the first deterministic almost-linear time vertex connectivity algorithm for all constants c. Our running time is $m^{1+o(1)}2^{O(c^{2})}$ time, which is almost-linear for all $c=o(\\sqrt{\\log n})$. This is the first deterministic algorithm that breaks the $O(n^{2})$-time bound on sparse graphs where $m=O(n)$, which is known for more than 50 years ago [Kleitman’69]. Towards our result, we give a new reduction framework to vertex expanders which in turn exploits our new almost-linear time construction of mimicking network for vertex connectivity. The previous construction by Kratsch and Wahlström [FOCS’12] requires large polynomial time and is randomized.",
                "ieee_keywords": [
                    "Computer science"
                ],
                "author_keywords": [
                    "Algorithmic Graph Theory",
                    "Vertex Connectivity."
                ]
            }
        ]
    },
    {
        "name": "Z. Morley Mao (茅斫青)",
        "publications": [
            {
                "title": "Sensor Adversarial Traits: Analyzing Robustness of 3D Object Detection Sensor Fusion Models",
                "link": "https://ieeexplore.ieee.org/document/9506183/",
                "date_of_publication": "23 August 2021",
                "doi": "10.1109/ICIP42928.2021.9506183",
                "citations": "1",
                "abstract": "A critical aspect of autonomous vehicles (AVs) is the object detection stage, which is increasingly being performed with sensor fusion models: multimodal 3D object detection models which utilize both 2D RGB image data and 3D data from a LIDAR sensor as inputs. In this work, we perform the first study to analyze the robustness of a high-performance, open source sensor fusion model architecture towards adversarial attacks and challenge the popular belief that the use of additional sensors automatically mitigate the risk of adversarial attacks. We find that despite the use of a LIDAR sensor, the model is vulnerable to our purposefully crafted image-based adversarial attacks including disappearance, universal patch, and spoofing. After identifying the underlying reason, we explore some potential defenses and provide some recommendations for improved sensor fusion models.",
                "ieee_keywords": [
                    "Training",
                    "Solid modeling",
                    "Analytical models",
                    "Three-dimensional displays",
                    "Laser radar",
                    "Object detection",
                    "Sensor fusion"
                ],
                "author_keywords": [
                    "Adversarial examples",
                    "multimodal",
                    "3D object detection",
                    "sensor fusion"
                ]
            },
            {
                "title": "Categorization of Anomalies in Smart Manufacturing Systems to Support the Selection of Detection Mechanisms",
                "link": "https://ieeexplore.ieee.org/document/7945261/",
                "date_of_publication": null,
                "doi": "10.1109/LRA.2017.2714135",
                "citations": "35",
                "abstract": "An important issue in anomaly detection in smart manufacturing systems is the lack of consistency in the formal definitions of anomalies, faults, and attacks. The term anomaly is used to cover a wide range of situations that are addressed by different types of solutions. In this letter, we categorize anomalies in machines, controllers, and networks along with their detection mechanisms, and unify them under a common framework to aid in the identification of potential solutions. The main contribution of the proposed categorization is that it allows the identification of gaps in anomaly detection in smart manufacturing systems.",
                "ieee_keywords": [
                    "Manufacturing systems",
                    "Market research",
                    "Robot sensing systems"
                ],
                "author_keywords": [
                    "Factory automation",
                    "intelligent and flexible manufacturing"
                ]
            },
            {
                "title": "MCNet: Crowdsourcing wireless performance measurements through the eyes of mobile devices",
                "link": "https://ieeexplore.ieee.org/document/6917407/",
                "date_of_publication": null,
                "doi": "10.1109/MCOM.2014.6917407",
                "citations": "32",
                "abstract": "Measurement of network performance in complex WiFi networks, such as in corporations and universities, is an important but challenging task, as wireless performance in networks with many access points is hard to measure and model. We demonstrate that crowdsourcing the task of measuring WiFi performance is an effective solution to this problem. By measuring performance directly with unmodified consumer mobile devices such as smartphones, it is possible to cheaply and easily detect problems that matter to users. Aggregated performance data across clients can then provide a global view of performance trends in an enterprise network. We demonstrate that periodic sampling allows the collection of representative data while keeping battery consumption low, and we leverage mobile sensor information to intelligently schedule these measurements. This system was deployed in two different large WLANs, where we discovered numerous previously undetected performance problems.",
                "ieee_keywords": [
                    "Performance evaluation",
                    "IEEE 802.11 Standards",
                    "Battery charge measurement",
                    "Crowdsourcing",
                    "Mobile communication",
                    "Wireless LAN",
                    "Mobile handsets"
                ],
                "author_keywords": []
            },
            {
                "title": "On the Cybersecurity of Traffic Signal Control System With Connected Vehicles",
                "link": "https://ieeexplore.ieee.org/document/9714728/",
                "date_of_publication": null,
                "doi": "10.1109/TITS.2022.3149449",
                "citations": "6",
                "abstract": "Connected vehicle (CV) technology brings both opportunities and challenges to the traffic signal control (TSC) system. While safety and mobility performance could be greatly improved by adopting CV technologies, the connectivity between vehicles and transportation infrastructure may increase the risks of cyber threats. In the past few years, studies related to cybersecurity on the TSC systems were conducted. However, there still lacks a systematic investigation that provides a comprehensive analysis framework. In this study, our aim is to fill the research gap by proposing a comprehensive analysis framework for the cybersecurity problem of the TSC in the CV environment. With potential threats towards the major components of the system and their corresponding impacts on safety and efficiency analyzed, data spoofing attack is considered the most plausible and realistic attack approach. Based on this finding, different attack strategies and defense solutions are discussed. A case study is presented to show the impact of the data spoofing attacks towards a selected CV based TSC system and corresponding mitigation countermeasures. This case study is conducted on a hybrid security testing platform, with virtual traffic and a real V2X communication network. To the best of our knowledge, this is the first study to present a comprehensive analysis framework to the cybersecurity problem of the CV-based TSC systems.",
                "ieee_keywords": [
                    "Computer security",
                    "Transportation",
                    "Safety",
                    "Timing",
                    "Testing",
                    "Connected vehicles",
                    "Vehicle-to-everything"
                ],
                "author_keywords": [
                    "Traffic signal control system",
                    "cybersecurity",
                    "connected vehicles",
                    "security testing platform"
                ]
            },
            {
                "title": "Toward an Automated Learning Control Architecture for Cyber-Physical Manufacturing Systems",
                "link": "https://ieeexplore.ieee.org/document/9751037/",
                "date_of_publication": null,
                "doi": "10.1109/ACCESS.2022.3165551",
                "citations": "7",
                "abstract": "Manufacturers are constantly looking to enhance the performance of their manufacturing systems by improving their ability to address disruptions and disturbances, while reducing cost and maximizing quantity and quality. Even though innovative mechanisms for adaptability and flexibility continuously contribute to the smart manufacturing evolutionary process, they generally stop short of providing a capability for coordinated on-line learning. This is especially true when that learning requires exploration outside of established operational boundaries or uses artificial intelligence (as opposed to purely human intelligence) as part of the dynamic implementation of learning. In this work, we provide a vision for the development of an automated learning control architecture to extend the adaptability and flexibility capabilities of manufacturing systems. As part of this vision, we describe a set of requirements and objectives that, if addressed, provide an environment to allow distributed and automated learning across the manufacturing ecosystem. We then provide an example communication and control architecture that meets these requirements and objectives by gathering information, building a dynamic knowledge base, distributing intelligence, making decisions, and adapting the control commands sent to the equipment and people across the manufacturing ecosystem. The example architecture leverages both centralized and distributed control strategies and the ability to switch between the strategies to gather and learn from information in the system. Example case studies are provided illustrating how this architecture can be used to improve manufacturing system performance. This work describes a vision for automated learning in manufacturing systems that would allow the system to go beyond predefined capabilities and automatically learn from...View more",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "AVMaestro: A Centralized Policy Enforcement Framework for Safe Autonomous-driving Environments",
                "link": "https://ieeexplore.ieee.org/document/9827092/",
                "date_of_publication": "19 July 2022",
                "doi": "10.1109/IV51971.2022.9827092",
                "citations": "1",
                "abstract": "Autonomous vehicles (AVs) are on the verge of changing the transportation industry. Despite the fast development of autonomous driving systems (ADSs), they still face safety and security challenges. Current defensive approaches usually focus on a narrow objective and are bound to specific platforms, making them difficult to generalize. To solve these limitations, we propose AVMaestro, an efficient and effective policy enforcement framework for full-stack ADSs. AVMaestro includes a code instrumentation module to systematically collect required information across the entire ADS, which will then be feed into a centralized data examination module, where users can utilize the global information to deploy defensive methods to protect AVs from various threats. AVMaestro is evaluated on top of Apollo-6.0 and experimental results confirm that it can be easily incorporated into the original ADS with almost negligible run-time delay. We further demonstrate that utilizing the global information can not only improve the accuracy of existing intrusion detection methods, but also potentially inspire new security applications.",
                "ieee_keywords": [
                    "Codes",
                    "Instruments",
                    "Transportation industry",
                    "Prototypes",
                    "Intrusion detection",
                    "Safety",
                    "Security"
                ],
                "author_keywords": []
            },
            {
                "title": "Production as a service: A centralized framework for small batch manufacturing",
                "link": "https://ieeexplore.ieee.org/document/8256133/",
                "date_of_publication": "15 January 2018",
                "doi": "10.1109/COASE.2017.8256133",
                "citations": "7",
                "abstract": "We present a Production as a Service (PaaS) framework that connects users (product developers) who have small batch production needs, with existing manufacturing facilities that have underutilized resources. PaaS is a web-based framework based on the service oriented architecture (SOA) design that breaks down the manufacturing of a product into several steps, or services, to incorporate various manufacturers with available capability in fulfilling the production request. Preservation of intellectual property (IP) is a major concern with SOA based manufacturing systems. Presented work aims to address this issue by using abstracted features to describe a product design, which enables manufacturers to provide quotations for the manufacturing steps without the need for the detailed technical data of the product. The paper also presents the optimization approaches employed to find feasible solutions based on the collected quotations for a production request by relaxing the precedence constraints on the manufacturing processes or the components. A case study is provided to illustrate the functionalities of the proposed framework. A detailed description of the PaaS architecture and the required APIs for its implementation are also provided.",
                "ieee_keywords": [
                    "Databases",
                    "Optimization",
                    "Service-oriented architecture",
                    "Manufacturing processes",
                    "Production facilities"
                ],
                "author_keywords": []
            },
            {
                "title": "The Mason Test: A Defense Against Sybil Attacks in Wireless Networks Without Trusted Authorities",
                "link": "https://ieeexplore.ieee.org/document/7029135/",
                "date_of_publication": null,
                "doi": "10.1109/TMC.2015.2398425",
                "citations": "22",
                "abstract": "Wireless networks are vulnerable to Sybil attacks, in which a malicious node poses as many identities in order to gain disproportionate influence. Many defenses based on spatial variability of wireless channels exist, but depend either on detailed, multi-tap channel estimation-something not exposed on commodity 802.11 devices-or valid RSSI observations from multiple trusted sources, e.g., corporate access points-something not directly available in ad hoc and delay-tolerant networks with potentially malicious neighbors. We extend these techniques to be practical for wireless ad hoc networks of commodity 802.11 devices. Specifically, we propose two efficient methods for separating the valid RSSI observations of behaving nodes from those falsified by malicious participants. Further, we note that prior signalprint methods are easily defeated by mobile attackers and develop an appropriate challenge-response defense. Finally, we present the Mason test, the first implementation of these techniques for ad hoc and delay-tolerant networks of commodity 802.11 devices. We illustrate its performance in several real-world scenarios.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Diagnosing Mobile Apps' Quality of Experience: Challenges and Promising Directions",
                "link": "https://ieeexplore.ieee.org/document/7373505/",
                "date_of_publication": null,
                "doi": "10.1109/MIC.2016.1",
                "citations": "3",
                "abstract": "How can mobile app developers characterize and diagnose problems that could hinder users' quality of experience?",
                "ieee_keywords": [
                    "Mobile communication",
                    "Computer applications",
                    "Internet",
                    "Web services",
                    "Quality of experience"
                ],
                "author_keywords": [
                    "Internet/Web technologies",
                    "quality of experience",
                    "QoE",
                    "mobile apps"
                ]
            },
            {
                "title": "Anomaly Detection Against GPS Spoofing Attacks on Connected and Autonomous Vehicles Using Learning From Demonstration",
                "link": "https://ieeexplore.ieee.org/document/10109166/",
                "date_of_publication": null,
                "doi": "10.1109/TITS.2023.3269029",
                "citations": "1",
                "abstract": "GPS spoofing attacks pose great challenges to connected vehicle (CVs) safety applications and localization of autonomous vehicles (AVs). In this paper, we propose to utilize transportation and vehicle engineering domain knowledge to detect GPS spoofing attacks towards CVs and AVs. A novel detection method using learning from demonstration is developed, which can be implemented in both vehicles and at the transportation infrastructure. A computational-efficient driving model, which can be learned from historical trajectories of the vehicles, is constructed to predict normal driving behaviors. Then a statistical method is developed to measure the dissimilarities between the observed trajectory and the predicted normal trajectory for anomaly detection. We validate the proposed method using two threat models (i.e., attacks targeting the multi-sensor fusion system of AVs and attacks targeting the intersection movement assist application of CVs) on two real-world datasets (i.e., KAIST and Michigan roundabout dataset). Results show that the proposed model is able to detect almost all of the attacks in time with low false positive and false negative rates.",
                "ieee_keywords": [
                    "Global Positioning System",
                    "Location awareness",
                    "Trajectory",
                    "Behavioral sciences",
                    "Threat modeling",
                    "Anomaly detection",
                    "Transportation"
                ],
                "author_keywords": [
                    "Anomaly detection",
                    "GPS spoofing attack",
                    "Black localization",
                    "intersection movement assist",
                    "connected and autonomous vehicles",
                    "learning from demonstration"
                ]
            }
        ]
    },
    {
        "name": "Jason Corso",
        "publications": [
            {
                "title": "Learning Compositional Sparse Bimodal Models",
                "link": "https://ieeexplore.ieee.org/document/7898500/",
                "date_of_publication": null,
                "doi": "10.1109/TPAMI.2017.2693987",
                "citations": "686",
                "abstract": "Various perceptual domains have underlying compositional semantics that are rarely captured in current models. We suspect this is because directly learning the compositional structure has evaded these models. Yet, the compositional structure of a given domain can be grounded in a separate domain thereby simplifying its learning. To that end, we propose a new approach to modeling bimodal perceptual domains that explicitly relates distinct projections across each modality and then jointly learns a bimodal sparse representation. The resulting model enables compositionality across these distinct projections and hence can generalize to unobserved percepts spanned by this compositional basis. For example, our model can be trained on red triangles and blue squares; yet, implicitly will also have learned red squares and blue triangles. The structure of the projections and hence the compositional basis is learned automatically; no assumption is made on the ordering of the compositional elements in either modality. Although our modeling paradigm is general, we explicitly focus on a tabletop building-blocks setting. To test our model, we have acquired a new bimodal dataset comprising images and spoken utterances of colored shapes (blocks) in the tabletop setting. Our experiments demonstrate the benefits of explicitly leveraging compositionality in both quantitative and human evaluation studies.",
                "ieee_keywords": [
                    "Dictionaries",
                    "Encoding",
                    "Semantics",
                    "Poles and towers",
                    "Robot sensing systems",
                    "Visualization"
                ],
                "author_keywords": [
                    "Multimodal learning",
                    "compositional learning",
                    "symbol grounding",
                    "artificial intelligence",
                    "tabletop robotics",
                    "human-robot interaction"
                ]
            },
            {
                "title": "Real-time model predictive control for keeping a quadrotor visible on the camera field-of-view of a ground robot",
                "link": "https://ieeexplore.ieee.org/document/7525254/",
                "date_of_publication": "01 August 2016",
                "doi": "10.1109/ACC.2016.7525254",
                "citations": "1",
                "abstract": "This paper considers a cooperative control design for an aerial/ground robot system, and addresses the problem of maintaining visibility of a quadrotor within the camera field-of-view of a ground robot in the presence of external disturbances. The quadrotor needs to be tracked by the ground robot with a monocular camera, and hence its motion should facilitate the ground vision-based tracking process by remaining in the effective camera sensing area. We design a model predictive controller (MPC) strategy where the visibility constraints of the camera and the control input constraints of the quadrotor are encoded into the cost function via barrier functions, and we adopt a fast MPC solver that is able to solve the optimization problem in real time. We also propose a method to enhance the robustness of the algorithm by suitably defining a restart method for the MPC solver. The applicability of the proposed algorithm is demonstrated through simulations and experimental results on real setups.",
                "ieee_keywords": [
                    "Cameras",
                    "Robot vision systems",
                    "Acceleration",
                    "Optimization"
                ],
                "author_keywords": []
            },
            {
                "title": "MINT: Deep Network Compression via Mutual Information-based Neuron Trimming",
                "link": "https://ieeexplore.ieee.org/document/9412590/",
                "date_of_publication": "05 May 2021",
                "doi": "10.1109/ICPR48806.2021.9412590",
                "citations": "213",
                "abstract": "Most approaches to deep neural network compression via pruning either directly evaluate a filter's importance using its weights or optimize an alternative objective function with sparsity constraints. While these methods offer a useful way to approximate contributions from similar filters, they often either ignore the dependency between layers or solve a more difficult optimization objective than standard cross-entropy. Our method, Mutual Information-based Neuron Trimming (MINT), approaches deep compression via pruning by enforcing sparsity based on the strength of the dependency between filters of adjacent layers, across every pair of layers in the network. The dependency is calculated using conditional geometric mutual information which evaluates the amount of similar information exchanged between filters using a graph-based criterion. When pruning a network, we ensure that retained filters contribute the majority of the information towards succeeding layers which ensures high performance. Our novel approach is highly competitive with existing state-of-the-art compression-via-pruning methods on standard benchmarks for this task: MNIST, CIFAR-10, and ILSVRC2012, across a variety of network architectures despite using only a single retraining pass. Also, we discuss our observations of a common denominator between our pruning methodology's response to adversarial attacks and calibration statistics when compared to the original network.",
                "ieee_keywords": [
                    "Sensitivity",
                    "Neurons",
                    "Redundancy",
                    "Filtering algorithms",
                    "Information filters",
                    "Robustness",
                    "Calibration"
                ],
                "author_keywords": []
            },
            {
                "title": "Ground-truth or DAER: Selective Re-query of Secondary Information",
                "link": "https://ieeexplore.ieee.org/document/9709633/",
                "date_of_publication": "28 February 2022",
                "doi": "10.1109/ICCV48922.2021.00074",
                "citations": "32",
                "abstract": "Many vision tasks use secondary information at inference time—a seed—to assist a computer vision model in solving a problem. For example, an initial bounding box is needed to initialize visual object tracking. To date, all such work makes the assumption that the seed is a good one. However, in practice, from crowdsourcing to noisy automated seeds, this is often not the case. We hence propose the problem of seed rejection—determining whether to reject a seed based on the expected performance degradation when it is provided in place of a gold-standard seed. We provide a formal definition to this problem, and focus on two meaningful subgoals: understanding causes of error and understanding the model’s response to noisy seeds conditioned on the primary input. With these goals in mind, we propose a novel training method and evaluation metrics for the seed rejection problem. We then use seeded versions of the viewpoint estimation and fine-grained classification tasks to evaluate these contributions. In these experiments, we show our method can reduce the number of seeds that need to be reviewed for a target performance by over 23% compared to strong baselines.",
                "ieee_keywords": [
                    "Training",
                    "Degradation",
                    "Crowdsourcing",
                    "Computer vision",
                    "Visualization",
                    "Computational modeling",
                    "Estimation"
                ],
                "author_keywords": [
                    "Vision + other modalities",
                    "Efficient training and inference methods",
                    "Machine learning architectures and formulations"
                ]
            },
            {
                "title": "Can humans fly? Action understanding with multiple classes of actors",
                "link": "https://ieeexplore.ieee.org/document/7298839/",
                "date_of_publication": "15 October 2015",
                "doi": "10.1109/CVPR.2015.7298839",
                "citations": "46",
                "abstract": "Can humans fly? Emphatically no. Can cars eat? Again, absolutely not. Yet, these absurd inferences result from the current disregard for particular types of actors in action understanding. There is no work we know of on simultaneously inferring actors and actions in the video, not to mention a dataset to experiment with. Our paper hence marks the first effort in the computer vision community to jointly consider various types of actors undergoing various actions. To start with the problem, we collect a dataset of 3782 videos from YouTube and label both pixel-level actors and actions in each video. We formulate the general actor-action understanding problem and instantiate it at various granularities: both video-level single- and multiple-label actor-action recognition and pixel-level actor-action semantic segmentation. Our experiments demonstrate that inference jointly over actors and actions outperforms inference independently over them, and hence concludes our argument of the value of explicit consideration of various actors in comprehensive action understanding.",
                "ieee_keywords": [
                    "Joints",
                    "Semantics",
                    "Birds",
                    "Pediatrics",
                    "Legged locomotion",
                    "Yttrium",
                    "Graphical models"
                ],
                "author_keywords": []
            },
            {
                "title": "Weakly Supervised Actor-Action Segmentation via Robust Multi-task Ranking",
                "link": "https://ieeexplore.ieee.org/document/8099598/",
                "date_of_publication": "09 November 2017",
                "doi": "10.1109/CVPR.2017.115",
                "citations": "25",
                "abstract": "Fine-grained activity understanding in videos has attracted considerable recent attention with a shift from action classification to detailed actor and action understanding that provides compelling results for perceptual needs of cutting-edge autonomous systems. However, current methods for detailed understanding of actor and action have significant limitations: they require large amounts of finely labeled data, and they fail to capture any internal relationship among actors and actions. To address these issues, in this paper, we propose a novel, robust multi-task ranking model for weakly supervised actor-action segmentation where only video-level tags are given for training samples. Our model is able to share useful information among different actors and actions while learning a ranking matrix to select representative supervoxels for actors and actions respectively. Final segmentation results are generated by a conditional random field that considers various ranking scores for different video parts. Extensive experimental results on the Actor-Action Dataset (A2D) demonstrate that the proposed approach outperforms the state-of-the-art weakly supervised methods and performs as well as the top-performing fully supervised method.",
                "ieee_keywords": [
                    "Videos",
                    "Robustness",
                    "Optimization",
                    "Semantics",
                    "Training",
                    "Support vector machines"
                ],
                "author_keywords": []
            },
            {
                "title": "A Temporally-Aware Interpolation Network for Video Frame Inpainting",
                "link": "https://ieeexplore.ieee.org/document/8892406/",
                "date_of_publication": null,
                "doi": "10.1109/TPAMI.2019.2951667",
                "citations": "17",
                "abstract": "In this work, we explore video frame inpainting, a task that lies at the intersection of general video inpainting, frame interpolation, and video prediction. Although our problem can be addressed by applying methods from other video interpolation or extrapolation tasks, doing so fails to leverage the additional context information that our problem provides. To this end, we devise a method specifically designed for video frame inpainting that is composed of two modules: a bidirectional video prediction module and a temporally-aware frame interpolation module. The prediction module makes two intermediate predictions of the missing frames, each conditioned on the preceding and following frames respectively, using a shared convolutional LSTM-based encoder-decoder. The interpolation module blends the intermediate predictions by using time information and hidden activations from the video prediction module to resolve disagreements between the predictions. Our experiments demonstrate that our approach produces smoother and more accurate results than state-of-the-art methods for general video inpainting, frame interpolation, and video prediction.",
                "ieee_keywords": [
                    "Interpolation",
                    "Task analysis",
                    "Predictive models",
                    "Data models",
                    "Sun",
                    "Computer science"
                ],
                "author_keywords": [
                    "Video inpainting",
                    "video prediction",
                    "frame interpolation",
                    "temporal upsampling"
                ]
            },
            {
                "title": "Video Object Segmentation-based Visual Servo Control and Object Depth Estimation on a Mobile Robot",
                "link": "https://ieeexplore.ieee.org/document/9093335/",
                "date_of_publication": "14 May 2020",
                "doi": "10.1109/WACV45572.2020.9093335",
                "citations": "10",
                "abstract": "To be useful in everyday environments, robots must be able to identify and locate real-world objects. In recent years, video object segmentation has made significant progress on densely separating such objects from background in real and challenging videos. Building off of this progress, this paper addresses the problem of identifying generic objects and locating them in 3D using a mobile robot with an RGB camera. We achieve this by, first, introducing a video object segmentation-based approach to visual servo control and active perception and, second, developing a new Hadamard-Broyden update formulation. Our segmentation-based methods are simple but effective, and our update formulation lets a robot quickly learn the relationship between actuators and visual features without any camera calibration. We validate our approach in experiments by learning a variety of actuator-camera configurations on a mobile HSR robot, which subsequently identifies, locates, and grasps objects from the YCB dataset and tracks people and other dynamic articulated objects in real-time.",
                "ieee_keywords": [
                    "Visualization",
                    "Cameras",
                    "Servosystems",
                    "Three-dimensional displays",
                    "Streaming media",
                    "Robot sensing systems"
                ],
                "author_keywords": []
            },
            {
                "title": "Novel Object Viewpoint Estimation Through Reconstruction Alignment",
                "link": "https://ieeexplore.ieee.org/document/9156616/",
                "date_of_publication": "05 August 2020",
                "doi": "10.1109/CVPR42600.2020.00318",
                "citations": "8",
                "abstract": "The goal of this paper is to estimate the viewpoint for a novel object. Standard viewpoint estimation approaches generally fail on this task due to their reliance on a 3D model for alignment or large amounts of class-specific training data and their corresponding canonical pose. We overcome those limitations by learning a reconstruct and align approach. Our key insight is that although we do not have an explicit 3D model or a predefined canonical pose, we can still learn to estimate the object's shape in the viewer's frame and then use an image to provide our reference model or canonical pose. In particular, we propose learning two networks: the first maps images to a 3D geometry-aware feature bottleneck and is trained via an image-to-image translation loss; the second learns whether two instances of features are aligned. At test time, our model finds the relative transformation that best aligns the bottleneck features of our test image to a reference image. We evaluate our method on novel object viewpoint estimation by generalizing across different datasets, analyzing the impact of our different modules, and providing a qualitative analysis of the learned features to identify what representation is being learnt for alignment.",
                "ieee_keywords": [
                    "Three-dimensional displays",
                    "Solid modeling",
                    "Shape",
                    "Two dimensional displays",
                    "Estimation",
                    "Image reconstruction",
                    "Task analysis"
                ],
                "author_keywords": []
            },
            {
                "title": "Robot-Supervised Learning for Object Segmentation",
                "link": "https://ieeexplore.ieee.org/document/9196543/",
                "date_of_publication": "15 September 2020",
                "doi": "10.1109/ICRA40945.2020.9196543",
                "citations": "4",
                "abstract": "To be effective in unstructured and changing environments, robots must learn to recognize new objects. Deep learning has enabled rapid progress for object detection and segmentation in computer vision; however, this progress comes at the price of human annotators labeling many training examples. This paper addresses the problem of extending learning-based segmentation methods to robotics applications where annotated training data is not available. Our method enables pixelwise segmentation of grasped objects. We factor the problem of segmenting the object from the background into two sub-problems: (1) segmenting the robot manipulator and object from the background and (2) segmenting the object from the manipulator. We propose a kinematics-based foreground segmentation technique to solve (1). To solve (2), we train a self-recognition network that segments the robot manipulator. We train this network without human supervision, leveraging our foreground segmentation technique from (1) to label a training set of images containing the robot manipulator without a grasped object. We demonstrate experimentally that our method outperforms state-of-the-art adaptable in-hand object segmentation. We also show that a training set composed of automatically labelled images of grasped objects improves segmentation performance on a test set of images of the same objects in the environment.",
                "ieee_keywords": [
                    "Robot kinematics",
                    "Manipulators",
                    "Image segmentation",
                    "Robot sensing systems",
                    "Object segmentation",
                    "Training"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Stella X. Yu",
        "publications": [
            {
                "title": "Modeling Semantic Correlation and Hierarchy for Real-World Wildlife Recognition",
                "link": "https://ieeexplore.ieee.org/document/10073535/",
                "date_of_publication": null,
                "doi": "10.1109/LSP.2023.3257725",
                "citations": "1",
                "abstract": "We explore the challenges of human-in-the-loop frameworks to label wildlife recognition datasets with a neural network. In wildlife imagery, the main challenges for a model to assist human annotation are two-fold: (1) the training dataset is usually imbalanced, which makes the model's suggestion biased, and (2) there are complex taxonomies in the classes. We establish a simple and efficient baseline, including the debiasing loss function and the hyperbolic network architecture, to address these issues. Moreover, we propose leveraging the semantic correlation to train the model more effectively by adding a co-occurrence layer to our model during training. We demonstrate the efficacy of our method in both a real-world wildlife areal survey recognition dataset and the public image classification dataset, CIFAR100-LT, CIFAR10-LT, and iNaturalist.",
                "ieee_keywords": [
                    "Wildlife",
                    "Training",
                    "Neural networks",
                    "Semantics",
                    "Correlation",
                    "Data models",
                    "Birds"
                ],
                "author_keywords": [
                    "Wildlife recognition",
                    "active learning",
                    "class imbalance"
                ]
            },
            {
                "title": "Compact and Optimal Deep Learning with Recurrent Parameter Generators",
                "link": "https://ieeexplore.ieee.org/document/10030806/",
                "date_of_publication": "06 February 2023",
                "doi": "10.1109/WACV56688.2023.00389",
                "citations": "51",
                "abstract": "Deep learning has achieved tremendous success by training increasingly large models, which are then compressed for practical deployment. We propose a drastically different approach to compact and optimal deep learning: We decouple the Degrees of freedom (DoF) and the actual number of parameters of a model, optimize a small DoF with predefined random linear constraints for a large model of an arbitrary architecture, in one-stage end-to-end learning.Specifically, we create a recurrent parameter generator (RPG), which repeatedly fetches parameters from a ring and unpacks them onto a large model with random permutation and sign flipping to promote parameter decorrelation. We show that gradient descent can automatically find the best model under constraints with in fact faster convergence.Our extensive experimentation reveals a log-linear relationship between model DoF and accuracy. Our RPG demonstrates remarkable DoF reduction, and can be further pruned and quantized for additional run-time performance gain. For example, in terms of top-1 accuracy on ImageNet, RPG achieves 96% of ResNet18’s performance with only 18% DoF (the equivalent of one convolutional layer) and 52% of ResNet34’s performance with only 0.25% DoF! Our work shows significant potential of constrained neural opti-mization in compact and optimal deep learning.",
                "ieee_keywords": [
                    "Deep learning",
                    "Training",
                    "Computer vision",
                    "Computer architecture",
                    "Performance gain",
                    "Generators",
                    "Decorrelation"
                ],
                "author_keywords": [
                    "Algorithms: Machine learning architectures",
                    "formulations",
                    "and algorithms (including transfer)"
                ]
            },
            {
                "title": "Cut and Learn for Unsupervised Object Detection and Instance Segmentation",
                "link": "https://ieeexplore.ieee.org/document/10203052/",
                "date_of_publication": "22 August 2023",
                "doi": "10.1109/CVPR52729.2023.00305",
                "citations": "1",
                "abstract": "We propose Cut-and-LEaRn (CutLER), a simple approach for training unsupervised object detection and seg-mentation models. We leverage the property of self-supervised models to ‘discover’ objects without supervision and amplify it to train a state-of-the-art localization model without any human labels. CutLER first uses our proposed MaskCut approach to generate coarse masks for multiple objects in an image, and then learns a detector on these masks using our robust loss function. We further improve performance by self-training the model on its predictions. Compared to prior work, CutLER is simpler, compatible with different detection architectures, and detects multiple objects. CutLER is also a zero-shot unsupervised detector and improves detection performance AP 50 by over 2.7× on 11 benchmarks across domains like video frames, paintings, sketches, etc. With finetuning, CutLER serves as a low-shot detector surpassing MoCo-v2 by 7.3% Ap box and 6.6% Ap mask on COCO when training with 5% labels.",
                "ieee_keywords": [
                    "Training",
                    "Representation learning",
                    "Location awareness",
                    "Computer vision",
                    "Detectors",
                    "Object detection",
                    "Computer architecture"
                ],
                "author_keywords": [
                    "Self-supervised or unsupervised representation learning"
                ]
            },
            {
                "title": "Bootstrapping Objectness from Videos by Relaxed Common Fate and Visual Grouping",
                "link": "https://ieeexplore.ieee.org/document/10204404/",
                "date_of_publication": "22 August 2023",
                "doi": "10.1109/CVPR52729.2023.01401",
                "citations": "2",
                "abstract": "We study learning object segmentation from unlabeled videos. Humans can easily segment moving objects without knowing what they are. The Gestalt law of common fate, i.e., what move at the same speed belong together, has inspired unsupervised object discovery based on motion segmentation. However, common fate is not a reliable indicator of objectness: Parts of an articulated / deformable object may not move at the same speed, whereas shadows / reflections of an object always move with it but are not part of it. Our insight is to bootstrap objectness by first learning image features from relaxed common fate and then refining them based on visual appearance grouping within the image itself and across images statistically. Specifically, we learn an image segmenter first in the loop of approximating optical flow with constant segment flow plus small within-segment residual flow, and then by refining it for more coherent appearance and statistical figure-ground relevance. On unsupervised video object segmentation, using only ResNet and convolutional heads, our model surpasses the state-of-the-art by absolute gains of 7/9/5% on DAVIS16 / STv2 / FBMS59 respectively, demonstrating the effectiveness of our ideas. Our code is publicly available.",
                "ieee_keywords": [
                    "Computer vision",
                    "Visualization",
                    "Image segmentation",
                    "Image motion analysis",
                    "Motion segmentation",
                    "Refining",
                    "Object segmentation"
                ],
                "author_keywords": [
                    "Video: Low-level analysis",
                    "motion",
                    "tracking"
                ]
            }
        ]
    },
    {
        "name": "Eytan Adar",
        "publications": [
            {
                "title": "Benefitting InfoVis with Visual Difficulties",
                "link": "https://ieeexplore.ieee.org/document/6064986/",
                "date_of_publication": null,
                "doi": "10.1109/TVCG.2011.175",
                "citations": "78",
                "abstract": "Many well-cited theories for visualization design state that a visual representation should be optimized for quick and immediate interpretation by a user. Distracting elements like decorative \"chartjunk\" or extraneous information are avoided so as not to slow comprehension. Yet several recent studies in visualization research provide evidence that non-efficient visual elements may benefit comprehension and recall on the part of users. Similarly, findings from studies related to learning from visual displays in various subfields of psychology suggest that introducing cognitive difficulties to visualization interaction can improve a user's understanding of important information. In this paper, we synthesize empirical results from cross-disciplinary research on visual information representations, providing a counterpoint to efficiency-based design theory with guidelines that describe how visual difficulties can be introduced to benefit comprehension and recall. We identify conditions under which the application of visual difficulties is appropriate based on underlying factors in visualization interaction like active processing and engagement. We characterize effective graph design as a trade-off between efficiency and learning difficulties in order to provide Information Visualization (InfoVis) researchers and practitioners with a framework for organizing explorations of graphs for which comprehension and recall are crucial. We identify implications of this view for the design and evaluation of information visualizations.",
                "ieee_keywords": [
                    "Psychology",
                    "Data visualization",
                    "Time factors",
                    "Cognition"
                ],
                "author_keywords": [
                    "Desirable difficulites",
                    "cognitive efficiency",
                    "active processing",
                    "engagement",
                    "individual differences.",
                    "MeSH Terms",
                    "Cognition",
                    "Comprehension",
                    "Computer Graphics",
                    "Humans",
                    "Learning",
                    "Models, Psychological",
                    "User-Computer Interface",
                    "Visual Perception"
                ]
            },
            {
                "title": "Learning Objectives, Insights, and Assessments: How Specification Formats Impact Design",
                "link": "https://ieeexplore.ieee.org/document/9552876/",
                "date_of_publication": null,
                "doi": "10.1109/TVCG.2021.3114811",
                "citations": "1",
                "abstract": "Despite the ubiquity of communicative visualizations, specifying communicative intent during design is ad hoc. Whether we are selecting from a set of visualizations, commissioning someone to produce them, or creating them ourselves, an effective way of specifying intent can help guide this process. Ideally, we would have a concise and shared specification language. In previous work, we have argued that communicative intents can be viewed as a learning/assessment problem (i.e., what should the reader learn and what test should they do well on). Learning-based specification formats are linked (e.g., assessments are derived from objectives) but some may more effectively specify communicative intent. Through a large-scale experiment, we studied three specification types: learning objectives, insights, and assessments. Participants, guided by one of these specifications, rated their preferences for a set of visualization designs. Then, we evaluated the set of visualization designs to assess which specification led participants to prefer the most effective visualizations. We find that while all specification types have benefits over no-specification, each format has its own advantages. Our results show that learning objective-based specifications helped participants the most in visualization selection. We also identify situations in which specifications may be insufficient and assessments are vital.",
                "ieee_keywords": [
                    "Visualization",
                    "Task analysis",
                    "Data visualization",
                    "Stakeholders",
                    "Usability",
                    "Taxonomy",
                    "Interviews"
                ],
                "author_keywords": [
                    "Communicative visualization",
                    "evaluation",
                    "visualization specification"
                ]
            },
            {
                "title": "A Deeper Understanding of Sequence in Narrative Visualization",
                "link": "https://ieeexplore.ieee.org/document/6634182/",
                "date_of_publication": null,
                "doi": "10.1109/TVCG.2013.119",
                "citations": "112",
                "abstract": "Conveying a narrative with visualizations often requires choosing an order in which to present visualizations. While evidence exists that narrative sequencing in traditional stories can affect comprehension and memory, little is known about how sequencing choices affect narrative visualization. We consider the forms and reactions to sequencing in narrative visualization presentations to provide a deeper understanding with a focus on linear, 'slideshow-style' presentations. We conduct a qualitative analysis of 42 professional narrative visualizations to gain empirical knowledge on the forms that structure and sequence take. Based on the results of this study we propose a graph-driven approach for automatically identifying effective sequences in a set of visualizations to be presented linearly. Our approach identifies possible transitions in a visualization set and prioritizes local (visualization-to-visualization) transitions based on an objective function that minimizes the cost of transitions from the audience perspective. We conduct two studies to validate this function. We also expand the approach with additional knowledge of user preferences for different types of local transitions and the effects of global sequencing strategies on memory, preference, and comprehension. Our results include a relative ranking of types of visualization transitions by the audience perspective and support for memory and subjective rating benefits of visualization sequences that use parallelism as a structural device. We discuss how these insights can guide the design of narrative visualization and systems that support optimization of visualization sequence.",
                "ieee_keywords": [
                    "Data visualization",
                    "Sequential analysis",
                    "Parallel processing",
                    "Encoding",
                    "Linear programming"
                ],
                "author_keywords": [
                    "Data visualization",
                    "Sequential analysis",
                    "Parallel processing",
                    "Encoding",
                    "Linear programming",
                    "narrative structure",
                    "Data storytelling",
                    "narrative visualization",
                    "MeSH Terms",
                    "Algorithms",
                    "Artificial Intelligence",
                    "Comprehension",
                    "Computer Graphics",
                    "Humans",
                    "Multimodal Imaging",
                    "Narration",
                    "Pattern Recognition, Visual",
                    "Reproducibility of Results",
                    "Sensitivity and Specificity",
                    "User-Computer Interface",
                    "Visual Perception"
                ]
            },
            {
                "title": "VizItCards: A Card-Based Toolkit for Infovis Design Education",
                "link": "https://ieeexplore.ieee.org/document/7539629/",
                "date_of_publication": null,
                "doi": "10.1109/TVCG.2016.2599338",
                "citations": "21",
                "abstract": "Shifts in information visualization practice are forcing a reconsideration of how infovis is taught. Traditional curricula that focused on conveying research-derived knowledge are slowly integrating design thinking as a key learning objective. In part, this is motivated by the realization that infovis is a wicked design problem, requiring a different kind of design work. In this paper we describe, VizItCards, a card-driven workshop developed for our graduate infovis class. The workshop is intended to provide practice with good design techniques and to simultaneously reinforce key concepts. VizItCards relies on principles of collaborative-learning and research on parallel design to generate positive collaborations and high-quality designs. From our experience of simulating a realistic design scenario in a classroom setting, we find that our students were able to meet key learning objectives and their design performance improved during the class. We describe variants of the workshop, discussing which techniques we think match to which learning goals.",
                "ieee_keywords": [
                    "Conferences",
                    "Collaboration",
                    "Visualization",
                    "Education",
                    "Data visualization",
                    "Human computer interaction",
                    "Standards"
                ],
                "author_keywords": [
                    "information visualization education",
                    "peer learning",
                    "toolkit",
                    "card",
                    "design workshop"
                ]
            },
            {
                "title": "Communicative Visualizations as a Learning Problem",
                "link": "https://ieeexplore.ieee.org/document/9222102/",
                "date_of_publication": null,
                "doi": "10.1109/TVCG.2020.3030375",
                "citations": "12",
                "abstract": "Significant research has provided robust task and evaluation languages for the analysis of exploratory visualizations. Unfortunately, these taxonomies fail when applied to communicative visualizations. Instead, designers often resort to evaluating communicative visualizations from the cognitive efficiency perspective: “can the recipient accurately decode my message/insight?” However, designers are unlikely to be satisfied if the message went `in one ear and out the other.' The consequence of this inconsistency is that it is difficult to design or select between competing options in a principled way. The problem we address is the fundamental mismatch between how designers want to describe their intent, and the language they have. We argue that visualization designers can address this limitation through a learning lens: that the recipient is a student and the designer a teacher. By using learning objectives, designers can better define, assess, and compare communicative visualizations. We illustrate how the learning-based approach provides a framework for understanding a wide array of communicative goals. To understand how the framework can be applied (and its limitations), we surveyed and interviewed members of the Data Visualization Society using their own visualizations as a probe. Through this study we identified the broad range of objectives in communicative visualizations and the prevalence of certain objective types.",
                "ieee_keywords": [
                    "Data visualization",
                    "Taxonomy",
                    "Task analysis",
                    "Visualization",
                    "Elbow",
                    "Tools",
                    "Correlation"
                ],
                "author_keywords": [
                    "Learning objectives",
                    "communicative visualization",
                    "visualization design"
                ]
            },
            {
                "title": "SmartCues: A Multitouch Query Approach for Details-on-Demand through Dynamically Computed Overlays",
                "link": "https://ieeexplore.ieee.org/document/8440833/",
                "date_of_publication": null,
                "doi": "10.1109/TVCG.2018.2865231",
                "citations": "3",
                "abstract": "Details-on-demand is a crucial feature in the visual information-seeking process but is often only implemented in highly constrained settings. The most common solution, hover queries (i.e., tooltips), are fast and expressive but are usually limited to single mark (e.g., a bar in a bar chart). `Queries' to retrieve details for more complex sets of objects (e.g., comparisons between pairs of elements, averages across multiple items, trend lines, etc.) are difficult for end-users to invoke explicitly. Further, the output of these queries require complex annotations and overlays which need to be displayed and dismissed on demand to avoid clutter. In this work we introduce SmartCues, a library to support details-on-demand through dynamically computed overlays. For end-users, SmartCues provides multitouch interactions to construct complex queries for a variety of details. For designers, SmartCues offers an interaction library that can be used out-of-the-box, and can be extended for new charts and detail types. We demonstrate how SmartCues can be implemented across a wide array of visualization types and, through a lab study, show that end users can effectively use SmartCues.",
                "ieee_keywords": [
                    "Bars",
                    "US Department of Defense",
                    "Task analysis",
                    "Visualization",
                    "Data visualization",
                    "Libraries",
                    "Color"
                ],
                "author_keywords": [
                    "Graphical overlays",
                    "details-on-demand",
                    "graph comprehension"
                ]
            },
            {
                "title": "Affective Learning Objectives for Communicative Visualizations",
                "link": "https://ieeexplore.ieee.org/document/9905872/",
                "date_of_publication": null,
                "doi": "10.1109/TVCG.2022.3209500",
                "citations": "729",
                "abstract": "When designing communicative visualizations, we often focus on goals that seek to convey patterns, relations, or comparisons (cognitive learning objectives). We pay less attention to affective intents–those that seek to influence or leverage the audience's opinions, attitudes, or values in some way. Affective objectives may range in outcomes from making the viewer care about the subject, strengthening a stance on an opinion, or leading them to take further action. Because such goals are often considered a violation of perceived ‘neutrality’ or are ‘political,’ designers may resist or be unable to describe these intents, let alone formalize them as learning objectives. While there are notable exceptions–such as advocacy visualizations or persuasive cartography–we find that visualization designers rarely acknowledge or formalize affective objectives. Through interviews with visualization designers, we expand on prior work on using learning objectives as a framework for describing and assessing communicative intent. Specifically, we extend and revise the framework to include a set of affective learning objectives. This structured taxonomy can help designers identify and declare their goals and compare and assess designs in a more principled way. Additionally, the taxonomy can enable external critique and analysis of visualizations. We illustrate the use of the taxonomy with a critical analysis of an affective visualization.",
                "ieee_keywords": [
                    "Data visualization",
                    "Taxonomy",
                    "Appraisal",
                    "Interviews",
                    "Visualization",
                    "Journalism",
                    "Image color analysis"
                ],
                "author_keywords": [
                    "Affective visualization",
                    "communicative visualization",
                    "learning objectives",
                    "MeSH Terms",
                    "Computer Graphics",
                    "Communication",
                    "Learning"
                ]
            },
            {
                "title": "Roboviz: A Game-Centered Project for Information Visualization Education",
                "link": "https://ieeexplore.ieee.org/document/9905748/",
                "date_of_publication": null,
                "doi": "10.1109/TVCG.2022.3209402",
                "citations": "272",
                "abstract": "Due to their pedagogical advantages, large final projects in information visualization courses have become standard practice. Students take on a client–real or simulated–a dataset, and a vague set of goals to create a complete visualization or visual analytics product. Unfortunately, many projects suffer from ambiguous goals, over or under-constrained client expectations, and data constraints that have students spending their time on non-visualization problems (e.g., data cleaning). These are important skills, but are often secondary course objectives, and unforeseen problems can majorly hinder students. We created an alternative for our information visualization course: Roboviz, a real-time game for students to play by building a visualization-focused interface. By designing the game mechanics around four different data types, the project allows students to create a wide array of interactive visualizations. Student teams play against their classmates with the objective to collect the most (good) robots. The flexibility of the strategies encourages variability, a range of approaches, and solving wicked design constraints. We describe the construction of this game and report on student projects over two years. We further show how the game mechanics can be extended or adapted to other game-based projects.",
                "ieee_keywords": [
                    "Data visualization",
                    "Games",
                    "Task analysis",
                    "Robots",
                    "Encoding",
                    "Conferences",
                    "Buildings"
                ],
                "author_keywords": [
                    "pedagogy",
                    "final project",
                    "game interfaces"
                ]
            },
            {
                "title": "Visualizing Uncertainty in Probabilistic Graphs with Network Hypothetical Outcome Plots (NetHOPs)",
                "link": "https://ieeexplore.ieee.org/document/9552465/",
                "date_of_publication": null,
                "doi": "10.1109/TVCG.2021.3114679",
                "citations": "395",
                "abstract": "Probabilistic graphs are challenging to visualize using the traditional node-link diagram. Encoding edge probability using visual variables like width or fuzziness makes it difficult for users of static network visualizations to estimate network statistics like densities, isolates, path lengths, or clustering under uncertainty. We introduce Network Hypothetical Outcome Plots (NetHOPs), a visualization technique that animates a sequence of network realizations sampled from a network distribution defined by probabilistic edges. NetHOPs employ an aggregation and anchoring algorithm used in dynamic and longitudinal graph drawing to parameterize layout stability for uncertainty estimation. We present a community matching algorithm to enable visualizing the uncertainty of cluster membership and community occurrence. We describe the results of a study in which 51 network experts used NetHOPs to complete a set of common visual analysis tasks and reported how they perceived network structures and properties subject to uncertainty. Participants' estimates fell, on average, within 11% of the ground truth statistics, suggesting NetHOPs can be a reasonable approach for enabling network analysts to reason about multiple properties under uncertainty. Participants appeared to articulate the distribution of network statistics slightly more accurately when they could manipulate the layout anchoring and the animation speed. Based on these findings, we synthesize design recommendations for developing and using animated visualizations for probabilistic networks.",
                "ieee_keywords": [
                    "Probabilistic logic",
                    "Visualization",
                    "Uncertainty",
                    "Layout",
                    "Task analysis",
                    "Stability analysis",
                    "Encoding"
                ],
                "author_keywords": [
                    "Network",
                    "Uncertainty",
                    "Application"
                ]
            }
        ]
    },
    {
        "name": "Qiaozhu Mei",
        "publications": [
            {
                "title": "Re-ranking Biomedical Literature for Precision Medicine with Pre-trained Neural Models",
                "link": "https://ieeexplore.ieee.org/document/9374401/",
                "date_of_publication": "12 March 2021",
                "doi": "10.1109/ICHI48887.2020.9374401",
                "citations": "95",
                "abstract": "We propose a biomedical literature retrieval approach that incorporates a domain-specific BERT model as an auxiliary re-ranker. Experiments on TREC Precision Medicine dataset show its effectiveness in improving retrieval performance by 6.2% in inferred NDCG and 6.8% in R-precision over the best-published results. The contribution of this study is to provide evidence of incorporating BERT in a biomedical literature retrieval system, which serves the overall goal to improve the information retrieval for precision medicine.",
                "ieee_keywords": [
                    "Precision medicine",
                    "Biological system modeling",
                    "Conferences",
                    "Bit error rate",
                    "Medical services",
                    "Information retrieval",
                    "Informatics"
                ],
                "author_keywords": [
                    "biomedical retrieval",
                    "pretrained models",
                    "BERT"
                ]
            },
            {
                "title": "Numerical Age Variations within Clinical Notes: The Potential Impact on De-Identification and Information Extraction",
                "link": "https://ieeexplore.ieee.org/document/8411808/",
                "date_of_publication": "19 July 2018",
                "doi": "10.1109/ICHI-W.2018.00022",
                "citations": "91",
                "abstract": "Many kinds of numbers and numerical concepts appear frequently in free text clinical notes from electronic health records, including patient ages. The variability in how ages are described may impact the success of information extraction strategies as well as the accuracy of de-identification systems. This brief paper describes an analysis of the variation in how numbers and numerical concepts are represented in clinical notes with respect to ages. We used an inverted index of approximately 100 million notes to obtain the frequency of various permutations of ages, including biologically implausible ages as well as age descriptions that might not be detected by many de-identification systems. Missing such rare, but nevertheless present, variations could result in missed information or even privacy violations.",
                "ieee_keywords": [
                    "Information retrieval",
                    "Biology",
                    "Electronic medical records",
                    "Data mining",
                    "Task analysis",
                    "Indexes",
                    "Clinical trials"
                ],
                "author_keywords": [
                    "lexical variation, natural language processing, information retrieval, de-identification"
                ]
            },
            {
                "title": "When BERT Meets Bilbo: A Learning Curve Analysis of Pretrained Language Model on Disease Classification",
                "link": "https://ieeexplore.ieee.org/document/9374339/",
                "date_of_publication": "12 March 2021",
                "doi": "10.1109/ICHI48887.2020.9374339",
                "citations": "1",
                "abstract": "Natural language processing tasks in the health domain often deal with limited amount of labeled data. Pre-trained language models show us a promising way to compensate for the lake of training data, such as Bidirectional Encoder Representations from Transformers (BERT). However, previous downstream tasks often used training data at such a large scale that is unlikely to obtain in health domain. In this work, We conducted a learning curve analysis on a disease classification task to study the behavior of BERT and baseline models can still benefit downstream tasks when training data are relatively small in the context of health NLP.",
                "ieee_keywords": [
                    "Training",
                    "Analytical models",
                    "Bit error rate",
                    "Training data",
                    "Data models",
                    "Natural language processing",
                    "Task analysis"
                ],
                "author_keywords": [
                    "learning curve",
                    "bidirectional encoder representations from transformers",
                    "disease classification"
                ]
            },
            {
                "title": "Mining Usage Data from Large-Scale Android Users: Challenges and Opportunities",
                "link": "https://ieeexplore.ieee.org/document/7833008/",
                "date_of_publication": "26 January 2017",
                "doi": "10.1145/2897073.2897721",
                "citations": "2",
                "abstract": "Mining usage data from a large number of Android users can assist various software engineering tasks. In collaboration with Wandoujia, a leading Android app marketplace in China, we have conducted a large empirical analysis based on mining app usage behaviors collected from millions of Android users. Our empirical findings can provide implications, challenges, and opportunities to app-centric software development, deployment, and maintenance.",
                "ieee_keywords": [
                    "Androids",
                    "Humanoid robots",
                    "Software engineering",
                    "Predictive models",
                    "Data mining",
                    "Data models",
                    "Measurement"
                ],
                "author_keywords": [
                    "Mobile apps",
                    "user behavior analysis"
                ]
            },
            {
                "title": "Named Entity Recognition from Table Headers in Randomized Controlled Trial Articles",
                "link": "https://ieeexplore.ieee.org/document/9374323/",
                "date_of_publication": "12 March 2021",
                "doi": "10.1109/ICHI48887.2020.9374323",
                "citations": "135",
                "abstract": "Tables in biomedical articles often contain important information of research findings. However, they are often not available for direct uses by downstream computational applications due to its unstructured nature, with both structural and semantic complexity. In this study, we developed a deep learning-based approach that takes contextual information into consideration to recognize biomedical entities in tables headers in Randomized Controlled Trial (RCT) articles, using a manually annotated corpus. Our evaluation shows that it achieved good performance with an F1 score of 92.60% for entity recognition in headers. We believe the proposed approach for table information extraction, as well as the developed annotated corpus, would be great resources for biomedical text mining, thus facilitating other biomedical research and applications.",
                "ieee_keywords": [
                    "Text mining",
                    "Conferences",
                    "Semantics",
                    "Process control",
                    "Medical services",
                    "Information retrieval",
                    "Informatics"
                ],
                "author_keywords": [
                    "information extraction",
                    "named entity recognition",
                    "natural language processing",
                    "recognition of table",
                    "deep learning"
                ]
            },
            {
                "title": "Improving Rare Disease Classification Using Imperfect Knowledge Graph",
                "link": "https://ieeexplore.ieee.org/document/8904588/",
                "date_of_publication": "21 November 2019",
                "doi": "10.1109/ICHI.2019.8904588",
                "citations": "1",
                "abstract": "Accurately recognizing rare diseases based on symptom description is a critical task. The lack of historical data for rare diseases poses a great challenge to machine learning-based approaches. In this study, we develop a text classification algorithm that represents a document as a combination of a “bag of words” and a “bag of knowledge terms where a “knowledge term” is a term shared between the document and the knowledge graph relevant to the disease classification task.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Understanding Diverse Usage Patterns from Large-Scale Appstore-Service Profiles",
                "link": "https://ieeexplore.ieee.org/document/7883939/",
                "date_of_publication": null,
                "doi": "10.1109/TSE.2017.2685387",
                "citations": "30",
                "abstract": "The prevalence of smart mobile devices has promoted the popularity of mobile applications (a.k.a. apps). Supporting mobility has become a promising trend in software engineering research. This article presents an empirical study of behavioral service profiles collected from millions of users whose devices are deployed with Wandoujia, a leading Android app-store service in China. The dataset of Wandoujia service profiles consists of two kinds of user behavioral data from using 0.28 million free Android apps, including (1) app management activities (i.e., downloading, updating, and uninstalling apps) from over 17 million unique users and (2) app network usage from over 6 million unique users. We explore multiple aspects of such behavioral data and present patterns of app usage. Based on the findings as well as derived knowledge, we also suggest some new open opportunities and challenges that can be explored by the research community, including app development, deployment, delivery, revenue, etc.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Systematic Analysis of Fine-Grained Mobility Prediction With On-Device Contextual Data",
                "link": "https://ieeexplore.ieee.org/document/9165195/",
                "date_of_publication": null,
                "doi": "10.1109/TMC.2020.3015921",
                "citations": "2",
                "abstract": "User mobility prediction is widely considered by the research community. Many studies have explored various algorithms to predict where a user is likely to visit based on their contexts and trajectories. Most of existing studies focus on specific targets of predictions. While successful cases are often reported, few discussions have been done on what happens if the prediction targets vary: whether coarser locations are easier to be predicted, and whether predicting the immediate next location on the trajectory is easier than predicting the destination. On the other hand, while spatiotemporal tags and content information are commonly used in current prediction tasks, few have utilized the finer grained, on-device user behavioral data, which are supposed to be more informative and indicative of user intentions. In this paper, we conduct a systematic study on the mobility prediction using a large-scale real-world dataset that contains plentiful contextual information. Based on a series of learning models, including a Markov model, two recurrent neural network models, and a multi-modal learning method, we perform extensive experiments to comprehensively investigate the predictability of different types of granularities of targets and the effectiveness of different types of signals. The results provide insightful knowledge on what can be predicted along with how, which sheds light on the real-world mobility prediction from a relatively general perspective.",
                "ieee_keywords": [
                    "Predictive models",
                    "Mobile computing",
                    "Trajectory",
                    "Task analysis",
                    "Semantics",
                    "Systematics",
                    "Smart phones"
                ],
                "author_keywords": [
                    "Mobility prediction",
                    "user behavior analysis",
                    "multi-modal learning"
                ]
            },
            {
                "title": "Edge Weight Regularization over Multiple Graphs for Similarity Learning",
                "link": "https://ieeexplore.ieee.org/document/5693991/",
                "date_of_publication": "20 January 2011",
                "doi": "10.1109/ICDM.2010.156",
                "citations": "13",
                "abstract": "The growth of the web has directly influenced the increase in the availability of relational data. One of the key problems in mining such data is computing the similarity between objects with heterogeneous feature types. For example, publications have many heterogeneous features like text, citations, authorship information, venue information, etc. In most approaches, similarity is estimated using each feature type in isolation and then combined in a linear fashion. However, this approach does not take advantage of the dependencies between the different feature spaces. In this paper, we propose a novel approach to combine the different sources of similarity using a regularization framework over edges in multiple graphs. We show that the objective function induced by the framework is convex. We also propose an efficient algorithm using coordinate descent to solve the optimization problem. We extrinsically evaluate the performance of the proposed unified similarity measure on two different tasks, clustering and classification. The proposed similarity measure outperforms three baselines and a state-of-the-art classification algorithm on a variety of standard, large data sets.",
                "ieee_keywords": [
                    "Equations",
                    "Clustering algorithms",
                    "Context",
                    "Convex functions",
                    "Laplace equations",
                    "Semantics",
                    "Data mining"
                ],
                "author_keywords": [
                    "Machine Learning",
                    "Similarity Learning",
                    "Heterogeneous Features",
                    "Classification",
                    "Clustering"
                ]
            },
            {
                "title": "PRADA: Prioritizing Android Devices for Apps by Mining Large-Scale Usage Data",
                "link": "https://ieeexplore.ieee.org/document/7886887/",
                "date_of_publication": "03 April 2017",
                "doi": "10.1145/2884781.2884828",
                "citations": "19",
                "abstract": "Selecting and prioritizing major device models are critical for mobile app developers to select testbeds and optimize resources such as marketing and quality-assurance resources. The heavily fragmented distribution of Android devices makes it challenging to select a few major device models out of thousands of models available on the market. Currently app developers usually rely on some reported or estimated general market share of device models. However, these estimates can be quite inaccurate, and more problematically, can be irrelevant to the particular app under consideration. To address this issue, we propose PRADA, the first approach to prioritizing Android device models for individual apps, based on mining large-scale usage data. PRADA adapts the concept of operational profiling (popularly used in software reliability engineering) for mobile apps - the usage of an app on a specific device model reflects the importance of that device model for the app. PRADA includes a collaborative filtering technique to predict the usage of an app on different device models, even if the app is entirely new (without its actual usage in the market yet), based on the usage data of a large collection of apps. We empirically demonstrate the effectiveness of PRADA over two popular app categories, i.e., Game and Media, covering over 3.86 million users and 14,000 device models collected through a leading Android management app in China.",
                "ieee_keywords": [
                    "Androids",
                    "Humanoid robots",
                    "Data models",
                    "Games",
                    "Testing",
                    "Data mining",
                    "Mobile communication"
                ],
                "author_keywords": [
                    "Mobile apps",
                    "Android fragmentation",
                    "prioritization",
                    "usage data"
                ]
            }
        ]
    },
    {
        "name": "J. Alex Halderman",
        "publications": [
            {
                "title": "Can Voters Detect Malicious Manipulation of Ballot Marking Devices?",
                "link": "https://ieeexplore.ieee.org/document/9152705/",
                "date_of_publication": "30 July 2020",
                "doi": "10.1109/SP40000.2020.00118",
                "citations": "11",
                "abstract": "Ballot marking devices (BMDs) allow voters to select candidates on a computer kiosk, which prints a paper ballot that the voter can review before inserting it into a scanner to be tabulated. Unlike paperless voting machines, BMDs provide voters an opportunity to verify an auditable physical record of their choices, and a growing number of U.S. jurisdictions are adopting them for all voters. However, the security of BMDs depends on how reliably voters notice and correct any adversarially induced errors on their printed ballots. In order to measure voters' error detection abilities, we conducted a large study (N = 241) in a realistic polling place setting using real voting machines that we modified to introduce an error into each printout. Without intervention, only 40% of participants reviewed their printed ballots at all, and only 6.6% told a poll worker something was wrong. We also find that carefully designed interventions can improve verification performance. Verbally instructing voters to review the printouts and providing a written slate of candidates for whom to vote both significantly increased review and reporting rates-although the improvements may not be large enough to provide strong security in close elections, especially when BMDs are used by all voters. Based on these findings, we make several evidence-based recommendations to help better defend BMD-based elections.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "To Strengthen Security, Change Developers' Incentives",
                "link": "https://ieeexplore.ieee.org/document/5439535/",
                "date_of_publication": null,
                "doi": "10.1109/MSP.2010.85",
                "citations": "5",
                "abstract": "Many of the most common software vulnerabilities, such as buffer overflows, cross-site scripting, and misapplications of cryptography, are wholly avoidable if software makers apply an appropriate level of training, testing, and care.Yet developers today have the \"wrong\" incentives, often leading them to underinvest in security or even to directly harm it. If we can understand these incentives and their causes, we might be able to reshape them and radically improve security.Software makers have shown a dramatic ability to strengthen their products' security given sufficient motivation.The most famous example is Microsoft's transformation over the past decade from a security laughingstock to a leader. In 2002, stung by several widely publicized vulnerabilities across its product line, the company began a major security initiative that produced lasting changes in its priorities, processes, and culture. Gone were the days of \"creating designs and code that emphasize features over security.\" Yet changes like these are exceptional. Microsoft's shift was motivated by an intense level of scrutiny and withering global publicity that few firms experience, and it had the unusual luxury of responding with vast engineering resources paid for by monopoly rents. Most developers face far weaker security incentives.",
                "ieee_keywords": [
                    "Security",
                    "Buffer overflow",
                    "Cryptography",
                    "Software testing",
                    "Monopoly"
                ],
                "author_keywords": [
                    "security economics",
                    "developers' incentives",
                    "transparency",
                    "liability",
                    "security and privacy"
                ]
            },
            {
                "title": "An Internet-wide view of ICS devices",
                "link": "https://ieeexplore.ieee.org/document/7906943/",
                "date_of_publication": "24 April 2017",
                "doi": "10.1109/PST.2016.7906943",
                "citations": "54",
                "abstract": "Industrial control systems have become ubiquitous, enabling the remote, electronic control of physical equipment and sensors. Originally designed to operate on closed networks, the protocols used by these devices have no built-in security. However, despite this, an alarming number of systems are connected to the public Internet and an attacker who finds a device often can cause catastrophic damage to physical infrastructure. We consider two aspects of ICS security in this work: (1) what devices have been inadvertently exposed on the public Internet, and (2) who is searching for vulnerable systems. First, we implement five common SCADA protocols in ZMap and conduct a survey of the public IPv4 address space finding more than 60K publicly accessible systems. Second, we use a large network telescope and high-interaction honeypots to find and profile actors searching for devices. We hope that our findings can both motivate and inform future work on securing industrial control systems.",
                "ieee_keywords": [
                    "Protocols",
                    "Security",
                    "Industrial control",
                    "IEC Standards",
                    "Integrated circuits",
                    "Companies"
                ],
                "author_keywords": []
            },
            {
                "title": "Tracking Certificate Misissuance in the Wild",
                "link": "https://ieeexplore.ieee.org/document/8418638/",
                "date_of_publication": "26 July 2018",
                "doi": "10.1109/SP.2018.00015",
                "citations": "34",
                "abstract": "Certificate Authorities (CAs) regularly make mechanical errors when issuing certificates. To quantify these errors, we introduce ZLint, a certificate linter that codifies the policies set forth by the CA/Browser Forum Baseline Requirements and RFC 5280 that can be tested in isolation. We run ZLint on browser-trusted certificates in Censys and systematically analyze how well CAs construct certificates. We find that the number errors has drastically reduced since 2012. In 2017, only 0.02% of certificates have errors. However, this is largely due to a handful of large authorities that consistently issue correct certificates. There remains a long tail of small authorities that regularly issue non-conformant certificates. We further find that issuing certificates with errors is correlated with other types of mismanagement and for large authorities, browser action. Drawing on our analysis, we conclude with a discussion on how the community can best use lint data to identify authorities with worrisome organizational practices and ensure long-term health of the Web PKI.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "FTP: The Forgotten Cloud",
                "link": "https://ieeexplore.ieee.org/document/7579767/",
                "date_of_publication": "03 October 2016",
                "doi": "10.1109/DSN.2016.52",
                "citations": "13",
                "abstract": "Once pervasive, the File Transfer Protocol (FTP) has been largely supplanted by HTTP, SCP, and BitTorrent for transferring data between hosts. Yet, in a comprehensive analysis of the FTP ecosystem as of 2015, we find that there are still more than 13~million FTP servers in the IPv4 address space, 1.1~million of which allow \"anonymous\" (public) access. These anonymous FTP servers leak sensitive information, such as tax documents and cryptographic secrets. More than 20,000 FTP servers allow public write access, which has facilitated malicious actors' use of free storage as well as malware deployment and click-fraud attacks. We further investigate real-world attacks by deploying eight FTP honeypots, shedding light on how attackers are abusing and exploiting vulnerable servers. We conclude with lessons and recommendations for securing FTP.",
                "ieee_keywords": [
                    "Servers",
                    "Protocols",
                    "Security",
                    "Ports (Computers)",
                    "Ecosystems",
                    "Robots",
                    "Malware"
                ],
                "author_keywords": []
            },
            {
                "title": "A security analysis of police computer systems",
                "link": "https://ieeexplore.ieee.org/document/7907023/",
                "date_of_publication": "24 April 2017",
                "doi": "10.1109/PST.2016.7907023",
                "citations": "154",
                "abstract": "Every day we entrust our privacy, safety, and security to police officers sworn to protect and serve. While many critical infrastructure computer systems have been well studied, the computer infrastructure supporting these officers remains to be surveyed in public literature. We remedy this by characterizing a sample police department's systems through a security lens, discussing weak points and areas for future work. Pen-testing of the security camera and web application systems were performed to give hard data points on the security of this department's systems. Our characterization shows that the security of the department we study is good overall, but enough weaknesses exist in this department and others to be concerned for the state of police security. We determine that compliance with FBI security policies is an appropriate first step for departments, but it is not sufficient. More research and increased support for security education and resource services is needed in order to defend these critical systems against adapting adversaries.",
                "ieee_keywords": [
                    "Security",
                    "Law enforcement",
                    "Local area networks",
                    "Computers",
                    "Cloud computing",
                    "Virtual private networks",
                    "Organizations"
                ],
                "author_keywords": []
            },
            {
                "title": "Implementing Attestable kiosks",
                "link": "https://ieeexplore.ieee.org/document/7906989/",
                "date_of_publication": "24 April 2017",
                "doi": "10.1109/PST.2016.7906989",
                "citations": "1",
                "abstract": "In this paper we explore the notion of a secure kiosk, a trusted computing platform built using off-the-shelf components. We demonstrate how kiosks serve as convenient primitives when designing secure computing protocols, as they allow for a very prescribed set of assumptions to be made about a system. We begin by defining the necessary properties of a kiosk, and then explain how each of these properties can (or cannot) be attained using current off-the-shelf hardware and software components. We construct a proof-of-concept implementation using TPM hardware and Windows 10. We also provide ASKVote, the Attestable and Secure Voting protocol to demonstrate the flexibility gained from the use of kiosks in a larger secure system.",
                "ieee_keywords": [
                    "Hardware",
                    "Protocols",
                    "Security",
                    "Registers",
                    "Computers"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Yejin Choi",
        "publications": [
            {
                "title": "Tactical Rewind: Self-Correction via Backtracking in Vision-And-Language Navigation",
                "link": "https://ieeexplore.ieee.org/document/8953538/",
                "date_of_publication": "09 January 2020",
                "doi": "10.1109/CVPR.2019.00690",
                "citations": "58",
                "abstract": "We present the Frontier Aware Search with backTracking (FAST) Navigator, a general framework for action decoding, that achieves state-of-the-art results on the 2018 Room-to-Room (R2R) Vision-and-Language navigation challenge. Given a natural language instruction and photo-realistic image views of a previously unseen environment, the agent was tasked with navigating from source to target location as quickly as possible. While all current approaches make local action decisions or score entire trajectories using beam search, ours balances local and global signals when exploring an unobserved environment. Importantly, this lets us act greedily but use global signals to backtrack when necessary. Applying FAST framework to existing state-of-the-art models achieved a 17% relative gain, an absolute 6% gain on Success rate weighted by Path Length.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Segment-Phrase Table for Semantic Segmentation, Visual Entailment and Paraphrasing",
                "link": "https://ieeexplore.ieee.org/document/7410367/",
                "date_of_publication": "18 February 2016",
                "doi": "10.1109/ICCV.2015.10",
                "citations": "9",
                "abstract": "We introduce Segment-Phrase Table (SPT), a large collection of bijective associations between textual phrases and their corresponding segmentations. Leveraging recent progress in object recognition and natural language semantics, we show how we can successfully build a high-quality segment-phrase table using minimal human supervision. More importantly, we demonstrate the unique value unleashed by this rich bimodal resource, for both vision as well as natural language understanding. First, we show that fine-grained textual labels facilitate contextual reasoning that helps in satisfying semantic constraints across image segments. This feature enables us to achieve state-of-the-art segmentation results on benchmark datasets. Next, we show that the association of high-quality segmentations to textual phrases aids in richer semantic understanding and reasoning of these textual phrases. Leveraging this feature, we motivate the problem of visual entailment and visual paraphrasing, and demonstrate its utility on a large dataset.",
                "ieee_keywords": [
                    "Image segmentation",
                    "Semantics",
                    "Visualization",
                    "Cognition",
                    "Pragmatics",
                    "Buildings",
                    "Training"
                ],
                "author_keywords": []
            },
            {
                "title": "EARLY FUSION for Goal Directed Robotic Vision",
                "link": "https://ieeexplore.ieee.org/document/8968165/",
                "date_of_publication": "28 January 2020",
                "doi": "10.1109/IROS40897.2019.8968165",
                "citations": "3",
                "abstract": "Building perceptual systems for robotics which perform well under tight computational budgets requires novel architectures which rethink the traditional computer vision pipeline. Modern vision architectures require the agent to build a summary representation of the entire scene, even if most of the input is irrelevant to the agent's current goal. In this work, we flip this paradigm, by introducing EARLYFUSION vision models that condition on a goal to build custom representations for downstream tasks. We show that these goal specific representations can be learned more quickly, are substantially more parameter efficient, and more robust than existing attention mechanisms in our domain. We demonstrate the effectiveness of these methods on a simulated item retrieval problem that is trained in a fully end-to-end manner via imitation learning.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "From Recognition to Cognition: Visual Commonsense Reasoning",
                "link": "https://ieeexplore.ieee.org/document/8953217/",
                "date_of_publication": "09 January 2020",
                "doi": "10.1109/CVPR.2019.00688",
                "citations": "200",
                "abstract": "Visual understanding goes well beyond object recognition. With one glance at an image, we can effortlessly imagine the world beyond the pixels: for instance, we can infer people's actions, goals, and mental states. While this task is easy for humans, it is tremendously difficult for today's vision systems, requiring higher-order cognition and commonsense reasoning about the world. We formalize this task as Visual Commonsense Reasoning. Given a challenging question about an image, a machine must answer correctly and then provide a rationale justifying its answer. Next, we introduce a new dataset, VCR, consisting of 290k multiple choice QA problems derived from 110k movie scenes. The key recipe for generating non-trivial and high-quality problems at scale is Adversarial Matching, a new approach to transform rich annotations into multiple choice questions with minimal bias. Experimental results show that while humans find VCR easy (over 90% accuracy), state-of-the-art vision models struggle (~45%). To move towards cognition-level understanding, we present a new reasoning engine, Recognition to Cognition Networks (R2C), that models the necessary layered inferences for grounding, contextualization, and reasoning. R2C helps narrow the gap between humans and machines (~65%); still, the challenge is far from solved, and we provide analysis that suggests avenues for future work.",
                "ieee_keywords": [
                    "Visualization",
                    "Grounding",
                    "Transforms",
                    "Motion pictures",
                    "Pattern recognition",
                    "Object recognition",
                    "Task analysis"
                ],
                "author_keywords": [
                    "Vision + Language",
                    "Recognition: Detection",
                    "Categorization",
                    "Retrieval",
                    "Scene Analysis and Understanding",
                    "Visual Reasonin"
                ]
            },
            {
                "title": "Fusing Pre-Trained Language Models with Multimodal Prompts through Reinforcement Learning",
                "link": "https://ieeexplore.ieee.org/document/10204166/",
                "date_of_publication": "22 August 2023",
                "doi": "10.1109/CVPR52729.2023.01044",
                "citations": "4",
                "abstract": "Language models are capable of commonsense reasoning: while domain-specific models can learn from explicit knowledge (e.g. commonsense graphs [6] ethical norms [25]), and larger models like GPT-3 [7] mani-fest broad commonsense reasoning capacity. Can their knowledge be extended to multimodal inputs such as images and audio without paired domain data? In this work, we propose ‡ ESPER (Extending Sensory PErception with Reinforcement learning) which enables text-only pretrained models to address multimodal tasks such as visual commonsense reasoning. Our key novelty is to use rein-forcement learning to align multimodal inputs to language model generations without direct supervision: for example, our reward optimization relies only on cosine similarity derived from CLIP [52] and requires no additional paired (image, text) data. Experiments demonstrate that ESPER outperforms baselines and prior work on a variety of multimodal text generation tasks ranging from captioning to commonsense reasoning; these include a new benchmark we collect and release, the ESP dataset, which tasks models with generating the text of several different domains for each image. Our code and data are publicly released at https://github.com/JiwanChung/esper.",
                "ieee_keywords": [
                    "Training",
                    "Visualization",
                    "Ethics",
                    "Reinforcement learning",
                    "Generators",
                    "Distance measurement",
                    "Pattern recognition"
                ],
                "author_keywords": [
                    "Vision",
                    "language",
                    "and reasoning"
                ]
            },
            {
                "title": "AI's 10 to Watch",
                "link": "https://ieeexplore.ieee.org/document/7389912/",
                "date_of_publication": null,
                "doi": "10.1109/MIS.2016.7",
                "citations": "2164",
                "abstract": "IEEE Intelligent Systems once again selected 10 young AI scientists as \" AI's 10 to Watch.\" This acknowledgment and celebration not only recognizes these young scientists and makes a positive impact in their academic career but also promotes the community and cutting-edge AI research among next-generation AI researchers, the industry, and the general public alike. The contributions are \"Collective Decision Making in Multi-Agent Systems,\" by Haris Aziz, \"From Causal Inference and Data Fusion to an Automated Scientist,\" by Elias Bareinboim, \"Language, Vision, and Social AI,\" by Yejin Choi, \"Algorithms for Machine Learning,\" by Daniel Hsu, \"Learning Agents,\" by Shivaram Kalyanakrishnan, \"Strategy and Bounded Rationality,\" by Reshef Meir, \";A Reasoning Engine for Tailoring Healthcare to the Individual,\" by Suchi Saria, \"Pushing the Limits of Knowledge Representation and Reasoning,\" by Gerardo I. Simari, \"Better Group Decision Making,\" by Lirong Xia, and \"Distributed Constraint Optimization,\" by William Yeoh.",
                "ieee_keywords": [
                    "Algorithm design and analysis",
                    "Computer science",
                    "Visualization",
                    "Machine learning algorithms",
                    "Resource management"
                ],
                "author_keywords": [
                    "artificial intelligence",
                    "AI 10 to watch",
                    "innovation",
                    "cutting edge",
                    "intelligent systems"
                ]
            },
            {
                "title": "Neural Motifs: Scene Graph Parsing with Global Context",
                "link": "https://ieeexplore.ieee.org/document/8578709/",
                "date_of_publication": "16 December 2018",
                "doi": "10.1109/CVPR.2018.00611",
                "citations": "408",
                "abstract": "We investigate the problem of producing structured graph representations of visual scenes. Our work analyzes the role of motifs: regularly appearing substructures in scene graphs. We present new quantitative insights on such repeated structures in the Visual Genome dataset. Our analysis shows that object labels are highly predictive of relation labels but not vice-versa. We also find that there are recurring patterns even in larger subgraphs: more than 50% of graphs contain motifs involving at least two relations. Our analysis motivates a new baseline: given object detections, predict the most frequent relation between object pairs with the given labels, as seen in the training set. This baseline improves on the previous state-of-the-art by an average of 3.6% relative improvement across evaluation settings. We then introduce Stacked Motif Networks, a new architecture designed to capture higher order motifs in scene graphs that further improves over our strong baseline by an average 7.1% relative gain. Our code is available at github.com/rowanz/neural-motifs.",
                "ieee_keywords": [
                    "Visualization",
                    "Head",
                    "Genomics",
                    "Bioinformatics",
                    "Semantics",
                    "Image edge detection",
                    "Wheels"
                ],
                "author_keywords": []
            },
            {
                "title": "MERLOT RESERVE: Neural Script Knowledge through Vision and Language and Sound",
                "link": "https://ieeexplore.ieee.org/document/9879062/",
                "date_of_publication": "27 September 2022",
                "doi": "10.1109/CVPR52688.2022.01589",
                "citations": "23",
                "abstract": "As humans, we navigate a multimodal world, building a holistic understanding from all our senses. We introduce @MERLOT RESERVE, a model that represents videos jointly over time - through a new training objective that learns from audio, subtitles, and video frames. Given a video, we replace snippets of text and audio with a MASK token; the model learns by choosing the correct masked-out snippet. Our objective learns faster than alternatives, and performs well at scale: we pretrain on 20 million YouTube videos. Empirical results show that @MERLOT RESERVE learns strong multimodal representations. When finetuned, it sets state-of-the-art on Visual Commonsense Reasoning (VCR), TVQA, and Kinetics-600; outperforming prior work by 5%, 7%, and 1.5% respectively. Ablations show that these tasks benefit from audio pretraining - even VCR, a QA task centered around images (without sound). Moreover, our objective enables out-of-the-box prediction, revealing strong multimodal commonsense understanding. In a fully zero-shot setting, our model obtains competitive results on four video tasks, even outperforming supervised approaches on the recently proposed Situated Reasoning (STAR) benchmark. We analyze why audio enables better vision-language representations, suggesting significant opportunities for future research. We conclude by discussing ethical and societal implications of multimodal pretraining.",
                "ieee_keywords": [
                    "Training",
                    "Representation learning",
                    "Visualization",
                    "Ethics",
                    "Video on demand",
                    "Navigation",
                    "Stars"
                ],
                "author_keywords": [
                    "Vision + language; Representation learning; Video analysis and understanding; Visual reasoning"
                ]
            },
            {
                "title": "VinVL: Revisiting Visual Representations in Vision-Language Models",
                "link": "https://ieeexplore.ieee.org/document/9577951/",
                "date_of_publication": "02 November 2021",
                "doi": "10.1109/CVPR46437.2021.00553",
                "citations": "141",
                "abstract": "This paper presents a detailed study of improving visual representations for vision language (VL) tasks and develops an improved object detection model to provide object-centric representations of images. Compared to the most widely used bottom-up and top-down model [2], the new model is bigger, better-designed for VL tasks, and pre-trained on much larger training corpora that combine multiple public annotated object detection datasets. Therefore, it can generate representations of a richer collection of visual objects and concepts. While previous VL research focuses mainly on improving the vision-language fusion model and leaves the object detection model improvement untouched, we show that visual features matter significantly in VL models. In our experiments we feed the visual features generated by the new object detection model into a Transformer-based VL fusion model OSCAR [20], and utilize an improved approach OSCAR+ to pre-train the VL model and fine-tune it on a wide range of downstream VL tasks. Our results show that the new visual features significantly improve the performance across all VL tasks, creating new state-of-the-art results on seven public benchmarks. Code, models and pre-extracted features are released at https://github.com/pzzhang/VinVL.",
                "ieee_keywords": [
                    "Training",
                    "Visualization",
                    "Computational modeling",
                    "Object detection",
                    "Benchmark testing",
                    "Feature extraction",
                    "Transformers"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Luke Zettlemoyer",
        "publications": [
            {
                "title": "“Can you give me another word for hyperbaric?”: Improving speech translation using targeted clarification questions",
                "link": "https://ieeexplore.ieee.org/document/6639302/",
                "date_of_publication": "21 October 2013",
                "doi": "10.1109/ICASSP.2013.6639302",
                "citations": "9",
                "abstract": "We present a novel approach for improving communication success between users of speech-to-speech translation systems by automatically detecting errors in the output of automatic speech recognition (ASR) and statistical machine translation (SMT) systems. Our approach initiates system-driven targeted clarification about errorful regions in user input and repairs them given user responses. Our system has been evaluated by unbiased subjects in live mode, and results show improved success of communication between users of the system.",
                "ieee_keywords": [
                    "Speech",
                    "Speech recognition",
                    "Syntactics",
                    "Lattices",
                    "Pragmatics",
                    "Merging",
                    "Hidden Markov models"
                ],
                "author_keywords": [
                    "Speech translation",
                    "error detection",
                    "error correction",
                    "spoken dialog systems"
                ]
            },
            {
                "title": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks",
                "link": "https://ieeexplore.ieee.org/document/9156851/",
                "date_of_publication": "05 August 2020",
                "doi": "10.1109/CVPR42600.2020.01075",
                "citations": "87",
                "abstract": "We present ALFRED (Action Learning From Realistic Environments and Directives), a benchmark for learning a mapping from natural language instructions and egocentric vision to sequences of actions for household tasks. ALFRED includes long, compositional tasks with non-reversible state changes to shrink the gap between research benchmarks and real-world applications. ALFRED consists of expert demonstrations in interactive visual environments for 25k natural language directives. These directives contain both high-level goals like “Rinse off a mug and place it in the coffee maker.” and low-level language instructions like “Walk to the coffee maker on the right.” ALFRED tasks are more complex in terms of sequence length, action space, and language than existing vision- and-language task datasets. We show that a baseline model based on recent embodied vision-and-language tasks performs poorly on ALFRED, suggesting that there is significant room for developing innovative grounded visual language understanding models with this benchmark.",
                "ieee_keywords": [
                    "Task analysis",
                    "Visualization",
                    "Navigation",
                    "Benchmark testing",
                    "Natural languages",
                    "Robots",
                    "Videos"
                ],
                "author_keywords": []
            },
            {
                "title": "Robot Programming by Demonstration with situated spatial language understanding",
                "link": "https://ieeexplore.ieee.org/document/7139462/",
                "date_of_publication": "02 July 2015",
                "doi": "10.1109/ICRA.2015.7139462",
                "citations": "27",
                "abstract": "Robot Programming by Demonstration (PbD) allows users to program a robot by demonstrating the desired behavior. Providing these demonstrations typically involves moving the robot through a sequence of states, often by physically manipulating it. This requires users to be co-located with the robot and have the physical ability to manipulate it. In this paper, we present a natural language based interface for PbD that removes these requirements and enables hands-free programming. We focus on programming object manipulation actions-our key insight is that such actions can be decomposed into known types of manipulator movements that are naturally described using spatial language; e.g., object reference expressions and prepositions. Our method takes a natural language command and the current world state to infer the intended movement command and its parametrization. We implement this method on a two-armed mobile manipulator and demonstrate the different types of manipulation actions that can be programmed with it. We compare it to a kinesthetic PbD interface and we demonstrate our method's ability to deal with incomplete language.",
                "ieee_keywords": [
                    "Programming",
                    "Natural languages",
                    "Computational modeling",
                    "Manipulators",
                    "Color",
                    "Context"
                ],
                "author_keywords": []
            },
            {
                "title": "Using syntactic and confusion network structure for out-of-vocabulary word detection",
                "link": "https://ieeexplore.ieee.org/document/6424215/",
                "date_of_publication": "31 January 2013",
                "doi": "10.1109/SLT.2012.6424215",
                "citations": "9",
                "abstract": "This paper addresses the problem of detecting words that are out-of-vocabulary (OOV) for a speech recognition system to improve automatic speech translation. The detection system leverages confidence prediction techniques given a confusion network representation and parsing with OOV word tokens to identify spans associated with true OOV words. Working in a resource-constrained domain, we achieve OOV detection F-scores of 60-66 and reduce word error rate by 12% relative to the case where OOV words are not detected.",
                "ieee_keywords": [
                    "Vocabulary",
                    "Speech recognition",
                    "Error analysis",
                    "Syntactics",
                    "Grammar",
                    "Speech",
                    "Lattices"
                ],
                "author_keywords": [
                    "OOV detection",
                    "speech recognition",
                    "parsing"
                ]
            },
            {
                "title": "Commonly Uncommon: Semantic Sparsity in Situation Recognition",
                "link": "https://ieeexplore.ieee.org/document/8100154/",
                "date_of_publication": "09 November 2017",
                "doi": "10.1109/CVPR.2017.671",
                "citations": "17",
                "abstract": "Semantic sparsity is a common challenge in structured visual classification problems, when the output space is complex, the vast majority of the possible predictions are rarely, if ever, seen in the training set. This paper studies semantic sparsity in situation recognition, the task of producing structured summaries of what is happening in images, including activities, objects and the roles objects play within the activity. For this problem, we find empirically that most substructures required for prediction are rare, and current state-of-the-art model performance dramatically decreases if even one such rare substructure exists in the target output. We avoid many such errors by (1) introducing a novel tensor composition function that learns to share examples across substructures more effectively and (2) semantically augmenting our training data with automatically gathered examples of rarely observed outputs using web data. When integrated within a complete CRF-based structured prediction model, the tensor-based approach outperforms existing state of the art by a relative improvement of 2.11% and 4.40% on top-5 verb and noun-role accuracy, respectively. Adding 5 million images with our semantic augmentation techniques gives further relative improvements of 6.23% and 9.57% on top-5 verb and noun-role accuracy.",
                "ieee_keywords": [
                    "Semantics",
                    "Tensile stress",
                    "Training",
                    "Image representation",
                    "Image recognition",
                    "Predictive models"
                ],
                "author_keywords": []
            },
            {
                "title": "Situation Recognition: Visual Semantic Role Labeling for Image Understanding",
                "link": "https://ieeexplore.ieee.org/document/7780966/",
                "date_of_publication": "12 December 2016",
                "doi": "10.1109/CVPR.2016.597",
                "citations": "95",
                "abstract": "This paper introduces situation recognition, the problem of producing a concise summary of the situation an image depicts including: (1) the main activity (e.g., clipping), (2) the participating actors, objects, substances, and locations (e.g., man, shears, sheep, wool, and field) and most importantly (3) the roles these participants play in the activity (e.g., the man is clipping, the shears are his tool, the wool is being clipped from the sheep, and the clipping is in a field). We use FrameNet, a verb and role lexicon developed by linguists, to define a large space of possible situations and collect a large-scale dataset containing over 500 activities, 1,700 roles, 11,000 objects, 125,000 images, and 200,000 unique situations. We also introduce structured prediction baselines and show that, in activity-centric images, situation-driven prediction of objects and activities outperforms independent object and activity recognition.",
                "ieee_keywords": [
                    "Semantics",
                    "Image recognition",
                    "Visualization",
                    "Wool",
                    "Labeling",
                    "Spraying"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Noah A. Smith",
        "publications": [
            {
                "title": "Natural Language Processing for Analyzing Disaster Recovery Trends Expressed in Large Text Corpora",
                "link": "https://ieeexplore.ieee.org/document/8601884/",
                "date_of_publication": "06 January 2019",
                "doi": "10.1109/GHTC.2018.8601884",
                "citations": "3",
                "abstract": "We are developing a new natural language processing (NLP) method to facilitate analysis of text corpora that describe long-term recovery. The aim of the method is to allow users to measure the degree that user-specified propositions about potential issues are embodied within the corpora, serving as a proxy for the disaster recovery process. The presented method employs a statistical syntax-based semantic matching model and was trained on a standard, publicly available training dataset. We applied the NLP method to a news story corpus that describes the recovery of Christchurch, New Zealand after the 2010-2011 Canterbury earthquake sequence. We used the model to compute semantic measurements of multiple potential recovery issues as expressed in the Christchurch news corpus that span 2011 to 2016. We evaluated method outputs through a user study involving twenty professional emergency managers. User study results show that the model can be effective when applied to a disaster-related news corpus. 85% of study participants were interested in a way to measure recovery issue propositions in news or other corpora. We are encouraged by the potential for future applications of our NLP method for after-action learning, recovery decision making, and disaster research.",
                "ieee_keywords": [
                    "Earthquakes",
                    "Natural language processing",
                    "Semantics",
                    "Seismic measurements",
                    "Government",
                    "Computational modeling"
                ],
                "author_keywords": [
                    "natural language processing",
                    "semantic matching",
                    "disaster recovery"
                ]
            },
            {
                "title": "End-to-End Neural Segmental Models for Speech Recognition",
                "link": "https://ieeexplore.ieee.org/document/8037994/",
                "date_of_publication": null,
                "doi": "10.1109/JSTSP.2017.2752462",
                "citations": "13",
                "abstract": "Segmental models are an alternative to frame-based models for sequence prediction, where hypothesized path weights are based on entire segment scores rather than a single frame at a time. Neural segmental models are segmental models that use neural network-based weight functions. Neural segmental models have achieved competitive results for speech recognition, and their end-to-end training has been explored in several studies. In this work, we review neural segmental models, which can be viewed as consisting of a neural network-based acoustic encoder and a finite-state transducer decoder. We study end-to-end segmental models with different weight functions, including ones based on frame-level neural classifiers and on segmental recurrent neural networks. We study how reducing the search space size impacts performance under different weight functions. We also compare several loss functions for end-to-end training. Finally, we explore training approaches, including multistage versus end-to-end training and multitask training that combines segmental and frame-level losses.",
                "ieee_keywords": [
                    "Hidden Markov models",
                    "Computational modeling",
                    "Automatic speech recognition",
                    "Speech recognition",
                    "Predictive models",
                    "Mel frequency cepstral coefficient"
                ],
                "author_keywords": [
                    "Connectionist temporal classification",
                    "end-to-end training",
                    "multitask training",
                    "segmental models"
                ]
            }
        ]
    },
    {
        "name": "Hannaneh Hajishirzi",
        "publications": [
            {
                "title": "Segment-Phrase Table for Semantic Segmentation, Visual Entailment and Paraphrasing",
                "link": "https://ieeexplore.ieee.org/document/7410367/",
                "date_of_publication": "18 February 2016",
                "doi": "10.1109/ICCV.2015.10",
                "citations": "9",
                "abstract": "We introduce Segment-Phrase Table (SPT), a large collection of bijective associations between textual phrases and their corresponding segmentations. Leveraging recent progress in object recognition and natural language semantics, we show how we can successfully build a high-quality segment-phrase table using minimal human supervision. More importantly, we demonstrate the unique value unleashed by this rich bimodal resource, for both vision as well as natural language understanding. First, we show that fine-grained textual labels facilitate contextual reasoning that helps in satisfying semantic constraints across image segments. This feature enables us to achieve state-of-the-art segmentation results on benchmark datasets. Next, we show that the association of high-quality segmentations to textual phrases aids in richer semantic understanding and reasoning of these textual phrases. Leveraging this feature, we motivate the problem of visual entailment and visual paraphrasing, and demonstrate its utility on a large dataset.",
                "ieee_keywords": [
                    "Image segmentation",
                    "Semantics",
                    "Visualization",
                    "Cognition",
                    "Pragmatics",
                    "Buildings",
                    "Training"
                ],
                "author_keywords": []
            },
            {
                "title": "Robust fine-tuning of zero-shot models",
                "link": "https://ieeexplore.ieee.org/document/9878622/",
                "date_of_publication": "27 September 2022",
                "doi": "10.1109/CVPR52688.2022.00780",
                "citations": "30",
                "abstract": "Large pre-trained models such as CLIP or ALIGN offer consistent accuracy across a range of data distributions when performing zero-shot inference (i.e., without fine-tuning on a specific dataset). Although existing fine-tuning methods substantially improve accuracy on a given target distribution, they often reduce robustness to distribution shifts. We address this tension by introducing a simple and effective method for improving robustness while fine-tuning: ensembling the weights of the zero-shot and fine-tuned models (WiSE-FT). Compared to standard fine-tuning, WiSE-FT provides large accuracy improvements under distribution shift, while preserving high accuracy on the target distribution. On ImageNet and five derived distribution shifts, WiSE-FT improves accuracy under distribution shift by 4 to 6 percentage points (pp) over prior work while increasing ImageNet accuracy by 1.6 pp. WiSE-FT achieves similarly large robustness gains (2 to 23 pp) on a diverse set of six further distribution shifts, and accuracy gains of 0.8 to 3.3 pp compared to standard fine-tuning on commonly used transfer learning datasets. These improvements come at no additional computational cost during fine-tuning or inference.",
                "ieee_keywords": [
                    "Computer vision",
                    "Computational modeling",
                    "Computer network reliability",
                    "Transfer learning",
                    "Neural networks",
                    "Robustness",
                    "Data models"
                ],
                "author_keywords": [
                    "Transfer/low-shot/long-tail learning; Machine learning"
                ]
            },
            {
                "title": "Potential-Based Advice for Stochastic Policy Learning",
                "link": "https://ieeexplore.ieee.org/document/9030194/",
                "date_of_publication": "12 March 2020",
                "doi": "10.1109/CDC40024.2019.9030194",
                "citations": "1",
                "abstract": "This paper augments the reward received by a reinforcement learning agent with potential functions in order to help the agent learn (possibly stochastic) optimal policies. We show that a potential-based reward shaping scheme is able to preserve optimality of stochastic policies, and demonstrate that the ability of an agent to learn an optimal policy is not affected when this scheme is augmented to soft Q-learning. We propose a method to impart potential-based advice schemes to policy gradient algorithms. An algorithm that considers an advantage actor-critic architecture augmented with this scheme is proposed, and we give guarantees on its convergence. Finally, we evaluate our approach on a puddle-jump grid world with indistinguishable states, and the continuous state and action mountain car environment from classical control. Our results indicate that these schemes allow the agent to learn a stochastic optimal policy faster and obtain a higher average reward.",
                "ieee_keywords": [
                    "Convergence",
                    "Training",
                    "Stochastic processes",
                    "Learning (artificial intelligence)",
                    "Task analysis",
                    "Entropy",
                    "Automobiles"
                ],
                "author_keywords": []
            },
            {
                "title": "ESPNetv2: A Light-Weight, Power Efficient, and General Purpose Convolutional Neural Network",
                "link": "https://ieeexplore.ieee.org/document/8953320/",
                "date_of_publication": "09 January 2020",
                "doi": "10.1109/CVPR.2019.00941",
                "citations": "173",
                "abstract": "We introduce a light-weight, power efficient, and general purpose convolutional neural network, ESPNetv2, for modeling visual and sequential data. Our network uses group point-wise and depth-wise dilated separable convolutions to learn representations from a large effective receptive field with fewer FLOPs and parameters. The performance of our network is evaluated on four different tasks: (1) object classification, (2) semantic segmentation, (3) object detection, and (4) language modeling. Experiments on these tasks, including image classification on the ImageNet and language modeling on the PenTree bank dataset, demonstrate the superior performance of our method over the state-of-the-art methods. Our network outperforms ESPNet by 4-5% and has 2-4x fewer FLOPs on the PASCAL VOC and the Cityscapes dataset. Compared to YOLOv2 on the MS-COCO object detection, ESPNetv2 delivers 4.4% higher accuracy with 6x fewer FLOPs. Our experiments show that ESPNetv2 is much more power efficient than existing state-of-the-art efficient methods including ShuffleNets and MobileNets. Our code is open-source and available at https://github.com/sacmehta/ESPNetv2.",
                "ieee_keywords": [
                    "Convolutional codes",
                    "Visualization",
                    "Computer vision",
                    "Computational modeling",
                    "Object detection",
                    "Data models",
                    "Pattern recognition"
                ],
                "author_keywords": [
                    "Deep Learning",
                    "Others",
                    "Recognition: Detection",
                    "Categorization",
                    "Retrieval",
                    "Representation Learning"
                ]
            },
            {
                "title": "Are You Smarter Than a Sixth Grader? Textbook Question Answering for Multimodal Machine Comprehension",
                "link": "https://ieeexplore.ieee.org/document/8100054/",
                "date_of_publication": "09 November 2017",
                "doi": "10.1109/CVPR.2017.571",
                "citations": "57",
                "abstract": "We introduce the task of Multi-Modal Machine Comprehension (M3C), which aims at answering multimodal questions given a context of text, diagrams and images. We present the Textbook Question Answering (TQA) dataset that includes 1,076 lessons and 26,260 multi-modal questions, taken from middle school science curricula. Our analysis shows that a significant portion of questions require complex parsing of the text and the diagrams and reasoning, indicating that our dataset is more complex compared to previous machine comprehension and visual question answering datasets. We extend state-of-the-art methods for textual machine comprehension and visual question answering to the TQA dataset. Our experiments show that these models do not perform well on TQA. The presented dataset opens new challenges for research in question answering and reasoning across multiple modalities.",
                "ieee_keywords": [
                    "Knowledge discovery",
                    "Visualization",
                    "Cognition",
                    "Training",
                    "Natural languages",
                    "Computer vision"
                ],
                "author_keywords": []
            },
            {
                "title": "DiCENet: Dimension-Wise Convolutions for Efficient Networks",
                "link": "https://ieeexplore.ieee.org/document/9277670/",
                "date_of_publication": null,
                "doi": "10.1109/TPAMI.2020.3041871",
                "citations": "15",
                "abstract": "We introduce a novel and generic convolutional unit, DiCE unit, that is built using dimension-wise convolutions and dimension-wise fusion. The dimension-wise convolutions apply light-weight convolutional filtering across each dimension of the input tensor while dimension-wise fusion efficiently combines these dimension-wise representations; allowing the DiCE unit to efficiently encode spatial and channel-wise information contained in the input tensor. The DiCE unit is simple and can be seamlessly integrated with any architecture to improve its efficiency and performance. Compared to depth-wise separable convolutions, the DiCE unit shows significant improvements across different architectures. When DiCE units are stacked to build the DiCENet model, we observe significant improvements over state-of-the-art models across various computer vision tasks including image classification, object detection, and semantic segmentation. On the ImageNet dataset, the DiCENet delivers 2-4 percent higher accuracy than state-of-the-art manually designed models (e.g., MobileNetv2 and ShuffleNetv2). Also, DiCENet generalizes better to tasks (e.g., object detection) that are often used in resource-constrained devices in comparison to state-of-the-art separable convolution-based efficient networks, including neural search-based methods (e.g., MobileNetv3 and MixNet).",
                "ieee_keywords": [
                    "Computer architecture",
                    "Tensors",
                    "Standards",
                    "Kernel",
                    "Convolutional codes",
                    "Task analysis",
                    "Object detection"
                ],
                "author_keywords": [
                    "Deep convolutional neural network",
                    "image classification",
                    "object detection",
                    "semantic segmentation",
                    "efficient networks",
                    "MeSH Terms",
                    "Algorithms",
                    "Image Processing, Computer-Assisted",
                    "Neural Networks, Computer"
                ]
            },
            {
                "title": "A Task-Oriented Approach for Cost-Sensitive Recognition",
                "link": "https://ieeexplore.ieee.org/document/7780611/",
                "date_of_publication": "12 December 2016",
                "doi": "10.1109/CVPR.2016.242",
                "citations": "2",
                "abstract": "With the recent progress in visual recognition, we have already started to see a surge of vision related real-world applications. These applications, unlike general scene understanding, are task oriented and require specific information from visual data. Considering the current growth in new sensory devices, feature designs, feature learning methods, and algorithms, the search in the space of features and models becomes combinatorial. In this paper, we propose a novel cost-sensitive task-oriented recognition method that is based on a combination of linguistic semantics and visual cues. Our task-oriented framework is able to generalize to unseen tasks for which there is no training data and outperforms state-of-the-art cost-based recognition baselines on our new task-based dataset.",
                "ieee_keywords": [
                    "Visualization",
                    "Training data",
                    "Feature extraction",
                    "Vocabulary",
                    "Pragmatics",
                    "Semantics",
                    "Training"
                ],
                "author_keywords": []
            },
            {
                "title": "Discriminative and consistent similarities in instance-level Multiple Instance Learning",
                "link": "https://ieeexplore.ieee.org/document/7298674/",
                "date_of_publication": "15 October 2015",
                "doi": "10.1109/CVPR.2015.7298674",
                "citations": "1",
                "abstract": "In this paper we present a bottom-up method to instance-level Multiple Instance Learning (MIL) that learns to discover positive instances with globally constrained reasoning about local pairwise similarities. We discover positive instances by optimizing for a ranking such that positive (top rank) instances are highly and consistently similar to each other and dissimilar to negative instances. Our approach takes advantage of a discriminative notion of pairwise similarity coupled with a structural cue in the form of a consistency metric that measures the quality of each similarity. We learn a similarity function for every pair of instances in positive bags by how similarly they differ from instances in negative bags, the only certain labels in MIL. Our experiments demonstrate that our method consistently outperforms state-of-the-art MIL methods both at bag-level and instance-level predictions in standard benchmarks, image category recognition, and text categorization datasets.",
                "ieee_keywords": [
                    "Training",
                    "Optimization",
                    "Standards",
                    "Support vector machines",
                    "Benchmark testing",
                    "Joints",
                    "Reliability"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Simon Shaolei Du",
        "publications": [
            {
                "title": "TOI-836: A super-Earth and mini-Neptune transiting a nearby K-dwarf",
                "link": "https://ieeexplore.ieee.org/document/10057528/",
                "date_of_publication": null,
                "doi": "10.1093/mnras/stad306",
                "citations": "Abstract",
                "abstract": "We present the discovery of two exoplanets transiting TOI-836 (TIC 440887364) using data from TESS Sector 11 and Sector 38. TOI-836 is a bright (T = 8.5 mag), high proper motion (∼200 mas yr −1 ), low metallicity ([Fe/H]≈−0.28) K-dwarf with a mass of 0.68 ± 0.05 M ⊙ and a radius of 0.67 ± 0.01 R ⊙ . We obtain photometric follow-up observations with a variety of facilities, and we use these data sets to determine that the inner planet, TOI-836 b, is a 1.70 ± 0.07 R ⊕ super-Earth in a 3.82-d orbit, placing it directly within the so-called ‘radius valley’. The outer planet, TOI-836 c, is a 2.59 ± 0.09 R ⊕ mini-Neptune in an 8.60-d orbit. Radial velocity measurements reveal that TOI-836 b has a mass of 4.5 ± 0.9 M ⊕ , while TOI-836 c has a mass of 9.6 ± 2.6 M ⊕ . Photometric observations show Transit Timing Variations (TTVs) on the order of 20 min for TOI-836 c, although there are no detectable TTVs for TOI-836 b. The TTVs of planet TOI-836 c may be caused by an undetected exterior planet.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "In Situ Reference Datasets From the TropiSAR and AfriSAR Campaigns in Support of Upcoming Spaceborne Biomass Missions",
                "link": "https://ieeexplore.ieee.org/document/8417912/",
                "date_of_publication": null,
                "doi": "10.1109/JSTARS.2018.2851606",
                "citations": "44",
                "abstract": "Tropical forests are a key component of the global carbon cycle. Yet, there are still high uncertainties in forest carbon stock and flux estimates, notably because of their spatial and temporal variability across the tropics. Several upcoming spaceborne missions have been designed to address this gap. High-quality ground data are essential for accurate calibration/validation so that spaceborne biomass missions can reach their full potential in reducing uncertainties regarding forest carbon stocks and fluxes. The BIOMASS mission, a P-band SAR satellite from the European Space Agency (ESA), aims at improving carbon stock mapping and reducing uncertainty in the carbon fluxes from deforestation, forest degradation, and regrowth. In situ activities in support of the BIOMASS mission were carried out in French Guiana and Gabon during the TropiSAR and AfriSAR campaigns. During these campaigns, airborne P-band SAR, forest inventory, and lidar data were collected over six study sites. This paper describes the methods used for forest inventory and lidar data collection and analysis, and presents resulting plot estimates and aboveground biomass maps. These reference datasets along with intermediate products (e.g., canopy height models) can be accessed through ESA's Forest Observation System and the Dryad data repository and will be useful for BIOMASS but also to other spaceborne biomass missions such as GEDI, NISAR, and Tandem-L for calibration/validation purposes. During data quality control and analysis, prospects for reducing uncertainties have been identified, and this paper finishes with a series of recommendations for future tropical forest field campaigns to better serve the remote sensing community.",
                "ieee_keywords": [
                    "Forestry",
                    "Biomass",
                    "Carbon",
                    "Electronic mail",
                    "Uncertainty",
                    "Earth",
                    "Remote sensing"
                ],
                "author_keywords": [
                    "Carbon",
                    "ecology",
                    "environmental monitoring",
                    "remote sensing",
                    "surface topography",
                    "synthetic aperture radar",
                    "vegetation"
                ]
            }
        ]
    },
    {
        "name": "Yin Tat Lee",
        "publications": [
            {
                "title": "A Faster Interior Point Method for Semidefinite Programming",
                "link": "https://ieeexplore.ieee.org/document/9317892/",
                "date_of_publication": "19 January 2021",
                "doi": "10.1109/FOCS46700.2020.00089",
                "citations": "15",
                "abstract": "Semidefinite programs (SDPs) are a fundamental class of optimization problems with important recent applications in approximation algorithms, quantum complexity, robust learning, algorithmic rounding, and adversarial deep learning. This paper presents a faster interior point method to solve generic SDPs with variable size n \\times n and m constraints in time \\begin{equation*} \\tilde{O}(\\sqrt{n}(mn^{2}+m^{\\omega}+n^{\\omega})\\log(1/\\epsilon)), \\end{equation*} where \\omega is the exponent of matrix multiplication and \\epsilon is the relative accuracy. In the predominant case of m\\geq n , our runtime outperforms that of the previous fastest SDP solver, which is based on the cutting plane method [JLSW20]. Our algorithm's runtime can be naturally interpreted as follows: O(\\sqrt{n}\\log(1/\\epsilon)) is the number of iterations needed for our interior point method, mn^{2} is the input size, and m^{\\omega}+n^{\\omega} is the time to invert the Hessian and slack matrix in each iteration. These constitute natural barriers to further improving the runtime of interior point methods for solving generic SDPs.",
                "ieee_keywords": [
                    "Runtime",
                    "Manganese",
                    "Optimization",
                    "Complexity theory",
                    "Approximation algorithms",
                    "Time factors",
                    "Programming"
                ],
                "author_keywords": [
                    "SDP",
                    "Numerical Linear Algebra",
                    "Optimization"
                ]
            },
            {
                "title": "Bipartite Matching in Nearly-linear Time on Moderately Dense Graphs",
                "link": "https://ieeexplore.ieee.org/document/9317961/",
                "date_of_publication": "19 January 2021",
                "doi": "10.1109/FOCS46700.2020.00090",
                "citations": "18",
                "abstract": "We present an $\\tilde{O}(m+n^{1.5})$ -time randomized algorithm for maximum cardinality bipartite matching and related problems (e.g. transshipment, negative-weight shortest paths, and optimal transport) on m-edge, n-node graphs. For maximum cardinality bipartite matching on moderately dense graphs, i.e. $m=\\Omega(n^{1.5})$ , our algorithm runs in time nearly linear in the input size and constitutes the first improvement over the classic $O(m\\sqrt{n})$ -time [Dinic 1970; Hopcroft-Karp 1971; Karzanov 1973] and $\\widetilde{O}(n^{\\omega})$ -time algorithms [Ibarra-Moran 1981] (where currently $\\omega\\approx 2.373$ ). On sparser graphs, i.e. when $m=n^{9/8+\\delta}$ for any constant $\\delta > 0$ , our result improves upon the recent advances of [Madry 2013] and [Liu-Sidford 2020b, 2020a] which achieve an $\\widetilde{O}(m^{4/3+o(1)})$ runtime. We obtain these results by combining and advancing recent lines of research in interior point methods (IPMs) and dynamic graph algorithms. First, we simplify and improve the IPM of [v.d.Brand-Lee-Sidford-Song 2020], providing a general primal-dual IPM framework and new sampling-based techniques for handling infeasibility induced by approximate linear system solvers. Second, we provide a simple sublinear-time algorithm for detecting and sampling high-energy edges in electric flows on expanders and show that when combined with recent advances in dynamic expander decompositions, this yields efficient data structures for maintaining the iterates of both [v.d.Brand et al.] and our new IPMs. Combining this general machinery yields a simpler $\\widetilde{O}(n\\sqrt{m})$ time algorithm for matching based on the logarithmic barrier function, and our state-of-the-art $\\widetilde{O}(m+n^{1.5})$ time algorithm for matching based on the [Lee-Sidford 2014] barrier (as regularized in [v.d.Brand et al.]).",
                "ieee_keywords": [
                    "Runtime",
                    "Data structures",
                    "Heuristic algorithms",
                    "Approximation algorithms",
                    "Symmetric matrices",
                    "Sparse matrices",
                    "Machine learning algorithms"
                ],
                "author_keywords": [
                    "bipartite matching",
                    "shortest paths",
                    "transshipment",
                    "optimal transport",
                    "nearly linear time",
                    "interior point method",
                    "linear program"
                ]
            },
            {
                "title": "Faster Matroid Intersection",
                "link": "https://ieeexplore.ieee.org/document/8948634/",
                "date_of_publication": "06 January 2020",
                "doi": "10.1109/FOCS.2019.00072",
                "citations": "4",
                "abstract": "In this paper we consider the classic matroid intersection problem: given two matroids M 1 = (V, I 1 ) and M 2 = (V, I 2 ) defined over a common ground set V , compute a set S ∈ I 1 ∩ I 2 of largest possible cardinality, denoted by r. We consider this problem both in the setting where each Mi is accessed through an independence oracle, i.e. a routine which returns whether or not a set S ∈ I i in T ind time, and the setting where each Mi is accessed through a rank oracle, i.e. a routine which returns the size of the largest independent subset of S in M i in T rank time. In each setting we provide faster exact and approximate algorithms. Given an independence oracle, we provide an exact O(nr log r · T ind ) time algorithm. This improves upon previous best known running times of O(nr 1.5 ·T ind ) due to Cunningham O(n 2 ·T ind in 1986 and + n 3 ) due to Lee, Sidford, and Wong in 2015. We also provide two algorithms which compute a (1- ε-approximate solution to matroid intersection running in times O(n 1.5 /ε 1.5 · Tind) and O((n 2 r -1 ε -2 + r 1.5 ε -4.5 ) · Tind), respectively. These results improve upon the O(nr/ε · T ind )time algorithm of Cunningham (noted recently by Chekuri and Quanrud). Given a rank oracle, we provide algorithms with even better dependence on n and r. We provide an O(n√r log n · T rank )time exact algorithm and an O(nε -1 log n · T rank )-time algorithm which obtains a (1 - 0)-approximation to the matroid intersection problem. The former result improves over the O(nr · T rank + n 3 )-time algorithm by Lee, Sidford, and Wong. The rank oracle is of particular interest as the matroid intersection problem with this oracle is a special case (via Edmond's minimax characterization of matroid intersection) of the submodular function minimization (SFM) problem with an evaluation oracle, and understanding SFM query complexity is an outstanding open question.",
                "ieee_keywords": [
                    "Approximation algorithms",
                    "Minimization",
                    "Complexity theory",
                    "Image edge detection",
                    "Computer science",
                    "Optimization",
                    "Distance measurement"
                ],
                "author_keywords": [
                    "Matroids",
                    "Combinatorial Optimization",
                    "Submodular Functions"
                ]
            },
            {
                "title": "Eldan's Stochastic Localization and the KLS Hyperplane Conjecture: An Improved Lower Bound for Expansion",
                "link": "https://ieeexplore.ieee.org/document/8104128/",
                "date_of_publication": "13 November 2017",
                "doi": "10.1109/FOCS.2017.96",
                "citations": "24",
                "abstract": "We show that the KLS constant for n-dimensional isotropic logconcavemeasures is O(n^{1/4}), improving on the current best bound ofO(n^{1/3}√{\\log n}). As corollaries we obtain the same improvedbound on the thin-shell estimate, Poincar\\e constant and Lipschitzconcentration constant and an alternative proof of this bound forthe isotropic constant; it also follows that the ball walk for samplingfrom an isotropic logconcave density in \\R^{n} converges in O^{*}(n^{2.5})steps from a warm start.",
                "ieee_keywords": [
                    "Covariance matrices",
                    "Density measurement",
                    "Convergence",
                    "Geometry",
                    "Standards",
                    "Computer science",
                    "Needles"
                ],
                "author_keywords": [
                    "Convex Geometry",
                    "Concentration",
                    "Isoperimetry",
                    "Cheeger constant",
                    "Poincar\\'e constant",
                    "Slicing conjecture",
                    "Thin-shell conjecture",
                    "Logconcave distributions",
                    "High-dimensional Sampling"
                ]
            }
        ]
    },
    {
        "name": "Byron Boots",
        "publications": [
            {
                "title": "Stackelberg Games for Learning Emergent Behaviors During Competitive Autocurricula",
                "link": "https://ieeexplore.ieee.org/document/10160875/",
                "date_of_publication": "04 July 2023",
                "doi": "10.1109/ICRA48891.2023.10160875",
                "citations": "27",
                "abstract": "Autocurricular training is an important sub-area of multi-agent reinforcement learning (MARL) that allows multiple agents to learn emergent skills in an unsupervised co-evolving scheme. The robotics community has experimented auto-curricular training with physically grounded problems, such as robust control and interactive manipulation tasks. However, the asymmetric nature of these tasks makes the generation of sophisticated policies challenging. Indeed, the asymmetry in the environment may implicitly or explicitly provide an advantage to a subset of agents which could, in turn, lead to a low-quality equilibrium. This paper proposes a novel game-theoretic algorithm, Stackelberg Multi-Agent Deep Deterministic Policy Gradient (ST-MADDPG), which formulates a two-player MARL problem as a Stackelberg game with one player as the ‘leader’ and the other as the ‘follower’ in a hierarchical interaction structure wherein the leader has an advantage. We first demonstrate that the leader's advantage from ST-MADDPG can be used to alleviate the inherent asymmetry in the environment. By exploiting the leader's advantage, ST-MADDPG improves the quality of a co-evolution process and results in more sophisticated and complex strategies that work well even against an unseen strong opponent.",
                "ieee_keywords": [
                    "Training",
                    "Robust control",
                    "Automation",
                    "Games",
                    "Reinforcement learning",
                    "Approximation algorithms",
                    "Behavioral sciences"
                ],
                "author_keywords": []
            },
            {
                "title": "Stein Variational Probabilistic Roadmaps",
                "link": "https://ieeexplore.ieee.org/document/9811656/",
                "date_of_publication": "12 July 2022",
                "doi": "10.1109/ICRA46639.2022.9811656",
                "citations": "170",
                "abstract": "Efficient and reliable generation of global path plans are necessary for safe execution and deployment of autonomous systems. In order to generate planning graphs which adequately resolve the topology of a given environment, many sampling-based motion planners resort to coarse, heuristically-driven strategies which often fail to generalize to new and varied surroundings. Further, many of these approaches are not designed to contend with partial-observability. We posit that such uncertainty in environment geometry can, in fact, help drive the sampling process in generating feasible, and probabilistically-safe planning graphs. We propose a method for Probabilistic Roadmaps which relies on particle-based Variational Inference to efficiently cover the posterior distribution over feasible regions in configuration space. Our approach, Stein Variational Probabilistic Roadmap (SV-PRM), results in sample-efficient generation of planning-graphs and large improvements over traditional sampling approaches. We demonstrate the approach on a variety of challenging planning problems, including real-world probabilistic occupancy maps and high-dof manipulation problems common in robotics. Video, additional material and results can be found here: https://sites.google.com/view/stein-prm.",
                "ieee_keywords": [
                    "Geometry",
                    "Uncertainty",
                    "Automation",
                    "Autonomous systems",
                    "Probabilistic logic",
                    "Reliability engineering",
                    "Planning"
                ],
                "author_keywords": []
            },
            {
                "title": "Grasping with Chopsticks: Combating Covariate Shift in Model-free Imitation Learning for Fine Manipulation",
                "link": "https://ieeexplore.ieee.org/document/9561662/",
                "date_of_publication": "18 October 2021",
                "doi": "10.1109/ICRA48506.2021.9561662",
                "citations": "6",
                "abstract": "Billions of people use chopsticks, a simple yet versatile tool, for fine manipulation of everyday objects. The small, curved, and slippery tips of chopsticks pose a challenge for picking up small objects, making them a suitably complex test case. This paper leverages human demonstrations to develop an autonomous chopsticks-equipped robotic manipulator. Due to the lack of accurate models for fine manipulation, we explore model-free imitation learning, which traditionally suffers from the covariate shift phenomenon that causes poor generalization. We propose two approaches to reduce covariate shift, neither of which requires access to an interactive expert or a model, unlike previous approaches. First, we alleviate singlestep prediction errors by applying an invariant operator to increase the data support at critical steps for grasping. Second, we generate synthetic corrective labels by adding bounded noise and combining parametric and non-parametric methods to prevent error accumulation. We demonstrate our methods on a real chopstick-equipped robot that we built, and observe the agent’s success rate increase from 37.3% to 80%, which is comparable to the human expert performance of 82.6%.",
                "ieee_keywords": [
                    "Automation",
                    "Conferences",
                    "Grasping",
                    "Tools",
                    "Manipulators"
                ],
                "author_keywords": []
            },
            {
                "title": "Space-time functional gradient optimization for motion planning",
                "link": "https://ieeexplore.ieee.org/document/6907818/",
                "date_of_publication": "29 September 2014",
                "doi": "10.1109/ICRA.2014.6907818",
                "citations": "25",
                "abstract": "Functional gradient algorithms (e.g. CHOMP) have recently shown great promise for producing locally optimal motion for complex many degree-of-freedom robots. A key limitation of such algorithms is the difficulty in incorporating constraints and cost functions that explicitly depend on time. We present T-CHOMP, a functional gradient algorithm that overcomes this limitation by directly optimizing in space-time. We outline a framework for joint space-time optimization, derive an efficient trajectory-wide update for maintaining time monotonicity, and demonstrate the significance of T-CHOMP over CHOMP in several scenarios. By manipulating time, T-CHOMP produces lower-cost trajectories leading to behavior that is meaningfully different from CHOMP.",
                "ieee_keywords": [
                    "Trajectory",
                    "Robots",
                    "Timing",
                    "Optimization",
                    "Planning",
                    "Collision avoidance",
                    "Vectors"
                ],
                "author_keywords": []
            },
            {
                "title": "Learning predictive models of a depth camera & manipulator from raw execution traces",
                "link": "https://ieeexplore.ieee.org/document/6907443/",
                "date_of_publication": "29 September 2014",
                "doi": "10.1109/ICRA.2014.6907443",
                "citations": "13",
                "abstract": "In this paper, we attack the problem of learning a predictive model of a depth camera and manipulator directly from raw execution traces. While the problem of learning manipulator models from visual and proprioceptive data has been addressed before, existing techniques often rely on assumptions about the structure of the robot or tracked features in observation space. We make no such assumptions. Instead, we formulate the problem as that of learning a high-dimensional controlled stochastic process. We leverage recent work on nonparametric predictive state representations to learn a generative model of the depth camera and robotic arm from sequences of uninterpreted actions and observations. We perform several experiments in which we demonstrate that our learned model can accurately predict future depth camera observations in response to sequences of motor commands.",
                "ieee_keywords": [
                    "Joints",
                    "Kernel",
                    "Predictive models",
                    "Cameras",
                    "Robot vision systems"
                ],
                "author_keywords": []
            },
            {
                "title": "Geometric Fabrics: Generalizing Classical Mechanics to Capture the Physics of Behavior",
                "link": "https://ieeexplore.ieee.org/document/9682604/",
                "date_of_publication": null,
                "doi": "10.1109/LRA.2022.3143311",
                "citations": "6",
                "abstract": "Classical mechanical systems are central to controller design in energy shaping methods of geometric control. However, their expressivity is limited by position-only metrics and the intimate link between metric and geometry. Recent work on Riemannian Motion Policies (RMPs) has shown that shedding these restrictions results in powerful design tools, but at the expense of theoretical stability guarantees. In this work, we generalize classical mechanics to what we call geometric fabrics, whose expressivity and theory enable the design of systems that outperform RMPs in practice. Geometric fabrics strictly generalize classical mechanics forming a new physics of behavior by first generalizing them to Finsler geometries and then explicitly bending them to shape their behavior while maintaining stability. We develop the theory of fabrics and present both a collection of controlled experiments examining their theoretical properties and a set of robot system experiments showing improved performance over a well-engineered and hardened implementation of RMPs, our current state-of-the-art in controller design.",
                "ieee_keywords": [
                    "Fabrics",
                    "Geometry",
                    "Measurement",
                    "Mechanical systems",
                    "Damping",
                    "Mathematical models",
                    "Bending"
                ],
                "author_keywords": [
                    "Dynamics",
                    "Optimization and Optimal Control"
                ]
            },
            {
                "title": "Collaborative Interaction Models for Optimized Human-Robot Teamwork",
                "link": "https://ieeexplore.ieee.org/document/9341369/",
                "date_of_publication": "10 February 2021",
                "doi": "10.1109/IROS45743.2020.9341369",
                "citations": "2",
                "abstract": "Effective human-robot collaboration requires informed anticipation. The robot must anticipate the human's actions, but also react quickly and intuitively when its predictions are wrong. The robot must plan its actions to account for the human's own plan, with the knowledge that the human's behavior will change based on what the robot actually does. This cyclical game of predicting a human's future actions and generating a corresponding motion plan is extremely difficult to model using standard techniques. In this work, we describe a novel Model Predictive Control (MPC)-based framework for finding optimal trajectories in a collaborative, multi-agent setting, in which we simultaneously plan for the robot while predicting the actions of its external collaborators. We use human-robot handovers to demonstrate that with a strong model of the collaborator, our framework produces fluid, reactive human-robot interactions in novel, cluttered environments. Our method efficiently generates coordinated trajectories, and achieves a high success rate in handover, even in the presence of significant sensor noise.",
                "ieee_keywords": [
                    "Robot kinematics",
                    "Handover",
                    "Predictive models",
                    "Robot sensing systems",
                    "Trajectory",
                    "Teamwork",
                    "Standards"
                ],
                "author_keywords": []
            },
            {
                "title": "Neural Contact Fields: Tracking Extrinsic Contact with Tactile Sensing",
                "link": "https://ieeexplore.ieee.org/document/10160526/",
                "date_of_publication": "04 July 2023",
                "doi": "10.1109/ICRA48891.2023.10160526",
                "citations": "100",
                "abstract": "We present Neural Contact Fields, a method that brings together neural fields and tactile sensing to address the problem of tracking extrinsic contact between object and environment. Knowing where the external contact occurs is a first step towards methods that can actively control it in facilitating downstream manipulation tasks. Prior work for localizing environmental contacts typically assume a contact type (e.g. point or line), does not capture contact/no-contact transitions, and only works with basic geometric-shaped objects. Neural Contact Fields are the first method that can track arbitrary multi-modal extrinsic contacts without making any assumptions about the contact type. Our key insight is to estimate the probability of contact for any 3D point in the latent space of object's shapes, given vision-based tactile inputs that sense the local motion resulting from the external contact. In experiments, we find that Neural Contact Fields are able to localize multiple contact patches without making any assumptions about the geometry of the contact, and capture contact/no-contact transitions for known categories of objects with unseen shapes in unseen environment configurations. In addition to Neural Contact Fields, we also release our YCB-Extrinsic-Contact dataset of simulated extrinsic contact interactions to enable further research in this area. Project page: https://github.com/carolinahiguera/NCF",
                "ieee_keywords": [
                    "Geometry",
                    "Three-dimensional displays",
                    "Automation",
                    "Shape",
                    "Tracking",
                    "Robot sensing systems",
                    "Sensors"
                ],
                "author_keywords": []
            },
            {
                "title": "Towards Coordinated Robot Motions: End-to-End Learning of Motion Policies on Transform Trees",
                "link": "https://ieeexplore.ieee.org/document/9636097/",
                "date_of_publication": "16 December 2021",
                "doi": "10.1109/IROS51168.2021.9636097",
                "citations": "128",
                "abstract": "Generating robot motion that fulfills multiple tasks simultaneously is challenging due to the geometric constraints imposed on the robot. In this paper, we propose to solve multi-task problems through learning structured policies from human demonstrations. Our structured policy is inspired by RMPflow, a framework for combining subtask policies on different spaces. The policy structure provides the user an interface to 1) specifying the spaces that are directly relevant to the completion of the tasks, and 2) designing policies for certain tasks that do not need to be learned. We derive an end-to-end learning objective that is suitable for the multi-task problem, emphasizing the distance between generated motions and demonstrations measured on task spaces. Furthermore, the motion generated from the learned policy class is guaranteed to be stable. We validate the effectiveness of our proposed learning framework through qualitative and quantitative evaluations on three robotic tasks on a 7-DOF Rethink Sawyer robot.",
                "ieee_keywords": [
                    "Robot motion",
                    "Robot kinematics",
                    "Transforms",
                    "Multitasking",
                    "Extraterrestrial measurements",
                    "Task analysis",
                    "Intelligent robots"
                ],
                "author_keywords": []
            },
            {
                "title": "Differentiable Gaussian Process Motion Planning",
                "link": "https://ieeexplore.ieee.org/document/9197260/",
                "date_of_publication": "15 September 2020",
                "doi": "10.1109/ICRA40945.2020.9197260",
                "citations": "15",
                "abstract": "Modern trajectory optimization based approaches to motion planning are fast, easy to implement, and effective on a wide range of robotics tasks. However, trajectory optimization algorithms have parameters that are typically set in advance (and rarely discussed in detail). Setting these parameters properly can have a significant impact on the practical performance of the algorithm, sometimes making the difference between finding a feasible plan or failing at the task entirely. We propose a method for leveraging past experience to learn how to automatically adapt the parameters of Gaussian Process Motion Planning (GPMP) algorithms. Specifically, we propose a differentiable extension to the GPMP2 algorithm, so that it can be trained end-to-end from data. We perform several experiments that validate our algorithm and illustrate the benefits of our proposed learning-based approach to motion planning.",
                "ieee_keywords": [
                    "Planning",
                    "Conferences",
                    "Automation",
                    "Gaussian processes",
                    "Artificial intelligence",
                    "Trajectory optimization",
                    "Robots"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Shwetak Patel",
        "publications": [
            {
                "title": "Multi-Channel Facial Photoplethysmography Sensing",
                "link": "https://ieeexplore.ieee.org/document/9176700/",
                "date_of_publication": "27 August 2020",
                "doi": "10.1109/EMBC44109.2020.9176700",
                "citations": "1",
                "abstract": "Motivated by the need for continuous cardiovascular monitoring, we present a system for performing photoplethysmography sensing at multiple facial locations. As a proof-of-concept, our system incorporates an optical sensor array into a wearable face mask form factor for application in a surgical hemodynamic monitoring use case. Here we demonstrate that our design can accurately detect pulse timing by validating estimated heart rate against ground truth electrocardiogram recordings. In an experiment across 10 experimental subjects, our system achieves an error standard deviation of 2.84 beats per minute. This system shows promise for performing non-invasive, continuous pulse waveform recording from multiple locations on the face.",
                "ieee_keywords": [
                    "Heart rate",
                    "Optical sensors",
                    "Face",
                    "Biomedical monitoring",
                    "Surgery",
                    "Blood pressure"
                ],
                "author_keywords": []
            },
            {
                "title": "Noninvasive hemoglobin measurement using unmodified smartphone camera and white flash",
                "link": "https://ieeexplore.ieee.org/document/8037323/",
                "date_of_publication": "14 September 2017",
                "doi": "10.1109/EMBC.2017.8037323",
                "citations": "16",
                "abstract": "We show that a mobile phone can measure hemoglobin levels using the built-in RGB camera and white LED without modification. Prior work has demonstrated that a smartphone using the built-in RGB camera with the aid of visible and IR lights can achieve a Pearson correlation results between 0.69-0.82 and an RMSE value between 1.26-1.56 g/dL. Our system builds upon the prior work and demonstrates that with only the built-in white LED, the estimation of hemoglobin level has a Pearson correlation of 0.62 with an RMSE of 1.27 g/dL. This extension work demonstrates that it is feasible to measure hemoglobin without using an IR source.",
                "ieee_keywords": [
                    "Cameras",
                    "Blood",
                    "Light emitting diodes",
                    "Absorption",
                    "Color",
                    "Fingers",
                    "Wavelength measurement"
                ],
                "author_keywords": []
            },
            {
                "title": "Dante vision: In-air and touch gesture sensing for natural surface interaction with combined depth and thermal cameras",
                "link": "https://ieeexplore.ieee.org/document/6152472/",
                "date_of_publication": "16 February 2012",
                "doi": "10.1109/ESPA.2012.6152472",
                "citations": "22",
                "abstract": "Researchers have paid considerable attention to natural user interfaces, especially sensing gestures and touches upon an un-instrumented surface from an overhead camera. We present a system that combines depth sensing from a Microsoft Kinect and temperature sensing from a thermal imaging camera to infer a variety of gestures and touches for controlling a natural user interface. The system, coined Dante, is capable of (1) inferring multiple touch points from multiple users (92.6% accuracy), (2) detecting and classifying each user using their depth and thermal footprint (87.7% accuracy), and (3) detecting touches on objects placed upon the table top (91.7% accuracy). The system can also classify the pressure of chording motions. The system is real time, with an average processing delay of 40 ms.",
                "ieee_keywords": [
                    "Cameras",
                    "Thumb",
                    "Sensors",
                    "Real time systems",
                    "Image segmentation"
                ],
                "author_keywords": [
                    "depth imaging",
                    "thermal imaging",
                    "natural user interface",
                    "sensor fusion"
                ]
            },
            {
                "title": "O-pH: Optical pH Monitor to Measure Dental Biofilm Acidity and Assist in Enamel Health Monitoring",
                "link": "https://ieeexplore.ieee.org/document/9720240/",
                "date_of_publication": null,
                "doi": "10.1109/TBME.2022.3153659",
                "citations": "1081",
                "abstract": "Objective: Bacteria in the dental biofilm produceacid after consumption of carbohydrates which if left unmonitored leads to caries formation. We present O-pH, a device that can measure dental biofilm acidity and provide quantitative feedback to assist in oral health monitoring. Method: O-pH utilizes a ratiometric pH sensing method by capturing fluorescence of Sodium Fluorescein, an FDA approved chemical dye. The device was calibrated to a lab pH meter using buffered fluorescein solution with a correlation coefficient of 0.97. The calibration was further verified in vitro on additional buffered solution, artificial, and extracted teeth. An in vivo study on 30 pediatric subjects was performed to measure pH before (rest pH) and after (drop pH) a sugar rinse, and the resultant difference in pH (diff pH) was calculated. The study enrolled subjects with low (Post-Cleaning) and heavy (Pre-Cleaning) biofilm load, having both unhealthy/healthy surfaces. Further, we modified point-based O-pH to an image-based device using a multimode-scanning fiber endoscope (mm-SFE) and tested in vivo on one subject. Results and Conclusion: We found significant difference between Post-Cleaning and Pre-Cleaning group using drop pH and diff pH. Additionally, in Pre-Cleaning group, the rest and drop pH is lower at the caries surfaces compared to healthy surfaces. Similar trend was not noticed in the Post-Cleaning group. mm-SFE pH scope recorded image-based pH heatmap of a subject with an average diff pH of 1.5. Significance: This work builds an optical pH prototype and presents a pioneering study for non-invasively measuring pH of dental biofilm clinically.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "GroupEnergyTable: An Interactive Tabletop for Energy Conservation",
                "link": "https://ieeexplore.ieee.org/document/6228599/",
                "date_of_publication": null,
                "doi": "10.1109/MPRV.2012.43",
                "citations": "270",
                "abstract": "The GroupEnergyTable is an interactive tabletop that lets users explore shared electricity and transportation data, view energy tips, and set goals. A two-month study shows how the GroupEnergyTable can help decrease electricity use and change travel habits.",
                "ieee_keywords": [
                    "Sustainable development",
                    "Green design",
                    "Interactive systems",
                    "Power distribution",
                    "Transportation"
                ],
                "author_keywords": [
                    "interactive surfaces",
                    "ubiquitous computing",
                    "sustainability",
                    "pervasive computing"
                ]
            },
            {
                "title": "IDCam: Precise Item Identification for AR Enhanced Object Interactions",
                "link": "https://ieeexplore.ieee.org/document/8719279/",
                "date_of_publication": "23 May 2019",
                "doi": "10.1109/RFID.2019.8719279",
                "citations": "4",
                "abstract": "Augmented reality (AR) promises to revolutionize the way people interact with their surroundings by seamlessly overlaying virtual information onto the physical world. To improve the quality of such information, AR systems need to identify the object with which the user is interacting. AR systems today heavily rely on computer vision for object identification; however, state-of-the-art computer vision systems can only identify the general object categories, rather than their precise identity. In this work, we propose IDCam, a system that fuses RFID and computer vision for precise item identification in AR object-oriented interactions. IDCam simultaneously tracks users' hands using a depth camera and generates motion traces for RFID-tagged objects. The system then correlates traces from vision and RFID to match item identities with user interactions. We tested our system through a simulated retail scenario where 5 participants interacted with a clothing rack simultaneously. In our evaluation study deployed in a lab environment, IDCam identified item interactions with an accuracy of 82.0% within 2 seconds.",
                "ieee_keywords": [
                    "Computer vision",
                    "Object recognition",
                    "RFID tags",
                    "Tracking",
                    "Headphones"
                ],
                "author_keywords": [
                    "Augmented Reality",
                    "RFID",
                    "Object Recognition",
                    "Sensor Fusion",
                    "Object Interaction"
                ]
            },
            {
                "title": "WiBreathe: Estimating respiration rate using wireless signals in natural settings in the home",
                "link": "https://ieeexplore.ieee.org/document/7146519/",
                "date_of_publication": "02 July 2015",
                "doi": "10.1109/PERCOM.2015.7146519",
                "citations": "68",
                "abstract": "Sensing respiration rate has many applications in monitoring various health conditions, such as sleep apnea and chronic obstructive pulmonary disease. In this paper, we present WiBreathe, a wireless, high fidelity and non-invasive breathing monitor that leverages wireless signals at 2.4 GHz to estimate an individual's respiration rate. Our work extends past approaches of using wireless signals for respiratory monitoring by using only a single transmitter-receiver pair at the same frequency range of commodity Wi-Fi signals to estimate the respiratory rate of an individual. This is done irrespective of whether they are in line of sight or not (e.g., through walls). Furthermore, we demonstrate the capability of WiBreathe in detecting multiple people and by extension, their respiration rates. We evaluate our approach in various natural environments and show that we can track breathing with the accuracy of 1.54 breaths per minute when compared to a clinical respiratory chest band.",
                "ieee_keywords": [
                    "Frequency estimation",
                    "Monitoring",
                    "Wireless communication",
                    "Harmonic analysis",
                    "Wireless sensor networks",
                    "Biomedical monitoring",
                    "Error analysis"
                ],
                "author_keywords": [
                    "health sensing",
                    "wireless",
                    "respiration rate",
                    "noninvasive"
                ]
            },
            {
                "title": "IDAct: Towards Unobtrusive Recognition of User Presence and Daily Activities",
                "link": "https://ieeexplore.ieee.org/document/8719103/",
                "date_of_publication": "23 May 2019",
                "doi": "10.1109/RFID.2019.8719103",
                "citations": "10",
                "abstract": "The Internet of Things (IoT) promises to revolutionize the way people interact with their surrounding environment and the objects within it by creating a ubiquitous network of physical devices. However, recent advancements have been focused on creating battery-powered electronics. There remains a huge gap between the collection of smart devices and the massive number of everyday physical objects. In this work, we bridge this gap by enhancing the sensing capabilities of everyday objects using commercial long-range RFID. We apply signal processing and machine learning techniques towards its communication channel parameters to detect the presence of users and to understand their daily activities. Different from prior work, our system can adapt to different environments and objects types. In a naturalistic user study deployed in a home environment, IDAct detected user presence with an F1 score of 96.7% and recognizes 24 different daily activities with an F1 score of 82.8%.",
                "ieee_keywords": [
                    "Radio frequency",
                    "Activity recognition",
                    "Interference",
                    "Conferences",
                    "Passive RFID tags"
                ],
                "author_keywords": [
                    "Activity Recognition",
                    "Object Usage Sensing",
                    "Presence Sensing",
                    "RFID"
                ]
            },
            {
                "title": "Powering Pervasive Computing Systems",
                "link": "https://ieeexplore.ieee.org/document/7994569/",
                "date_of_publication": null,
                "doi": "10.1109/MPRV.2017.2940955",
                "citations": "7",
                "abstract": "In this article, the authors discuss new trends in powering pervasive computing systems and review what we have learned in the last 15 years of building such systems. Major advances in cloud computing, RF, power harvesting, and networking have enabled new asymmetric architectures that reduce our reliance on batteries. Although the authors see a slowing of Moore's law, and although battery density technology has remained relatively flat, there are still opportunities in improving energy efficiency. This article is part of a special issue on pervasive computing revisited.",
                "ieee_keywords": [
                    "Batteries",
                    "Radio frequency",
                    "Pervasive computing",
                    "Energy harvesting",
                    "Sensors",
                    "Mobile handsets",
                    "Mobile communication"
                ],
                "author_keywords": [
                    "power",
                    "sensing",
                    "mobile computing",
                    "low-power computing",
                    "pervasive computing",
                    "mobile",
                    "green computing",
                    "hardware"
                ]
            },
            {
                "title": "Design and Performance of an Optimal Inertial Power Harvester for Human-Powered Devices",
                "link": "https://ieeexplore.ieee.org/document/5611532/",
                "date_of_publication": null,
                "doi": "10.1109/TMC.2010.202",
                "citations": "61",
                "abstract": "We present an empirical study of the long-term practicality of using human motion to generate operating power for body-mounted consumer electronics and health sensors. We have collected a large continuous acceleration data set from eight experimental subjects going about their normal daily routine for three days each. Each subject is instrumented with a data collection apparatus that simultaneously logs 3-axis, 80 Hz acceleration data from six body locations. We use this data set to optimize a first-principles physical model of the commonly used velocity damped resonant generator (VDRG) by selecting physical parameters such as resonant frequency and damping coefficient to maximize the harvested power. Our results show that with reasonable assumptions on size, mass, placement, and efficiency of VDRG harvesters, most body-mounted wireless sensors and even some consumer electronics devices can be powered continuously and indefinitely from everyday motion. We have optimized the power harvesters for each individual and for each body location. In addition, we present the potential of designing a damping- and frequency-tunable power harvester that could mitigate the power reduction of a generator generalized for \"average” subjects. We present the full details on the collection of the acceleration data sets, the development of the VDRG model, and a numerical simulator, and discuss some of the future challenges that remain in this promising field of research.",
                "ieee_keywords": [
                    "Generators",
                    "Acceleration",
                    "Humans",
                    "Accelerometers",
                    "Resonant frequency",
                    "Sensors",
                    "Damping"
                ],
                "author_keywords": [
                    "Human power harvesting",
                    "inertial generator",
                    "optimal design",
                    "tunable generator",
                    "human-powered device."
                ]
            }
        ]
    },
    {
        "name": "Sewoong Oh",
        "publications": [
            {
                "title": "Joint Channel Coding and Modulation via Deep Learning",
                "link": "https://ieeexplore.ieee.org/document/9153885/",
                "date_of_publication": "03 August 2020",
                "doi": "10.1109/SPAWC48557.2020.9153885",
                "citations": "10",
                "abstract": "Channel coding and modulation are two fundamental building blocks of physical layer wireless communications. We propose a neural network based end-to-end communication system, where both the channel coding and the modulation blocks are modeled as neural networks. Our proposed architecture combines Turbo Autoencoder together with feed-forward neural networks for modulation, and hence called TurboAE-MOD. Turbo Autoencoder was introduced in [1] and consists of a neural network based channel encoder (convolutional neural networks with an interleaver) and a neural network based decoder (iterations of convolutional neural networks with interleavers and de-interleavers in between). By allowing joint training of the channel coding and modulation in an end-to-end manner, we demonstrate that TurboAE-MOD performs comparable to modern codes stacked with canonical modulations for moderate block lengths. We also demonstrate that TurboAE-MOD learns interesting modulation patterns that are amenable to meaningful interpretations.",
                "ieee_keywords": [
                    "Modulation",
                    "Decoding",
                    "Neural networks",
                    "Training",
                    "Machine learning",
                    "Turbo codes"
                ],
                "author_keywords": [
                    "Channel Coding",
                    "Modulation",
                    "Autoencoder",
                    "Turbo Principle",
                    "Deep Learning"
                ]
            },
            {
                "title": "Feedback Turbo Autoencoder",
                "link": "https://ieeexplore.ieee.org/document/9053254/",
                "date_of_publication": "09 April 2020",
                "doi": "10.1109/ICASSP40776.2020.9053254",
                "citations": "6",
                "abstract": "Designing channel codes is one of the core research areas for modern communication systems. Canonical channel codes asymptotically achieve near-capacity performance under large block length regime for additive white gaussian noise channels. However, this achieved success does not generalize to many channels. Channels with output feedback, proposed by Shannon, is one of such channels where practical codes have been unknown for several decades.Recently it has been demonstrated that deep learning based code outperforms the state-of-the-art codes for channels with output feedback. While the success is promising and inspiring, there are a few major challenges that need to be addressed. Firstly, the channel assumes a feedback with a unit step delay, which is not very practical. Second is the lack of generalization to larger block lengths. In this work, we propose Feedback Auto Turbo Encoder (FTAE) which harmoniously combines interleaver and iterative decoding with CNN architectures and demonstrate the blocklength gain and improved performance in the block feedback setting.",
                "ieee_keywords": [
                    "Deep learning",
                    "Conferences",
                    "Signal processing",
                    "Delays",
                    "Speech processing",
                    "Output feedback",
                    "Iterative decoding"
                ],
                "author_keywords": [
                    "Information Theory",
                    "Channel Coding",
                    "Deep Learning",
                    "Feedback Channel",
                    "Autoencoder"
                ]
            },
            {
                "title": "DEEPTURBO: Deep Turbo Decoder",
                "link": "https://ieeexplore.ieee.org/document/8815400/",
                "date_of_publication": "29 August 2019",
                "doi": "10.1109/SPAWC.2019.8815400",
                "citations": "28",
                "abstract": "Present-day communication systems routinely use codes that approach the channel capacity when coupled with a computationally efficient decoder. However, the decoder is typically designed for the Gaussian noise channel, and is known to be sub-optimal for non-Gaussian noise distribution. Deep learning methods offer a new approach for designing decoders that can be trained and tailored for arbitrary channel statistics. We focus on Turbo codes, and propose (DEEPTURBO), a novel deep learning based architecture for Turbo decoding. The standard Turbo decoder (TURBO) iteratively applies the Bahl-Cocke-Jelinek-Raviv (BCJR) algorithm with an interleaver in the middle. A neural architecture for TURBO decoding, termed (NEURALBCJR), was proposed recently to create a module that imitates the BCJR algorithm using supervised learning, and to use the interleaver architecture along with this module, which is then fine-tuned using end-to-end training. However, knowledge of the BCJR algorithm is required to design such an architecture, which also constrains the resulting learnt decoder. Here we remedy this requirement and propose a fully end-to-end trained neural decoder - Deep Turbo Decoder (DEEPTURBO). With novel learnable decoder structure and training methodology, DEEPTURBO reveals superior performance under both AWGN and non-AWGN settings as compared to the other two decoders - TURBOand NEURALBCJR.",
                "ieee_keywords": [
                    "Decoding",
                    "Iterative decoding",
                    "Signal processing algorithms",
                    "Training",
                    "Turbo codes",
                    "Signal to noise ratio",
                    "AWGN channels"
                ],
                "author_keywords": []
            },
            {
                "title": "LEARN Codes: Inventing Low-Latency Codes via Recurrent Neural Networks",
                "link": "https://ieeexplore.ieee.org/document/8761286/",
                "date_of_publication": "15 July 2019",
                "doi": "10.1109/ICC.2019.8761286",
                "citations": "20",
                "abstract": "Designing channel codes under low latency constraints is one of the most demanding requirements in 5G standards. However, sharp characterizations of the performances of traditional codes are only available in the large block lengths limit. Code designs are guided by those asymptotic analyses and require large block lengths and long latency to achieve the desired error rate. Furthermore, when the codes designed for one channel (e.g. Additive White Gaussian Noise (AWGN) channel) are used for another (e.g. non-AWGN channels), heuristics are necessary to achieve any non trivial performance - thereby severely lacking in robustness as well as adaptivity. Obtained by jointly designing recurrent neural network (RNN) based encoder and decoder, we propose an end-to-end learned neural code which outperforms canonical convolutional code under block settings. With this gained experience of designing a novel neural block code, we propose a new class of codes under low latency constraint - Low-latency Efficient Adaptive Robust Neural (LEARN) codes, which outperform the state-of-the-art low latency codes as well as exhibit robustness and adaptivity properties. LEARN codes show the potential of designing new versatile and universal codes for future communications via tools of modern deep learning coupled with communication engineering insights.",
                "ieee_keywords": [
                    "Decoding",
                    "Recurrent neural networks",
                    "Training",
                    "Convolutional codes",
                    "Deep learning",
                    "Robustness",
                    "Block codes"
                ],
                "author_keywords": []
            },
            {
                "title": "LEARN Codes: Inventing Low-Latency Codes via Recurrent Neural Networks",
                "link": "https://ieeexplore.ieee.org/document/9070167/",
                "date_of_publication": null,
                "doi": "10.1109/JSAIT.2020.2988577",
                "citations": "20",
                "abstract": "Designing channel codes under low-latency constraints is one of the most demanding requirements in 5G standards. However, a sharp characterization of the performance of traditional codes is available only in the large block-length limit. Guided by such asymptotic analysis, code designs require large block lengths as well as latency to achieve the desired error rate. Tail-biting convolutional codes and other recent state-of-the-art short block codes, while promising reduced latency, are neither robust to channel-mismatch nor adaptive to varying channel conditions. When the codes designed for one channel (e.g., Additive White Gaussian Noise (AWGN) channel) are used for another (e.g., non-AWGN channels), heuristics are necessary to achieve non-trivial performance. In this paper, we first propose an end-to-end learned neural code, obtained by jointly designing a Recurrent Neural Network (RNN) based encoder and decoder. This code outperforms canonical convolutional code under block settings. We then leverage this experience to propose a new class of codes under low-latency constraints, which we call Low-latency Efficient Adaptive Robust Neural (LEARN) codes. These codes outperform state-of-the-art low-latency codes and exhibit robustness and adaptivity properties. LEARN codes show the potential to design new versatile and universal codes for future communications via tools of modern deep learning coupled with communication engineering insights.",
                "ieee_keywords": [
                    "Decoding",
                    "Convolutional codes",
                    "Robustness",
                    "Recurrent neural networks",
                    "Training",
                    "Delays",
                    "AWGN channels"
                ],
                "author_keywords": [
                    "Channel coding",
                    "low latency",
                    "communications",
                    "deep learning",
                    "robustness",
                    "adaptivity"
                ]
            },
            {
                "title": "Deepcode: Feedback Codes via Deep Learning",
                "link": "https://ieeexplore.ieee.org/document/9062338/",
                "date_of_publication": null,
                "doi": "10.1109/JSAIT.2020.2986752",
                "citations": "20",
                "abstract": "The design of codes for communicating reliably over a statistically well defined channel is an important endeavor involving deep mathematical research and wide-ranging practical applications. In this work, we present the first family of codes obtained via deep learning, which significantly outperforms state-of-the-art codes designed over several decades of research. The communication channel under consideration is the Gaussian noise channel with feedback, whose study was initiated by Shannon; feedback is known theoretically to improve reliability of communication, but no practical codes that do so have ever been successfully constructed. We break this logjam by integrating information theoretic insights harmoniously with recurrent-neural-network based encoders and decoders to create novel codes that outperform known codes by 3 orders of magnitude in reliability and achieve a 3dB gain in terms of SNR. We also demonstrate several desirable properties of the codes: (a) generalization to larger block lengths, (b) composability with known codes, and (c) adaptation to practical constraints. This result also has broader ramifications for coding theory: even when the channel has a clear mathematical model, deep learning methodologies, when combined with channel-specific information-theoretic insights, can potentially beat state-of-the-art codes constructed over decades of mathematical research.",
                "ieee_keywords": [
                    "Noise measurement",
                    "Decoding",
                    "Reliability theory",
                    "Machine learning",
                    "AWGN channels"
                ],
                "author_keywords": [
                    "Channel coding",
                    "deep learning",
                    "neural networks",
                    "recurrent neural networks",
                    "feedback communication",
                    "Schalkwijk–Kailath scheme"
                ]
            },
            {
                "title": "Reed-Muller Subcodes: Machine Learning-Aided Design of Efficient Soft Recursive Decoding",
                "link": "https://ieeexplore.ieee.org/document/9517885/",
                "date_of_publication": "01 September 2021",
                "doi": "10.1109/ISIT45174.2021.9517885",
                "citations": "7",
                "abstract": "Reed-Muller (RM) codes are conjectured to achieve the capacity of any binary-input memoryless symmetric (BMS) channel, and are observed to have a comparable performance to that of random codes in terms of scaling laws. On the negative side, RM codes lack efficient decoders with performance close to that of a maximum likelihood decoder for general parameters. Also, they only admit certain discrete sets of rates. In this paper, we focus on subcodes of RM codes with flexible rates that can take any code dimension from 1 to $n$ . where $n$ is the blocklength. We first extend the recursive projection-aggregation (RPA) algorithm proposed recently by Ye and Abbe for decoding RM codes. To lower the complexity of our decoding algorithm, referred to as subRPA, we investigate different ways for pruning the projections. We then derive the soft-decision based version of our algorithm, called soft-subRPA, that is shown to improve upon the performance of subRPA. Furthermore, it enables training a machine learning (ML) model to search for good sets of projections that minimize the decoding error rate. Training our ML model enables achieving very close to the performance of full-projection decoding with a significantly reduced number of projections. For instance, our simulation results on a (64,14) RM subcode show almost identical performance for full-projection decoding and pruned-projection decoding with 15 projections picked via training our ML model. This is equivalent to lowering the complexity by a factor of more than 4 without sacrificing the decoding performance.",
                "ieee_keywords": [
                    "Training",
                    "Machine learning algorithms",
                    "Error analysis",
                    "Simulation",
                    "Machine learning",
                    "Complexity theory",
                    "Maximum likelihood decoding"
                ],
                "author_keywords": []
            },
            {
                "title": "Leveraging Synergies Between AI and Networking to Build Next Generation Edge Networks",
                "link": "https://ieeexplore.ieee.org/document/10061749/",
                "date_of_publication": "13 March 2023",
                "doi": "10.1109/CIC56439.2022.00013",
                "citations": "170",
                "abstract": "Networking and Artificial Intelligence (AI) are two of the most transformative information technologies over the last few decades. Building upon the synergies of these two powerful technologies, we envision designing next generation of edge networks to be highly efficient, reliable, robust and secure. To this end, in this paper, we delve into interesting and fundamental research challenges and opportunities that span two major broad and symbiotic areas: AI for Networks and Networks for AI. The former deals with the development of new AI tools and techniques that can enable the next generation AI-assisted networks; while the latter focuses on developing networking techniques and tools that will facilitate the vision of distributed intelligence, resulting in a virtuous research cycle where advances in one will help accelerate advances in the other. A wide range of applications will be further discussed to illustrate the importance of the foundational advances developed in these two areas.",
                "ieee_keywords": [
                    "Wireless communication",
                    "Symbiosis",
                    "Privacy",
                    "Uncertainty",
                    "Program processors",
                    "Reliability engineering",
                    "Security"
                ],
                "author_keywords": []
            },
            {
                "title": "Machine Learning-Aided Efficient Decoding of Reed–Muller Subcodes",
                "link": "https://ieeexplore.ieee.org/document/10193768/",
                "date_of_publication": null,
                "doi": "10.1109/JSAIT.2023.3298362",
                "citations": "41",
                "abstract": "Reed-Muller (RM) codes achieve the capacity of general binary-input memoryless symmetric channels and are conjectured to have a comparable performance to that of random codes in terms of scaling laws. However, such results are established assuming maximum-likelihood decoders for general code parameters. Also, RM codes only admit limited sets of rates. Efficient decoders such as successive cancellation list (SCL) decoder and recently-introduced recursive projection-aggregation (RPA) decoders are available for RM codes at finite lengths. In this paper, we focus on subcodes of RM codes with flexible rates. We first extend the RPA decoding algorithm to RM subcodes. To lower the complexity of our decoding algorithm, referred to as subRPA, we investigate different approaches to prune the projections. Next, we derive the soft-decision based version of our algorithm, called soft-subRPA, that not only improves upon the performance of subRPA but also enables a differentiable decoding algorithm. Building upon the soft-subRPA algorithm, we then provide a framework for training a machine learning (ML) model to search for good sets of projections that minimize the decoding error rate. Training our ML model enables achieving very close to the performance of full-projection decoding with a significantly smaller number of projections. We also show that the choice of the projections in decoding RM subcodes matters significantly, and our ML-aided projection pruning scheme is able to find a good selection, i.e., with negligible performance degradation compared to the full-projection case, given a reasonable number of projections. Topic: Dimensions of Channel Coding: Special Issue Dedicated to the Memory of Alexander Vardy",
                "ieee_keywords": [
                    "Codes",
                    "Maximum likelihood decoding",
                    "Complexity theory",
                    "Machine learning algorithms",
                    "Generators",
                    "Training",
                    "Polar codes"
                ],
                "author_keywords": [
                    "Reed-muller (RM) codes",
                    "machine learning",
                    "low-complexity decoding",
                    "recursive projection-aggregation (RPA) decoding",
                    "projection pruning"
                ]
            },
            {
                "title": "PacGAN: The Power of Two Samples in Generative Adversarial Networks",
                "link": "https://ieeexplore.ieee.org/document/9046238/",
                "date_of_publication": null,
                "doi": "10.1109/JSAIT.2020.2983071",
                "citations": "26",
                "abstract": "Generative adversarial networks (GANs) are innovative techniques for learning generative models of complex data distributions from samples. Despite remarkable improvements in generating realistic images, one of their major shortcomings is the fact that in practice, they tend to produce samples with little diversity, even when trained on diverse datasets. This phenomenon, known as mode collapse, has been the main focus of several recent advances in GANs. Yet there is little understanding of why mode collapse happens and why recently-proposed approaches mitigate mode collapse. We propose a principled approach to handle mode collapse called packing . The main idea is to modify the discriminator to make decisions based on multiple samples from the same class, either real or artificially generated. We borrow analysis tools from binary hypothesis testing—in particular the seminal result of (Blackwell, 1953)—to prove a fundamental connection between packing and mode collapse. We show that packing naturally penalizes generators with mode collapse, thereby favoring generator distributions with less mode collapse during the training process. Numerical experiments on benchmark datasets suggests that packing provides significant improvements in practice as well.",
                "ieee_keywords": [
                    "Generators",
                    "Gallium nitride",
                    "Training",
                    "Testing",
                    "Data models",
                    "Neural networks",
                    "Information theory"
                ],
                "author_keywords": [
                    "Generative adversarial networks",
                    "mode collapse",
                    "hypothesis testing",
                    "data processing inequalities"
                ]
            }
        ]
    },
    {
        "name": "Arvind Krishnamurthy",
        "publications": [
            {
                "title": "A Hardware–Software Blueprint for Flexible Deep Learning Specialization",
                "link": "https://ieeexplore.ieee.org/document/8764458/",
                "date_of_publication": null,
                "doi": "10.1109/MM.2019.2928962",
                "citations": "67",
                "abstract": "This article describes the Versatile Tensor Accelerator (VTA), a programmable DL architecture designed to be extensible in the face of evolving workloads. VTA achieves “flexible specialization” via a parameterizable architecture, two-level Instruction Set Architecture (ISA), and a Just in Time (JIT) compiler.",
                "ieee_keywords": [
                    "Machine learning",
                    "Computer architecture",
                    "Runtime",
                    "Task analysis",
                    "Computational modeling",
                    "Deep learning"
                ],
                "author_keywords": []
            },
            {
                "title": "MetaSync: Coordinating Storage across Multiple File Synchronization Services",
                "link": "https://ieeexplore.ieee.org/document/7436648/",
                "date_of_publication": null,
                "doi": "10.1109/MIC.2016.44",
                "citations": "5",
                "abstract": "Cloud-based file synchronization services such as Dropbox are a worldwide resource for many millions of users. However, individual services often have tight resource limits, suffer from temporary outages or shutdowns, and sometimes silently corrupt or leak user data. As a solution, the authors design, implement, and evaluate MetaSync, a secure and reliable file synchronization service using multiple cloud synchronization services as untrusted storage providers. To provide global consistency among the storage providers, the authors devise a novel variant of Paxos that enables efficient updates on top of the unmodified APIs exported by each service. MetaSync provides better availability and performance, stronger confidentiality and integrity, and larger storage for end users.",
                "ieee_keywords": [
                    "Synchronization",
                    "Cloud computing",
                    "Metadata",
                    "Proposals",
                    "Reliability",
                    "Servers",
                    "Storage automation",
                    "Internet and Web services"
                ],
                "author_keywords": [
                    "Internet/Web technologies",
                    "cloud computing",
                    "file synchronization",
                    "cloud storage",
                    "reliability",
                    "consensus"
                ]
            },
            {
                "title": "Unifying FSM-inference algorithms through declarative specification",
                "link": "https://ieeexplore.ieee.org/document/6606571/",
                "date_of_publication": "26 September 2013",
                "doi": "10.1109/ICSE.2013.6606571",
                "citations": "28",
                "abstract": "Logging system behavior is a staple development practice. Numerous powerful model inference algorithms have been proposed to aid developers in log analysis and system understanding. Unfortunately, existing algorithms are difficult to understand, extend, and compare. This paper presents InvariMint, an approach to specify model inference algorithms declaratively. We applied InvariMint to two model inference algorithms and present evaluation results to illustrate that InvariMint (1) leads to new fundamental insights and better understanding of existing algorithms, (2) simplifies creation of new algorithms, including hybrids that extend existing algorithms, and (3) makes it easy to compare and contrast previously published algorithms. Finally, algorithms specified with InvariMint can outperform their procedural versions.",
                "ieee_keywords": [
                    "Inference algorithms",
                    "Postal services",
                    "Approximation algorithms",
                    "Algorithm design and analysis",
                    "Doped fiber amplifiers",
                    "Educational institutions",
                    "Electronic mail"
                ],
                "author_keywords": []
            },
            {
                "title": "Using Declarative Specification to Improve the Understanding, Extensibility, and Comparison of Model-Inference Algorithms",
                "link": "https://ieeexplore.ieee.org/document/6951474/",
                "date_of_publication": null,
                "doi": "10.1109/TSE.2014.2369047",
                "citations": "35",
                "abstract": "It is a staple development practice to log system behavior. Numerous powerful model-inference algorithms have been proposed to aid developers in log analysis and system understanding. Unfortunately, existing algorithms are typically declared procedurally, making them difficult to understand, extend, and compare. This paper presents InvariMint, an approach to specify model-inference algorithms declaratively. We applied the InvariMint declarative approach to two model-inference algorithms. The evaluation results illustrate that InvariMint (1) leads to new fundamental insights and better understanding of existing algorithms, (2) simplifies creation of new algorithms, including hybrids that combine or extend existing algorithms, and (3) makes it easy to compare and contrast previously published algorithms. InvariMint's declarative approach can outperform procedural implementations. For example, on a log of 50,000 events, InvariMint's declarative implementation of the kTails algorithm completes in 12 seconds, while a procedural implementation completes in 18 minutes. We also found that InvariMint's declarative version of the Synoptic algorithm can be over 170 times faster than the procedural implementation.",
                "ieee_keywords": [
                    "Inference algorithms",
                    "Postal services",
                    "Electronic mail",
                    "Algorithm design and analysis",
                    "Software algorithms",
                    "Educational institutions",
                    "Approximation algorithms"
                ],
                "author_keywords": [
                    "Model inference",
                    "API mining",
                    "specification mining",
                    "process mining",
                    "declarative specification",
                    "inference understanding",
                    "inference extensibility",
                    "inference comparison",
                    "InvariMint",
                    "kTails",
                    "synoptic",
                    "Model inference",
                    "API mining",
                    "specification mining",
                    "process mining",
                    "declarative specification",
                    "inference understanding",
                    "inference extensibility",
                    "inference comparison",
                    "InvariMint",
                    "kTails",
                    "synoptic"
                ]
            },
            {
                "title": "Fast Video Classification via Adaptive Cascading of Deep Models",
                "link": "https://ieeexplore.ieee.org/document/8099719/",
                "date_of_publication": "09 November 2017",
                "doi": "10.1109/CVPR.2017.236",
                "citations": "36",
                "abstract": "Recent advances have enabled oracle classifiers that can classify across many classes and input distributions with high accuracy without retraining. However, these classifiers are relatively heavyweight, so that applying them to classify video is costly. We show that day-to-day video exhibits highly skewed class distributions over the short term, and that these distributions can be classified by much simpler models. We formulate the problem of detecting the short-term skews online and exploiting models based on it as a new sequential decision making problem dubbed the Online Bandit Problem, and present a new algorithm to solve it. When applied to recognizing faces in TV shows and movies, we realize end-to-end classification speedups of 2.4-7.8x/2.6-11.2x (on GPU/CPU) relative to a state-of-the-art convolutional neural network, at competitive accuracy.",
                "ieee_keywords": [
                    "Face recognition",
                    "Training",
                    "Motion pictures",
                    "Adaptation models",
                    "Neural networks",
                    "Cameras"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Michael I. Jordan",
        "publications": [
            {
                "title": "TOI-431/HIP 26013: a super-Earth and a sub-Neptune transiting a bright, early K dwarf, with a third RV planet",
                "link": "https://ieeexplore.ieee.org/document/9584212/",
                "date_of_publication": null,
                "doi": "10.1093/mnras/stab2313",
                "citations": "Abstract",
                "abstract": "We present the bright (V mag = 9.12), multiplanet system TOI-431, characterized with photometry and radial velocities (RVs). We estimate the stellar rotation period to be 30.5 ± 0.7 d using archival photometry and RVs. Transiting Exoplanet Survey Satellite (TESS) objects of Interest (TOI)-431 b is a super-Earth with a period of 0.49 d, a radius of 1.28 ± 0.04 R ⊕ , a mass of 3.07 ± 0.35 M ⊕ , and a density of 8.0 ± 1.0 g cm −3 ; TOI-431 d is a sub-Neptune with a period of 12.46 d, a radius of 3.29 ± 0.09 R ⊕ , a mass of $9.90^{+1.53}_{-1.49}$ M ⊕ , and a density of 1.36 ± 0.25 g cm −3 . We find a third planet, TOI-431 c, in the High Accuracy Radial velocity Planet Searcher RV data, but it is not seen to transit in the TESS light curves. It has an Msin i of $2.83^{+0.41}_{-0.34}$ M ⊕ , and a period of 4.85 d. TOI-431 d likely has an extended atmosphere and is one of the most well-suited TESS discoveries for atmospheric characterization, while the super-Earth TOI-431 b may be a stripped core. These planets straddle the radius gap, presenting an interesting case-study for atmospheric evolution, and TOI-431 b is a prime TESS discovery for the study of rocky planet phase curves.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "SOUL: An Energy-Efficient Unsupervised Online Learning Seizure Detection Classifier",
                "link": "https://ieeexplore.ieee.org/document/9773289/",
                "date_of_publication": null,
                "doi": "10.1109/JSSC.2022.3172231",
                "citations": "2",
                "abstract": "Implantable devices that record neural activity and detect seizures have been adopted to issue warnings or trigger neurostimulation to suppress epileptic seizures. Typical seizure detection systems rely on high-accuracy offline-trained machine learning classifiers that require manual retraining when seizure patterns change over long periods of time. For an implantable seizure detection system, a low-power, at-the-edge, online learning algorithm can be employed to dynamically adapt to the neural signal drifts, thereby maintaining high accuracy without external intervention. This work proposes SOUL: Stochastic-gradient-descent-based Online Unsupervised Logistic regression classifier. After an initial offline training phase, continuous online unsupervised classifier updates are applied in situ , which improves sensitivity in patients with drifting seizure features. SOUL was tested on two human electroencephalography (EEG) datasets: the Children’s Hospital Boston and the Massachusetts Institute of Technology (CHB-MIT) scalp EEG dataset and a long ( $\\!>$ 100 h) intracranial EEG dataset. It was able to achieve an average sensitivity of 97.5% and 97.9% for the two datasets, respectively, at >95% specificity. Sensitivity improved by at most 8.2% on long-term data when compared to a typical seizure detection classifier. SOUL was fabricated in Taiwan Semiconductor Manufacturing Company (TSMC’s) 28 nm process occupying 0.1 mm 2 and achieves 1.5 nJ/classification energy efficiency, which is at least $24\\times $ more efficient than state-of-the-art.",
                "ieee_keywords": [
                    "Logistics",
                    "Training",
                    "Electroencephalography",
                    "Feature extraction",
                    "System-on-chip",
                    "Optimization",
                    "Sensitivity"
                ],
                "author_keywords": [
                    "Classification",
                    "logistic regression",
                    "online learning",
                    "seizure detection",
                    "stochastic gradient descent (SGD)"
                ]
            }
        ]
    },
    {
        "name": "Yi Ma (马毅)",
        "publications": [
            {
                "title": "ASASSN-18am/SN 2018gk: an overluminous Type IIb supernova from a massive progenitor",
                "link": "https://ieeexplore.ieee.org/document/9491295/",
                "date_of_publication": null,
                "doi": "10.1093/mnras/stab629",
                "citations": "Abstract",
                "abstract": "ASASSN-18am/SN 2018gk is a newly discovered member of the rare group of luminous, hydrogen-rich supernovae (SNe) with a peak absolute magnitude of M V ≈ −20 mag that is in between normal core-collapse SNe and superluminous SNe. These SNe show no prominent spectroscopic signatures of ejecta interacting with circumstellar material (CSM), and their powering mechanism is debated. ASASSN-18am declines extremely rapidly for a Type II SN, with a photospheric-phase decline rate of ∼6.0 mag (100 d) −1 . Owing to the weakening of H i and the appearance of He i in its later phases, ASASSN-18am is spectroscopically a Type IIb SN with a partially stripped envelope. However, its photometric and spectroscopic evolution shows significant differences from typical SNe IIb. Using a radiative diffusion model, we find that the light curve requires a high synthesized 56 Ni mass $M_{\\rm Ni} \\sim 0.4\\, \\rm {M_{\\odot }}$ and ejecta with high kinetic energy E kin = (7–10) × 10 51  erg. Introducing a magnetar central engine still requires $M_{\\rm Ni} \\sim 0.3\\, \\rm {M_{\\odot }}$ and E kin = 3 × 10 51  erg. The high 56 Ni mass is consistent with strong iron-group nebular lines in its spectra, which are also similar to several SNe Ic-BL with high 56 Ni yields. The earliest spectrum shows ‘flash ionization’ features, from which we estimate a mass-loss rate of $\\dot{M}\\approx 2\\times 10^{-4} \\, \\rm \\rm {M_{\\odot }}\\,yr^{-1}$ . This wind density is too low to power the luminous light curve by ejecta–CSM interaction. We measure expansion velocities as high as 17 000  $\\rm {\\, km\\, s^{-1}}$ for Hα, which is remarkably high compared to other SNe II. We estimate an oxygen core mass of 1.8–3.4 M ⊙ using the [O i] luminosity measured from a nebular-phase spectrum, implying a progenitor with a zero-age main-sequence mass of 19–26 M ⊙ .",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "NeRD: Neural 3D Reflection Symmetry Detector",
                "link": "https://ieeexplore.ieee.org/document/9577295/",
                "date_of_publication": "02 November 2021",
                "doi": "10.1109/CVPR46437.2021.01568",
                "citations": "8",
                "abstract": "Recent advances have shown that symmetry, a structural prior that most objects exhibit, can support a variety of single-view 3D understanding tasks. However, detecting 3D symmetry from an image remains a challenging task. Previous works either assume the symmetry is given or detect the symmetry with a heuristic-based method. In this paper, we present NeRD, a Neural 3D Reflection Symmetry Detector, which combines the strength of learning-based recognition and geometry-based reconstruction to accurately recover the normal direction of objects’ mirror planes. Specifically, we enumerate the symmetry planes with a coarse-to-fine strategy and find the best ones by building 3D cost volumes to examine the intra-image pixel correspondence from the symmetry. Our experiments show that the symmetry planes detected with our method are significantly more accurate than the planes from direct CNN regression on both synthetic and real datasets. More importantly, we also demonstrate that the detected symmetry can be used to improve the performance of downstream tasks such as pose estimation and depth map regression by a wide margin over existing methods. The code of this paper has been made public at https://github.com/zhou13/nerd.",
                "ieee_keywords": [
                    "Computer vision",
                    "Three-dimensional displays",
                    "Costs",
                    "Pose estimation",
                    "Detectors",
                    "Reflection",
                    "Pattern recognition"
                ],
                "author_keywords": []
            },
            {
                "title": "Learning to Parse Wireframes in Images of Man-Made Environments",
                "link": "https://ieeexplore.ieee.org/document/8578170/",
                "date_of_publication": "16 December 2018",
                "doi": "10.1109/CVPR.2018.00072",
                "citations": "80",
                "abstract": "In this paper, we propose a learning-based approach to the task of automatically extracting a \"wireframe\" representation for images of cluttered man-made environments. The wireframe (see Fig. 1) contains all salient straight lines and their junctions of the scene that encode efficiently and accurately large-scale geometry and object shapes. To this end, we have built a very large new dataset of over 5,000 images with wireframes thoroughly labelled by humans. We have proposed two convolutional neural networks that are suitable for extracting junctions and lines with large spatial support, respectively. The networks trained on our dataset have achieved significantly better performance than state-of-the-art methods for junction detection and line segment detection, respectively. We have conducted extensive experiments to evaluate quantitatively and qualitatively the wireframes obtained by our method, and have convincingly shown that effectively and efficiently parsing wireframes for images of man-made environments is a feasible goal within reach. Such wireframes could benefit many important visual tasks such as feature correspondence, 3D reconstruction, vision-based mapping, localization, and navigation. The data and source code are available at https://github.com/huangkuns/wireframe.",
                "ieee_keywords": [
                    "Junctions",
                    "Image segmentation",
                    "Three-dimensional displays",
                    "Feature extraction",
                    "Image edge detection",
                    "Geometry",
                    "Task analysis"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Eric Paulos",
        "publications": [
            {
                "title": "Dark energy survey year 1 results: Constraining baryonic physics in the Universe",
                "link": "https://ieeexplore.ieee.org/document/9480579/",
                "date_of_publication": null,
                "doi": "10.1093/mnras/stab357",
                "citations": "Abstract",
                "abstract": "Measurements of large-scale structure are interpreted using theoretical predictions for the matter distribution, including potential impacts of baryonic physics. We constrain the feedback strength of baryons jointly with cosmology using weak lensing and galaxy clustering observables (3 × 2pt) of Dark Energy Survey (DES) Year 1 data in combination with external information from baryon acoustic oscillations (BAO) and Planck cosmic microwave background polarization. Our baryon modelling is informed by a set of hydrodynamical simulations that span a variety of baryon scenarios; we span this space via a Principal Component (PC) analysis of the summary statistics extracted from these simulations. We show that at the level of DES Y1 constraining power, one PC is sufficient to describe the variation of baryonic effects in the observables, and the first PC amplitude (Q 1 ) generally reflects the strength of baryon feedback. With the upper limit of Q 1 prior being bound by the Illustris feedback scenarios, we reach $\\sim 20{{\\ \\rm per\\ cent}}$ improvement in the constraint of $S_8=\\sigma _8(\\Omega _{\\rm m}/0.3)^{0.5}=0.788^{+0.018}_{-0.021}$ compared to the original DES 3 × 2pt analysis. This gain is driven by the inclusion of small-scale cosmic shear information down to 2.5 arcmin, which was excluded in previous DES analyses that did not model baryonic physics. We obtain $S_8=0.781^{+0.014}_{-0.015}$ for the combined DES Y1+Planck EE+BAO analysis with a non-informative Q 1 prior. In terms of the baryon constraints, we measure $Q_1=1.14^{+2.20}_{-2.80}$ for DES Y1 only and $Q_1=1.42^{+1.63}_{-1.48}$ for DESY1+Planck EE+BAO, allowing us to exclude one of the most extreme AGN feedback hydrodynamical scenario at more than 2σ.",
                "ieee_keywords": [],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Martin Wainwright",
        "publications": [
            {
                "title": "An Efficient 10GBASE-T Ethernet LDPC Decoder Design With Low Error Floors",
                "link": "https://ieeexplore.ieee.org/document/5437474/",
                "date_of_publication": null,
                "doi": "10.1109/JSSC.2010.2042255",
                "citations": "112",
                "abstract": "A grouped-parallel low-density parity-check (LDPC) decoder is designed for the (2048,1723) Reed-Solomon-based LDPC (RS-LDPC) code suitable for 10GBASE-T Ethernet. A two-step decoding scheme reduces the wordlength to 4 bits while lowering the error floor to below 10 $^{-14}~$BER. The proposed post-processor is conveniently integrated with the decoder, adding minimal area and power. The decoder architecture is optimized by groupings so as to localize irregular interconnects and regularize global interconnects and the overall wiring overhead is minimized. The ${\\hbox {5.35~mm}}^{2}$, 65 nm CMOS chip achieves a decoding throughput of 47.7 Gb/s. With scaled frequency and voltage, the chip delivers a 6.67 Gb/s throughput necessary for 10GBASE-T while dissipating 144 mW of power.",
                "ieee_keywords": [
                    "Ethernet networks",
                    "Parity check codes",
                    "Iterative decoding",
                    "Integrated circuit interconnections",
                    "Wiring",
                    "Digital video broadcasting",
                    "Routing",
                    "Throughput",
                    "WiMAX",
                    "Silicon"
                ],
                "author_keywords": [
                    "Error floors",
                    "iterative decoder architecture",
                    "low-density parity-check (LDPC) code",
                    "message-passing decoding",
                    "post-processing"
                ]
            }
        ]
    },
    {
        "name": "David Wagner",
        "publications": [
            {
                "title": "The mass and galaxy distribution around SZ-selected clusters",
                "link": "https://ieeexplore.ieee.org/document/9584412/",
                "date_of_publication": null,
                "doi": "10.1093/mnras/stab2505",
                "citations": "Abstract",
                "abstract": "We present measurements of the radial profiles of the mass and galaxy number density around Sunyaev–Zel'dovich (SZ)-selected clusters using both weak lensing and galaxy counts. The clusters are selected from the Atacama Cosmology Telescope Data Release 5 and the galaxies from the Dark Energy Survey Year 3 data set. With signal-to-noise ratio of 62 (45) for galaxy (weak lensing) profiles over scales of about 0.2–20 h−1 Mpc, these are the highest precision measurements for SZ-selected clusters to date. Because SZ selection closely approximates mass selection, these measurements enable several tests of theoretical models of the mass and light distribution around clusters. Our main findings are: (1) The splashback feature is detected at a consistent location in both the mass and galaxy profiles and its location is consistent with predictions of cold dark matter N-body simulations. (2) The full mass profile is also consistent with the simulations. (3) The shapes of the galaxy and lensing profiles are remarkably similar for our sample over the entire range of scales, from well inside the cluster halo to the quasilinear regime. We measure the dependence of the profile shapes on the galaxy sample, redshift, and cluster mass. We extend the Diemer & Kravtsov model for the cluster profiles to the linear regime using perturbation theory and show that it provides a good match to the measured profiles. We also compare the measured profiles to predictions of the standard halo model and simulations that include hydrodynamics. Applications of these results to cluster mass estimation, cosmology, and astrophysics are discussed.",
                "ieee_keywords": [],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Kannan Ramchandran",
        "publications": [
            {
                "title": "A Real-Time, 1.89-GHz Bandwidth, 175-kHz Resolution Sparse Spectral Analysis RISC-V SoC in 16-nm FinFET",
                "link": "https://ieeexplore.ieee.org/document/8738896/",
                "date_of_publication": null,
                "doi": "10.1109/JSSC.2019.2913099",
                "citations": "4",
                "abstract": "A 1.89-GHz bandwidth, 175-kHz resolution spectral analysis system-on-chip (SoC), integrating a subsampling analog-to-digital converter (ADC) frontend with a digital reconstruction backend and implementing a 21 600-point sparse Fourier transform based on the fast Fourier aliasing-based sparse transform (FFAST) algorithm has been co-designed by using the Constructing Hardware in a Scala Embedded Language (Chisel) and Berkeley Analog Generator (BAG) circuit generator frameworks in 16-nm CMOS. Three sets of 25×, 27×, and 32× subsampling successive approximation register (SAR) ADCs acquire signal with ~5.4-6.3 effective number of bits (ENOB)/slice. The digital backend consists of mixed-radix 864-, 800-, and 675-point fast Fourier transforms (FFTs), a signal location estimator, and a peeling decoder that recovers aliased signals from a sparsely populated spectrum. A single-issue, in-order, fifth-generation reduced instruction set (RISC-V) Rocket processor interacts with the spectrum analyzer for post-processing and calibration. The ADC consumes 49.8 mW with a 3.78-GHz reference clock. At 400 MHz and 0.7-V digital supply voltage (VDD), the Rocket core and the FFAST digital signal processing (DSP) together consume 133.5 mW.",
                "ieee_keywords": [
                    "Time-frequency analysis",
                    "Bandwidth",
                    "Hardware",
                    "Time-domain analysis",
                    "Real-time systems",
                    "System-on-chip",
                    "Generators"
                ],
                "author_keywords": [
                    "Analog-to-digital converters (ADCs)",
                    "Berkeley Analog Generator (BAG)",
                    "Constructing Hardware in a Scala Embedded Language (Chisel)",
                    "fast Fourier transform (FFT)",
                    "hardware generators",
                    "fifth-generation reduced instruction set computer (RISC-V)",
                    "spectrum sensing"
                ]
            }
        ]
    },
    {
        "name": "Shankar Sastry",
        "publications": [
            {
                "title": "Markov Decision Process Routing Games",
                "link": "https://ieeexplore.ieee.org/document/7945016/",
                "date_of_publication": "12 June 2017",
                "doi": null,
                "citations": "399",
                "abstract": "We explore an extension of nonatomic routing games that we call Markov decision process routing games where each agent chooses a transition policy between nodes in a network rather than a path from an origin node to a destination node, i.e. each agent in the population solves a Markov decision process rather than a shortest path problem. We define the appropriate version of a Wardrop equilibrium as well as a potential function for this game in the finite horizon (total reward) case. This work can be thought of as a routing- game-based formulation of continuous population stochastic games (mean-field games or anonymous sequential games). We apply our model to the problem of ridesharing drivers competing for customers.",
                "ieee_keywords": [
                    "Games",
                    "Sociology",
                    "Statistics",
                    "Routing",
                    "Markov processes",
                    "Rats"
                ],
                "author_keywords": [
                    "Stochastic games",
                    "routing games",
                    "mean-field games",
                    "anonymous sequential games",
                    "Markov decision processes"
                ]
            }
        ]
    },
    {
        "name": "Edward A. LEE",
        "publications": [
            {
                "title": "Deploying Hard Real-Time Control Software on Chip-Multiprocessors",
                "link": "https://ieeexplore.ieee.org/document/5591875/",
                "date_of_publication": "30 September 2010",
                "doi": "10.1109/RTCSA.2010.43",
                "citations": "4",
                "abstract": "Deploying real-time control systems software on multiprocessors requires distributing tasks on multiple processing nodes and coordinating their executions using a protocol. One such protocol is the discrete-event (DE) model of computation. In this paper, we investigate distributed discrete-event (DE) with null-message protocol (NMP) on a multicore system for real-time control software. We illustrate analytically and experimentally that even with the null-message deadlock avoidance scheme in the protocol, the system can deadlock due to inter-core message dependencies. We identify two central reasons for such deadlocks: 1) the lack of an upper-bound on packet transmission rates and processing capability, and 2) an unknown upper-bound on the communication network delay. To address these, we propose using architectural features such as timing control and real-time network-on-chips to prevent such message-dependent deadlocks. We employ these architectural techniques in conjunction with a distributed DE strategy called PTIDES for an illustrative car wash station example and later follow it with a more realistic tunnelling ball device application.",
                "ieee_keywords": [
                    "System recovery",
                    "Delay",
                    "Real time systems",
                    "Jitter",
                    "Process control",
                    "Protocols"
                ],
                "author_keywords": [
                    "Real-time software",
                    "Chip-multiprocessors",
                    "Discrete-Event"
                ]
            }
        ]
    },
    {
        "name": "Yakun Sophia Shao",
        "publications": [
            {
                "title": "A 0.32–128 TOPS, Scalable Multi-Chip-Module-Based Deep Neural Network Inference Accelerator With Ground-Referenced Signaling in 16 nm",
                "link": "https://ieeexplore.ieee.org/document/8959403/",
                "date_of_publication": null,
                "doi": "10.1109/JSSC.2019.2960488",
                "citations": "44",
                "abstract": "Custom accelerators improve the energy efficiency, area efficiency, and performance of deep neural network (DNN) inference. This article presents a scalable DNN accelerator consisting of 36 chips connected in a mesh network on a multi-chip-module (MCM) using ground-referenced signaling (GRS). While previous accelerators fabricated on a single monolithic chip are optimal for specific network sizes, the proposed architecture enables flexible scaling for efficient inference on a wide range of DNNs, from mobile to data center domains. Communication energy is minimized with large on-chip distributed weight storage and a hierarchical network-on-chip and network-on-package, and inference energy is minimized through extensive data reuse. The 16-nm prototype achieves 1.29-TOPS/mm 2 area efficiency, 0.11 pJ/op (9.5 TOPS/W) energy efficiency, 4.01-TOPS peak performance for a one-chip system, and 127.8 peak TOPS and 1903 images/s ResNet-50 batch-1 inference for a 36-chip system.",
                "ieee_keywords": [
                    "Tensors",
                    "Neural networks",
                    "Computer architecture",
                    "System-on-chip",
                    "Prototypes",
                    "Convolution",
                    "Bandwidth"
                ],
                "author_keywords": [
                    "Deep neural networks (DNNs)",
                    "ground-referenced signaling (GRS)",
                    "inference accelerator",
                    "multi-chip modules",
                    "single-ended signaling"
                ]
            },
            {
                "title": "SNAP: An Efficient Sparse Neural Acceleration Processor for Unstructured Sparse Deep Neural Network Inference",
                "link": "https://ieeexplore.ieee.org/document/9310233/",
                "date_of_publication": null,
                "doi": "10.1109/JSSC.2020.3043870",
                "citations": "23",
                "abstract": "Recent developments in deep neural network (DNN) pruning introduces data sparsity to enable deep learning applications to run more efficiently on resourceand energy-constrained hardware platforms. However, these sparse models require specialized hardware structures to exploit the sparsity for storage, latency, and efficiency improvements to the full extent. In this work, we present the sparse neural acceleration processor (SNAP) to exploit unstructured sparsity in DNNs. SNAP uses parallel associative search to discover valid weight (W) and input activation (IA) pairs from compressed, unstructured, sparse W and IA data arrays. The associative search allows SNAP to maintain a 75% average compute utilization. SNAP follows a channel-first dataflow and uses a two-level partial sum (psum) reduction dataflow to eliminate access contention at the output buffer and cut the psum writeback traffic by 22× compared with state-of-the-art DNN accelerator designs. SNAP's psum reduction dataflow can be configured in two modes to support general convolution (CONV) layers, pointwise CONV, and fully connected layers. A prototype SNAP chip is implemented in a 16-nm CMOS technology. The 2.3-mm2 test chip is measured to achieve a peak effectual efficiency of 21.55 TOPS/W (16 b) at 0.55 V and 260 MHz for CONV layers with 10% weight and activation densities. Operating on a pruned ResNet-50 network, the test chip achieves a peak throughput of 90.98 frames/s at 0.80 V and 480 MHz, dissipating 348 mW.",
                "ieee_keywords": [
                    "Arrays",
                    "Indexes",
                    "Neural networks",
                    "Throughput",
                    "Convolution",
                    "Acceleration",
                    "Pipelines"
                ],
                "author_keywords": [
                    "Channel index matching",
                    "deep neural network (DNN)",
                    "energy-efficient accelerator",
                    "sparse neural network",
                    "unstructured sparsity"
                ]
            },
            {
                "title": "Guest Editorial Introduction to the Special Issue on the 2022 IEEE International Solid-State Circuits Conference (ISSCC)",
                "link": "https://ieeexplore.ieee.org/document/9999568/",
                "date_of_publication": null,
                "doi": "10.1109/JSSC.2022.3225691",
                "citations": "938",
                "abstract": "The International Technical Program Committee (ITPC) of the IEEE International Solid-State Circuits Conference (ISSCC) selects outstanding articles from the papers presented at the conference and invites the authors to submit an extended manuscript to the special issue of IEEE Journal of Solid- State Circuits (JSSC). This January issue contains the selected papers from the Digital Architectures and Systems, Digital Circuits, Machine Learning, Memory, and Wireline subcommittees.",
                "ieee_keywords": [],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Borivoje Nikolic",
        "publications": [
            {
                "title": "An Eight-Core 1.44-GHz RISC-V Vector Processor in 16-nm FinFET",
                "link": "https://ieeexplore.ieee.org/document/9606937/",
                "date_of_publication": null,
                "doi": "10.1109/JSSC.2021.3118046",
                "citations": "4",
                "abstract": "This work presents a RISC-V system-on-chip (SoC) with eight application cores containing programmable-precision vector accelerators. The SoC is built by using a generator-based design methodology, which enables the integration of open-source and project-specific building blocks to develop differentiated functionality. The digital component generators use Chisel, the analog component generators use the Berkeley Analog Generator (BAG), and the physical design flow is implemented with Hammer. The chip totals 125 M gates and is implemented in a 16-nm finFET process. The vector accelerator achieves peak energy efficiency per task of 209 half-precision, 92 single-precision, and 56 double-precision GFLOPs/W for a matrix multiplication kernel at 0.55 V and 339 MHz.",
                "ieee_keywords": [
                    "Clocks",
                    "Registers",
                    "Phase locked loops",
                    "Electrical engineering",
                    "Generators",
                    "System-on-chip",
                    "Physical design"
                ],
                "author_keywords": [
                    "Agile hardware design",
                    "energy efficiency",
                    "generators",
                    "multicore",
                    "RISC-V",
                    "vector"
                ]
            },
            {
                "title": "A Real-Time, 1.89-GHz Bandwidth, 175-kHz Resolution Sparse Spectral Analysis RISC-V SoC in 16-nm FinFET",
                "link": "https://ieeexplore.ieee.org/document/8738896/",
                "date_of_publication": null,
                "doi": "10.1109/JSSC.2019.2913099",
                "citations": "4",
                "abstract": "A 1.89-GHz bandwidth, 175-kHz resolution spectral analysis system-on-chip (SoC), integrating a subsampling analog-to-digital converter (ADC) frontend with a digital reconstruction backend and implementing a 21 600-point sparse Fourier transform based on the fast Fourier aliasing-based sparse transform (FFAST) algorithm has been co-designed by using the Constructing Hardware in a Scala Embedded Language (Chisel) and Berkeley Analog Generator (BAG) circuit generator frameworks in 16-nm CMOS. Three sets of 25×, 27×, and 32× subsampling successive approximation register (SAR) ADCs acquire signal with ~5.4-6.3 effective number of bits (ENOB)/slice. The digital backend consists of mixed-radix 864-, 800-, and 675-point fast Fourier transforms (FFTs), a signal location estimator, and a peeling decoder that recovers aliased signals from a sparsely populated spectrum. A single-issue, in-order, fifth-generation reduced instruction set (RISC-V) Rocket processor interacts with the spectrum analyzer for post-processing and calibration. The ADC consumes 49.8 mW with a 3.78-GHz reference clock. At 400 MHz and 0.7-V digital supply voltage (VDD), the Rocket core and the FFAST digital signal processing (DSP) together consume 133.5 mW.",
                "ieee_keywords": [
                    "Time-frequency analysis",
                    "Bandwidth",
                    "Hardware",
                    "Time-domain analysis",
                    "Real-time systems",
                    "System-on-chip",
                    "Generators"
                ],
                "author_keywords": [
                    "Analog-to-digital converters (ADCs)",
                    "Berkeley Analog Generator (BAG)",
                    "Constructing Hardware in a Scala Embedded Language (Chisel)",
                    "fast Fourier transform (FFT)",
                    "hardware generators",
                    "fifth-generation reduced instruction set computer (RISC-V)",
                    "spectrum sensing"
                ]
            },
            {
                "title": "A Mixed-Signal RISC-V Signal Analysis SoC Generator With a 16-nm FinFET Instance",
                "link": "https://ieeexplore.ieee.org/document/8765410/",
                "date_of_publication": null,
                "doi": "10.1109/JSSC.2019.2924090",
                "citations": "10",
                "abstract": "This paper demonstrates a signal analysis systemon-chip (SoC) consisting of a general-purpose RISC-V core with vector extensions and a fixed-function signal-processing accelerator. Both the application core and the accelerators are design instances produced through an agile design-space exploration process by generators that allow for a wide range of parameter configurations. The signal processing chain consists of generated instances of a time-interleaved analog-to-digital converter (ADC) followed by a digital tuner, a finite-impulse response (FIR) filter, a polyphase filter, and a fast Fourier transform (FFT) all connected to the five-stage, in-order RISC-V Rocket processor via an AXI4 bus. The generator-based design methodology is detailed, along with the agile design process of producing the fabricated design instance. The 5 mm × 5 mm chip is implemented in a 16-nm FinFET process and operates at 410 MHz at 750 mV drawing 600 mW. Presented applications show coupled functionality of the application processor and accelerator performing spectrometry and radar receive processing, and a comparison with other state-of-the-art application-specific integrated circuits (ASICs) proves that generators can produce performance-competitive designs.",
                "ieee_keywords": [
                    "Generators",
                    "Hardware",
                    "Hardware design languages",
                    "Signal analysis",
                    "Tools",
                    "FinFETs"
                ],
                "author_keywords": [
                    "Agile hardware design",
                    "analog-to-digital converter (ADC)",
                    "CMOS",
                    "fast Fourier transform",
                    "filter",
                    "FinFET",
                    "generators",
                    "radar signal processing",
                    "spectrometer"
                ]
            },
            {
                "title": "A RISC-V Vector Processor With Simultaneous-Switching Switched-Capacitor DC–DC Converters in 28 nm FDSOI",
                "link": "https://ieeexplore.ieee.org/document/7422720/",
                "date_of_publication": null,
                "doi": "10.1109/JSSC.2016.2519386",
                "citations": "42",
                "abstract": "This work demonstrates a RISC-V vector microprocessor implemented in 28 nm FDSOI with fully integrated simultaneous-switching switched-capacitor DC-DC (SC DC-DC) converters and adaptive clocking that generates four on-chip voltages between 0.45 and 1 V using only 1.0 V core and 1.8 V IO voltage inputs. The converters achieve high efficiency at the system level by switching simultaneously to avoid charge-sharing losses and by using an adaptive clock to maximize performance for the resulting voltage ripple. Details about the implementation of the DC-DC switches, DC-DC controller, and adaptive clock are provided, and the sources of conversion loss are analyzed based on measured results. This system pushes the capabilities of dynamic voltage scaling by enabling fast transitions (20 ns), simple packaging (no off-chip passives), low area overhead (16%), high conversion efficiency (80%-86%), and high energy efficiency (26.2 DP GFLOPS/W) for mobile devices.",
                "ieee_keywords": [
                    "Voltage control",
                    "Microprocessors",
                    "System-on-chip",
                    "Clocks",
                    "Computer architecture",
                    "Inductors",
                    "Switches"
                ],
                "author_keywords": [
                    "Adaptive clock",
                    "DC–DC conversion",
                    "dynamic voltage and frequency scaling (DVFS)",
                    "fully integrated converter",
                    "integrated voltage regulator",
                    "noninterleaved",
                    "RISC-V",
                    "simultaneous-switching",
                    "switched-capacitor",
                    "Adaptive clock",
                    "DC–DC conversion",
                    "dynamic voltage and frequency scaling (DVFS)",
                    "fully integrated converter",
                    "integrated voltage regulator",
                    "noninterleaved",
                    "RISC-V",
                    "simultaneous-switching",
                    "switched-capacitor"
                ]
            },
            {
                "title": "A RISC-V Processor SoC With Integrated Power Management at Submicrosecond Timescales in 28 nm FD-SOI",
                "link": "https://ieeexplore.ieee.org/document/7915682/",
                "date_of_publication": null,
                "doi": "10.1109/JSSC.2017.2690859",
                "citations": "28",
                "abstract": "This paper presents a RISC-V system-on-chip (SoC) with integrated voltage regulation, adaptive clocking, and power management implemented in a 28 nm fully depleted silicon-on-insulator process. A fully integrated simultaneous-switching switched-capacitor DC-DC converter supplies an application core using a clock from a free-running adaptive clock generator, achieving high system conversion efficiency (82%-89%) and energy efficiency (41.8 double-precision GFLOPS/W) while delivering up to 231 mW of power. A second core serves as an integrated power-management unit that can measure system state and actuate changes to core voltage and frequency, allowing the implementation of a wide variety of power-management algorithms that can respond at submicrosecond timescales while adding just 2.0% area overhead. A voltage dithering program allows operation across a wide continuous voltage range (0.45 V-1 V), while an adaptive voltage-scaling algorithm reduces the energy consumption of a synthetic benchmark by 39.8% with negligible performance penalty, demonstrating practical microsecond-scale power management for mobile SoCs.",
                "ieee_keywords": [
                    "Clocks",
                    "Voltage control",
                    "Generators",
                    "Delays",
                    "Electrical engineering",
                    "Adaptive systems",
                    "Switches"
                ],
                "author_keywords": [
                    "Adaptive clock",
                    "energy-efficient processor",
                    "fine-grained adaptive voltage scaling (AVS)",
                    "integrated voltage regulator",
                    "power management unit (PMU)",
                    "RISC-V",
                    "voltage dithering"
                ]
            },
            {
                "title": "A 65-nm CMOS Wideband TDD Front-End With Integrated T/R Switching via PA Re-Use",
                "link": "https://ieeexplore.ieee.org/document/7946189/",
                "date_of_publication": null,
                "doi": "10.1109/JSSC.2017.2702669",
                "citations": "7",
                "abstract": "Time-division duplex (TDD) systems rely on off-chip transmit/receive (T/R) switches to isolate the RX from the high-output power of the TX, while existing on-chip T/R switching solutions are narrow-band or high loss. This paper presents a wideband integrated T/R switching technique that eliminates the conventional, lossy series T/R switch from the signal path. The system reconfigures the PA as an LNA during the receive mode, and utilizes only DC mode control switches to enable TDD co-existence. To demonstrate this technique, a polar transmitter that can be re-purposed into a common-gate LNA is implemented in 65-nm CMOS. With an integrated front-end balun transformer, the transmitter achieves 20-dBm peak output power with 32.7% peak drain efficiency. In the receive mode, the PA is reconfigured into a wideband 3.4-5.4-GHz LNA achieving -6.7-dBm P1dB, and 5.1-dB noise figure.",
                "ieee_keywords": [
                    "Impedance",
                    "Wideband",
                    "Switches",
                    "Power generation",
                    "Ports (Computers)",
                    "Transistors",
                    "Topology"
                ],
                "author_keywords": [
                    "Low-noise amplifier",
                    "PA re-use",
                    "polar transmitter",
                    "power amplifier",
                    "T/R switch",
                    "TDD front-end",
                    "transformer"
                ]
            },
            {
                "title": "A 71-to-86-GHz 16-Element by 16-Beam Multi-User Beamforming Integrated Receiver Sub-Array for Massive MIMO",
                "link": "https://ieeexplore.ieee.org/document/9593276/",
                "date_of_publication": null,
                "doi": "10.1109/JSSC.2021.3118641",
                "citations": "8",
                "abstract": "This article presents a 71–86 GHz 16-element array receiver application-specified integrated circuit (ASIC) for Massive multiple-input–multiple-output (MIMO) wireless uplink, featuring a multiple-output analog beamformer (BF) supporting up to 16 spatially multiplexed users at the same time. The ASIC includes 16 direct-conversion mixer-first RX front-ends, local oscillator (LO) generation and distribution, and a $16\\times 16$ fully connected baseband analog beamformer, which derives each user stream as a linear combination of all the 16 antennas. The 16 mm 2 28 nm CMOS ASIC is packaged on an organic interposer including a linear patch antenna array. Over-the-air measurements demonstrate up to 2 Gb/s single-user data-rate, and four simultaneous links at 500 Mb/s each, with number of users and data rate only limited by setup constraints. Circuits are optimized for low power consumption in order to enable scaling to massive arrays, and consumes 1.7 W total power, for a power figure of 7 mW/antenna/user.",
                "ieee_keywords": [
                    "Array signal processing",
                    "Antenna arrays",
                    "Computer architecture",
                    "Wireless communication",
                    "Millimeter wave communication",
                    "Antennas",
                    "Complexity theory",
                    "MIMO"
                ],
                "author_keywords": [
                    "60 GHz",
                    "baseband beamforming",
                    "millimeter-wave (mm-wave)",
                    "multi-user beamforming",
                    "multi-user multiple-input–multiple-output (MU-MIMO)",
                    "packaged array",
                    "spatial multiplexing",
                    "two-stage beamforming"
                ]
            },
            {
                "title": "Reprogrammable Redundancy for SRAM-Based Cache",
                "link": "https://ieeexplore.ieee.org/document/7983409/",
                "date_of_publication": null,
                "doi": "10.1109/JSSC.2017.2715798",
                "citations": "4",
                "abstract": "Reducing the operating voltage of digital systems improves energy efficiency, and the minimum operating voltage of a system (V min ) is commonly limited by SRAM bitcells. Common techniques to lower SRAM V min focus on using circuit-level periphery-assist techniques to prevent bitcell failures at low voltage. Alternatively, this paper proposes architecture-level techniques to allow caches to tolerate significant numbers of failing bitcells at low voltage while maintaining correct operation. The presented processor lowers SRAM-based cache V min using three architectural techniques-bit bypass, dynamic column redundancy, and line disable-that use low-overhead reprogrammable redundancy (RR) to increase the maximum tolerable bitcell failure rate and decrease the minimum operating voltage in processor caches. In a fabricated 28-nm RISC-V-based processor chip, these RR techniques add 2% area overhead to the cache and reduce the V min of the 1-MB L2 cache by 25%, resulting in a 49% power reduction.",
                "ieee_keywords": [
                    "Random access memory",
                    "Redundancy",
                    "Error correction codes",
                    "Computer architecture",
                    "Low voltage",
                    "Microprocessors",
                    "Circuit faults"
                ],
                "author_keywords": [
                    "Bit bypass (BB)",
                    "dynamic column redundancy (DCR)",
                    "error-correcting codes (ECC)",
                    "line disable (LD)",
                    "redundancy",
                    "reprogrammable redundancy (RR)",
                    "SRAM",
                    "Vmin"
                ]
            },
            {
                "title": "Analysis and Design of Integrated Active Cancellation Transceiver for Frequency Division Duplex Systems",
                "link": "https://ieeexplore.ieee.org/document/7948802/",
                "date_of_publication": null,
                "doi": "10.1109/JSSC.2017.2700360",
                "citations": "39",
                "abstract": "An active transmitter (TX) cancellation scheme enabling integration of the antenna interface for frequency division duplex systems is presented. A replica of the TX current is synthesized in shunt with the receiver (RX) by a digital-to-analog converter (DAC). The replica DAC virtually shorts out the TX signal at the RX input while having minimal impact on the TX insertion loss. Propagation of the TX phase noise in the RX band is analyzed and shown to be feed-forward cancelled in the proposed system. A prototype chip including integrated TX, measurement RX, and cancellation circuits, operating on a single-shared antenna, is implemented in 65-nm CMOS. The cancellation replica demonstrates > 50 dB cancellation of a +12.6 dBm peak 20-MHz TX signal across a wide range of center frequencies and up to 5:1 voltage standing-wave ratio at the antenna interface. The RX is able to down-convert the RX signal at 40-MHz offset with <; 4.3 dB noise figure degradation in the presence of the residual TX signal.",
                "ieee_keywords": [
                    "Impedance",
                    "Antennas",
                    "Phase noise",
                    "Transceivers",
                    "Bandwidth",
                    "Ports (Computers)",
                    "Power demand"
                ],
                "author_keywords": [
                    "Active cancellation",
                    "digital-to-analog converter (DAC)",
                    "electronicsubtraction",
                    "frequency division duplex (FDD)",
                    "integratedduplexer",
                    "phase noise cancellation",
                    "self-interferencecancellation",
                    "shared antenna interface",
                    "switched-capacitorpower amplifier (SCPA)",
                    "wideband transceiver"
                ]
            },
            {
                "title": "A 65-nm CMOS",
                "link": "https://ieeexplore.ieee.org/document/8254342/",
                "date_of_publication": null,
                "doi": "10.1109/JSSC.2017.2782084",
                "citations": "12",
                "abstract": "A 700-MHz to 1.6-GHz RF power digital-to-analog converter with programmable integrated harmonic cancellation and mixed-signal filtering is presented. Harmonic cancellation is implemented by splitting the power amplifier (PA) into segments, driving different segments of the PA with phase-shifted versions of the local oscillator (LO) signals, and summing at the output. Mixed-signal filtering is realized in a similar fashion but with segments driven with delayed versions of the input data. The phase shift and data delays are reconfigurable and implemented to operate across a wide frequency range. To boost efficiency, 25% duty cycle LO signals are used. A technique to correct for IQ constellation distortion induced by these 25% duty cycle LO signals is introduced and verified in measurements. The transmitter (TX) operates at a maximum sample rate of 500 MSa/s and achieves an output power of 25.6 dBm for an output load of 100 Ω when harmonic cancellation is enabled. The TX demonstrates 24-42 dB of 3rd harmonic cancellation for continuous wave signals across a 700-MHz to 2-GHz frequency range, achieving an HD3 as low as -57 dB. The TX achieves an HD3 reduction of 33 dB and an 18-dB notch at 40-MHz offset with 20-MHz long-term evolution data.",
                "ieee_keywords": [
                    "Harmonic analysis",
                    "Power harmonic filters",
                    "Radio frequency",
                    "Frequency conversion",
                    "Frequency measurement",
                    "Delays"
                ],
                "author_keywords": [
                    "Active cancellation",
                    "CMOS",
                    "digital-to-analog converter (DAC)",
                    "power amplifier (PA)",
                    "RF",
                    "switched-capacitor PA (SCPA)",
                    "wideband transmitter (TX)"
                ]
            }
        ]
    },
    {
        "name": "Thomas Ristenpart",
        "publications": [
            {
                "title": "The Spyware Used in Intimate Partner Violence",
                "link": "https://ieeexplore.ieee.org/document/8418618/",
                "date_of_publication": "26 July 2018",
                "doi": "10.1109/SP.2018.00061",
                "citations": "47",
                "abstract": "Survivors of intimate partner violence increasingly report that abusers install spyware on devices to track their location, monitor communications, and cause emotional and physical harm. To date there has been only cursory investigation into the spyware used in such intimate partner surveillance (IPS). We provide the first in-depth study of the IPS spyware ecosystem. We design, implement, and evaluate a measurement pipeline that combines web and app store crawling with machine learning to find and label apps that are potentially dangerous in IPS contexts. Ultimately we identify several hundred such IPS-relevant apps. While we find dozens of overt spyware tools, the majority are \"dual-use\" apps - they have a legitimate purpose (e.g., child safety or anti-theft), but are easily and effectively repurposed for spying on a partner. We document that a wealth of online resources are available to educate abusers about exploiting apps for IPS. We also show how some dual-use app developers are encouraging their use in IPS via advertisements, blogs, and customer support services. We analyze existing anti-virus and anti-spyware tools, which universally fail to identify dual-use apps as a threat.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "SoK: Hate, Harassment, and the Changing Landscape of Online Abuse",
                "link": "https://ieeexplore.ieee.org/document/9519435/",
                "date_of_publication": "26 August 2021",
                "doi": "10.1109/SP40001.2021.00028",
                "citations": "19",
                "abstract": "We argue that existing security, privacy, and antiabuse protections fail to address the growing threat of online hate and harassment. In order for our community to understand and address this gap, we propose a taxonomy for reasoning about online hate and harassment. Our taxonomy draws on over 150 interdisciplinary research papers that cover disparate threats ranging from intimate partner violence to coordinated mobs. In the process, we identify seven classes of attacks—such as toxic content and surveillance—that each stem from different attacker capabilities and intents. We also provide longitudinal evidence from a three-year survey that hate and harassment is a pervasive, growing experience for online users, particularly for at-risk communities like young adults and people who identify as LGBTQ+. Responding to each class of hate and harassment requires a unique strategy and we highlight five such potential research directions that ultimately empower individuals, communities, and platforms to do so.",
                "ieee_keywords": [
                    "Privacy",
                    "Social networking (online)",
                    "Taxonomy",
                    "Distance measurement",
                    "Cognition",
                    "Computer security"
                ],
                "author_keywords": [
                    "hate",
                    "harassment",
                    "emerging-threats",
                    "at-risk"
                ]
            },
            {
                "title": "Honey Encryption: Encryption beyond the Brute-Force Barrier",
                "link": "https://ieeexplore.ieee.org/document/6876246/",
                "date_of_publication": null,
                "doi": "10.1109/MSP.2014.67",
                "citations": "34",
                "abstract": "Honey encryption (HE) addresses the challenge of encrypting messages using keys that are vulnerable to guessing attacks, such as the passwords selected by ordinary users. HE creates a ciphertext that, when decrypted with an incorrect key or password, yields a valid-looking but bogus message. So, attackers can't tell when decryption has been successful. Counterintuitively, HE enables the encryption of a message using a weak password such that even a strong attacker--one with unlimited computing power--can't decrypt the message with certainty. You can use HE to encrypt the list of passwords in a password manager, credentials used in SSH (Secure Shell), and so on. HE fuses the creative use of honey objects and decoys in system security with the rigor and principled application imparted by cryptography.",
                "ieee_keywords": [
                    "Encryption",
                    "Databases",
                    "Computer security",
                    "Privacy",
                    "Encoding"
                ],
                "author_keywords": [
                    "honey encryption",
                    "encryption",
                    "security",
                    "computer security",
                    "cryptography",
                    "cybercrime",
                    "hackers",
                    "password-based encryption",
                    "one-time pad"
                ]
            },
            {
                "title": "Beyond Credential Stuffing: Password Similarity Models Using Neural Networks",
                "link": "https://ieeexplore.ieee.org/document/8835247/",
                "date_of_publication": "16 September 2019",
                "doi": "10.1109/SP.2019.00056",
                "citations": "28",
                "abstract": "Attackers increasingly use passwords leaked from one website to compromise associated accounts on other websites. Such targeted attacks work because users reuse, or pick similar, passwords for different websites. We recast one of the core technical challenges underlying targeted attacks as the task of modeling similarity of human-chosen passwords. We show how to learn good password similarity models using a compilation of 1.4 billion leaked email, password pairs. Using our trained models of password similarity, we exhibit the most damaging targeted attack to date. Simulations indicate that our attack compromises more than 16% of user accounts in less than a thousand guesses, should one of their other passwords be known to the attacker and despite the use of state-of-the art countermeasures. We show via a case study involving a large university authentication service that the attacks are also effective in practice. We go on to propose the first-ever defense against such targeted attacks, by way of personalized password strength meters (PPSMs). These are password strength meters that can warn users when they are picking passwords that are vulnerable to attacks, including targeted ones that take advantage of the user's previously compromised passwords. We design and build a PPSM that can be compressed to less than 3 MB, making it easy to deploy in order to accurately estimate the strength of a password against all known guessing attacks.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "The Many Kinds of Creepware Used for Interpersonal Attacks",
                "link": "https://ieeexplore.ieee.org/document/9152794/",
                "date_of_publication": "30 July 2020",
                "doi": "10.1109/SP40000.2020.00069",
                "citations": "7",
                "abstract": "Technology increasingly facilitates interpersonal attacks such as stalking, abuse, and other forms of harassment. While prior studies have examined the ecosystem of software designed for stalking, there exists an unstudied, larger landscape of apps-what we call creepware-used for interpersonal attacks. In this paper, we initiate a study of creepware using access to a dataset detailing the mobile apps installed on over 50 million Android devices. We develop a new algorithm, CreepRank, that uses the principle of guilt by association to help surface previously unknown examples of creepware, which we then characterize through a combination of quantitative and qualitative methods. We discovered apps used for harassment, impersonation, fraud, information theft, concealment, and even apps that purport to defend victims against such threats. As a result of our work, the Google Play Store has already removed hundreds of apps for policy violations. More broadly, our findings and techniques improve understanding of the creepware ecosystem, and will inform future efforts that aim to mitigate interpersonal attacks.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Rethinking Security in the Era of Cloud Computing",
                "link": "https://ieeexplore.ieee.org/document/7945209/",
                "date_of_publication": null,
                "doi": "10.1109/MSP.2017.80",
                "citations": "25",
                "abstract": "Cloud computing has emerged as a dominant computing platform for the foreseeable future, resulting in an ongoing disruption to the way we build and deploy software. This disruption offers a rare opportunity to integrate new approaches to computer security. The aggregating effect of cloud computing and the role of cloud providers as trust anchors can significantly benefit computing security.",
                "ieee_keywords": [
                    "Cloud computing",
                    "Servers",
                    "Cryptography",
                    "Computational modeling",
                    "Computer security"
                ],
                "author_keywords": [
                    "cloud",
                    "cloud computing",
                    "security",
                    "cloud providers",
                    "honeywords"
                ]
            },
            {
                "title": "Leakage-Abuse Attacks against Order-Revealing Encryption",
                "link": "https://ieeexplore.ieee.org/document/7958603/",
                "date_of_publication": "26 June 2017",
                "doi": "10.1109/SP.2017.44",
                "citations": "112",
                "abstract": "Order-preserving encryption and its generalization order-revealing encryption (OPE/ORE) allow sorting, performing range queries, and filtering data - all while only having access to ciphertexts. But OPE and ORE ciphertexts necessarily leak information about plaintexts, and what level of security they provide in practice has been unclear. In this work, we introduce new leakage-abuse attacks that recover plaintexts from OPE/ORE-encrypted databases. Underlying our new attacks is a framework in which we cast the adversary's challenge as a non-crossing bipartite matching problem. This allows easy tailoring of attacks to a specific scheme's leakage profile. In a case study of customer records, we show attacks that recover 99% of first names, 97% of last names, and 90% of birthdates held in a database, despite all values being encrypted with the OPE scheme most widely used in practice. We also show the first attack against the recent frequency-hiding Kerschbaum scheme, to which no prior attacks have been demonstrated. Our attack recovers frequently occurring plaintexts most of the time.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Blind Certificate Authorities",
                "link": "https://ieeexplore.ieee.org/document/8835242/",
                "date_of_publication": "16 September 2019",
                "doi": "10.1109/SP.2019.00007",
                "citations": "6",
                "abstract": "We explore how to build a blind certificate authority (CA). Unlike conventional CAs, which learn the exact identity of those registering a public key, a blind CA can simultaneously validate an identity and provide a certificate binding a public key to it, without ever learning the identity. Blind CAs would therefore allow bootstrapping truly anonymous systems in which no party ever learns who participates. In this work we focus on constructing blind CAs that can bind an email address to a public key. To do so, we first introduce secure channel injection (SCI) protocols. These allow one party (in our setting, the blind CA) to insert a private message into another party's encrypted communications. We construct an efficient SCI protocol for communications delivered over TLS, and use it to realize anonymous proofs of account ownership for SMTP servers. Combined with a zero-knowledge certificate signing protocol, we build the first blind CA that allows Alice to obtain a X.509 certificate binding her email address alice@domain.com to a public key of her choosing without ever revealing ``alice'' to the CA. We show experimentally that our system works with standard email server implementations as well as Gmail.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Network Traffic Obfuscation and Automated Internet Censorship",
                "link": "https://ieeexplore.ieee.org/document/7782699/",
                "date_of_publication": null,
                "doi": "10.1109/MSP.2016.121",
                "citations": "14",
                "abstract": "Internet censors seek ways to identify and block internet access to information they deem objectionable. Increasingly, censors deploy advanced networking tools such as deep-packet inspection (DPI) to identify such connections. In response, activists and academic researchers have developed and deployed network traffic obfuscation mechanisms. These apply specialized cryptographic tools to attempt to hide from DPI the true nature and content of connections. This survey offers an overview of network traffic obfuscation and its role in circumventing Internet censorship.",
                "ieee_keywords": [
                    "Censorship",
                    "Protocols",
                    "Internet",
                    "Encryption",
                    "Payloads",
                    "Inspection",
                    "IP networks"
                ],
                "author_keywords": [
                    "security",
                    "DPI",
                    "deep-packet inspection",
                    "censors",
                    "cryptography"
                ]
            },
            {
                "title": "Cracking-Resistant Password Vaults Using Natural Language Encoders",
                "link": "https://ieeexplore.ieee.org/document/7163043/",
                "date_of_publication": "20 July 2015",
                "doi": "10.1109/SP.2015.36",
                "citations": "27",
                "abstract": "We now present a basic HE scheme. This scheme hides passwords, but takes the simple approach of storing domains in the clear with the ciphertext. Basic HE Scheme Upon input \\vec{D},\\vec{P},\\vec{h} this scheme proceeds as follows. We first apply the sub-grammar NLE SG to the subset of passwords in \\vec{P} for which h_{i}=1. To each of the remaining passwords with h_{i}=0 we apply the DTE UNIF. The result from both steps is a bit string S=S_{0}\\Vert S_{1}\\Vert\\cdots\\Vert S_{\\ell} with S_{0} the output from encoding the sub-grammar and each S_{i} either an encoding under PCFG using the sub-grammar (h_{i}=1) or an encoding under UNIF (h_{i}=0). The string S is then encrypted as follows. First, derive a key K= KDF (mpw,\\ \\mathrm{sa}) for a freshly generated uniform salt sa and where mpw is the user's master password. Here KDF is a password based key derivation function (PBKDF) that is strengthened to be as slow as tolerable during normal usage [25]. Then, encrypt each S_{i} (for i=0 to l) independently using AES in counter mode with key K and a fresh random IV. This produces a sequence of \\ell+1 CTR-mode ciphertexts \\vec{C}=(C_{0},\\ \\ldots,\\ C_{\\ell}). The final vault ciphertext includes a (conventional) encoding of \\vec{D},\\vec{h}, sa, \\vec{C}. Decryption works in a straightforward way. This HE scheme is relatively simple (once the NLE and DTE are fixed) and space efficient. However, \\vec{D} and \\vec{h} are stored in the clear and this means that attackers obtaining access to the ciphertext learn the domains for which the user has an account as well as which passwords were randomly generated. One approach to rectify this would be to specify a DTE for encoding \\vec{D}. One could use popularity statistics for domains, for example based on Alexa rankings. However note that this may sacrifice security against offline bruteforce attacks in the case that an attacker knows with high certainty the set of domains associated with a user's vault. This highlights a delicate challenge in the use of HE: if an attacker can easily obtain knowledge about a portion of the plaintext, it may be better to not apply HE to that portion of the plaintext. We may view domain information as such easily-obtainable side information that it is not worth encrypting. To provide domain privacy, though, we must do more. We now describe two approaches: HE-DH1 and HE-DH2. HE-DH1 In this scheme, we hide an individual user's domains in the set of all domains used by users and include in each user's vault “dummy” entries for unused domains. To achieve privacy of domains the goal will be that an attacker cannot distinguish between a dummy and real entry. In the following, we fix a set of popular domains \\vec{D}^{\\ast}=({D}_{1}^{\\ast},\\ \\ldots,\\ D_{s_{1}}^{\\ast}). We require that \\vec{D}^{\\ast} be a superset of all the domains used by users. We discuss how to achieve this momentarily. To encrypt an input \\vec{D},\\vec{h}, \\vec{P} we first encode the password \\mathrm{s} as described above for the basic scheme: apply SG to the set of human-generated passwords and UNIF to each of the computer-generated passwords. For any domain in \\vec{D}^{\\ast} but not in \\vec{D}, we generate a dummy encoding as follows. First we choose a bit h to select whether this domain should have a human-generated dummy entry or a computer-generated one. We discuss how to bias this bit selection below. Then we generate a random bit string of length equal to the length of outputs of PCFG (\\mathrm{if}\\ h=1) or UNIF (\\mathrm{if}\\ h=0). We then generate the full encoding S=S_{0}\\Vert S_{1}\\Vert\\cdots\\Vert S_{s1} by inserting the per-password encodings or dummy encodings to match the order from \\vec{D}^{\\ast}. We then encrypt S as in the basic scheme. The distribution alluded to above for dummy encodings does not affect confidentiality of mpw or P, but rather confidentiality of the domains associated to a user and whether the user has human or generated passwords for each such domain. One can, for example, set this distribution to be biased towards human-generated passwords. Note that decryption with either the correct mpw or an incorrect one produces s_{1} passwords. The s_{1}-\\ell honey passwords corresponding to the dummy entries obscure from an attacker which sites are in use by the user (even in the extreme case that the attacker has guessed mpw somehow). When mounting brute-force attacks, the dummy entries might hinder attempts to perform online checks of recovered passwords. In particular, if a domain's login web page follows best practices and does not leak whether a user has an account there (regardless of correctness of the provided password), then the attacker may not be able to distinguish between the situation in which a certain domain is not used by a user and the situation in which the decryption attempt resulted in a honey password. The downside of the above approach is that s_{1} may need to be very large and for each user the storage service (described below in Section VIII) must store \\mathcal{O}(s_{1}) bits of data. We can grow s_{1} over time by having clients inform the service of when a new domain should be added to D^{\\ast} and the server can insert dummy entries in previous vaults by just inserting random bit strings in the appropriate location for each vault ciphertext. (this is possible because a separate CTR-mode encryption is used for each vault entry.) When the system is first setup, an initial relatively small popular domains list can be seeded with highly popular domains. Another approach to reducing overheads is to bucket users into separate groups, each group having their own popular domains list. This enables tuning the size of vaults relative to per-group domain confidentiality. HE-DH2 In this scheme, we adopt an alternative approach to dealing with the long tail of domains, and use a honey-encrypted overflow table. Fix some number s_{2} > 0. For each of the domains not in the current popular domain set, we use the following procedure. First apply the PCFG (using the sub-grammar or UNIF appropriately to the password to get a bit string S^{\\prime}. Then hash the domain name and take the result modulo s_{2} to yield an index j\\in[0..s_{2}-1]. Set S_{s_{1}+j+1}^{\\ast} to S^{\\prime}. Some indices in [0..s_{2}-1] will be unused after handling all domains outside the popular domain set; we fill these with dummy encodings. The additional s_{2} seeds are each encrypted with CTR mode, making the final, full ciphertext \\vec{D}^{\\ast},\\vec{h}^{\\ast}, sa, \\vec{C}. Note that now, \\vec{C} contains s_{1}+s_{2}+ 1 individual CTR-mode encryption ciphertexts for the sub-grammar and s1+s2 individual, possibly dummy, passwords. By setting s_{2} large enough relative to the expected number of domains not in D^{\\ast} we can ensure that with high probability no two domains hash to the same location. Note that the domains associated with the overflow table are not stored with the ciphertext. To decrypt, the requested domain is checked to see if it is in D^{\\ast} and if not it is hashed to find the appropriate entry in the last s_{2} HE ciphertexts. Updating a Vault To update a password for a particular domain in the basic scheme or HE-DH1, one first decrypts the entire vault, changes the appropriate entry, and then encrypts the modified vault with fresh randomness (including the salt). needed to ensure the sub-grammar is consistent with the encoded content. For HE-DH2, one proceeds much the same, also decrypting each of the s_{2} entries in the overflow table. The appropriate domain's entry is updated (found either by looking in the popular domains list or, failing that, hashing the domain to be updated to find it in the overflow table). Finally, the modified vault is encrypted (with fresh randomness). Deletion of a password can be performed by converting the appropriate entry into a dummy entry while also updating the sub-grammar by removing any now unnecessary rules. Security Discussion Our primary goal is confidentiality of the plaintext passwords. All passwords are first encoded using an appropriate DTE and then encrypted using a PBE scheme. Should the user's master password be strong, even an offline brute-force attack is infeasible and, in particular, it will require as much work to break any of the schemes above as would be to break a conventional PBE encryption. Should the user's master password be weak, then by construction decrypting the ciphertext under any incorrect master password gives back a sample from the DTE distribution. In particular, we believe there to be no speed-up attacks that allow the attacker to rule out a particular incorrect master password without having to determine if the recovered plaintext is decoy or not. As we showed in the previous section, our NLE is good enough that distinguishing human-generated passwords is challenging even for sophisticated adversaries. The above is admittedly informal reasoning, and does not rule out improved attacks. We would prefer a formal analysis of plaintext vault recovery security akin to those given for simpler honey encryption schemes in [23], which would reduce security to solely depend on DTE quality. Those techniques rely on a closed-form description of the distribution of password vaults as produced by decoding uniform strings. Unfortunately we do not know how to determine one; even estimating the distribution of single passwords is impractical with sampled data [4]. Formal analysis remains an interesting open question. If an attacker obtains the encryptions of a vault before and after an update, then security falls back to that of conventional PBE. One simply decrypts both vaults under each guessed master password, and with high probability the contents of the two plaintext vaults will match (except where updates occurred) with high probability only with the correct master password. This is a limitation of all decoy-based approaches we are aware of and finding a solution for update security is an interesting open question. Another security goal is domain hiding. As discussed earlier, adding dummy ciphertexts (random bit strings) for the latter two schemes for unused domains means that an offline attacker will recover passwords for these domains as well. The same reasoning extends to the use of the overflow table. The complexity of the sub-grammar may leak some information about the overall number of human-generated passwords in-use, but not which of the domains marked as having human-generated passwords are dummy encodings. SECTION VIII. The NoCrack System We now turn to the design of the full honey-vault service that we call NoCrack. Our architecture closely follows deployed commercial systems, such as LastPass6 A web-storage service exposes a RESTful web API over HTTPS for backing up user vaults and synchronizing vaults across devices. To achieve the security benefits of HE, however, we must design this service carefully. The Challenge of Password-Based Logins One encounters an interesting challenge when attempting to build a decoy-based system which supports backup of user vaults: how to authenticate users to the service that is responsible for backing up their vaults. In particular, the status quo in industry is for users to choose a username and service password. The password would be sent over HTTPS to the server, hashed, and stored to authenticate future requests. But customers are likely to choose this service password to be the same as their vault's master password. If an attacker compromises the storage service and obtains both a user's encrypted vault and the service password hash, they can mount a brute-force attack against the service password hash, learn the service password, and then decrypt the vault. One might attempt to mitigate with this by securing the password hash separately from vaults. Or one could avoid backup of encrypted vaults entirely, but this would leave users responsible and violate our goal of matching features of existing services. We therefore go a different route, and forego password-based login to the storage service completely. Device Enrollment A new user registers with the service by providing an email address (also used as an identifier), to which a standard proof-of-ownership challenge is sent. To hinder abuse of the registration functionality, the service can rate limit such requests and require solution of an appropriate CAPTCHA [36]. The proof-of-ownership is an email including a randomly generated 128-bit temporary token (encoded in Base64 format, 22 characters long). The user copies this temporary token into the client program which submits the token over an enroll API call. The server verifies the temporary token, and returns to the client program a (long term) bearer token (also 128 bits) that can be used as a key to authenticate subsequent requests using HMAC. At this stage the client device is enrolled. Note that all communication is performed over TLS. Additional devices can be enrolled in a similar manner by having an already-enrolled client device to generate a token for the new device or sending a new temporary token via email. Should a user lose all access to a device with a current bearer token, they can easily obtain a new token via the same enrollment process. We note that two-factor authentication would be straightforward to support by requiring a proof-of-ownership of a phone number or a correct hardware token-generated onetime password to obtain a device bearer token. Synchronizing with the Server An enrolled client device can compare their local information with that stored under their account on the server. This involves ensuring the client and storage service have the same version of the vault, which, in normal usage, is cached on the client device. To save bandwidth, downloads and uploads can be done in an efficient manner via any standard “diff” mechanism - in particular our HE schemes support sending only portions of the ciphertext at a time. The Client We currently have only a command line client supported, but future versions could easily integrate with popular browsers via an extension. The client caches the vault locally, but never stores it in the clear on persistent storage. The client queries the service when run to determine if it needs to synchronize the vault. At the beginning of a browsing session, the user is prompted for the master password and the vault is decrypted. To check for typos, we can use dynamic security skins [15] (as suggested also for use with Kamouflage), which show a color or picture that is computed as a hash of the master password (but never stored). The output of the KDF can be cached in memory in order to decrypt individual domains as needed, while the master password itself is expunged from memory immediately. Note that the HE scheme does not handle login names; we assume that browser caching mechanisms can handle this for a user if they desire. Should a login detectably fail for the user due to master password typo and the user does not observe the incorrect security skin, the client can prompt the user to reenter their master password. By construction, there might be dummy password entries in NoCrack for some domains where the user does not have an account. The user and/or the browser is responsible to distinguish the domains where the user has an account. Implementation and Performance We implemented a prototype of NoCrack in Python-2.7. On the server side we used Flask and Sqlite3. To normalize domains we use the Python Public-Suffix library. All cryptographic operations use PyCrypto-2.6.1. We use AES within CTR mode encryption, and SHA-256 within PBKDF2 for key derivation. Many of the operations are parallelizable; we use the Python multiprocessing library for this but note that our prototype implementation does not yet fully take advantage of parallelization. The client and server consist of 3,102 total lines of code as counted by the utility cloc (not counting libraries). All experiments were performed on an Intel Core-i5 with 16 GB of RAM running Linux. We provide some basic performance numbers for our most complex honey encryption scheme HE-DH2, but emphasize that this is a naive implementation and some improvements will be easy. We fix various vault sizes s ∊{2, 200, 2,000, 20,000} and set s1 = s2 = s/2 (these are the sizes of the popular domains table and overflow table, respectively). We used integer representation size b = 128. for encoding fractions. We start by generating a random ciphertext of size appropriate for the values of s1, s2 assuming some short arbitrary domain size and that all passwords are human generated (the worst-case for performance). We then measure the time to recover a particular vault password as well as to add a password to the vault. We report in Table 5 the median times over 100 trials. Variance in timing was negligible. Time for recovering a single password is fast, and agnostic to the size of the vault. This is because our design allows random access into the vault. Time for adding passwords increases with s, since our scheme decrypts and decodes all s entries, updates the new password, then re-encodes and re-encrypts all s entries (this is required to keep the sub-grammar synchronized with the contents of the vault.) The bulk of the time is spent in encoding and re-encoding passwords. This operation is still only around one second for large vaults, and large vaults are needed only to support domain hiding. The encrypted vaults are also of reasonable size. We conclude that, while NoCrack does incur time and space overheads relative to conventionally encrypted vaults, the absolute performance is more than sufficient for the envisioned usage scenarios. Figure 5: Running times (median over 100 trials) of operations for different vault sizes s=s_{1}+s_{2} the final row is size of encrypted vaults on disk. Show All SECTION IX. Related Work Honey Objects The use of decoy objects such as honeypots or decoy documents [7], [8] is well-established in information security practice. More closely related to our work here are honeywords [24], decoy passwords associated with each user in a password database. The honeywords system involves fake individual passwords, rather than password sets, and does not help with decoy security for password vaults, our goal here. We also note that decoy document and honeyword systems are distributed: they assume explicit storage of secrets that distinguish decoy from real objects in a trustworthy location (a “honeychecker”) separate from the system containing the decoy objects. See [22] for a discussion of the distinction between such systems and those in which these secrets (e.g., master passwords) are provided by a user, as in NoCrack. An early decoy system involving encryption under user-furnished secrets was proposed by Hoover and Kausik [20]; it only supports encryption of specially crafted RSA private keys. Honey encryption [23] introduced a general framework for incorporating honey objects into encryption. As explained earlier, it does not prescribe constructions for specific message types, which gives rise to one of the major technical challenges we faced in building NoCrack. A detailed discussion of Kamouflage [3] was given in Section II. Password-Based Key Derivation Key stretching, where one slows down key derivation, was first defined by Kelsey et al. [28], and standardized later in PKCS#5 [25]. Boyen proposed halting password puzzles [9] in which the key-derivation will run indefinitely on incorrect password guesses and only terminates (after an unspecified length of time) upon correct guesses. Another approach is to incorporate memory-hard functions, which require a significant amount of RAM to compute efficiently, such as done in scrypt [33]. Each of these techniques slows down offline brute-force attacks, but do not force attackers to make online queries. Stateless Password Managers Several schemes exist for strengthening user passwords (and preventing direct password reuse) by hashing a master secret with domain names to dynamically generate per-domain passwords. An early example was the Lucent Personal Web Assistant (LPWA) [17]; later variants include PwdHash [34] and Password Multiplier a scheme by Halderman et al. [18]. Chiasson et al. conducted a usability study of both PwdHash and Password Multiplier and found the majority of users could not successfully use them as intended to generate strong passwords [13]. Another usability challenge is dealing with sites with a password policy banning the output of the password hash; for this reason NoCrack uses a simple set of rules for computer-generated passwords. Password Managers In addition to Kamouflage [3], several academic proposals have sought to improve the usability and security of stateful password managers. Passpet [39] generates random passwords per-domain and allows users to assign avatars to different websites to easily identify which passwords are used with which website. Tapas [31] is a prototype two-factor password manager which distributes passwords into shares between a computer and a mobile phone. Karole et al. [26] performed a usability evaluation comparing three common approaches to password vaults: online services, phone applications, and USB tokens. Interestingly, they found that the online service was by far the easiest for participants to use, although participants stated a clear preference for the phone-based solution because most didn't want to entrust all of their passwords to a cloud-based service. These findings are a compelling justification for NoCrack, which enables the convenience of cloud-based password vault backup with higher security against compromise. ACKNOWLEDGMENT We thank the Oakland 2015 anonymous reviewers for their valuable comments and feedback. We thank Michael Doescher for helping in several design choices of PCFG construction and cleaning the Pastebin dataset, Shoban Preeth Chandrabose for his feedback on the machine learning analysis of NoCrack, and Adam Everspaugh for valuable discussions and editorial assistance. This work was supported in part by NSF grants CNS-1330308 and CNS-1253870 and AFOSR grant FA9550-13-1-0138. Authors Figures References Citations Keywords Metrics Footnotes More Like This Syntax Analysis of Serbian Language using Context-free Grammars 2020 55th International Scientific Conference on Information, Communication and Energy Systems and Technologies (ICEST) Published: 2020 Recognising the English Language using Context Free Grammar with PyFormlang 2022 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT) Published: 2022 Show More References 1. M. Bellare, T. Ristenpart and S. Tessaro, \"Multi-instance security and its application to password-based cryptography\" in Advances in Cryptology - CRYPTO 2012, Springer Berlin Heidelberg, pp. 312-329, 2012. Show in Context Google Scholar 2. S. Billot and B. Lang, \"The structure of shared forests in ambiguous parsing\", Proceedings of the 27th Annual Meeting on Association for Computational Linguistics, pp. 143-151, 1989. Show in Context Google Scholar 3. H. Bojinov, E. Bursztein, X. Boyen and D. Boneh, \"Kamouflage: Loss-resistant password management\", ESORICS, pp. 286-302, 2010. Show in Context Google Scholar 4. J. Bonneau, \"Guessing human-chosen secrets\", May 2012. Show in Context 5. J. Bonneau, \"The science of guessing: analyzing an anonymized corpus of 70 million passwords\", Security and Privacy (SP) 2012 IEEE Symposium on, pp. 538-552, 2012. Show in Context Google Scholar 6. J. Bonneau, C. Herley, P. C. van Oorschot and F. Stajano, \"The Quest to Replace Passwords: A Framework for Comparative Evaluation of Web Authentication Schemes\", 2012 IEEE Symposium on Security and Privacy, May 2012. Show in Context CrossRef Google Scholar 7. B. M. Bowen, S. Hershkop, A. D. Keromytis and S. J. Stolfo, Baiting Inside Attackers Using Decoy Documents, pp. 51-70, 2009. Show in Context Google Scholar 8. B. M. Bowen, V. P. Kemerlis, P. Prabhu, A. D. Keromytis and S. J. Stolfo, \"Automating the injection of believable decoys to detect snooping\", WiSec. ACM, pp. 81-86, 2010. Show in Context CrossRef Google Scholar 9. X. Boyen, \"Halting Password Puzzles - Hard-to-break Encryption from Human-memorable Keys\", 16th USENIX Security Symposium, pp. 119-134, 2007. Show in Context Google Scholar 10. L. Breiman, \"Random Forests\", Machine learning, vol. 45, no. 1, pp. 5-32, 2001. Show in Context CrossRef Google Scholar 11. C. Castelluccia, M. Dürmuth and D. Perito, \"Adaptive password-strength meters from markov models\", NDSS, 2012. Show in Context Google Scholar 12. R. Chatterjee, J. Bonneau, A. Juels and T. Ristenpart, \"Cracking-resistant password vaults using natural-language encoders\" in , available from the authors' websites, 2015. Show in Context CrossRef Google Scholar 13. S. Chiasson, P. van Oorschot and R. Biddle, \"A Usability Study and Critique of Two Password Managers\", Proceedings of the 15. Show in Context Google Scholar 14. M. M. Devillers, \"Analyzing password strength\", Radboud University Nijmegen Tech. Rep 2010. Show in Context Google Scholar 15. R. Dhamija and J. D. Tygar, \"The battle against phishing: Dynamic security skins\", Proceedings of the 2005 symposium on Usable privacy and security, pp. 77-88, 2005. Show in Context CrossRef Google Scholar 16. G. Dick and H. Ceriel, \"Parsing techniques a practical guide\", 1990. Show in Context Google Scholar 17. E. Gabber, P. B. Gibbons, Y. Matias and A. J. Mayer, \"How to Make Personalized Web Browising Simple Secure and Anonymous\", FC '97: Proceedings of the 1st International Conference on Financial Cryptography, pp. 17-32, 1997. Show in Context Google Scholar 18. J. A. Halderman, B. Waters and E. W. Felten, \"A Convenient Method for Securely Managing Passwords\", WWW '05: Proceedings of the 14th International Conference on World Wide Web, pp. 471-479, 2005. Show in Context CrossRef Google Scholar 19. M. E. Hellman, \"A cryptanalytic time-memory trade-off\", Information Theory IEEE Transactions on, vol. 26, no. 4, pp. 401-406, 1980. Show in Context View Article Google Scholar 20. D. Hoover and B. Kausik, \"Software smart cards via cryptographic camouflage\", IEEE Symposium on Security and Privacy, pp. 208-215, 1999. Show in Context View Article Google Scholar 21. M. Jakobsson and M. Dhiman, \"The benefits of understanding passwords\" in Mobile Authentication, Springer, pp. 5-24, 2013. Show in Context CrossRef Google Scholar 22. A. Juels, \"A bodyguard of lies: the use of honey objects in information security\", SACMAT, pp. 1-4, 2014. Show in Context CrossRef Google Scholar 23. A. Juels and T. Ristenpart, \"Honey Encryption: Beyond the brute-force barrier\" in Advances in Cryptology - EUROCRYPT, Springer, pp. 523-540, 2014. Show in Context CrossRef Google Scholar 24. A. Juels and R. Rivest, \"Honeywords: Making password-cracking detectable\", ACM Conference on Computer and Communications Security-CCS, pp. 145-160, 2013, 2013. Show in Context CrossRef Google Scholar 25. B. Kaliski, \"PKCS #5: Password-based cryptography specification version 2.0\", RFC 2289, 2000. Show in Context Google Scholar 26. A. Karole, N. Saxena and N. Christin, \"A comparative usability evaluation of traditional password managers\", Information Security and Cryptology - ICISC 2010 ser. Lecture Notes in Computer Science, vol. 6829, pp. 233-251. Show in Context CrossRef Google Scholar 27. P. Kelley, S. Komanduri, M. Mazurek, R. Shay, T. Vidas, L. Bauer, et al., \"Guess again (and again and again): Measuring password strength by simulating password-cracking algorithms\", IEEE Symposium on Security and Privacy (SP), pp. 523-537, 2012. Show in Context View Article Google Scholar 28. J. Kelsey, B. Schneier, C. Hall and D. Wagner, \"Secure applications of low-entropy keys\" in Information Security, Springer, pp. 121-134, 1998. Show in Context CrossRef Google Scholar 29. Z. Li, W. He, D. Akhawe and D. Song, \"The emperor's new password manager: Security analysis of web-based password managers\", 23rd USENIX Security Symposium (USENIX Security 14), 2014. Show in Context Google Scholar 30. J. Ma, W. Yang, M. Luo and N. Li, \"A study of probabilistic password models\", Proceedings of the 2014 IEEE Symposium on Security and Privacy, pp. 689-704, 2014. Show in Context CrossRef Google Scholar 31. D. McCarney, D. Barrera, J. Clark, S. Chiasson and P. C. van Oorschot, \"Tapas: Design Implementation and Usability Evaluation of a Password Manager\", Proceedings of the 28th Annual Computer Security Applications Conference ser. ACSAC '12, pp. 89-98, 2012. Show in Context CrossRef Google Scholar 32. P. Oechslin, \"Making a faster cryptanalytic time-memory trade-off\" in Advances in Cryptology-CRYPTO 2003, Springer, pp. 617-630, 2003. Show in Context CrossRef Google Scholar 33. C. Percival, \"Stronger key derivation via sequential memory-hard functions\", 2009. Show in Context Google Scholar 34. B. Ross, C. Jackson, N. Miyake, D. Boneh and J. Mitchell, \"Stronger password authentication using browser extensions\", USENIX Security 2005. Show in Context Google Scholar 35. R. Veras, C. Collins and J. Thorpe, \"On the semantic patterns of passwords and their security impact\", Network and Distributed System Security Symposium (NDSS), 2014. Show in Context CrossRef Google Scholar 36. L. Von Ahn, M. Blum, N. J. Hopper and J. Langford, \"CAPTCHA: Using hard AI problems for security\" in Advances in Cryptology—EUROCRYPT 2003, Springer, pp. 294-311, 2003. Show in Context CrossRef Google Scholar 37. M. Weir, S. Aggarwal, B. de Medeiros and B. Glodek, \"Password cracking using probabilistic context-free grammars\", IEEE Symposium on Security and Privacy (SP), pp. 162-175, 2009. Show in Context View Article Google Scholar 38. L. Whitney, \"LastPass CEO reveals details on security breach\", CNet, May 2011. Show in Context Google Scholar 39. K.-P. Yee and K. Sitaker, \"Passpet: convenient password management and phishing protection\", Proceedings of the second symposium on Usable privacy and security, pp. 32-43, 2006. Show in Context CrossRef Google Scholar",
                "ieee_keywords": [],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Noah Snavely",
        "publications": [
            {
                "title": "Towards computational models of kinship verification",
                "link": "https://ieeexplore.ieee.org/document/5652590/",
                "date_of_publication": "03 December 2010",
                "doi": "10.1109/ICIP.2010.5652590",
                "citations": "179",
                "abstract": "We tackle the challenge of kinship verification using novel feature extraction and selection methods, automatically classifying pairs of face images as “related” or “unrelated” (in terms of kinship). First, we conducted a controlled online search to collect frontal face images of 150 pairs of public figures and celebrities, along with images of their parents or children. Next, we propose and evaluate a set of low-level image features for this classification problem. After selecting the most discriminative inherited facial features, we demonstrate a classification accuracy of 70.67% on a test set of image pairs using K-Nearest-Neighbors. Finally, we present an evaluation of human performance on this problem.",
                "ieee_keywords": [
                    "Face",
                    "Feature extraction",
                    "Facial features",
                    "Humans",
                    "Accuracy",
                    "Support vector machine classification",
                    "Image color analysis"
                ],
                "author_keywords": [
                    "Face recognition",
                    "inheritance",
                    "feature extraction"
                ]
            },
            {
                "title": "SfM with MRFs: Discrete-Continuous Optimization for Large-Scale Structure from Motion",
                "link": "https://ieeexplore.ieee.org/document/6327192/",
                "date_of_publication": null,
                "doi": "10.1109/TPAMI.2012.218",
                "citations": "105",
                "abstract": "Recent work in structure from motion (SfM) has built 3D models from large collections of images downloaded from the Internet. Many approaches to this problem use incremental algorithms that solve progressively larger bundle adjustment problems. These incremental techniques scale poorly as the image collection grows, and can suffer from drift or local minima. We present an alternative framework for SfM based on finding a coarse initial solution using hybrid discrete-continuous optimization and then improving that solution using bundle adjustment. The initial optimization step uses a discrete Markov random field (MRF) formulation, coupled with a continuous Levenberg-Marquardt refinement. The formulation naturally incorporates various sources of information about both the cameras and points, including noisy geotags and vanishing point (VP) estimates. We test our method on several large-scale photo collections, including one with measured camera positions, and show that it produces models that are similar to or better than those produced by incremental bundle adjustment, but more robustly and in a fraction of the time.",
                "ieee_keywords": [
                    "Cameras",
                    "Optimization",
                    "Robustness",
                    "Image reconstruction",
                    "Noise measurement",
                    "Belief propagation",
                    "Motion analysis"
                ],
                "author_keywords": [
                    "Structure from motion",
                    "3D reconstruction",
                    "Markov random fields",
                    "belief propagation"
                ]
            },
            {
                "title": "Recovering depth of a dynamic scene using real world motion prior",
                "link": "https://ieeexplore.ieee.org/document/6467083/",
                "date_of_publication": "21 February 2013",
                "doi": "10.1109/ICIP.2012.6467083",
                "citations": "1",
                "abstract": "Given a video of a dynamic scene captured using a dynamic camera, we present a method to recover a dense depth map of the scene with a focus on estimating the depth of the dynamic objects. We assume that the static portions of the scene help estimate the pose of the cameras. We recover a dense depth map of the scene via a plane sweep stereo approach. The relative motion of the dynamic object in the scene however, results in an inaccurate depth estimate. Estimating the accurate depth of the dynamic object is an ambiguous problem since both the depth and the real world speed of the object are unknown. In this work, we show that by using occlusions and putting constraints on the speed of the object we can bound the depth of the object. We can then incorporate this real world motion into the plane sweep stereo framework to obtain a more accurate depth for the dynamic object. We focus on videos with people walking in the scene and show the effectiveness of our approach through quantitative and qualitative results.",
                "ieee_keywords": [
                    "Dynamics",
                    "Cameras",
                    "Heuristic algorithms",
                    "Video sequences",
                    "Estimation",
                    "Stereo vision",
                    "Legged locomotion"
                ],
                "author_keywords": [
                    "Computer vision",
                    "Image sequences",
                    "Image sequence analysis",
                    "Depth from video"
                ]
            },
            {
                "title": "TOUCHDOWN: Natural Language Navigation and Spatial Reasoning in Visual Street Environments",
                "link": "https://ieeexplore.ieee.org/document/8954308/",
                "date_of_publication": "09 January 2020",
                "doi": "10.1109/CVPR.2019.01282",
                "citations": "73",
                "abstract": "We study the problem of jointly reasoning about language and vision through a navigation and spatial reasoning task. We introduce the Touchdown task and dataset, where an agent must first follow navigation instructions in a Street View environment to a goal position, and then guess a location in its observed environment described in natural language to find a hidden object. The data contains 9326 examples of English instructions and spatial descriptions paired with demonstrations. We perform qualitative linguistic analysis, and show that the data displays a rich use of spatial reasoning. Empirical analysis shows the data presents an open challenge to existing methods.",
                "ieee_keywords": [
                    "Visualization",
                    "Navigation",
                    "Urban areas",
                    "Linguistics",
                    "Data collection",
                    "Cognition",
                    "Spatial databases"
                ],
                "author_keywords": [
                    "Vision + Language",
                    "Datasets and Evaluation",
                    "Visual Reasoning"
                ]
            },
            {
                "title": "Deep Feature Interpolation for Image Content Changes",
                "link": "https://ieeexplore.ieee.org/document/8100128/",
                "date_of_publication": "09 November 2017",
                "doi": "10.1109/CVPR.2017.645",
                "citations": "145",
                "abstract": "We propose Deep Feature Interpolation (DFI), a new datadriven baseline for automatic high-resolution image transformation. As the name suggests, DFI relies only on simple linear interpolation of deep convolutional features from pre-trained convnets. We show that despite its simplicity, DFI can perform high-level semantic transformations like “make older/younger”, “make bespectacled”, “add smile”, among others, surprisingly well-sometimes even matching or outperforming the state-of-the-art. This is particularly unexpected as DFI requires no specialized network architecture or even any deep network to be trained for these tasks. DFI therefore can be used as a new baseline to evaluate more complex algorithms and provides a practical answer to the question of which image transformation tasks are still challenging after the advent of deep learning.",
                "ieee_keywords": [
                    "Interpolation",
                    "Hair",
                    "Image resolution",
                    "Image reconstruction",
                    "Image color analysis",
                    "Manifolds"
                ],
                "author_keywords": []
            },
            {
                "title": "3D exploitation of 2D ground-level & aerial imagery",
                "link": "https://ieeexplore.ieee.org/document/6176363/",
                "date_of_publication": "03 April 2012",
                "doi": "10.1109/AIPR.2011.6176363",
                "citations": "2",
                "abstract": "Working with 2.3K+ digital images shot around MIT, we form a SIFT graph which imposes an initial topological ordering upon the quasi-random set of input urban photos. We next employ iterative bundle adjustment algorithms developed by Snavely et al to recover the MIT photos' geometrical structure. After georegistering the 2.3K+ images to an aerial ladar map, we plot recovered camera geolocations on a Google Map and conduct virtual tours through the 3D field of 2D urban photos. Similar computer vision techniques are applied to video footage collected over a rural scene by an aerial glider. 1.5K video frames are reconstructed and georegistered via the UAV's GPS track. We demonstrate how several difficult aerial video exploitation problems become tractable when a geometry-based analysis approach is adopted.",
                "ieee_keywords": [
                    "Three dimensional displays",
                    "Image reconstruction",
                    "Browsers",
                    "Google",
                    "Buildings",
                    "Digital cameras"
                ],
                "author_keywords": []
            },
            {
                "title": "Who’s Waldo? Linking People Across Text and Images",
                "link": "https://ieeexplore.ieee.org/document/9711365/",
                "date_of_publication": "28 February 2022",
                "doi": "10.1109/ICCV48922.2021.00141",
                "citations": "51",
                "abstract": "We present a task and benchmark dataset for person-centric visual grounding, the problem of linking between people named in a caption and people pictured in an image. In contrast to prior work in visual grounding, which is predominantly object-based, our new task masks out the names of people in captions in order to encourage methods trained on such image–caption pairs to focus on contextual cues, such as the rich interactions between multiple people, rather than learning associations between names and appearances. To facilitate this task, we introduce a new dataset, Who’s Waldo, mined automatically from image–caption data on Wikimedia Commons. We propose a Transformer-based method that outperforms several strong baselines on this task, and release our data to the research community to spur work on contextual models that consider both vision and language. Code and data are available at: https://whoswaldo.github.io",
                "ieee_keywords": [
                    "Visualization",
                    "Computer vision",
                    "Codes",
                    "Grounding",
                    "Force",
                    "Benchmark testing",
                    "Transformers"
                ],
                "author_keywords": [
                    "Vision + language",
                    "Datasets and evaluation"
                ]
            },
            {
                "title": "DualSDF: Semantic Shape Manipulation Using a Two-Level Representation",
                "link": "https://ieeexplore.ieee.org/document/9157166/",
                "date_of_publication": "05 August 2020",
                "doi": "10.1109/CVPR42600.2020.00765",
                "citations": "40",
                "abstract": "We are seeing a Cambrian explosion of 3D shape representations for use in machine learning. Some representations seek high expressive power in capturing high-resolution detail. Other approaches seek to represent shapes as compositions of simple parts, which are intuitive for people to understand and easy to edit and manipulate. However, it is difficult to achieve both fidelity and interpretability in the same representation. We propose DualSDF, a representation expressing shapes at two levels of granularity, one capturing fine details and the other representing an abstracted proxy shape using simple and semantically consistent shape primitives. To achieve a tight coupling between the two representations, we use a variational objective over a shared latent space. Our two-level model gives rise to a new shape manipulation technique in which a user can interactively manipulate the coarse proxy shape and see the changes instantly mirrored in the high-resolution shape. Moreover, our model actively augments and guides the manipulation towards producing semantically meaningful shapes, making complex manipulations possible with minimal user input.",
                "ieee_keywords": [
                    "Shape",
                    "Three-dimensional displays",
                    "Neural networks",
                    "Solid modeling",
                    "Surface reconstruction",
                    "Automobiles",
                    "Couplings"
                ],
                "author_keywords": []
            },
            {
                "title": "Visual Chirality",
                "link": "https://ieeexplore.ieee.org/document/9156531/",
                "date_of_publication": "05 August 2020",
                "doi": "10.1109/CVPR42600.2020.01231",
                "citations": "13",
                "abstract": "How can we tell whether an image has been mirrored? While we understand the geometry of mirror reflections very well, less has been said about how it affects distributions of imagery at scale, despite widespread use for data augmentation in computer vision. In this paper, we investigate how the statistics of visual data are changed by reflection. We refer to these changes as ``visual chirality,'' after the concept of geometric chirality---the notion of objects that are distinct from their mirror image. Our analysis of visual chirality reveals surprising results, including low-level chiral signals pervading imagery stemming from image processing in cameras, to the ability to discover visual chirality in images of people and faces. Our work has implications for data augmentation, self-supervised learning, and image forensics.",
                "ieee_keywords": [
                    "Visualization",
                    "Task analysis",
                    "Computer vision",
                    "Training",
                    "Mirrors",
                    "Image processing",
                    "Cameras"
                ],
                "author_keywords": []
            },
            {
                "title": "Extreme Rotation Estimation using Dense Correlation Volumes",
                "link": "https://ieeexplore.ieee.org/document/9577404/",
                "date_of_publication": "02 November 2021",
                "doi": "10.1109/CVPR46437.2021.01433",
                "citations": "3",
                "abstract": "We present a technique for estimating the relative 3D rotation of an RGB image pair in an extreme setting, where the images have little or no overlap. We observe that, even when images do not overlap, there may be rich hidden cues as to their geometric relationship, such as light source directions, vanishing points, and symmetries present in the scene. We propose a network design that can automatically learn such implicit cues by comparing all pairs of points between the two input images. Our method therefore constructs dense feature correlation volumes and processes these to predict relative 3D rotations. Our predictions are formed over a fine-grained discretization of rotations, bypassing difficulties associated with regressing 3D rotations. We demonstrate our approach on a large variety of extreme RGB image pairs, including indoor and outdoor images captured under different lighting conditions and geographic locations. Our evaluation shows that our model can successfully estimate relative rotations among non-overlapping images without compromising performance over overlapping image pairs. 1",
                "ieee_keywords": [
                    "Solid modeling",
                    "Three-dimensional displays",
                    "Correlation",
                    "Computational modeling",
                    "Urban areas",
                    "Lighting",
                    "Training data"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Kilian Weinberger",
        "publications": [
            {
                "title": "Ithaca365: Dataset and Driving Perception under Repeated and Challenging Weather Conditions",
                "link": "https://ieeexplore.ieee.org/document/9879752/",
                "date_of_publication": "27 September 2022",
                "doi": "10.1109/CVPR52688.2022.02069",
                "citations": "3",
                "abstract": "Advances in perception for self-driving cars have accelerated in recent years due to the availability of large-scale datasets, typically collected at specific locations and under nice weather conditions. Yet, to achieve the high safety requirement, these perceptual systems must operate robustly under a wide variety of weather conditions including snow and rain. In this paper, we present a new dataset to enable robust autonomous driving via a novel data collection process - data is repeatedly recorded along a 15 km route under diverse scene (urban, highway, rural, campus), weather (snow, rain, sun), time (day/night), and traffic conditions (pedestrians, cyclists and cars). The dataset includes images and point clouds from cameras and LiDAR sensors, along with high-precision GPS/INS to establish correspondence across routes. The dataset includes road and object annotations using amodal masks to capture partial occlusions and 3D bounding boxes. We demonstrate the uniqueness of this dataset by analyzing the performance of baselines in amodal segmentation of road and objects, depth estimation, and 3D object detection. The repeated routes opens new research directions in object discovery, continual learning, and anomaly detection. Link to Ithaca365: https://ithaca365.mae.cornell.edu/",
                "ieee_keywords": [
                    "Point cloud compression",
                    "Three-dimensional displays",
                    "Rain",
                    "Snow",
                    "Roads",
                    "Pose estimation",
                    "Sensor systems and applications"
                ],
                "author_keywords": [
                    "Datasets and evaluation; 3D from multi-view and sensors; Machine learning; Navigation and autonomous driving; Pose estimation and tracking; Scene analysis and understanding; Vision applications and systems"
                ]
            },
            {
                "title": "Wav2Seq: Pre-Training Speech-to-Text Encoder-Decoder Models Using Pseudo Languages",
                "link": "https://ieeexplore.ieee.org/document/10096988/",
                "date_of_publication": "05 May 2023",
                "doi": "10.1109/ICASSP49357.2023.10096988",
                "citations": "1",
                "abstract": "We introduce Wav2Seq, the first self-supervised approach to pre-train both parts of encoder-decoder models for speech data. We induce a pseudo language as a compact discrete representation, and formulate a self-supervised pseudo speech recognition task — transcribing audio inputs into pseudo subword sequences. This process stands on its own, or can be applied as low-cost second-stage pre-training. We experiment with automatic speech recognition (ASR), spoken named entity recognition, and speech-to-text translation. We set new state-of-the-art results for end-to-end spoken named entity recognition, and show consistent improvements on 8 language pairs for speech-to-text translation, even when competing methods use additional text data for training. On ASR, our approach enables encoder-decoder methods to benefit from pre-training for all parts of the network, and shows comparable performance to highly optimized recent methods.",
                "ieee_keywords": [
                    "Training",
                    "Text recognition",
                    "Self-supervised learning",
                    "Signal processing",
                    "Data models",
                    "Acoustics",
                    "Task analysis"
                ],
                "author_keywords": [
                    "Self-supervision",
                    "Encoder-decoder",
                    "Speech recognition",
                    "Spoken named enitiy recognition",
                    "Speech translation"
                ]
            },
            {
                "title": "Image-to-Image Translation for Autonomous Driving from Coarsely-Aligned Image Pairs",
                "link": "https://ieeexplore.ieee.org/document/10160815/",
                "date_of_publication": "04 July 2023",
                "doi": "10.1109/ICRA48891.2023.10160815",
                "citations": "79",
                "abstract": "A self-driving car must be able to reliably handle adverse weather conditions (e.g., snowy) to operate safely. In this paper, we investigate the idea of turning sensor inputs (i.e., images) captured in an adverse condition into a benign one (i.e., sunny), upon which the downstream tasks (e.g., semantic segmentation) can attain high accuracy. Prior work primarily formulates this as an unpaired image-to-image translation problem due to the lack of paired images captured under the exact same camera poses and semantic layouts. While perfectly-aligned images are not available, one can easily obtain coarsely-paired images. For instance, many people drive the same routes daily in both good and adverse weather; thus, images captured at close-by GPS locations can form a pair. Though data from repeated traversals are unlikely to capture the same foreground objects, we posit that they provide rich contextual information to supervise the image translation model. To this end, we propose a novel training objective leveraging coarsely-aligned image pairs. We show that our coarsely-aligned training scheme leads to a better image translation quality and improved downstream tasks, such as semantic segmentation, monocular depth estimation, and visual localization.",
                "ieee_keywords": [
                    "Training",
                    "Location awareness",
                    "Visualization",
                    "Semantic segmentation",
                    "Semantics",
                    "Stochastic processes",
                    "Turning"
                ],
                "author_keywords": []
            },
            {
                "title": "Resource Aware Person Re-identification Across Multiple Resolutions",
                "link": "https://ieeexplore.ieee.org/document/8578937/",
                "date_of_publication": "16 December 2018",
                "doi": "10.1109/CVPR.2018.00839",
                "citations": "153",
                "abstract": "Not all people are equally easy to identify: color statistics might be enough for some cases while others might require careful reasoning about high- and low-level details. However, prevailing person re-identification(re-ID) methods use one-size-fits-all high-level embeddings from deep convolutional networks for all cases. This might limit their accuracy on difficult examples or makes them needlessly expensive for the easy ones. To remedy this, we present a new person re-ID model that combines effective embeddings built on multiple convolutional network layers, trained with deep-supervision. On traditional re-ID benchmarks, our method improves substantially over the previous state-of-the-art results on all five datasets that we evaluate on. We then propose two new formulations of the person re-ID problem under resource-constraints, and show how our model can be used to effectively trade off accuracy and computation in the presence of resource constraints.",
                "ieee_keywords": [
                    "Task analysis",
                    "Measurement",
                    "Image color analysis",
                    "Feature extraction",
                    "Cognition",
                    "Semantics",
                    "Computer architecture"
                ],
                "author_keywords": []
            },
            {
                "title": "Learning to Detect Mobile Objects from LiDAR Scans Without Labels",
                "link": "https://ieeexplore.ieee.org/document/9879816/",
                "date_of_publication": "27 September 2022",
                "doi": "10.1109/CVPR52688.2022.00120",
                "citations": "5",
                "abstract": "Current 3D object detectors for autonomous driving are almost entirely trained on human-annotated data. Although of high quality, the generation of such data is laborious and costly, restricting them to a few specific locations and object types. This paper proposes an alternative approach entirely based on unlabeled data, which can be collected cheaply and in abundance almost everywhere on earth. Our approach leverages several simple common sense heuristics to create an initial set of approximate seed labels. For example, relevant traffic participants are generally not persistent across multiple traversals of the same route, do not fly, and are never under ground. We demonstrate that these seed labels are highly effective to bootstrap a surprisingly accurate detector through repeated self-training without a single human annotated label. Code is available at https://github.com/YurongYou/MODEST.",
                "ieee_keywords": [
                    "Training",
                    "Three-dimensional displays",
                    "Laser radar",
                    "Navigation",
                    "Detectors",
                    "Pattern recognition",
                    "Sensors"
                ],
                "author_keywords": [
                    "Recognition: detection",
                    "categorization",
                    "retrieval; 3D from multi-view and sensors; Navigation and autonomous driving; Transfer/low-shot/long-tail learning"
                ]
            },
            {
                "title": "End-to-End Pseudo-LiDAR for Image-Based 3D Object Detection",
                "link": "https://ieeexplore.ieee.org/document/9157553/",
                "date_of_publication": "05 August 2020",
                "doi": "10.1109/CVPR42600.2020.00592",
                "citations": "78",
                "abstract": "Reliable and accurate 3D object detection is a necessity for safe autonomous driving. Although LiDAR sensors can provide accurate 3D point cloud estimates of the environment, they are also prohibitively expensive for many settings. Recently, the introduction of pseudo-LiDAR (PL) has led to a drastic reduction in the accuracy gap between methods based on LiDAR sensors and those based on cheap stereo cameras. PL combines state-of-the-art deep neural networks for 3D depth estimation with those for 3D object detection by converting 2D depth map outputs to 3D point cloud inputs. However, so far these two networks have to be trained separately. In this paper, we introduce a new framework based on differentiable Change of Representation (CoR) modules that allow the entire PL pipeline to be trained end-to-end. The resulting framework is compatible with most state-of-the-art networks for both tasks and in combination with PointRCNN improves over PL consistently across all benchmarks --- yielding the highest entry on the KITTI image-based 3D object detection leaderboard at the time of submission. Our code will be made available at https://github.com/mileyan/pseudo-LiDAR_e2e.",
                "ieee_keywords": [
                    "Three-dimensional displays",
                    "Detectors",
                    "Object detection",
                    "Laser radar",
                    "Pipelines",
                    "Quantization (signal)",
                    "Estimation"
                ],
                "author_keywords": []
            },
            {
                "title": "Anytime Stereo Image Depth Estimation on Mobile Devices",
                "link": "https://ieeexplore.ieee.org/document/8794003/",
                "date_of_publication": "12 August 2019",
                "doi": "10.1109/ICRA.2019.8794003",
                "citations": "77",
                "abstract": "Many applications of stereo depth estimation in robotics require the generation of accurate disparity maps in real time under significant computational constraints. Current state-of-the-art algorithms force a choice between either generating accurate mappings at a slow pace, or quickly generating inaccurate ones, and additionally these methods typically require far too many parameters to be usable on power- or memory-constrained devices. Motivated by these shortcomings, we propose a novel approach for disparity prediction in the anytime setting. In contrast to prior work, our end-to-end learned approach can trade off computation and accuracy at inference time. Depth estimation is performed in stages, during which the model can be queried at any time to output its current best estimate. Our final model can process 1242×375 resolution images within a range of 10-35 FPS on an NVIDIA Jetson TX2 module with only marginal increases in error - using two orders of magnitude fewer parameters than the most competitive baseline. The source code is available at https://github.com/mileyan/AnyNet.",
                "ieee_keywords": [
                    "Estimation",
                    "Image resolution",
                    "Feature extraction",
                    "Computational modeling",
                    "Three-dimensional displays",
                    "Cameras",
                    "Predictive models"
                ],
                "author_keywords": []
            },
            {
                "title": "Train in Germany, Test in the USA: Making 3D Object Detectors Generalize",
                "link": "https://ieeexplore.ieee.org/document/9156543/",
                "date_of_publication": "05 August 2020",
                "doi": "10.1109/CVPR42600.2020.01173",
                "citations": "41",
                "abstract": "In the domain of autonomous driving, deep learning has substantially improved the 3D object detection accuracy for LiDAR and stereo camera data alike. While deep networks are great at generalization, they are also notorious to overfit to all kinds of spurious artifacts, such as brightness, car sizes and models, that may appear consistently throughout the data. In fact, most datasets for autonomous driving are collected within a narrow subset of cities within one country, typically under similar weather conditions. In this paper we consider the task of adapting 3D object detectors from one dataset to another. We observe that naively, this appears to be a very challenging task, resulting in drastic drops in accuracy levels. We provide extensive experiments to investigate the true adaptation challenges and arrive at a surprising conclusion: the primary adaptation hurdle to overcome are differences in car sizes across geographic areas. A simple correction based on the average car size yields a strong correction of the adaptation gap. Our proposed method is simple and easily incorporated into most 3D object detection frameworks. It provides a first baseline for 3D object detection adaptation across countries, and gives hope that the underlying problem may be more within grasp than one may have hoped to believe. Our code is available at https://github. com/cxy1997/3D_adapt_auto_driving.",
                "ieee_keywords": [
                    "Three-dimensional displays",
                    "Laser radar",
                    "Automobiles",
                    "Detectors",
                    "Object detection",
                    "Cameras",
                    "Training"
                ],
                "author_keywords": []
            },
            {
                "title": "Pseudo-LiDAR From Visual Depth Estimation: Bridging the Gap in 3D Object Detection for Autonomous Driving",
                "link": "https://ieeexplore.ieee.org/document/8954293/",
                "date_of_publication": "09 January 2020",
                "doi": "10.1109/CVPR.2019.00864",
                "citations": "433",
                "abstract": "3D object detection is an essential task in autonomous driving. Recent techniques excel with highly accurate detection rates, provided the 3D input data is obtained from precise but expensive LiDAR technology. Approaches based on cheaper monocular or stereo imagery data have, until now, resulted in drastically lower accuracies --- a gap that is commonly attributed to poor image-based depth estimation. However, in this paper we argue that it is not the quality of the data but its representation that accounts for the majority of the difference. Taking the inner workings of convolutional neural networks into consideration, we propose to convert image-based depth maps to pseudo-LiDAR representations --- essentially mimicking the LiDAR signal. With this representation we can apply different existing LiDAR-based detection algorithms. On the popular KITTI benchmark, our approach achieves impressive improvements over the existing state-of-the-art in image-based performance --- raising the detection accuracy of objects within the 30m range from the previous state-of-the-art of 22% to an unprecedented 74%. At the time of submission our algorithm holds the highest entry on the KITTI 3D object detection leaderboard for stereo-image-based approaches.",
                "ieee_keywords": [
                    "Visualization",
                    "Three-dimensional displays",
                    "Laser radar",
                    "Estimation",
                    "Object detection",
                    "Pattern recognition",
                    "Convolutional neural networks"
                ],
                "author_keywords": [
                    "Robotics + Driving",
                    "3D from Multiview and Sensors",
                    "Recognition: Detection",
                    "Categorization",
                    "Retrieval"
                ]
            },
            {
                "title": "LDLS: 3-D Object Segmentation Through Label Diffusion From 2-D Images",
                "link": "https://ieeexplore.ieee.org/document/8735751/",
                "date_of_publication": null,
                "doi": "10.1109/LRA.2019.2922582",
                "citations": "16",
                "abstract": "Object segmentation in three-dimensional (3-D) point clouds is a critical task for robots capable of 3-D perception. Despite the impressive performance of deep learning-based approaches on object segmentation in 2-D images, deep learning has not been applied nearly as successfully for 3-D point cloud segmentation. Deep networks generally require large amounts of labeled training data, which are readily available for 2-D images but are difficult to produce for 3-D point clouds. In this letter, we present Label Diffusion Lidar Segmentation (LDLS), a novel approach for 3-D point cloud segmentation, which leverages 2-D segmentation of an RGB image from an aligned camera to avoid the need for training on annotated 3-D data. We obtain 2-D segmentation predictions by applying Mask-RCNN to the RGB image, and then link this image to a 3-D lidar point cloud by building a graph of connections among 3-D points and 2-D pixels. This graph then directs a semi-supervised label diffusion process, where the 2-D pixels act as source nodes that diffuse object label information through the 3-D point cloud, resulting in a complete 3-D point cloud segmentation. We conduct empirical studies on the KITTI benchmark dataset and on a mobile robot, demonstrating wide applicability and superior performance of LDLS compared with the previous state of the art in 3-D point cloud segmentation, without any need for either 3-D training data or fine tuning of the 2-D image segmentation model.",
                "ieee_keywords": [
                    "Three-dimensional displays",
                    "Two dimensional displays",
                    "Image segmentation",
                    "Laser radar",
                    "Sensors",
                    "Cameras",
                    "Task analysis"
                ],
                "author_keywords": [
                    "Object detection",
                    "segmentation and categorization",
                    "RGB-D perception"
                ]
            }
        ]
    },
    {
        "name": "Alexander M. Rush",
        "publications": [
            {
                "title": "9.8 A 25mm2 SoC for IoT Devices with 18ms Noise-Robust Speech-to-Text Latency via Bayesian Speech Denoising and Attention-Based Sequence-to-Sequence DNN Speech Recognition in 16nm FinFET",
                "link": "https://ieeexplore.ieee.org/document/9366062/",
                "date_of_publication": "03 March 2021",
                "doi": "10.1109/ISSCC42613.2021.9366062",
                "citations": "10",
                "abstract": "Automatic speech recognition (ASR) using deep learning is essential for user interfaces on IoT devices. However, previously published ASR chips [4-7] do not consider realistic operating conditions, which are typically noisy and may include more than one speaker. Furthermore, several of these works have implemented only small-vocabulary tasks, such as keyword-spotting (KWS), where context-blind deep neural network (DNN) algorithms are adequate. However, for large-vocabulary tasks (e.g., >100k words), the more complex bidirectional RNNs with an attention mechanism [1] provide context learning in long sequences, which improve ASR accuracy by up to 62% on the 200kwords LibriSpeech dataset, compared to a simpler unidirectional RNN (Fig. 9.8.1). Attention-based networks emphasize the most relevant parts of the source sequence during each decoding time step. In doing so, the encoder sequence is treated as a soft-addressable memory whose positions are weighted based on the state of the decoder RNN. Bidirectional RNNs learn past and future temporal information by concatenating forward and backward time steps.",
                "ieee_keywords": [
                    "Neural networks",
                    "User interfaces",
                    "Decoding",
                    "Solid state circuits",
                    "Noise robustness",
                    "Noise measurement",
                    "Task analysis"
                ],
                "author_keywords": []
            },
            {
                "title": "SM6: A 16nm System-on-Chip for Accurate and Noise-Robust Attention-Based NLP Applications : The 33rd Hot Chips Symposium – August 22-24, 2021",
                "link": "https://ieeexplore.ieee.org/document/9567180/",
                "date_of_publication": "20 October 2021",
                "doi": "10.1109/HCS52781.2021.9567180",
                "citations": "234",
                "abstract": "In this work, we present SM6, an SoC architecture for real-time denoised speech and NLP pipelines, featuring (1) MSSE: an unsupervised probabilistic sound source separation accelerator, (2) FlexNLP: a programmable inference accelerator for attention-based seq2seq DNNs using adaptive floating-point datatypes for wide dynamic range computations, (3) a dual-core Arm Cortex A53 CPU cluster, which provides on-demand SIMD FFT processing, and operating system support. In adverse acoustic conditions, MSSE allows FlexNLP to store up to 6x smaller ASR models obviating the very inefficient strategy of scaling up the DNN model to achieve noise robustness. MSSE and FlexNLP produce efficiency ranges of 4.33-17.6 Gsamples/s/W and 2.6-7.8TFLOPs/W, respectively, with per-frame end-to-end latencies of 15-45ms.",
                "ieee_keywords": [
                    "Source separation",
                    "Computational modeling",
                    "Operating systems",
                    "Pipelines",
                    "Brain modeling",
                    "Probabilistic logic",
                    "Real-time systems"
                ],
                "author_keywords": []
            },
            {
                "title": "GenNI: Human-AI Collaboration for Data-Backed Text Generation",
                "link": "https://ieeexplore.ieee.org/document/9552430/",
                "date_of_publication": null,
                "doi": "10.1109/TVCG.2021.3114845",
                "citations": "3",
                "abstract": "Table2Text systems generate textual output based on structured data utilizing machine learning. These systems are essential for fluent natural language interfaces in tools such as virtual assistants; however, left to generate freely these ML systems often produce misleading or unexpected outputs. GenNI (Generation Negotiation Interface) is an interactive visual system for high-level human-AI collaboration in producing descriptive text. The tool utilizes a deep learning model designed with explicit control states. These controls allow users to globally constrain model generations, without sacrificing the representation power of the deep learning models. The visual interface makes it possible for users to interact with AI systems following a Refine-Forecast paradigm to ensure that the generation system acts in a manner human users find suitable. We report multiple use cases on two experiments that improve over uncontrolled generation approaches, while at the same time providing fine-grained control. A demo and source code are available at https://genni.vizhub.ai.",
                "ieee_keywords": [
                    "Computational modeling",
                    "Visualization",
                    "Tools",
                    "Data models",
                    "Collaboration",
                    "Task analysis",
                    "Deep learning"
                ],
                "author_keywords": [
                    "Tabular Data",
                    "Text/Document Data",
                    "Machine Learning",
                    "Statistics",
                    "Modelling",
                    "Simulation Applications"
                ]
            },
            {
                "title": "A 16-nm SoC for Noise-Robust Speech and NLP Edge AI Inference With Bayesian Sound Source Separation and Attention-Based DNNs",
                "link": "https://ieeexplore.ieee.org/document/9791855/",
                "date_of_publication": null,
                "doi": "10.1109/JSSC.2022.3179303",
                "citations": "1088",
                "abstract": "The proliferation of personal artificial intelligence (AI) -assistant technologies with speech-based conversational AI interfaces is driving the exponential growth in the consumer Internet of Things (IoT) market. As these technologies are being applied to keyword spotting (KWS), automatic speech recognition (ASR), natural language processing (NLP), and text-to-speech (TTS) applications, it is of paramount importance that they provide uncompromising performance for context learning in long sequences, which is a key benefit of the attention mechanism, and that they work seamlessly in polyphonic environments. In this work, we present a 25-mm2 system-on-chip (SoC) in 16-nm FinFET technology, codenamed SM6, which executes end-to-end speech-enhancing attention-based ASR and NLP workloads. The SoC includes: 1) FlexASR, a highly reconfigurable NLP inference processor optimized for whole-model acceleration of bidirectional attention-based sequence-to-sequence (seq2seq) deep neural networks (DNNs); 2) a Markov random field source separation engine (MSSE), a probabilistic graphical model accelerator for unsupervised inference via Gibbs sampling, used for sound source separation; 3) a dual-core Arm Cortex A53 CPU cluster, which provides on-demand single Instruction/multiple data (SIMD) fast fourier transform (FFT) processing and performs various application logic (e.g., expectation–maximization (EM) algorithm and 8-bit floating-point (FP8) quantization); and 4) an always-ON M0 subsystem for audio detection and power management. Measurement results demonstrate the efficiency ranges of 2.6–7.8 TFLOPs/W and 4.33–17.6 Gsamples/s/W for FlexASR and MSSE, respectively; MSSE denoising performance allowing 6 $\\times $ smaller ASR model to be stored on-chip with negligible accuracy loss; and 2.24-mJ energy consumption while achieving real-time throughput, end-to-end, and per-frame ASR latencies of 18 ms.",
                "ieee_keywords": [
                    "Task analysis",
                    "Computational modeling",
                    "Internet of Things",
                    "Source separation",
                    "Decoding",
                    "System-on-chip",
                    "Recurrent neural networks"
                ],
                "author_keywords": [
                    "Attention mechanism",
                    "Gibbs sampling",
                    "hardware accelerators",
                    "Internet of Things (IoT)",
                    "natural language processing (NLP)",
                    "recurrent neural networks (RNNs)",
                    "sound source separation",
                    "speech recognition",
                    "system-on-chip (SoC)"
                ]
            },
            {
                "title": "Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models",
                "link": "https://ieeexplore.ieee.org/document/9908590/",
                "date_of_publication": null,
                "doi": "10.1109/TVCG.2022.3209479",
                "citations": "3",
                "abstract": "State-of-the-art neural language models can now be used to solve ad-hoc language tasks through zero-shot prompting without the need for supervised training. This approach has gained popularity in recent years, and researchers have demonstrated prompts that achieve strong accuracy on specific NLP tasks. However, finding a prompt for new tasks requires experimentation. Different prompt templates with different wording choices lead to significant accuracy differences. PromptIDE allows users to experiment with prompt variations, visualize prompt performance, and iteratively optimize prompts. We developed a workflow that allows users to first focus on model feedback using small data before moving on to a large data regime that allows empirical grounding of promising prompts using quantitative measures of the task. The tool then allows easy deployment of the newly created ad-hoc models. We demonstrate the utility of PromptIDE (demo: http://prompt.vizhub.ai ) and our workflow using several real-world use cases.",
                "ieee_keywords": [
                    "Task analysis",
                    "Visualization",
                    "Analytical models",
                    "Training",
                    "Natural language processing",
                    "Transformers",
                    "Computational modeling"
                ],
                "author_keywords": [
                    "Natural language processing",
                    "language modeling",
                    "zero-shot models"
                ]
            }
        ]
    },
    {
        "name": "bharath hariharan",
        "publications": [
            {
                "title": "Ithaca365: Dataset and Driving Perception under Repeated and Challenging Weather Conditions",
                "link": "https://ieeexplore.ieee.org/document/9879752/",
                "date_of_publication": "27 September 2022",
                "doi": "10.1109/CVPR52688.2022.02069",
                "citations": "3",
                "abstract": "Advances in perception for self-driving cars have accelerated in recent years due to the availability of large-scale datasets, typically collected at specific locations and under nice weather conditions. Yet, to achieve the high safety requirement, these perceptual systems must operate robustly under a wide variety of weather conditions including snow and rain. In this paper, we present a new dataset to enable robust autonomous driving via a novel data collection process - data is repeatedly recorded along a 15 km route under diverse scene (urban, highway, rural, campus), weather (snow, rain, sun), time (day/night), and traffic conditions (pedestrians, cyclists and cars). The dataset includes images and point clouds from cameras and LiDAR sensors, along with high-precision GPS/INS to establish correspondence across routes. The dataset includes road and object annotations using amodal masks to capture partial occlusions and 3D bounding boxes. We demonstrate the uniqueness of this dataset by analyzing the performance of baselines in amodal segmentation of road and objects, depth estimation, and 3D object detection. The repeated routes opens new research directions in object discovery, continual learning, and anomaly detection. Link to Ithaca365: https://ithaca365.mae.cornell.edu/",
                "ieee_keywords": [
                    "Point cloud compression",
                    "Three-dimensional displays",
                    "Rain",
                    "Snow",
                    "Roads",
                    "Pose estimation",
                    "Sensor systems and applications"
                ],
                "author_keywords": [
                    "Datasets and evaluation; 3D from multi-view and sensors; Machine learning; Navigation and autonomous driving; Pose estimation and tracking; Scene analysis and understanding; Vision applications and systems"
                ]
            },
            {
                "title": "PointFlow: 3D Point Cloud Generation With Continuous Normalizing Flows",
                "link": "https://ieeexplore.ieee.org/document/9010395/",
                "date_of_publication": "27 February 2020",
                "doi": "10.1109/ICCV.2019.00464",
                "citations": "195",
                "abstract": "As 3D point clouds become the representation of choice for multiple vision and graphics applications, the ability to synthesize or reconstruct high-resolution, high-fidelity point clouds becomes crucial. Despite the recent success of deep learning models in discriminative tasks of point clouds, generating point clouds remains challenging. This paper proposes a principled probabilistic framework to generate 3D point clouds by modeling them as a distribution of distributions. Specifically, we learn a two-level hierarchy of distributions where the first level is the distribution of shapes and the second level is the distribution of points given a shape. This formulation allows us to both sample shapes and sample an arbitrary number of points from a shape. Our generative model, named PointFlow, learns each level of the distribution with a continuous normalizing flow. The invertibility of normalizing flows enables the computation of the likelihood during training and allows us to train our model in the variational inference framework. Empirically, we demonstrate that PointFlow achieves state-of-the-art performance in point cloud generation. We additionally show that our model can faithfully reconstruct point clouds and learn useful representations in an unsupervised manner. The code is available at https://github.com/stevenygd/PointFlow.",
                "ieee_keywords": [
                    "Three-dimensional displays",
                    "Shape",
                    "Computational modeling",
                    "Training",
                    "Task analysis",
                    "Gallium nitride",
                    "Solid modeling"
                ],
                "author_keywords": []
            },
            {
                "title": "Exploiting Playbacks in Unsupervised Domain Adaptation for 3D Object Detection in Self-Driving Cars",
                "link": "https://ieeexplore.ieee.org/document/9811722/",
                "date_of_publication": "12 July 2022",
                "doi": "10.1109/ICRA46639.2022.9811722",
                "citations": "5",
                "abstract": "Self-driving cars must detect other traffic participants like vehicles and pedestrians in 3D in order to plan safe routes and avoid collisions. State-of-the-art 3D object detectors, based on deep learning, have shown promising accuracy but are prone to over-fit domain idiosyncrasies, making them fail in new environments-a serious problem for the robustness of self-driving cars. In this paper, we propose a novel learning approach that reduces this gap by fine-tuning the detector on high-quality pseudo-labels in the target domain - pseudo-labels that are automatically generated after driving based on replays of previously recorded driving sequences. In these replays, object tracks are smoothed forward and backward in time, and detections are interpolated and extrapolated-crucially, leveraging future information to catch hard cases such as missed detections due to occlusions or far ranges. We show, across five autonomous driving datasets, that fine-tuning the object detector on these pseudo-labels substantially reduces the domain gap to new driving environments, yielding strong improvements detection reliability and accuracy.",
                "ieee_keywords": [
                    "Deep learning",
                    "Three-dimensional displays",
                    "Target tracking",
                    "Automation",
                    "Detectors",
                    "Object detection",
                    "Robustness"
                ],
                "author_keywords": [
                    "Object Detection",
                    "Segmentation and Categorization",
                    "Computer Vision for Automation",
                    "Transfer Learning",
                    "Deep Learning for Visual Perception"
                ]
            },
            {
                "title": "Image-to-Image Translation for Autonomous Driving from Coarsely-Aligned Image Pairs",
                "link": "https://ieeexplore.ieee.org/document/10160815/",
                "date_of_publication": "04 July 2023",
                "doi": "10.1109/ICRA48891.2023.10160815",
                "citations": "79",
                "abstract": "A self-driving car must be able to reliably handle adverse weather conditions (e.g., snowy) to operate safely. In this paper, we investigate the idea of turning sensor inputs (i.e., images) captured in an adverse condition into a benign one (i.e., sunny), upon which the downstream tasks (e.g., semantic segmentation) can attain high accuracy. Prior work primarily formulates this as an unpaired image-to-image translation problem due to the lack of paired images captured under the exact same camera poses and semantic layouts. While perfectly-aligned images are not available, one can easily obtain coarsely-paired images. For instance, many people drive the same routes daily in both good and adverse weather; thus, images captured at close-by GPS locations can form a pair. Though data from repeated traversals are unlikely to capture the same foreground objects, we posit that they provide rich contextual information to supervise the image translation model. To this end, we propose a novel training objective leveraging coarsely-aligned image pairs. We show that our coarsely-aligned training scheme leads to a better image translation quality and improved downstream tasks, such as semantic segmentation, monocular depth estimation, and visual localization.",
                "ieee_keywords": [
                    "Training",
                    "Location awareness",
                    "Visualization",
                    "Semantic segmentation",
                    "Semantics",
                    "Stochastic processes",
                    "Turning"
                ],
                "author_keywords": []
            },
            {
                "title": "Resource Aware Person Re-identification Across Multiple Resolutions",
                "link": "https://ieeexplore.ieee.org/document/8578937/",
                "date_of_publication": "16 December 2018",
                "doi": "10.1109/CVPR.2018.00839",
                "citations": "153",
                "abstract": "Not all people are equally easy to identify: color statistics might be enough for some cases while others might require careful reasoning about high- and low-level details. However, prevailing person re-identification(re-ID) methods use one-size-fits-all high-level embeddings from deep convolutional networks for all cases. This might limit their accuracy on difficult examples or makes them needlessly expensive for the easy ones. To remedy this, we present a new person re-ID model that combines effective embeddings built on multiple convolutional network layers, trained with deep-supervision. On traditional re-ID benchmarks, our method improves substantially over the previous state-of-the-art results on all five datasets that we evaluate on. We then propose two new formulations of the person re-ID problem under resource-constraints, and show how our model can be used to effectively trade off accuracy and computation in the presence of resource constraints.",
                "ieee_keywords": [
                    "Task analysis",
                    "Measurement",
                    "Image color analysis",
                    "Feature extraction",
                    "Cognition",
                    "Semantics",
                    "Computer architecture"
                ],
                "author_keywords": []
            },
            {
                "title": "Learning to Detect Mobile Objects from LiDAR Scans Without Labels",
                "link": "https://ieeexplore.ieee.org/document/9879816/",
                "date_of_publication": "27 September 2022",
                "doi": "10.1109/CVPR52688.2022.00120",
                "citations": "5",
                "abstract": "Current 3D object detectors for autonomous driving are almost entirely trained on human-annotated data. Although of high quality, the generation of such data is laborious and costly, restricting them to a few specific locations and object types. This paper proposes an alternative approach entirely based on unlabeled data, which can be collected cheaply and in abundance almost everywhere on earth. Our approach leverages several simple common sense heuristics to create an initial set of approximate seed labels. For example, relevant traffic participants are generally not persistent across multiple traversals of the same route, do not fly, and are never under ground. We demonstrate that these seed labels are highly effective to bootstrap a surprisingly accurate detector through repeated self-training without a single human annotated label. Code is available at https://github.com/YurongYou/MODEST.",
                "ieee_keywords": [
                    "Training",
                    "Three-dimensional displays",
                    "Laser radar",
                    "Navigation",
                    "Detectors",
                    "Pattern recognition",
                    "Sensors"
                ],
                "author_keywords": [
                    "Recognition: detection",
                    "categorization",
                    "retrieval; 3D from multi-view and sensors; Navigation and autonomous driving; Transfer/low-shot/long-tail learning"
                ]
            },
            {
                "title": "Extreme Rotation Estimation using Dense Correlation Volumes",
                "link": "https://ieeexplore.ieee.org/document/9577404/",
                "date_of_publication": "02 November 2021",
                "doi": "10.1109/CVPR46437.2021.01433",
                "citations": "3",
                "abstract": "We present a technique for estimating the relative 3D rotation of an RGB image pair in an extreme setting, where the images have little or no overlap. We observe that, even when images do not overlap, there may be rich hidden cues as to their geometric relationship, such as light source directions, vanishing points, and symmetries present in the scene. We propose a network design that can automatically learn such implicit cues by comparing all pairs of points between the two input images. Our method therefore constructs dense feature correlation volumes and processes these to predict relative 3D rotations. Our predictions are formed over a fine-grained discretization of rotations, bypassing difficulties associated with regressing 3D rotations. We demonstrate our approach on a large variety of extreme RGB image pairs, including indoor and outdoor images captured under different lighting conditions and geographic locations. Our evaluation shows that our model can successfully estimate relative rotations among non-overlapping images without compromising performance over overlapping image pairs. 1",
                "ieee_keywords": [
                    "Solid modeling",
                    "Three-dimensional displays",
                    "Correlation",
                    "Computational modeling",
                    "Urban areas",
                    "Lighting",
                    "Training data"
                ],
                "author_keywords": []
            },
            {
                "title": "End-to-End Pseudo-LiDAR for Image-Based 3D Object Detection",
                "link": "https://ieeexplore.ieee.org/document/9157553/",
                "date_of_publication": "05 August 2020",
                "doi": "10.1109/CVPR42600.2020.00592",
                "citations": "78",
                "abstract": "Reliable and accurate 3D object detection is a necessity for safe autonomous driving. Although LiDAR sensors can provide accurate 3D point cloud estimates of the environment, they are also prohibitively expensive for many settings. Recently, the introduction of pseudo-LiDAR (PL) has led to a drastic reduction in the accuracy gap between methods based on LiDAR sensors and those based on cheap stereo cameras. PL combines state-of-the-art deep neural networks for 3D depth estimation with those for 3D object detection by converting 2D depth map outputs to 3D point cloud inputs. However, so far these two networks have to be trained separately. In this paper, we introduce a new framework based on differentiable Change of Representation (CoR) modules that allow the entire PL pipeline to be trained end-to-end. The resulting framework is compatible with most state-of-the-art networks for both tasks and in combination with PointRCNN improves over PL consistently across all benchmarks --- yielding the highest entry on the KITTI image-based 3D object detection leaderboard at the time of submission. Our code will be made available at https://github.com/mileyan/pseudo-LiDAR_e2e.",
                "ieee_keywords": [
                    "Three-dimensional displays",
                    "Detectors",
                    "Object detection",
                    "Laser radar",
                    "Pipelines",
                    "Quantization (signal)",
                    "Estimation"
                ],
                "author_keywords": []
            },
            {
                "title": "Train in Germany, Test in the USA: Making 3D Object Detectors Generalize",
                "link": "https://ieeexplore.ieee.org/document/9156543/",
                "date_of_publication": "05 August 2020",
                "doi": "10.1109/CVPR42600.2020.01173",
                "citations": "41",
                "abstract": "In the domain of autonomous driving, deep learning has substantially improved the 3D object detection accuracy for LiDAR and stereo camera data alike. While deep networks are great at generalization, they are also notorious to overfit to all kinds of spurious artifacts, such as brightness, car sizes and models, that may appear consistently throughout the data. In fact, most datasets for autonomous driving are collected within a narrow subset of cities within one country, typically under similar weather conditions. In this paper we consider the task of adapting 3D object detectors from one dataset to another. We observe that naively, this appears to be a very challenging task, resulting in drastic drops in accuracy levels. We provide extensive experiments to investigate the true adaptation challenges and arrive at a surprising conclusion: the primary adaptation hurdle to overcome are differences in car sizes across geographic areas. A simple correction based on the average car size yields a strong correction of the adaptation gap. Our proposed method is simple and easily incorporated into most 3D object detection frameworks. It provides a first baseline for 3D object detection adaptation across countries, and gives hope that the underlying problem may be more within grasp than one may have hoped to believe. Our code is available at https://github. com/cxy1997/3D_adapt_auto_driving.",
                "ieee_keywords": [
                    "Three-dimensional displays",
                    "Laser radar",
                    "Automobiles",
                    "Detectors",
                    "Object detection",
                    "Cameras",
                    "Training"
                ],
                "author_keywords": []
            },
            {
                "title": "Pseudo-LiDAR From Visual Depth Estimation: Bridging the Gap in 3D Object Detection for Autonomous Driving",
                "link": "https://ieeexplore.ieee.org/document/8954293/",
                "date_of_publication": "09 January 2020",
                "doi": "10.1109/CVPR.2019.00864",
                "citations": "433",
                "abstract": "3D object detection is an essential task in autonomous driving. Recent techniques excel with highly accurate detection rates, provided the 3D input data is obtained from precise but expensive LiDAR technology. Approaches based on cheaper monocular or stereo imagery data have, until now, resulted in drastically lower accuracies --- a gap that is commonly attributed to poor image-based depth estimation. However, in this paper we argue that it is not the quality of the data but its representation that accounts for the majority of the difference. Taking the inner workings of convolutional neural networks into consideration, we propose to convert image-based depth maps to pseudo-LiDAR representations --- essentially mimicking the LiDAR signal. With this representation we can apply different existing LiDAR-based detection algorithms. On the popular KITTI benchmark, our approach achieves impressive improvements over the existing state-of-the-art in image-based performance --- raising the detection accuracy of objects within the 30m range from the previous state-of-the-art of 22% to an unprecedented 74%. At the time of submission our algorithm holds the highest entry on the KITTI 3D object detection leaderboard for stereo-image-based approaches.",
                "ieee_keywords": [
                    "Visualization",
                    "Three-dimensional displays",
                    "Laser radar",
                    "Estimation",
                    "Object detection",
                    "Pattern recognition",
                    "Convolutional neural networks"
                ],
                "author_keywords": [
                    "Robotics + Driving",
                    "3D from Multiview and Sensors",
                    "Recognition: Detection",
                    "Categorization",
                    "Retrieval"
                ]
            }
        ]
    },
    {
        "name": "Claire Cardie",
        "publications": [
            {
                "title": "Physics-Inspired Neural Networks for Efficient Device Compact Modeling",
                "link": "https://ieeexplore.ieee.org/document/7778193/",
                "date_of_publication": null,
                "doi": "10.1109/JXCDC.2016.2636161",
                "citations": "44",
                "abstract": "We present a novel physics-inspired neural network (Pi-NN) approach for compact modeling. Development of high-quality compact models for devices is a key to connect device science with applications. One recent approach is to treat compact modeling as a regression problem in machine learning. The most common learning algorithm to develop compact models is the multilayer perceptron (MLP) neural network. However, device compact models derived using the MLP neural networks often exhibit unphysical behavior, which is eliminated in the Pi-NN approach proposed in this paper, since the Pi-NN incorporates fundamental device physics. As a result, smooth, accurate, and computationally efficient device models can be learned from discrete data points by using Pi-NN. This paper sheds new light on the future of the neural network compact modeling. The novel neural network architecture for transistor modeling.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Intentonomy: a Dataset and Study towards Human Intent Understanding",
                "link": "https://ieeexplore.ieee.org/document/9578470/",
                "date_of_publication": "02 November 2021",
                "doi": "10.1109/CVPR46437.2021.01279",
                "citations": "6",
                "abstract": "An image is worth a thousand words, conveying information that goes beyond the mere visual content therein. In this paper, we study the intent behind social media images with an aim to analyze how visual information can facilitate recognition of human intent. Towards this goal, we introduce an intent dataset, Intentonomy, comprising 14K images covering a wide range of everyday scenes. These images are manually annotated with 28 intent categories derived from a social psychology taxonomy. We then systematically study whether, and to what extent, commonly used visual information, i.e., object and context, contribute to human motive understanding. Based on our findings, we conduct further study to quantify the effect of attending to object and context classes as well as textual information in the form of hashtags when training an intent classifier. Our results quantitatively and qualitatively shed light on how visual and textual information can produce observable effects when predicting intent. 1",
                "ieee_keywords": [
                    "Training",
                    "Visualization",
                    "Computer vision",
                    "Image recognition",
                    "Social networking (online)",
                    "Taxonomy",
                    "Psychology"
                ],
                "author_keywords": []
            },
            {
                "title": "Multi-aspect Sentiment Analysis with Topic Models",
                "link": "https://ieeexplore.ieee.org/document/6137364/",
                "date_of_publication": "23 January 2012",
                "doi": "10.1109/ICDMW.2011.125",
                "citations": "109",
                "abstract": "We investigate the efficacy of topic model based approaches to two multi-aspect sentiment analysis tasks: multi-aspect sentence labeling and multi-aspect rating prediction. For sentence labeling, we propose a weakly-supervised approach that utilizes only minimal prior knowledge - in the form of seed words - to enforce a direct correspondence between topics and aspects. This correspondence is used to label sentences with performance that approaches a fully supervised baseline. For multi-aspect rating prediction, we find that overall ratings can be used in conjunction with our sentence labelings to achieve reasonable performance compared to a fully supervised baseline. When gold-standard aspect-ratings are available, we find that topic model based features can be used to improve unsophisticated supervised baseline performance, in agreement with previous multi-aspect rating prediction work. This improvement is diminished, however, when topic model features are paired with a more competitive supervised baseline - a finding not acknowledged in previous work.",
                "ieee_keywords": [
                    "Hidden Markov models",
                    "Labeling",
                    "Analytical models",
                    "Predictive models",
                    "Support vector machines",
                    "Accuracy",
                    "Conferences"
                ],
                "author_keywords": [
                    "multi-aspect sentiment analysis",
                    "topic modeling"
                ]
            },
            {
                "title": "Exploring Visual Engagement Signals for Representation Learning",
                "link": "https://ieeexplore.ieee.org/document/9710255/",
                "date_of_publication": "28 February 2022",
                "doi": "10.1109/ICCV48922.2021.00417",
                "citations": "1",
                "abstract": "Visual engagement in social media platforms comprises interactions with photo posts including comments, shares, and likes. In this paper, we leverage such Visual Engagement clues as supervisory signals for representation learning. However, learning from engagement signals is non-trivial as it is not clear how to bridge the gap between low-level visual information and high-level social interactions. We present VisE, a weakly supervised learning approach, which maps social images to pseudo labels derived by clustered engagement signals. We then study how models trained in this way benefit subjective downstream computer vision tasks such as emotion recognition or political bias detection. Through extensive studies, we empirically demonstrate the effectiveness of VisE across a diverse set of classification tasks beyond the scope of conventional recognition 1 .",
                "ieee_keywords": [
                    "Representation learning",
                    "Visualization",
                    "Computer vision",
                    "Emotion recognition",
                    "Social networking (online)",
                    "Computational modeling",
                    "Supervised learning"
                ],
                "author_keywords": [
                    "Vision applications and systems",
                    "Recognition and classification",
                    "Representation learning",
                    "Vision + language"
                ]
            }
        ]
    },
    {
        "name": "Daniel D. Lee",
        "publications": [
            {
                "title": "An On-Chip 2-D DFT Accelerator Ultrasonic Wavefront for Convolutional Neural Networks",
                "link": "https://ieeexplore.ieee.org/document/9703556/",
                "date_of_publication": "10 February 2022",
                "doi": "10.23919/USNC-URSI51813.2021.9703556",
                "citations": "3",
                "abstract": "A 2-dimensional discrete Fourier transform (2-D DFT) is a required preprocessing step for convolutional neural networks (CNNs) to perform matrix multiplication in convolutional layers. Here we present an ultrasonic wavefront-based architecture for CNNs that harness the wave propagation diffraction physic to perform Fourier transform (FT) effectively. The computation is improved to O(N) compare to Fast Fourier transform (FFT) with O(N(log 2 N). In addition, analysis of the proposed ultrasonic wavefront scheme is described in this paper.",
                "ieee_keywords": [
                    "Fast Fourier transforms",
                    "Diffraction",
                    "Propagation",
                    "Discrete Fourier transforms",
                    "Meetings",
                    "Computer architecture",
                    "Acoustics"
                ],
                "author_keywords": [
                    "Discrete Fourier transform",
                    "FT",
                    "FFT",
                    "DFT",
                    "ultrasonic wavefront",
                    "CNN",
                    "convolutional neural network"
                ]
            },
            {
                "title": "Semi-Automated Tracking: A Balanced Approach for Self-Monitoring Applications",
                "link": "https://ieeexplore.ieee.org/document/7807194/",
                "date_of_publication": null,
                "doi": "10.1109/MPRV.2017.18",
                "citations": "82",
                "abstract": "The authors present an approach for designing self-monitoring technology called \"semi-automated tracking,\" which combines both manual and automated data collection methods. Through this approach, they aim to lower the capture burdens, collect data that is typically hard to track automatically, and promote awareness to help people achieve their self-monitoring goals. They first specify three design considerations for semi-automated tracking: data capture feasibility, the purpose of self-monitoring, and the motivation level. They then provide examples of semi-automated tracking applications in the domains of sleep, mood, and food tracking to demonstrate strategies they developed to find the right balance between manual tracking and automated tracking, combining each of their benefits while minimizing their associated limitations.",
                "ieee_keywords": [
                    "Monitoring",
                    "Sensors",
                    "Insulation life",
                    "Data collection",
                    "Mood tracking",
                    "Pervasive computing",
                    "Medical devices",
                    "Internet of things",
                    "Informatics"
                ],
                "author_keywords": [
                    "self-monitoring",
                    "semi-automated tracking",
                    "personal informatics",
                    "sleep tracking",
                    "food tracking",
                    "mood tracking",
                    "pervasive computing",
                    "healthcare",
                    "mobile",
                    "Internet of Things",
                    "bioinformatics",
                    "data analysis"
                ]
            },
            {
                "title": "Learning from Demonstration using a Curvature Regularized Variational Auto-Encoder (CurvVAE)",
                "link": "https://ieeexplore.ieee.org/document/9981930/",
                "date_of_publication": "26 December 2022",
                "doi": "10.1109/IROS47612.2022.9981930",
                "citations": "53",
                "abstract": "Learning intricate manipulation skills from human demonstrations requires good sample efficiency. We introduce a novel learning algorithm, the Curvature-regularized Variational Auto-Encoder (CurvVAE), to achieve this goal. The CurvVAE is able to model the natural variations in human-demonstrated trajectory data without overfitting. It does so by regularizing the curvature of the learned manifold. To showcase our algorithm, our robot learns an interpretable model of the variation in how humans acquire soft, slippery banana slices with a fork. We evaluate our learned trajectories on a physical robot system, resulting in banana slice acquisition performance better than current state-of-the-art.",
                "ieee_keywords": [
                    "Manifolds",
                    "Data models",
                    "Trajectory",
                    "Intelligent robots"
                ],
                "author_keywords": []
            },
            {
                "title": "Learning Data Manifolds with a Cutting Plane Method",
                "link": "https://ieeexplore.ieee.org/document/8850629/",
                "date_of_publication": null,
                "doi": "10.1162/neco_a_01119",
                "citations": "Abstract",
                "abstract": "We consider the problem of classifying data manifolds where each manifold represents invariances that are parameterized by continuous degrees of freedom. Conventional data augmentation methods rely on sampling large numbers of training examples from these manifolds. Instead, we propose an iterative algorithm, MCP, based on a cutting plane approach that efficiently solves a quadratic semi-infinite programming problem to find the maximum margin solution. We provide a proof of convergence as well as a polynomial bound on the number of iterations required for a desired tolerance in the objective function. The efficiency and performance of MCP are demonstrated in high-dimensional simulations and on image manifolds generated from the ImageNet data set. Our results indicate that MCP is able to rapidly learn good classifiers and shows superior generalization performance compared with conventional maximum margin methods using data augmentation methods.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Nearest Neighbor Density Functional Estimation From Inverse Laplace Transform",
                "link": "https://ieeexplore.ieee.org/document/9712283/",
                "date_of_publication": null,
                "doi": "10.1109/TIT.2022.3151231",
                "citations": "1",
                "abstract": "A new approach to L_{2} -consistent estimation of a general density functional using k -nearest neighbor distances is proposed, where the functional under consideration is in the form of the expectation of some function f of the densities at each point. The estimator is designed to be asymptotically unbiased, using the convergence of the normalized volume of a k -nearest neighbor ball to a Gamma distribution in the large-sample limit, and naturally involves the inverse Laplace transform of a scaled version of the function f . Some instantiations of the proposed estimator recover existing k -nearest neighbor based estimators of Shannon and Rényi entropies and Kullback–Leibler and Rényi divergences, and discover new consistent estimators for many other functionals such as logarithmic entropies and divergences. The L_{2} -consistency of the proposed estimator is established for a broad class of densities for general functionals, and the convergence rate in mean squared error is established as a function of the sample size for smooth, bounded densities.",
                "ieee_keywords": [
                    "Laplace equations",
                    "Entropy",
                    "Estimation",
                    "Convergence",
                    "Electronic mail",
                    "Random variables",
                    "Measurement"
                ],
                "author_keywords": [
                    "Density functional estimation",
                    "information measure",
                    "nearest neighbor",
                    "inverse Laplace transform"
                ]
            },
            {
                "title": "Learning Q-network for Active Information Acquisition",
                "link": "https://ieeexplore.ieee.org/document/8968173/",
                "date_of_publication": "28 January 2020",
                "doi": "10.1109/IROS40897.2019.8968173",
                "citations": "6",
                "abstract": "In this paper, we propose a novel Reinforcement Learning approach for solving the Active Information Acquisition problem, which requires an agent to choose a sequence of actions in order to acquire information about a process of interest using on-board sensors. The classic challenges in the information acquisition problem are the dependence of a planning algorithm on known models and the difficulty of computing information-theoretic cost functions over arbitrary distributions. In contrast, the proposed framework of reinforcement learning does not require any knowledge on models and alleviates the problems during an extended training stage. It results in policies that are efficient to execute online and applicable for real-time control of robotic systems. Furthermore, the state-of-the-art planning methods are typically restricted to short horizons, which may become problematic with local minima. Reinforcement learning naturally handles the issue of planning horizon in information problems as it maximizes a discounted sum of rewards over a long finite or infinite time horizon. We discuss the potential benefits of the proposed framework and compare the performance of the novel algorithm to an existing information acquisition method for multi-target tracking scenarios.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Deep Reinforcement Learning for Active Target Tracking",
                "link": "https://ieeexplore.ieee.org/document/9561258/",
                "date_of_publication": "18 October 2021",
                "doi": "10.1109/ICRA48506.2021.9561258",
                "citations": "810",
                "abstract": "We solve active target tracking, one of the essential tasks in autonomous systems, using a deep reinforcement learning (RL) approach. In this problem, an autonomous agent is tasked with acquiring information about targets of interests using its on-board sensors. The classical challenges in this problem are system model dependence and the difficulty of computing information-theoretic cost functions for a long planning horizon. RL provides solutions for these challenges as the length of its effective planning horizon does not affect the computational complexity, and it drops the strong dependency of an algorithm on system models. In particular, we introduce Active Tracking Target Network (ATTN), a unified deep RL policy that is capable of solving major sub-tasks of active target tracking – in-sight tracking, navigation, and exploration. The policy shows robust behavior for tracking agile and anomalous targets with a partially known target model. Additionally, the same policy is able to navigate in obstacle environments to reach distant targets as well as explore the environment when targets are positioned in unexpected locations.",
                "ieee_keywords": [
                    "Target tracking",
                    "Navigation",
                    "Computational modeling",
                    "Conferences",
                    "Reinforcement learning",
                    "Cost function",
                    "Planning"
                ],
                "author_keywords": []
            },
            {
                "title": "Learning Optimal Resource Allocations in Wireless Systems",
                "link": "https://ieeexplore.ieee.org/document/8680025/",
                "date_of_publication": null,
                "doi": "10.1109/TSP.2019.2908906",
                "citations": "129",
                "abstract": "This paper considers the design of optimal resource allocation policies in wireless communication systems, which are generically modeled as a functional optimization problem with stochastic constraints. These optimization problems have the structure of a learning problem in which the statistical loss appears as a constraint, motivating the development of learning methodologies to attempt their solution. To handle stochastic constraints, training is undertaken in the dual domain. It is shown that this can be done with small loss of optimality when using near-universal learning parameterizations. In particular, since deep neural networks (DNNs) are near universal, their use is advocated and explored. DNNs are trained here with a model-free primal-dual method that simultaneously learns a DNN parameterization of the resource allocation policy and optimizes the primal and dual variables. Numerical simulations demonstrate the strong performance of the proposed approach on a number of common wireless resource allocation problems.",
                "ieee_keywords": [
                    "Resource management",
                    "Wireless communication",
                    "Optimization",
                    "Training",
                    "Fading channels",
                    "Measurement",
                    "Interference"
                ],
                "author_keywords": [
                    "Wireless systems",
                    "deep learning",
                    "resource allocation",
                    "strong duality"
                ]
            },
            {
                "title": "Dual Domain Learning of Optimal Resource Allocations in Wireless Systems",
                "link": "https://ieeexplore.ieee.org/document/8683150/",
                "date_of_publication": "17 April 2019",
                "doi": "10.1109/ICASSP.2019.8683150",
                "citations": "2",
                "abstract": "We consider the problem of finding optimal resource allocations subject to system constraints in a generic class of problems in wireless communications. These problems are inherently challenging due to functional optimization and potential non-convexities. However, these problems can be observed to take the form of a regression problem, although one in which the statistical loss function appears as a constraint. This motivates the use of machine learning model parameterizations. To apply gradient-based solution algorithms that do not require model knowledge, we convert the constrained optimization problem to an unconstrained one using Lagrangian duality. Despite the non-convexity in the problem, we formally show that the sub-optimality of the dual domain problem is small when the learning parameterization is sufficiently dense. We then present a primal-dual learning algorithm that looks for solutions to the dual problem using model-free gradient estimates. In a numerical simulation, we demonstrate the near-optimality of the proposed model-free algorithm using a neural network parametrization for a capacity maximization problem.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Efficient Sampling With Q-Learning to Guide Rapidly Exploring Random Trees",
                "link": "https://ieeexplore.ieee.org/document/8412544/",
                "date_of_publication": null,
                "doi": "10.1109/LRA.2018.2856927",
                "citations": "15",
                "abstract": "This letter presents a novel approach for efficient sampling of Rapidly-exploring Random Trees (RRTs) based upon learning a state-action value function (Q-function). Our sampling method selects the optimal node to extend in the search tree via the learned state value computed from the node feature representation. Our softmax node selection procedure avoids becoming stuck at local minima and maintains the asymptotic completeness property of RRTs. We employ several features in learning the Q-function, including radial basis function (RBF) scoring of collision and collision-free regions in the configuration space. Since this approach allows the RRT to explore efficiently while avoiding obstacles via the Q-function, the RRT planner is continually adapted to the surrounding environment in an online manner. We compare our proposed method with traditional sampling-based planning algorithms in a number of robot arm planning scenarios and demonstrate the utility and effectiveness of our approach.",
                "ieee_keywords": [
                    "Planning",
                    "Trajectory",
                    "Function approximation",
                    "Collision avoidance",
                    "Manipulators",
                    "Probabilistic logic"
                ],
                "author_keywords": [
                    "Motion and path planning",
                    "learning and adaptive systems",
                    "manipulation planning"
                ]
            }
        ]
    },
    {
        "name": "Jon Kleinberg",
        "publications": [
            {
                "title": "Predicting Reciprocity in Social Networks",
                "link": "https://ieeexplore.ieee.org/document/6113094/",
                "date_of_publication": "02 January 2012",
                "doi": "10.1109/PASSAT/SocialCom.2011.110",
                "citations": "35",
                "abstract": "In social media settings where users send messages to one another, the issue of reciprocity naturally arises: does the communication between two users take place only in one direction, or is it reciprocated? In this paper we study the problem of reciprocity prediction: given the characteristics of two users, we wish to determine whether the communication between them is reciprocated or not. We approach this problem using decision trees and regression models to determine good indicators of reciprocity. We extract a network based on directed @-messages sent between users on Twitter, and identify measures based on the attributes of nodes and their network neighborhoods that can be used to construct good predictors of reciprocity. Moreover, we find that reciprocity prediction forms interesting contrasts with earlier network prediction tasks, including link prediction, as well as the inference of strengths and signs of network links.",
                "ieee_keywords": [
                    "Twitter",
                    "Accuracy",
                    "Decision trees",
                    "Educational institutions",
                    "Media",
                    "Computer science"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Christopher De Sa",
        "publications": [
            {
                "title": "QPyTorch: A Low-Precision Arithmetic Simulation Framework",
                "link": "https://ieeexplore.ieee.org/document/9463516/",
                "date_of_publication": "29 June 2021",
                "doi": "10.1109/EMC2-NIPS53020.2019.00010",
                "citations": "24",
                "abstract": "Low-precision training reduces computational cost and produces efficient models. Recent research in developing new low-precision training algorithms often relies on simulation to empirically evaluate the statistical effects of quantization while avoiding the substantial overhead of building specific hardware. To support this empirical research, we introduce QPyTorch, a low-precision arithmetic simulation framework. Built natively in PyTorch, QPyTorch provides a convenient interface that minimizes the efforts needed to reliably convert existing codes to study low-precision training. QPyTorch is general, and supports a variety of combinations of precisions, number formats, and rounding options. Additionally, it leverages an efficient fused-kernel approach to reduce simulator overhead, which enables simulation of large-scale, realistic problems. QPyTorch is publicly available at https://github.com/Tiiiger/QPyTorch.",
                "ieee_keywords": [
                    "Training",
                    "Quantization (signal)",
                    "Computational modeling",
                    "Conferences",
                    "Machine learning",
                    "Hardware",
                    "Energy efficiency"
                ],
                "author_keywords": [
                    "low-precision training",
                    "low-precision arithmetic"
                ]
            },
            {
                "title": "Building Efficient Deep Neural Networks With Unitary Group Convolutions",
                "link": "https://ieeexplore.ieee.org/document/8954295/",
                "date_of_publication": "09 January 2020",
                "doi": "10.1109/CVPR.2019.01156",
                "citations": "16",
                "abstract": "We propose unitary group convolutions (UGConvs), a building block for CNNs which compose a group convolution with unitary transforms in feature space to learn a richer set of representations than group convolution alone. UGConvs generalize two disparate ideas in CNN architecture, channel shuffling (i.e. ShuffleNet) and block-circulant networks (i.e. CirCNN), and provide unifying insights that lead to a deeper understanding of each technique. We experimentally demonstrate that dense unitary transforms can outperform channel shuffling in DNN accuracy. On the other hand, different dense transforms exhibit comparable accuracy performance. Based on these observations we propose HadaNet, a UGConv network using Hadamard transforms. HadaNets achieve similar accuracy to circulant networks with lower computation complexity, and better accuracy than ShuffleNets with the same number of parameters and floating-point multiplies.",
                "ieee_keywords": [],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Zhiru Zhang",
        "publications": [
            {
                "title": "Logic Synthesis Meets Machine Learning: Trading Exactness for Generalization",
                "link": "https://ieeexplore.ieee.org/document/9473972/",
                "date_of_publication": "16 July 2021",
                "doi": "10.23919/DATE51398.2021.9473972",
                "citations": "4",
                "abstract": "Logic synthesis is a fundamental step in hardware design whose goal is to find structural representations of Boolean functions while minimizing delay and area. If the function is completely-specified, the implementation accurately represents the function. If the function is incompletely-specified, the implementation has to be true only on the care set. While most of the algorithms in logic synthesis rely on SAT and Boolean methods to exactly implement the care set, we investigate learning in logic synthesis, attempting to trade exactness for generalization. This work is directly related to machine learning where the care set is the training set and the implementation is expected to generalize on a validation set. We present learning incompletely-specified functions based on the results of a competition conducted at IWLS 2020. The goal of the competition was to implement 100 functions given by a set of care minterms for training, while testing the implementation using a set of validation minterms sampled from the same function. We make this benchmark suite available and offer a detailed comparative analysis of the different approaches to learning.",
                "ieee_keywords": [
                    "Training",
                    "Machine learning algorithms",
                    "Boolean functions",
                    "Conferences",
                    "Supervised learning",
                    "Programming",
                    "Hardware"
                ],
                "author_keywords": []
            },
            {
                "title": "LAMDA: Learning-Assisted Multi-stage Autotuning for FPGA Design Closure",
                "link": "https://ieeexplore.ieee.org/document/8735535/",
                "date_of_publication": "13 June 2019",
                "doi": "10.1109/FCCM.2019.00020",
                "citations": "18",
                "abstract": "A primary barrier to rapid hardware specialization with FPGAs stems from weak guarantees of existing CAD tools on achieving design closure. Current methodologies require extensive manual efforts to configure a large set of options across multiple stages of the toolflow, intended to achieve high quality-of-results. Due to the size and complexity of the design space spanned by these options, coupled with the time-consuming evaluation of each design point, exploration for reconfigurable computing has become remarkably challenging. To tackle this challenge, we present a learning-assisted autotuning framework called LAMDA, which accelerates FPGA design closure by utilizing design-specific features extracted from early stages of the design flow to guide the tuning process with significant runtime savings. LAMDA automatically configures logic synthesis, technology mapping, placement, and routing to achieve design closure efficiently. Compared with a state-of-the-art FPGA-targeted autotuning system, LAMDA realizes faster timing closure on various realistic benchmarks using Intel Quartus Pro.",
                "ieee_keywords": [
                    "Tools",
                    "Timing",
                    "Field programmable gate arrays",
                    "Estimation",
                    "Databases",
                    "Tuning",
                    "Runtime"
                ],
                "author_keywords": [
                    "Reconfigurable Computing",
                    "Machine Learning",
                    "Autotuning",
                    "Timing Closure",
                    "CAD",
                    "FPGAs"
                ]
            },
            {
                "title": "FPGA-Based Real-Time Charged Particle Trajectory Reconstruction at the Large Hadron Collider",
                "link": "https://ieeexplore.ieee.org/document/7966650/",
                "date_of_publication": "03 July 2017",
                "doi": "10.1109/FCCM.2017.27",
                "citations": "2",
                "abstract": "The upgrades of the Compact Muon Solenoid particle physics experiment at CERN's Large Hadron Collider provide a major challenge for the real-time collision data selection. This paper presents a novel approach to pattern recognition and charged particle trajectory reconstruction using an all-FPGA solution. The challenges include a large input data rate of about 20 to 40 Tbps, processing a new batch of input data every 25 ns, each consisting of about 10,000 precise position measurements of particles (`stubs'), perform the pattern recognition on these stubs to find the trajectories, and produce the list of parameters describing these trajectories within 4 μs. A proposed solution to this problem is described, in particular, the implementation of the pattern recognition and particle trajectory determination using an all-FPGA system. The results of an end-to-end demonstrator system based on Xilinx Virtex-7 FPGAs that meets timing and performance requirements are presented.",
                "ieee_keywords": [
                    "Trajectory",
                    "Field programmable gate arrays",
                    "Detectors",
                    "Large Hadron Collider",
                    "Real-time systems",
                    "Pattern recognition",
                    "Hardware"
                ],
                "author_keywords": [
                    "particle physics",
                    "field programmable gate arrays",
                    "pattern recognition",
                    "hardware",
                    "detectors",
                    "LHC environment",
                    "physics computing",
                    "l1 track trigger",
                    "High energy physics instrumentation computing",
                    "trigger circuits"
                ]
            },
            {
                "title": "Evaluating Celerity: A 16-nm 695 Giga-RISC-V Instructions/s Manycore Processor With Synthesizable PLL",
                "link": "https://ieeexplore.ieee.org/document/8903494/",
                "date_of_publication": null,
                "doi": "10.1109/LSSC.2019.2953847",
                "citations": "12",
                "abstract": "This letter presents a 16-nm 496-core RISC-V network-onchip (NoC). The mesh achieves 1.4 GHz at 0.98 V, yielding a peak throughput of 695 Giga RISC-V instructions/s (GRVIS), a peak energy efficiency of 314.89 GRVIS/W, and a record 825320 CoreMark benchmark score. Unlike previously reported [1], this new score was obtained without modifying the core benchmark code. The main feature is the NoC architecture, which uses only 1881 μm 2 per router node, enables highly scalable and dense compute, and provides up to 361 Tb/s of aggregate bandwidth.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Fast and Accurate Estimation of Quality of Results in High-Level Synthesis with Machine Learning",
                "link": "https://ieeexplore.ieee.org/document/8457644/",
                "date_of_publication": "11 September 2018",
                "doi": "10.1109/FCCM.2018.00029",
                "citations": "61",
                "abstract": "While high-level synthesis (HLS) offers sophisticated techniques to optimize designs for area and performance, HLS-estimated resource usage and timing often deviate significantly from actual quality of results (QoR) achieved by FPGA-targeted designs. Inaccurate HLS estimates prevent designers from performing meaningful design space exploration without resorting to the time-consuming downstream implementation process. To address this challenge, we first build a large collection of C-to-FPGA results from a diverse set of realistic HLS applications and identify relevant features from HLS reports for estimating post-implementation metrics. We then leverage these features and data to train and compare a number of promising machine learning models to effectively and efficiently bridge the accuracy gap. Experiments demonstrate that our proposed approach is able to dramatically reduce the estimation errors for different families of FPGA devices. By extracting domain-specific insights from our experiments, we explore the implications of our models and predictive influence of various features for enabling fast and accurate QoR estimation in HLS. We have released our dataset to springboard future efforts in this area.",
                "ieee_keywords": [
                    "Feature extraction",
                    "Estimation",
                    "Machine learning",
                    "Predictive models",
                    "Timing",
                    "Table lookup",
                    "Task analysis"
                ],
                "author_keywords": [
                    "high level synthesis",
                    "field programmable gate array",
                    "machine learning",
                    "quality of results"
                ]
            },
            {
                "title": "CASA: Correlation-aware speculative adders",
                "link": "https://ieeexplore.ieee.org/document/7298247/",
                "date_of_publication": "15 October 2015",
                "doi": "10.1145/2627369.2627635",
                "citations": "9",
                "abstract": "Speculative adders divide addition into subgroups and execute them in parallel for higher execution speed and energy efficiency, but at the risk of generating incorrect results. In this paper, we propose a lightweight correlation-aware speculative addition (CASA) method, which exploits the correlation between input data and carry-in values observed in real-life benchmarks to improve the accuracy of speculative adders. Experimental results show that applying the CASA method leads to a significant reduction in error rate with only marginal overhead in timing, area, and power consumption.",
                "ieee_keywords": [
                    "Adders",
                    "Error analysis",
                    "Correlation",
                    "Benchmark testing",
                    "Accuracy",
                    "Logic gates",
                    "Timing"
                ],
                "author_keywords": [
                    "Speculative Adders",
                    "Low Error Rates",
                    "Low Power"
                ]
            },
            {
                "title": "A 28nm 8-bit Floating-Point Tensor Core based CNN Training Processor with Dynamic Activation/Weight Sparsification",
                "link": "https://ieeexplore.ieee.org/document/9911359/",
                "date_of_publication": "20 October 2022",
                "doi": "10.1109/ESSCIRC55480.2022.9911359",
                "citations": "495",
                "abstract": "We present an 8-bit floating-point (FP8) training processor which implements (1) highly parallel tensor cores (fused multiply-add trees) that maintain high utilization throughout forward propagation (FP), backward propagation (BP), and weight update (WU) phases of the training process, (2) hardware-efficient channel gating for dynamic output activation sparsity, (3) dynamic weight sparsity based on group Lasso, and (4) gradient skipping based on FP prediction error. We develop a custom ISA to flexibly support different CNN topologies and training parameters. The 28nm prototype chip demonstrates large improvements in FLOPs reduction (7.3 ×), energy efficiency (16.4 TFLOPS/W), and overall training latency speedup (4.7×), for both supervised and self-supervised training tasks.",
                "ieee_keywords": [
                    "Training",
                    "Tensors",
                    "Neural networks",
                    "Prototypes",
                    "Europe",
                    "Energy efficiency",
                    "Topology"
                ],
                "author_keywords": [
                    "Convolutional neural networks",
                    "deep neural network training",
                    "structured sparsity",
                    "hardware accelerator"
                ]
            },
            {
                "title": "The Celerity Open-Source 511-Core RISC-V Tiered Accelerator Fabric: Fast Architectures and Design Methodologies for Fast Chips",
                "link": "https://ieeexplore.ieee.org/document/8344478/",
                "date_of_publication": null,
                "doi": "10.1109/MM.2018.022071133",
                "citations": "54",
                "abstract": "Rapidly emerging workloads require rapidly developed chips. The Celerity 16-nm open-source SoC was implemented in nine months using an architectural trifecta to minimize development time: a general-purpose tier comprised of open-source Linux-capable RISC-V cores, a massively parallel tier comprised of a RISC-V tiled manycore array that can be scaled to arbitrary sizes, and a specialization tier that uses high-level synthesis (HLS) to create an algorithmic neural-network accelerator. These tiers are tied together with an efficient heterogeneous remote store programming model on top of a flexible partial global address space memory system.",
                "ieee_keywords": [
                    "Memory management",
                    "Programming",
                    "Open source software",
                    "Energy efficiency",
                    "System-on-chip",
                    "Reduced instruction set computing"
                ],
                "author_keywords": [
                    "hardware",
                    "microchips"
                ]
            },
            {
                "title": "A Tensor Processing Framework for CPU-Manycore Heterogeneous Systems",
                "link": "https://ieeexplore.ieee.org/document/9509755/",
                "date_of_publication": null,
                "doi": "10.1109/TCAD.2021.3103825",
                "citations": "687",
                "abstract": "Future CPU-manycore heterogeneous systems can provide high peak throughput by integrating thousands of simple, independent, energy-efficient cores in a single die. However, there are two key challenges to translating this high peak throughput into improved end-to-end workload performance: 1) manycore co-processors rely on simple hardware putting significant demands on the software programmer and 2) manycore co-processors use in-order cores that struggle to tolerate long memory latencies. To address the manycore programmability challenge, this article presents a dense and sparse tensor processing framework based on PyTorch that enables domain experts to easily accelerate off-the-shelf workloads on CPU-manycore heterogeneous systems. To address the manycore memory latency challenge, we use our extended PyTorch framework to explore the potential for decoupled access/execute (DAE) software and hardware mechanisms. More specifically, we propose two software-only techniques, naïve-software DAE and systolic-software DAE, along with a lightweight hardware access accelerator to further improve area-normalized throughput. We evaluate our techniques using a combination of PyTorch operator microbenchmarking and real-world PyTorch workloads running on a detailed register-transfer-level model of a 128-core manycore architecture. Our evaluation on three real-world dense and sparse tensor workloads suggests these workloads can achieve approximately 2– $6\\times $ performance improvement when scaled to a future 2000-core CPU-manycore heterogeneous system compared to an 18-core out-of-order CPU baseline, while potentially achieving higher area-normalized throughput and improved energy efficiency compared to general-purpose graphics processing units.",
                "ieee_keywords": [
                    "Computer architecture",
                    "Software",
                    "Hardware",
                    "Throughput",
                    "Multicore processing",
                    "Tensors",
                    "Central Processing Unit"
                ],
                "author_keywords": [
                    "Accelerator architectures",
                    "open source software",
                    "parallel programming",
                    "software libraries"
                ]
            },
            {
                "title": "SuSy: A Programming Model for Productive Construction of High-Performance Systolic Arrays on FPGAs",
                "link": "https://ieeexplore.ieee.org/document/9256583/",
                "date_of_publication": "25 November 2020",
                "doi": null,
                "citations": "709",
                "abstract": "Systolic algorithms are one of the killer applications on spatial architectures such as FPGAs and CGRAs. However, it requires a tremendous amount of human effort to design and implement a high-performance systolic array for a given algorithm using the traditional RTL-based methodology. On the other hand, existing high-level synthesis (HLS) tools either (1) force the programmers to do “micro-coding” where too many optimizations must be carried out through tedious code restructuring and insertion of vendor-specific pragmas, or (2) give them too little control to influence a push-button compilation flow to achieve high quality of results. To tackle these challenges, we introduce SuSy, a programming framework composed of a domain-specific language (DSL) and a compilation flow that enables programmers to productively build high-performance systolic arrays on FPGAs. With SuSy, programmers express the design functionality in the form of uniform recurrence equations (UREs), which can describe algorithms from a wide spectrum of applications as long as the underlying computation has a uniform dependence structure. The URE description in SuSy is followed by a set of decoupled spatial mapping primitives that specify how to map the equations to a spatial architecture. More concretely, programmers can apply space-time transformations and several other memory and I/O optimizations to build a highly efficient systolic architecture productively. Experimental results show that SuSy can describe various algorithms with UREs and generate high-performance systolic arrays by spatial optimizations. For instance, the SGEMM benchmark written in SuSy can approach the performance of the manual design optimized by experts, while using 30× fewer lines of code.",
                "ieee_keywords": [
                    "Optimization",
                    "Field programmable gate arrays",
                    "DSL",
                    "Programming",
                    "Mathematical model",
                    "Arrays",
                    "Buildings"
                ],
                "author_keywords": [
                    "DSL",
                    "FPGA",
                    "Systolic Array",
                    "Space-Time Transformation",
                    "URE"
                ]
            }
        ]
    },
    {
        "name": "Dinesh Manocha",
        "publications": [
            {
                "title": "Multi-Agent Ergodic Coverage in Urban Environments",
                "link": "https://ieeexplore.ieee.org/document/9561257/",
                "date_of_publication": "18 October 2021",
                "doi": "10.1109/ICRA48506.2021.9561257",
                "citations": "238",
                "abstract": "An important aspect of dynamic urban coverage is how building collision avoidance is incorporated into the overall coverage mission. We consider a multi-agent urban dynamic coverage problem in which a team of flying agents uses downward facing cameras to observe the street-level environment outside of buildings. Cameras are assumed to be ineffective above a maximum altitude (lower than building height), such that agents must move around or over buildings to complete their mission. The main objective of this paper is to compare three different building avoidance strategies that are compatible with dynamic ergodic methods. To provide context for these results, we also compare our results to three other common coverage methods including: boustrophedon coverage (lawn-mower sweep), Voronoi region based coverage, and a naive grid method. All algorithms are evaluated in simulation with respect to four performance metrics (percent coverage, revisit count, revisit time, and the integral of area viewed over time), across team sizes ranging from 1 to 25 agents, and in five types of urban environments of varying density and height. We find that the relative performance of algorithms changes based on the ratio of team size to search area, as well the height and density characteristics of the urban environment.",
                "ieee_keywords": [
                    "Automation",
                    "Heuristic algorithms",
                    "Conferences",
                    "Buildings",
                    "Urban areas",
                    "Cameras",
                    "Distance measurement"
                ],
                "author_keywords": [
                    "Multi-Agent",
                    "Coverage",
                    "Lawn-mower",
                    "Ergodic",
                    "Boustrophedon",
                    "Voronoi",
                    "Urban Environment"
                ]
            },
            {
                "title": "Forecasting Trajectory and Behavior of Road-Agents Using Spectral Clustering in Graph-LSTMs",
                "link": "https://ieeexplore.ieee.org/document/9126166/",
                "date_of_publication": null,
                "doi": "10.1109/LRA.2020.3004794",
                "citations": "68",
                "abstract": "We present a novel approach for traffic forecasting in urban traffic scenarios using a combination of spectral graph analysis and deep learning. We predict both the low-level information (future trajectories) as well as the high-level information (road-agent behavior) from the extracted trajectory of each road-agent. Our formulation represents the proximity between the road agents using a weighted dynamic geometric graph (DGG). We use a two-stream graph-LSTM network to perform traffic forecasting using these weighted DGGs. The first stream predicts the spatial coordinates of road-agents, while the second stream predicts whether a road-agent is going to exhibit overspeeding, underspeeding, or neutral behavior by modeling spatial interactions between road-agents. Additionally, we propose a new regularization algorithm based on spectral clustering to reduce the error margin in long-term prediction (3-5 seconds) and improve the accuracy of the predicted trajectories. Moreover, we prove a theoretical upper bound on the regularized prediction error. We evaluate our approach on the Argoverse, Lyft, Apolloscape, and NGSIM datasets and highlight the benefits over prior trajectory prediction methods. In practice, our approach reduces the average prediction error by approximately 75% over prior algorithms and achieves a weighted average accuracy of 91.2% for behavior prediction. Additionally, our spectral regularization improves long-term prediction by up to 70%.",
                "ieee_keywords": [
                    "Trajectory",
                    "Prediction algorithms",
                    "Forecasting",
                    "Roads",
                    "Signal processing algorithms",
                    "Predictive models",
                    "Vehicle dynamics"
                ],
                "author_keywords": [
                    "Intelligent transportation systems",
                    "autonomous agents"
                ]
            },
            {
                "title": "EmotiCon: Context-Aware Multimodal Emotion Recognition Using Frege’s Principle",
                "link": "https://ieeexplore.ieee.org/document/9156904/",
                "date_of_publication": "05 August 2020",
                "doi": "10.1109/CVPR42600.2020.01424",
                "citations": "70",
                "abstract": "We present EmotiCon, a learning-based algorithm for context-aware perceived human emotion recognition from videos and images. Motivated by Frege's Context Principle from psychology, our approach combines three interpretations of context for emotion recognition. Our first interpretation is based on using multiple modalities (e.g.faces and gaits) for emotion recognition. For the second interpretation, we gather semantic context from the input image and use a self-attention-based CNN to encode this information. Finally, we use depth maps to model the third interpretation related to socio-dynamic interactions and proximity among agents. We demonstrate the efficiency of our network through experiments on EMOTIC, a benchmark dataset. We report an Average Precision (AP) score of 35.48 across 26 classes, which is an improvement of 7-8 over prior methods. We also introduce a new dataset, GroupWalk, which is a collection of videos captured in multiple real-world settings of people walking. We report an AP of 65.83 across 4 categories on GroupWalk, which is also an improvement over prior methods.",
                "ieee_keywords": [
                    "Emotion recognition",
                    "Videos",
                    "Psychology",
                    "TV",
                    "Semantics",
                    "Databases",
                    "Benchmark testing"
                ],
                "author_keywords": []
            },
            {
                "title": "DenseCAvoid: Real-time Navigation in Dense Crowds using Anticipatory Behaviors",
                "link": "https://ieeexplore.ieee.org/document/9197379/",
                "date_of_publication": "15 September 2020",
                "doi": "10.1109/ICRA40945.2020.9197379",
                "citations": "32",
                "abstract": "We present DenseCAvoid, a novel algorithm for navigating a robot through dense crowds and avoiding collisions by anticipating pedestrian behaviors. Our formulation uses visual sensors and a pedestrian trajectory prediction algorithm to track pedestrians in a set of input frames and compute bounding boxes that extrapolate to the pedestrian positions in a future time. Our hybrid approach combines this trajectory prediction with a Deep Reinforcement Learning-based collision avoidance method to train a policy to generate smoother, safer, and more robust trajectories during run-time. We train our policy in realistic 3-D simulations of static and dynamic scenarios with multiple pedestrians. In practice, our hybrid approach generalizes well to unseen, real-world scenarios and can navigate a robot through dense crowds (~1-2 humans per square meter) in indoor scenarios, including narrow corridors and lobbies. As compared to cases where prediction was not used, we observe that our method reduces the occurrence of the robot freezing in a crowd by up to 48%, and performs comparably with respect to trajectory lengths and mean arrival times to goal.",
                "ieee_keywords": [
                    "Collision avoidance",
                    "Navigation",
                    "Trajectory",
                    "Robot sensing systems",
                    "Robustness",
                    "Tracking"
                ],
                "author_keywords": []
            },
            {
                "title": "LSwarm: Efficient Collision Avoidance for Large Swarms With Coverage Constraints in Complex Urban Scenes",
                "link": "https://ieeexplore.ieee.org/document/8767930/",
                "date_of_publication": null,
                "doi": "10.1109/LRA.2019.2929981",
                "citations": "24",
                "abstract": "In this letter, we address the problem of collision avoidance for a swarm of UAVs used for continuous surveillance of an urban environment. Our method, LSwarm, efficiently avoids collisions with static obstacles, dynamic obstacles and other agents in three-dimensional urban environments while considering coverage constraints. LSwarm calculates collision avoiding velocities that maximize the conformity of an agent to an optimal path given by a global coverage strategy and ensure sufficient resolution of the coverage data collected by each agent. Our algorithm is formulated based on optimal reciprocal collision avoidance and is scalable with respect to the size of the swarm. We evaluate the coverage performance of LSwarm in realistic simulations of a swarm of quadrotors in complex urban models. In practice, our approach can compute collision avoiding velocities for a swarm composed of tens to hundreds of agents in a few milliseconds on dense urban scenes consisting of tens of buildings.",
                "ieee_keywords": [
                    "Collision avoidance",
                    "Sensors",
                    "Surveillance",
                    "Urban areas",
                    "Multi-robot systems",
                    "Path planning",
                    "Uncertainty"
                ],
                "author_keywords": [
                    "Collision Avoidance",
                    "Path Planning for Multiple Mobile Robots or Agents",
                    "Simulation and Animation"
                ]
            },
            {
                "title": "Redirection Using Alignment",
                "link": "https://ieeexplore.ieee.org/document/9419281/",
                "date_of_publication": "06 May 2021",
                "doi": "10.1109/VRW52623.2021.00072",
                "citations": "151",
                "abstract": "Date of Conference: 27 March 2021 - 01 April 2021 Date Added to IEEE Xplore: 06 May 2021 ISBN Information: INSPEC Accession Number: 20676993 DOI: 10.1109/VRW52623.2021.00072 Publisher: IEEE Conference Location: Lisbon, Portugal 1 Introduction and Background Exploration of virtual environments (VEs) using locomotion interfaces that enable natural walking is often preferred over interfaces that use artificial locomotion [7]. One such interface is redirected walking (RDW), which works by slowly rotating the VE around the user while they locomote, which causes them to adjust their physical trajectory to remain on their intended virtual path [6]. Using RDW, we can steer users away from physical obstacles that may otherwise obstruct their path in the physical environment (PE). The algorithm responsible for steering users is known as a redirection controller [5]. Sign in to Continue Reading Authors Figures References Keywords IEEE Keywords Legged locomotion , Three-dimensional displays , Conferences , Virtual environments , User interfaces , Trajectory INSPEC: Controlled Indexing virtual reality INSPEC: Non-Controlled Indexing natural walking , VR locomotion , physical trajectory , physical obstacles , redirected walking , physical environments , virtual environments , virtual reality Author Keywords Redirected Walking , Alignment , Locomotion Metrics More Like This From visual simulation to virtual reality to games Computer Published: 2005 Virtual Reality: How Much Immersion Is Enough? Computer Published: 2007 Show More References",
                "ieee_keywords": [
                    "Legged locomotion",
                    "Three-dimensional displays",
                    "Conferences",
                    "Virtual environments",
                    "User interfaces",
                    "Trajectory"
                ],
                "author_keywords": [
                    "Redirected Walking",
                    "Alignment",
                    "Locomotion"
                ]
            },
            {
                "title": "M3DETR: Multi-representation, Multi-scale, Mutual-relation 3D Object Detection with Transformers",
                "link": "https://ieeexplore.ieee.org/document/9706932/",
                "date_of_publication": "15 February 2022",
                "doi": "10.1109/WACV51458.2022.00235",
                "citations": "38",
                "abstract": "We present a novel architecture for 3D object detection, M3DETR, which combines different point cloud representations (raw, voxels, bird-eye view) with different feature scales based on multi-scale feature pyramids. M3DETR is the first approach that unifies multiple point cloud representations, feature scales, as well as models mutual relationships between point clouds simultaneously using transformers. We perform extensive ablation experiments that highlight the benefits of fusing representation and scale, and modeling the relationships. Our method achieves state-of-the-art performance on the KITTI 3D object detection dataset and Waymo Open Dataset. Results show that M3DETR improves the baseline significantly by 1.48% mAP for all classes on Waymo Open Dataset. In particular, our approach ranks 1 st on the well-known KITTI 3D Detection Benchmark for both car and cyclist classes, and ranks 1 st on Waymo Open Dataset with single frame point cloud input. Our code is available at: https://github.com/rayguan97/M3DETR.",
                "ieee_keywords": [
                    "Point cloud compression",
                    "Visualization",
                    "Computer vision",
                    "Three-dimensional displays",
                    "Laser radar",
                    "Object detection",
                    "Transformers"
                ],
                "author_keywords": [
                    "3D Computer Vision Object Detection/Recognition/Categorization"
                ]
            },
            {
                "title": "ProxEmo: Gait-based Emotion Learning and Multi-view Proxemic Fusion for Socially-Aware Robot Navigation",
                "link": "https://ieeexplore.ieee.org/document/9340710/",
                "date_of_publication": "10 February 2021",
                "doi": "10.1109/IROS45743.2020.9340710",
                "citations": "24",
                "abstract": "We present ProxEmo, a novel end-to-end emotion prediction algorithm for socially aware robot navigation among pedestrians. Our approach predicts the perceived emotions of a pedestrian from walking gaits, which is then used for emotion-guided navigation taking into account social and proxemic constraints. To classify emotions, we propose a multi-view skeleton graph convolution-based model that works on a commodity camera mounted onto a moving robot. Our emotion recognition is integrated into a mapless navigation scheme and makes no assumptions about the environment of pedestrian motion. It achieves a mean average emotion prediction precision of 82.47% on the Emotion-Gait benchmark dataset. We outperform current state-of-art algorithms for emotion recognition from 3D gaits. We highlight its benefits in terms of navigation in indoor scenes using a Clearpath Jackal robot.",
                "ieee_keywords": [
                    "Legged locomotion",
                    "Emotion recognition",
                    "Three-dimensional displays",
                    "Navigation",
                    "Robot vision systems",
                    "Prediction algorithms",
                    "Skeleton"
                ],
                "author_keywords": []
            },
            {
                "title": "Text2Gestures: A Transformer-Based Network for Generating Emotive Body Gestures for Virtual Agents",
                "link": "https://ieeexplore.ieee.org/document/9417647/",
                "date_of_publication": "10 May 2021",
                "doi": "10.1109/VR50410.2021.00037",
                "citations": "22",
                "abstract": "We present Text2Gestures, a transformer-based learning method to interactively generate emotive full-body gestures for virtual agents aligned with natural language text inputs. Our method generates emotionally expressive gestures by utilizing the relevant biomechanical features for body expressions, also known as affective features. We also consider the intended task corresponding to the text and the target virtual agents' intended gender and handedness in our generation pipeline. We train and evaluate our network on the MPI Emotional Body Expressions Database and observe that our network produces state-of-the-art performance in generating gestures for virtual agents aligned with the text for narration or conversation. Our network can generate these gestures at interactive rates on a commodity GPU. We conduct a web-based user study and observe that around 91% of participants indicated our generated gestures to be at least plausible on a five-point Likert Scale. The emotions perceived by the participants from the gestures are also strongly positively correlated with the corresponding intended emotions, with a minimum Pearson coefficient of 0.77 in the valence dimension.",
                "ieee_keywords": [
                    "Learning systems",
                    "Three-dimensional displays",
                    "Correlation",
                    "Databases",
                    "Natural languages",
                    "Pipelines",
                    "Graphics processing units"
                ],
                "author_keywords": [
                    "Computing methodologies-Virtual reality",
                    "Computing methodologies-Intelligent agents",
                    "Computer systems organization-Neural networks"
                ]
            },
            {
                "title": "CMetric: A Driving Behavior Measure using Centrality Functions",
                "link": "https://ieeexplore.ieee.org/document/9341720/",
                "date_of_publication": "10 February 2021",
                "doi": "10.1109/IROS45743.2020.9341720",
                "citations": "13",
                "abstract": "We present a new measure, CMetric, to classify driver behaviors using centrality functions. Our formulation combines concepts from computational graph theory and social traffic psychology to quantify and classify the behavior of human drivers. CMetric is used to compute the probability of a vehicle executing a driving style, as well as the intensity used to execute the style. Our approach is designed for realtime autonomous driving applications, where the trajectory of each vehicle or road-agent is extracted from a video. We compute a dynamic geometric graph (DGG) based on the positions and proximity of the road-agents and centrality functions corresponding to closeness and degree. These functions are used to compute the CMetric based on style likelihood and style intensity estimates. Our approach is general and makes no assumption about traffic density, heterogeneity, or how driving behaviors change over time. We present an algorithm to compute CMetric and demonstrate its performance on real-world traffic datasets. To test the accuracy of CMetric, we introduce a new evaluation protocol (called \"Time Deviation Error\") that measures the difference between human prediction and the prediction made by CMetric.",
                "ieee_keywords": [
                    "Protocols",
                    "Psychology",
                    "Prediction algorithms",
                    "Time measurement",
                    "Trajectory",
                    "Vehicle dynamics",
                    "Vehicles"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Tom Goldstein",
        "publications": [
            {
                "title": "Son of Zorn's lemma: Targeted style transfer using instance-aware semantic segmentation",
                "link": "https://ieeexplore.ieee.org/document/7952376/",
                "date_of_publication": "19 June 2017",
                "doi": "10.1109/ICASSP.2017.7952376",
                "citations": "9",
                "abstract": "Style transfer is an important task in which the style of a source image is mapped onto that of a target image. The method is useful for synthesizing derivative works of a particular artist or specific painting. This work considers targeted style transfer, in which the style of a template image is used to alter only part of a target image. For example, an artist may wish to alter the style of only one particular object in a target image without altering the object's general morphology or surroundings. This is useful, for example, in augmented reality applications (such as the recently released Pokémon go), where one wants to alter the appearance of a single real-world object in an image frame to make it appear as a cartoon. Most notably, the rendering of real-world objects into cartoon characters has been used in a number of films and television show, such as the upcoming series Son of Zorn. We present a method for targeted style transfer that simultaneously segments and stylizes single objects selected by the user. The method uses a Markov random field model to smooth and anti-alias outlier pixels near object boundaries, so that stylized objects naturally blend into their surroundings.",
                "ieee_keywords": [
                    "Image segmentation",
                    "Semantics",
                    "Markov processes",
                    "Image color analysis",
                    "Feature extraction",
                    "Computational modeling",
                    "Neural networks"
                ],
                "author_keywords": [
                    "Style transfer",
                    "Instance-aware semantic segmentation",
                    "Convolution neural network",
                    "Markov random fields",
                    "Image filtering"
                ]
            },
            {
                "title": "Witchcraft: Efficient PGD Attacks with Random Step Size",
                "link": "https://ieeexplore.ieee.org/document/9052930/",
                "date_of_publication": "09 April 2020",
                "doi": "10.1109/ICASSP40776.2020.9052930",
                "citations": "2",
                "abstract": "State-of-the-art adversarial attacks on neural networks use expensive iterative methods and numerous random restarts from different initial points. Iterative FGSM-based methods without restarts trade off performance for computational efficiency because they do not adequately explore the image space and are highly sensitive to the choice of step size. We propose a variant of Projected Gradient Descent (PGD) that uses a random step size to improve performance without resorting to expensive random restarts. Our method, Wide Iterative Stochastic crafting (WITCHcraft), achieves results superior to the classical PGD attack on the CIFAR-10 and MNIST data sets but without additional computational cost. This simple modification of PGD makes crafting attacks more economical, which is important in situations like adversarial training where attacks need to be crafted in real time.",
                "ieee_keywords": [
                    "Training",
                    "Signal processing",
                    "Real-time systems",
                    "Space exploration",
                    "Computational efficiency",
                    "Iterative methods",
                    "Speech processing"
                ],
                "author_keywords": [
                    "Adversarial",
                    "Attack",
                    "PGD",
                    "CNN",
                    "CIFAR"
                ]
            },
            {
                "title": "Estimating Sparse Signals with Smooth Support via Convex Programming and Block Sparsity",
                "link": "https://ieeexplore.ieee.org/document/7781005/",
                "date_of_publication": "12 December 2016",
                "doi": "10.1109/CVPR.2016.636",
                "citations": "2",
                "abstract": "Conventional algorithms for sparse signal recovery and sparse representation rely on l1-norm regularized variational methods. However, when applied to the reconstruction of sparse images, i.e., images where only a few pixels are non-zero, simple l1-norm-based methods ignore potential correlations in the support between adjacent pixels. In a number of applications, one is interested in images that are not only sparse, but also have a support with smooth (or contiguous) boundaries. Existing algorithms that take into account such a support structure mostly rely on nonconvex methods and-as a consequence-do not scale well to high-dimensional problems and/or do not converge to global optima. In this paper, we explore the use of new block l1-norm regularizers, which enforce image sparsity while simultaneously promoting smooth support structure. By exploiting the convexity of our regularizers, we develop new computationally-efficient recovery algorithms that guarantee global optimality. We demonstrate the efficacy of our regularizers on a variety of imaging tasks including compressive image recovery, image restoration, and robust PCA.",
                "ieee_keywords": [
                    "TV",
                    "Computer vision",
                    "Imaging",
                    "Image coding",
                    "Robustness",
                    "Minimization",
                    "Memory management"
                ],
                "author_keywords": []
            },
            {
                "title": "Adversarial Differentiable Data Augmentation for Autonomous Systems",
                "link": "https://ieeexplore.ieee.org/document/9561205/",
                "date_of_publication": "18 October 2021",
                "doi": "10.1109/ICRA48506.2021.9561205",
                "citations": "4",
                "abstract": "Autonomous systems often rely on neural networks to achieve high performance on planning and control problems. Unfortunately, neural networks suffer severely when input images become degraded in ways that are not reflected in the training data. This is particularly problematic for robotic systems like autonomous vehicles (AV) for which reliability is paramount. In this work, we consider robust optimization methods for hardening control systems against image corruptions and other unexpected domain shifts. Recent work on robust optimization for neural nets has been focused largely on combating adversarial attacks. In this work, we borrow ideas from the adversarial training and data augmentation literature to enhance robustness to image corruptions and domain shifts. To this end, we train networks while augmenting image data with a battery of image degradations. Unlike traditional augmentation methods, we choose the parameters for each degradation adversarially so as to maximize system performance. By formulating image degradations in a way that is differentiable with respect to degradation parameters, we enable the use of efficient optimization methods (PGD) for choosing worst-case augmentation parameters. We demonstrate the efficacy of this method on the learning to steer task for AVs. By adversarially training against image corruptions, we produce networks that are highly robust to image corruptions. We show that the proposed differentiable augmentation schemes result in higher levels of robustness and accuracy for a range of settings as compared to baseline and state-of-the-art augmentation methods.",
                "ieee_keywords": [
                    "Degradation",
                    "Training",
                    "Neural networks",
                    "Optimization methods",
                    "Training data",
                    "Robustness",
                    "Data models"
                ],
                "author_keywords": []
            },
            {
                "title": "Can Neural Nets Learn the Same Model Twice? Investigating Reproducibility and Double Descent from the Decision Boundary Perspective",
                "link": "https://ieeexplore.ieee.org/document/9878514/",
                "date_of_publication": "27 September 2022",
                "doi": "10.1109/CVPR52688.2022.01333",
                "citations": "3",
                "abstract": "We discuss methods for visualizing neural network decision boundaries and decision regions. We use these visual-izations to investigate issues related to reproducibility and generalization in neural network training. We observe that changes in model architecture (and its associate inductive bias) cause visible changes in decision boundaries, while multiple runs with the same architecture yield results with strong similarities, especially in the case of wide architectures. We also use decision boundary methods to visualize double descent phenomena. We see that decision boundary reproducibility depends strongly on model width. Near the threshold of interpolation, neural network decision bound-aries become fragmented into many small decision regions, and these regions are non-reproducible. Meanwhile, very narrows and very wide networks have high levels of re-producibility in their decision boundaries with relatively few decision regions. We discuss how our observations re-late to the theory of double descent phenomena in convex models. Code is available at https://github.com/somepago/dbViz.",
                "ieee_keywords": [
                    "Training",
                    "Interpolation",
                    "Computer vision",
                    "Computational modeling",
                    "Neural networks",
                    "Machine learning",
                    "Computer architecture"
                ],
                "author_keywords": [
                    "Machine learning; Deep learning architectures and techniques; Others"
                ]
            },
            {
                "title": "Robust Optimization as Data Augmentation for Large-scale Graphs",
                "link": "https://ieeexplore.ieee.org/document/9878654/",
                "date_of_publication": "27 September 2022",
                "doi": "10.1109/CVPR52688.2022.00016",
                "citations": "4",
                "abstract": "Data augmentation helps neural networks generalize better by enlarging the training set, but it remains an open question how to effectively augment graph data to enhance the performance of GNNs (Graph Neural Networks). While most existing graph regularizers focus on manipulating graph topological structures by adding/removing edges, we offer a method to augment node features for better performance. We propose FLAG (Free Large-scale Adversarial Augmentation on Graphs), which iteratively augments node features with gradient-based adversarial perturbations during training. By making the model invariant to small fluctuations in input data, our method helps models generalize to out-of-distribution samples and boosts model performance at test time. FLAG is a general-purpose approach for graph data, which universally works in node classification, link prediction, and graph classification tasks. FLAG is also highly flexible and scalable, and is deployable with arbitrary GNN backbones and large-scale datasets. We demon-strate the efficacy and stability of our method through ex-tensive experiments and ablation studies. We also provide intuitive observations for a deeper understanding of our method. We open source our implementation at https://github.com/devnkong/FLAG.",
                "ieee_keywords": [
                    "Training",
                    "Privacy",
                    "Social networking (online)",
                    "Perturbation methods",
                    "Pipelines",
                    "Data models",
                    "Stability analysis"
                ],
                "author_keywords": [
                    "Machine learning; Others"
                ]
            },
            {
                "title": "Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models",
                "link": "https://ieeexplore.ieee.org/document/10203298/",
                "date_of_publication": "22 August 2023",
                "doi": "10.1109/CVPR52729.2023.00586",
                "citations": "1",
                "abstract": "Cutting-edge diffusion models produce images with high quality and customizability, enabling them to be used for commercial art and graphic design purposes. But do diffusion models create unique works of art, or are they replicating content directly from their training sets? In this work, we study image retrieval frameworks that enable us to compare generated images with training samples and detect when content has been replicated. Applying our frameworks to diffusion models trained on multiple datasets including Oxford flowers, Celeb-A, ImageNet, and LAION, we discuss how factors such as training set size impact rates of content replication. We also identify cases where diffusion models, including the popular Stable Diffusion model, blatantly copy from their training data. Project page: https://somepago.github.io/diffrep.html",
                "ieee_keywords": [
                    "Training",
                    "Graphics",
                    "Art",
                    "Computational modeling",
                    "Image retrieval",
                    "Training data",
                    "Flowering plants"
                ],
                "author_keywords": [
                    "Transparency",
                    "fairness",
                    "accountability",
                    "privacy",
                    "ethics in vision"
                ]
            },
            {
                "title": "PhasePack: A Phase Retrieval Library",
                "link": "https://ieeexplore.ieee.org/document/9030878/",
                "date_of_publication": "12 March 2020",
                "doi": "10.1109/SampTA45681.2019.9030878",
                "citations": "7",
                "abstract": "Phase retrieval deals with the estimation of complex-valued signals solely from the magnitudes of linear measurements. While there has been a recent explosion in the development of phase retrieval algorithms, the lack of a common interface has made it difficult to compare new methods against the state-of-the-art. The purpose of PhasePack is to create a common software interface for a wide range of phase retrieval algorithms and to provide a common testbed using both synthetic data and empirical imaging datasets. PhasePack is able to benchmark a large number of recent phase retrieval methods against one another to generate comparisons using a range of different performance metrics. The software package handles single method testing as well as multiple method comparisons.The algorithm implementations in PhasePack differ slightly from their original descriptions in the literature in order to achieve faster speed and improved robustness. In particular, PhasePack uses adaptive stepsizes, line-search methods, and fast eigensolvers to speed up and automate convergence.",
                "ieee_keywords": [
                    "Phase measurement",
                    "Imaging",
                    "Extraterrestrial measurements",
                    "Length measurement",
                    "Image reconstruction",
                    "Software algorithms",
                    "Convergence"
                ],
                "author_keywords": []
            },
            {
                "title": "Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses",
                "link": "https://ieeexplore.ieee.org/document/9743317/",
                "date_of_publication": null,
                "doi": "10.1109/TPAMI.2022.3162397",
                "citations": "20",
                "abstract": "As machine learning systems grow in scale, so do their training data requirements, forcing practitioners to automate and outsource the curation of training data in order to achieve state-of-the-art performance. The absence of trustworthy human supervision over the data collection process exposes organizations to security vulnerabilities; training data can be manipulated to control and degrade the downstream behaviors of learned models. The goal of this work is to systematically categorize and discuss a wide range of dataset vulnerabilities and exploits, approaches for defending against these threats, and an array of open problems in this space.",
                "ieee_keywords": [
                    "Data models",
                    "Training",
                    "Training data",
                    "Security",
                    "Toxicology",
                    "Unsolicited e-mail",
                    "Servers"
                ],
                "author_keywords": [
                    "Data poisoning",
                    "backdoor attacks",
                    "dataset security"
                ]
            },
            {
                "title": "A New Rank Constraint on Multi-view Fundamental Matrices, and Its Application to Camera Location Recovery",
                "link": "https://ieeexplore.ieee.org/document/8099742/",
                "date_of_publication": "09 November 2017",
                "doi": "10.1109/CVPR.2017.259",
                "citations": "11",
                "abstract": "Accurate estimation of camera matrices is an important step in structure from motion algorithms. In this paper we introduce a novel rank constraint on collections of fundamental matrices in multi-view settings. We show that in general, with the selection of proper scale factors, a matrix formed by stacking fundamental matrices between pairs of images has rank 6. Moreover, this matrix forms the symmetric part of a rank 3 matrix whose factors relate directly to the corresponding camera matrices. We use this new characterization to produce better estimations of fundamental matrices by optimizing an L1-cost function using Iterative Re-weighted Least Squares and Alternate Direction Method of Multiplier. We further show that this procedure can improve the recovery of camera locations, particularly in multi-view settings in which fewer images are available.",
                "ieee_keywords": [
                    "Cameras",
                    "Symmetric matrices",
                    "Jacobian matrices",
                    "Calibration",
                    "Estimation",
                    "Stacking",
                    "Optimization"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "John P. Dickerson",
        "publications": [
            {
                "title": "Dealing with Lashkar-e-Taiba: A Multi-player Game-Theoretic Perspective",
                "link": "https://ieeexplore.ieee.org/document/6061230/",
                "date_of_publication": "27 October 2011",
                "doi": "10.1109/EISIC.2011.33",
                "citations": "9",
                "abstract": "Lashkar-e-Taiba (LET) is one of the deadliest terrorist groups in the world. With over 100 attacks worldwide since 2004, LET has become a political force within Pakistan, a proxy fighting force for the Pakistani Army, and a terror group that can carry out complex, coordinated attacks such as the 2008 Mumbai attacks. In this paper, we develop a game-theoretic analysis of how to deal with LET using a 5-player game whose players include LET, India, the Pakistani military, the (civilian) Pakistani government, and the US. We use an expert on LET and Pakistan to develop a payoff matrix and compute pure and mixed Nash equilibria (NE) in this payoff matrix. We study several of these NEs in detail. Our analysis shows that: (i) there are 6 pure NEs in which LET eliminates its armed wing, (ii) increasing external financial/military support for Pakistan leads to no NEs where LET reduces violence, (iii) almost all NEs in which LET significantly reduces violence involve coordinated actions by both the US and India.",
                "ieee_keywords": [
                    "Government",
                    "Terrorism",
                    "Lead",
                    "Games",
                    "Game theory",
                    "Educational institutions"
                ],
                "author_keywords": [
                    "Game theory",
                    "Computational modelling",
                    "International relations"
                ]
            },
            {
                "title": "PREVE: A Policy Recommendation Engine based on Vector Equilibria applied to reducing LeT's attacks",
                "link": "https://ieeexplore.ieee.org/document/6785837/",
                "date_of_publication": "10 April 2014",
                "doi": "10.1145/2492517.2500241",
                "citations": "1",
                "abstract": "We consider the problem of dealing with the terrorist group Lashkar-e-Taiba (LeT), responsible for the 2008 Mumbai attacks, as a five-player game. However, as different experts vary in their assessment of players' payoffs in this game (and other games), we identify multi-payoff equilibria through a novel combination of vector payoffs and well-supported ε-approximate equilibria. We develop a grid search algorithm for computing such equilibria, and provide experimental validation using three payoff matrices filled in by experts in India-Pakistan relations. The resulting system, called PREVE, allows us to analyze the equilibria thus generated and suggest policies to reduce attacks by LeT. We briefly discuss the suggested policies and identify their strengths and weaknesses.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Distributionally Robust Cycle and Chain Packing With Application To Organ Exchange",
                "link": "https://ieeexplore.ieee.org/document/9715358/",
                "date_of_publication": "23 February 2022",
                "doi": "10.1109/WSC52266.2021.9715358",
                "citations": "39",
                "abstract": "We consider the cycle packing problems motivated by kidney exchange. In kidney exchange, patients with willing but incompatible donors enter into an organized market and trade donors in cyclic structures. Exchange programs attempt to match patients and donors utilizing the quality of matches. Current methods use a point estimate for the utility of a potential match that is drawn from an unknown distribution over possible true qualities. We apply the conditional value-at-risk paradigm to the size-constrained cycle and chain packing problem. We derive sample average approximation and distributionally-robust-optimization approaches to maximize the true quality of matched organs in the face of uncertainty over the quality of potential matches. We test our approach on the realistic kidney exchange data and show they outperform the state-of-the-art approaches. In the experiments, we use randomly generated exchange graphs resembling the structure of real exchanges, using anonymized data from the United Network for Organ Sharing.",
                "ieee_keywords": [
                    "Uncertainty",
                    "Kidney",
                    "Faces"
                ],
                "author_keywords": []
            },
            {
                "title": "Using sentiment to detect bots on Twitter: Are humans more opinionated than bots?",
                "link": "https://ieeexplore.ieee.org/document/6921650/",
                "date_of_publication": "16 October 2014",
                "doi": "10.1109/ASONAM.2014.6921650",
                "citations": "125",
                "abstract": "In many Twitter applications, developers collect only a limited sample of tweets and a local portion of the Twitter network. Given such Twitter applications with limited data, how can we classify Twitter users as either bots or humans? We develop a collection of network-, linguistic-, and application-oriented variables that could be used as possible features, and identify specific features that distinguish well between humans and bots. In particular, by analyzing a large dataset relating to the 2014 Indian election, we show that a number of sentimentrelated factors are key to the identification of bots, significantly increasing the Area under the ROC Curve (AUROC). The same method may be used for other applications as well.",
                "ieee_keywords": [
                    "Twitter",
                    "Nominations and elections",
                    "Semantics",
                    "Syntactics",
                    "Principal component analysis",
                    "Conferences"
                ],
                "author_keywords": []
            },
            {
                "title": "Forecasting Country Stability in North Africa",
                "link": "https://ieeexplore.ieee.org/document/6975598/",
                "date_of_publication": "06 December 2014",
                "doi": "10.1109/JISIC.2014.60",
                "citations": "1",
                "abstract": "We develop a novel approach to predict certain type of stability events (battles, battles won by a government, riots/protests, violence against civilians) in countries by monitoring the content of a mix of traditional news, blog, and social media data. Specifically, we show that by monitoring sentiment on both pro- and anti-government entities within a country, even with a relative paucity of longitudinal data (36 time points), we can predict these stability related events with just over 80% classification accuracy. We report on our methods, together with a description of a prototype system called Sentibility that tracks country stability related events. In addition, we cast light on the key entities, sentiments on whom were correlated strongly (positively or negatively) by both Pearson and Spearman correlation coefficients, with such stability events in 3 countries: Egypt, Morocco, and Sudan.",
                "ieee_keywords": [
                    "Stability analysis",
                    "Accuracy",
                    "Correlation",
                    "Government",
                    "Databases",
                    "Power system stability",
                    "Educational institutions"
                ],
                "author_keywords": [
                    "Sentiment analysis",
                    "forecasting stability events"
                ]
            }
        ]
    },
    {
        "name": "Abhinav Shrivastava",
        "publications": [
            {
                "title": "Towards Scalable Neural Representation for Diverse Videos",
                "link": "https://ieeexplore.ieee.org/document/10205388/",
                "date_of_publication": "22 August 2023",
                "doi": "10.1109/CVPR52729.2023.00594",
                "citations": "2",
                "abstract": "Implicit neural representations (INR) have gained increasing attention in representing 3D scenes and images, and have been recently applied to encode videos (e.g., NeRV [1], E-NeRV [2]). While achieving promising results, existing INR-based methods are limited to encoding a handful of short videos (e.g., seven 5-second videos in the UVG dataset) with redundant visual content, leading to a model design that fits individual video frames independently and is not efficiently scalable to a large number of diverse videos. This paper focuses on developing neural representations for a more practical setup - encoding long and/or a large number of videos with diverse visual content. We first show that instead of dividing videos into small subsets and encoding them with separate models, encoding long and diverse videos jointly with a unified model achieves better compression results. Based on this observation, we propose D-NeRV, a novel neural representation framework designed to encode diverse videos by (i) decoupling clip-specific visual content from motion information, (ii) introducing temporal reasoning into the implicit neural network, and (iii) employing the task-oriented flow as intermediate output to reduce spatial redundancies. Our new model largely surpasses NeRV and traditional video compression techniques on UCF101 and UVG datasets on the video compression task. Moreover, when used as an efficient data-loader, D-NeRV achieves 3%-10% higher accuracy than NeRV on action recognition tasks on the UCF101 dataset under the same compression ratios.",
                "ieee_keywords": [
                    "Visualization",
                    "Three-dimensional displays",
                    "Neural networks",
                    "Termination of employment",
                    "Video compression",
                    "Encoding",
                    "Cognition"
                ],
                "author_keywords": [
                    "Deep learning architectures and techniques"
                ]
            },
            {
                "title": "Improved Modeling of 3D Shapes with Multi-view Depth Maps",
                "link": "https://ieeexplore.ieee.org/document/9320100/",
                "date_of_publication": "19 January 2021",
                "doi": "10.1109/3DV50981.2020.00017",
                "citations": "4",
                "abstract": "We present a simple yet effective general-purpose framework for modeling 3D shapes by leveraging recent advances in 2D image generation using CNNs. Using just a single depth image of the object, we can output a dense multi-view depth map representation of 3D objects. Our simple encoder-decoder framework, comprised of a novel identity encoder and class-conditional viewpoint generator, generates 3D consistent depth maps. Our experimental results demonstrate the two-fold advantage of our approach. First, we can directly borrow architectures that work well in the 2D image domain to 3D. Second, we can effectively generate high-resolution 3D shapes with low computational memory. Our quantitative evaluations show that our method is superior to existing depth map methods for reconstructing and synthesizing 3D objects and is competitive with other representations, such as point clouds, voxel grids, and implicit functions. Code and other material will be made available at http://multiview-shapes. umiacs.io.",
                "ieee_keywords": [
                    "Three-dimensional displays",
                    "Shape",
                    "Solid modeling",
                    "Convolution",
                    "Generators",
                    "Image reconstruction",
                    "Two dimensional displays"
                ],
                "author_keywords": [
                    "multivew",
                    "depthmap",
                    "shapes",
                    "generative models",
                    "stylegan",
                    "implict mle",
                    "gan",
                    "vae",
                    "autoencoders",
                    "reconstruction"
                ]
            },
            {
                "title": "The Lottery Ticket Hypothesis for Object Recognition",
                "link": "https://ieeexplore.ieee.org/document/9578168/",
                "date_of_publication": "02 November 2021",
                "doi": "10.1109/CVPR46437.2021.00082",
                "citations": "14",
                "abstract": "Recognition tasks, such as object recognition and key-point estimation, have seen widespread adoption in recent years. Most state-of-the-art methods for these tasks use deep networks that are computationally expensive and have huge memory footprints. This makes it exceedingly difficult to deploy these systems on low power embedded devices. Hence, the importance of decreasing the storage requirements and the amount of computation in such models is paramount. The recently proposed Lottery Ticket Hypothesis (LTH) states that deep neural networks trained on large datasets contain smaller subnetworks that achieve on par performance as the dense networks. In this work, we perform the first empirical study investigating LTH for model pruning in the context of object detection, instance segmentation, and keypoint estimation. Our studies reveal that lottery tickets obtained from Imagenet pretraining do not transfer well to the downstream tasks. We provide guidance on how to find lottery tickets with up to 80% overall sparsity on different sub-tasks without incurring any drop in the performance. Finally, we analyse the behavior of trained tickets with respect to various task attributes such as object size, frequency, and difficulty of detection.",
                "ieee_keywords": [
                    "Training",
                    "Performance evaluation",
                    "Computational modeling",
                    "Pipelines",
                    "Estimation",
                    "Object detection",
                    "Software"
                ],
                "author_keywords": []
            },
            {
                "title": "Hierarchical Video Prediction using Relational Layouts for Human-Object Interactions",
                "link": "https://ieeexplore.ieee.org/document/9578203/",
                "date_of_publication": "02 November 2021",
                "doi": "10.1109/CVPR46437.2021.01197",
                "citations": "1",
                "abstract": "Learning to model and predict how humans interact with objects while performing an action is challenging, and most of the existing video prediction models are ineffective in modeling complicated human-object interactions. Our work builds on hierarchical video prediction models, which disentangle the video generation process into two stages: predicting a high-level representation, such as pose sequence, and then learning a pose-to-pixels translation model for pixel generation. An action sequence for a human-object interaction task is typically very complicated, involving the evolution of pose, person’s appearance, object locations, and object appearances over time. To this end, we propose a Hierarchical Video Prediction model using Relational Layouts. In the first stage, we learn to predict a sequence of layouts. A layout is a high-level representation of the video containing both pose and objects’ information for every frame. The layout sequence is learned by modeling the relationships between the pose and objects using relational reasoning and recurrent neural networks. The layout sequence acts as a strong structure prior to the second stage that learns to map the layouts into pixel space. Experimental evaluation of our method on two datasets, UMD-HOI and Bimanual, shows significant improvements in standard video evaluation metrics such as LPIPS, PSNR, and SSIM. We also perform a detailed qualitative analysis of our model to demonstrate various generalizations.",
                "ieee_keywords": [
                    "Measurement",
                    "Computer vision",
                    "Recurrent neural networks",
                    "Computational modeling",
                    "Layout",
                    "Predictive models",
                    "Cognition"
                ],
                "author_keywords": []
            },
            {
                "title": "Render4Completion: Synthesizing Multi-View Depth Maps for 3D Shape Completion",
                "link": "https://ieeexplore.ieee.org/document/9022145/",
                "date_of_publication": "05 March 2020",
                "doi": "10.1109/ICCVW.2019.00506",
                "citations": "30",
                "abstract": "We propose a novel approach for 3D shape completion by synthesizing multi-view depth maps. While previous work for shape completion relies on volumetric representations, meshes, or point clouds, we propose to use multi-view depth maps from a set of fixed viewing angles as our shape representation. This allows us to be free of the memory limitations of volumetric representations and point clouds by casting shape completion into an image-to-image translation problem. Specifically, we render depth maps of the incomplete shape from a fixed set of viewpoints, and perform depth map completion in each view. Different from image-to-image translation networks that process each view separately, our novel multi-view completion net (MVCN) leverages information from all views of a 3D shape to help the completion of each single view. This enables MVCN to leverage more information from different depth views to achieve high accuracy in single depth view completion, and improve the consistency among the completed depth images in different views. Benefiting from the multi-view representation and novel network structure, MVCN significantly improves the accuracy of 3D shape completion in large-scale benchmarks compared to the state of the art.",
                "ieee_keywords": [
                    "Shape",
                    "Three-dimensional displays",
                    "Gallium nitride",
                    "Feature extraction",
                    "Training",
                    "Task analysis",
                    "Image resolution"
                ],
                "author_keywords": [
                    "3D shape completion",
                    "3D vision"
                ]
            },
            {
                "title": "Learned Spatial Representations for Few-shot Talking-Head Synthesis",
                "link": "https://ieeexplore.ieee.org/document/9710986/",
                "date_of_publication": "28 February 2022",
                "doi": "10.1109/ICCV48922.2021.01357",
                "citations": "11",
                "abstract": "We propose a novel approach for few-shot talking-head synthesis. While recent works in neural talking heads have produced promising results, they can still produce images that do not preserve the identity of the subject in source images. We posit this is a result of the entangled representation of each subject in a single latent code that models 3D shape information, identity cues, colors, lighting and even background details. In contrast, we propose to factorize the representation of a subject into its spatial and style components. Our method generates a target frame in two steps. First, it predicts a discrete and dense spatial layout for the target image. Second, an image generator utilizes the predicted layout for spatial denormalization and synthesizes the target frame. We experimentally show that this disentangled representation leads to a significant improvement over previous methods, both quantitatively and qualitatively.",
                "ieee_keywords": [
                    "Solid modeling",
                    "Three-dimensional displays",
                    "Head",
                    "Shape",
                    "Image color analysis",
                    "Layout",
                    "Lighting"
                ],
                "author_keywords": [
                    "Image and video synthesis",
                    "Neural generative models",
                    "Representation learning"
                ]
            },
            {
                "title": "Towards Discovery and Attribution of Open-world GAN Generated Images",
                "link": "https://ieeexplore.ieee.org/document/9710737/",
                "date_of_publication": "28 February 2022",
                "doi": "10.1109/ICCV48922.2021.01383",
                "citations": "9",
                "abstract": "With the recent progress in Generative Adversarial Networks (GANs), it is imperative for media and visual forensics to develop detectors which can identify and attribute images to the model generating them. Existing works have shown to attribute images to their corresponding GAN sources with high accuracy. However, these works are limited to a closed set scenario, failing to generalize to GANs unseen during train time and are therefore, not scalable with a steady influx of new GANs. We present an iterative algorithm for discovering images generated from previously unseen GANs by exploiting the fact that all GANs leave distinct fingerprints on their generated images. Our algorithm consists of multiple components including network training, out-of-distribution detection, clustering, merge and refine steps. Through extensive experiments, we show that our algorithm discovers unseen GANs with high accuracy and also generalizes to GANs trained on unseen real datasets. We additionally apply our algorithm to attribution and discovery of GANs in an online fashion as well as to the more standard task of real/fake detection. Our experiments demonstrate the effectiveness of our approach to discover new GANs and can be used in an open-world setup.",
                "ieee_keywords": [
                    "Training",
                    "Visualization",
                    "Pipelines",
                    "Clustering algorithms",
                    "Fingerprint recognition",
                    "Media",
                    "Generative adversarial networks"
                ],
                "author_keywords": [
                    "Image and video manipulation detection and integrity methods",
                    "Representation learning",
                    "Transfer/Low-shot/Semi/Unsupervised Learning"
                ]
            },
            {
                "title": "StEP: Style-based Encoder Pre-training for Multi-modal Image Synthesis",
                "link": "https://ieeexplore.ieee.org/document/9578574/",
                "date_of_publication": "02 November 2021",
                "doi": "10.1109/CVPR46437.2021.00371",
                "citations": "6",
                "abstract": "We propose a novel approach for multi-modal Image-to-image (I2I) translation. To tackle the one-to-many relationship between input and output domains, previous works use complex training objectives to learn a latent embedding, jointly with the generator, that models the variability of the output domain. In contrast, we directly model the style variability of images, independent of the image synthesis task. Specifically, we pre-train a generic style encoder using a novel proxy task to learn an embedding of images, from arbitrary domains, into a low-dimensional style latent space. The learned latent space introduces several advantages over previous traditional approaches to multi-modal I2I translation. First, it is not dependent on the target dataset, and generalizes well across multiple domains. Second, it learns a more powerful and expressive latent space, which improves the fidelity of style capture and transfer. The proposed style pre-training also simplifies the training objective and speeds up the training significantly. Furthermore, we provide a detailed study of the contribution of different loss terms to the task of multi-modal I2I translation, and propose a simple alternative to VAEs to enable sampling from unconstrained latent spaces. Finally, we achieve state-of-the-art results on six challenging benchmarks with a simple training objective that includes only a GAN loss and a reconstruction loss.",
                "ieee_keywords": [
                    "Training",
                    "Computer vision",
                    "Codes",
                    "Image synthesis",
                    "Benchmark testing",
                    "Generative adversarial networks",
                    "Generators"
                ],
                "author_keywords": []
            },
            {
                "title": "Pose and Joint-Aware Action Recognition",
                "link": "https://ieeexplore.ieee.org/document/9706760/",
                "date_of_publication": "15 February 2022",
                "doi": "10.1109/WACV51458.2022.00022",
                "citations": "10",
                "abstract": "Recent progress on action recognition has mainly focused on RGB and optical flow features. In this paper, we approach the problem of joint-based action recognition. Unlike other modalities, constellation of joints and their motion generate models with succinct human motion information for activity recognition. We present a new model for joint-based action recognition, which first extracts motion features from each joint separately through a shared motion encoder before performing collective reasoning. Our joint selector module re-weights the joint information to select the most discriminative joints for the task. We also propose a novel joint-contrastive loss that pulls together groups of joint features which convey the same action. We strengthen the joint-based representations by using a geometry-aware data augmentation technique which jitters pose heatmaps while retaining the dynamics of the action. We show large improvements over the current state-of-the-art joint-based approaches on JHMDB, HMDB, Charades, AVA action recognition datasets. A late fusion with RGB and Flow-based approaches yields additional improvements. Our model also outperforms the existing baseline on Mimetics, a dataset with out-of-context actions.",
                "ieee_keywords": [
                    "Heating systems",
                    "Computer vision",
                    "Computational modeling",
                    "Jitter",
                    "Activity recognition",
                    "Feature extraction",
                    "Data mining"
                ],
                "author_keywords": [
                    "Action and Behavior Recognition Biometrics -> Human Motion Analysis/Capture"
                ]
            },
            {
                "title": "LayoutTransformer: Layout Generation and Completion with Self-attention",
                "link": "https://ieeexplore.ieee.org/document/9710883/",
                "date_of_publication": "28 February 2022",
                "doi": "10.1109/ICCV48922.2021.00104",
                "citations": "18",
                "abstract": "We address the problem of scene layout generation for diverse domains such as images, mobile applications, documents, and 3D objects. Most complex scenes, natural or human-designed, can be expressed as a meaningful arrangement of simpler compositional graphical primitives. Generating a new layout or extending an existing layout re- quires understanding the relationships between these primitives. To do this, we propose LayoutTransformer, a novel framework that leverages self-attention to learn contextual relationships between layout elements and generate novel layouts in a given domain. Our framework allows us to generate a new layout either from an empty set or from an initial seed set of primitives, and can easily scale to support an arbitrary of primitives per layout. Furthermore, our analyses show that the model is able to automatically capture the semantic properties of the primitives. We propose simple improvements in both representation of layout primitives, as well as training methods to demonstrate competitive performance in very diverse data domains such as object bounding boxes in natural images (COCO bounding box), documents (PubLayNet), mobile applications (RICO dataset) as well as 3D shapes (Part-Net). Code and other materials will be made available at https://kampta.github.io/layout.",
                "ieee_keywords": [
                    "Training",
                    "Computer vision",
                    "Analytical models",
                    "Three-dimensional displays",
                    "Codes",
                    "Shape",
                    "Layout"
                ],
                "author_keywords": [
                    "Scene text and document understanding",
                    "Image and video synthesis",
                    "Neural generative models",
                    "Representation learning",
                    "Vision + other modalities",
                    "Vision applications and systems"
                ]
            }
        ]
    },
    {
        "name": "MohammadTaghi Hajiaghayi",
        "publications": [
            {
                "title": "PREVE: A Policy Recommendation Engine based on Vector Equilibria applied to reducing LeT's attacks",
                "link": "https://ieeexplore.ieee.org/document/6785837/",
                "date_of_publication": "10 April 2014",
                "doi": "10.1145/2492517.2500241",
                "citations": "1",
                "abstract": "We consider the problem of dealing with the terrorist group Lashkar-e-Taiba (LeT), responsible for the 2008 Mumbai attacks, as a five-player game. However, as different experts vary in their assessment of players' payoffs in this game (and other games), we identify multi-payoff equilibria through a novel combination of vector payoffs and well-supported ε-approximate equilibria. We develop a grid search algorithm for computing such equilibria, and provide experimental validation using three payoff matrices filled in by experts in India-Pakistan relations. The resulting system, called PREVE, allows us to analyze the equilibria thus generated and suggest policies to reduce attacks by LeT. We briefly discuss the suggested policies and identify their strengths and weaknesses.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "PIE: A Data-Driven Payoff Inference Engine for Strategic Security Applications",
                "link": "https://ieeexplore.ieee.org/document/8952764/",
                "date_of_publication": null,
                "doi": "10.1109/TCSS.2019.2957178",
                "citations": "167",
                "abstract": "Although most game theory models assume that payoff matrices are provided as input, getting payoff matrices in strategic games (e.g., corporate negotiations and counter-terrorism operations) has proven difficult. To tackle this challenge, we propose a payoff inference engine (PIE) that finds payoffs assuming that players in a game follow a myopic best response or a regret minimization heuristic. This assumption yields a set of constraints (possibly nonlinear) on the payoffs with a multiplicity of solutions. PIE finds payoffs by considering solutions of these constraints and their variants via three heuristics. First, we approximately compute a centroid of the resulting polytope of the constraints. Second, we use a soft constraint approach that allows violation of constraints by penalizing violations in the objective function. Third, we develop a novel approach to payoff inference based on support vector machines (SVMs). Unlike past work on payoff inference, PIE has the following advantages. PIE supports reasoning about multiplayer games, not just one or two players, it can use short histories, not long ones which may not be available in many real-world situations, it does not require all players to be fully rational, and it is one to two orders of magnitude more scalable than past work. We run experiments on a synthetic data set where we generate payoff functions for the players and see how well our algorithms can learn them, a real-world coarse-grained counter-terrorism data set about a set of different terrorist groups, and a real-world fine-grained data set about a specific terrorist group. As the ground truth about payoffs for the terrorist groups cannot be tested directly, we test PIE by using the payoffs to make predictions about the actions of the groups and corresponding governments (even though this is not the purpose of this article). We show that compared with recent work on payoff inference, PIE has both higher accuracy and much shorter runtime. (Show More)",
                "ieee_keywords": [
                    "Games",
                    "Terrorism",
                    "Biological system modeling",
                    "History",
                    "Computational modeling",
                    "Government",
                    "Computer science"
                ],
                "author_keywords": [
                    "Counter terrorism",
                    "game theory",
                    "payoff inference"
                ]
            },
            {
                "title": "Online Node-Weighted Steiner Forest and Extensions via Disk Paintings",
                "link": "https://ieeexplore.ieee.org/document/6686192/",
                "date_of_publication": "19 December 2013",
                "doi": "10.1109/FOCS.2013.66",
                "citations": "11",
                "abstract": "We give the first polynomial-time online algorithm for the node-weighted Steiner forest problem with a poly-logarithmic competitive ratio. The competitive ratio of our algorithm is optimal up to a logarithmic factor. For the special case of graphs with an excluded fixed minor (e.g., planar graphs), we obtain a logarithmic competitive ratio, which is optimal up to a constant, using a different online algorithm. Both these results are obtained as special cases of generic results for a large class of problems that can be encoded as online 0, 1-proper functions. Our results are obtained by using a new framework for online network design problems that we call disk paintings. The central idea in this technique is to amortize the cost of primal updates to a set of carefully selected mutually disjoint fixed-radius dual disks centered at a subset of terminals. We hope that this framework will be useful for other online network design problems.",
                "ieee_keywords": [
                    "Painting",
                    "Algorithm design and analysis",
                    "Approximation algorithms",
                    "Color",
                    "Polynomials",
                    "Computer science",
                    "Joining processes"
                ],
                "author_keywords": [
                    "Online Algorithm",
                    "Network Design",
                    "Steiner Tree",
                    "Steiner Forest"
                ]
            },
            {
                "title": "Towards an efficient algorithmic framework for pricing cellular data service",
                "link": "https://ieeexplore.ieee.org/document/5935231/",
                "date_of_publication": "30 June 2011",
                "doi": "10.1109/INFCOM.2011.5935231",
                "citations": "1",
                "abstract": "As wireless service providers move from flat-fee unlimited data plans to tiered usage-based ones, there has been little published research on how such tiered plans should be designed. In this paper, we tackle this problem from an algorithmic perspective: formulating the problem of tiered data pricing plans for a wireless provider, and proposing an efficient algorithmic framework to compute the plans. Our algorithmic framework can be applied to the usage and cost data of any provider to obtain the pricing functions specific to that provider.",
                "ieee_keywords": [
                    "Pricing",
                    "Cost function",
                    "Data models",
                    "Wireless communication",
                    "Approximation algorithms",
                    "Internet",
                    "Approximation methods"
                ],
                "author_keywords": []
            },
            {
                "title": "Two is Better Than One: Dual Embeddings for Complementary Product Recommendations",
                "link": "https://ieeexplore.ieee.org/document/10030004/",
                "date_of_publication": "06 February 2023",
                "doi": "10.1109/ICKG55886.2022.00024",
                "citations": "45",
                "abstract": "Embedding based product recommendations have gained popularity in recent years due to its ability to easily integrate to large-scale systems and allowing nearest neighbor searches in real-time. The bulk of studies in this area has predominantly been focused on similar item recommendations. Research on complementary item recommendations, on the other hand, still remains considerably under-explored. We define similar items as items that are interchangeable in terms of their utility and complementary items as items that serve different purposes, yet are compatible when used with one another. In this paper, we apply a novel approach to finding complementary items by leveraging dual embedding representations for products. We demonstrate that the notion of relatedness discovered in NLP for skip-gram negative sampling (SGNS) models translates effectively to the concept of complementarity when training item representations using co-purchase data. Since sparsity of purchase data is a major challenge in real-world scenarios, we further augment the model using synthetic samples to extend coverage. This allows the model to provide complementary recommendations for items that do not share co-purchase data by leveraging other abundantly available data modalities such as images, text, clicks etc. We establish the effectiveness of our approach in improving both coverage and quality of recommendations on real world data for a major online retail company. We further show the importance of task specific hyperparameter tuning in training SGNS. Our model is effective yet simple to implement, making it a great candidate for generating complementary item recommendations at any e-commerce website.",
                "ieee_keywords": [
                    "Training",
                    "Software packages",
                    "Reinforcement learning",
                    "Data models",
                    "Real-time systems",
                    "Electronic commerce",
                    "Task analysis"
                ],
                "author_keywords": [
                    "Recommendation Systems",
                    "Candidate Retrieval",
                    "Complementary Product Recommendations",
                    "SGNS",
                    "Representation Learning"
                ]
            }
        ]
    },
    {
        "name": "Soheil Feizi",
        "publications": [
            {
                "title": "Segment and Complete: Defending Object Detectors against Adversarial Patch Attacks with Robust Patch Detection",
                "link": "https://ieeexplore.ieee.org/document/9878604/",
                "date_of_publication": "27 September 2022",
                "doi": "10.1109/CVPR52688.2022.01455",
                "citations": "1",
                "abstract": "Object detection plays a key role in many security-critical systems. Adversarial patch attacks, which are easy to implement in the physical world, pose a serious threat to state-of-the-art object detectors. Developing reliable defenses for object detectors against patch attacks is critical but severely understudied. In this paper, we propose Segment and Complete defense (SAC), a general framework for defending object detectors against patch attacks through detection and removal of adversarial patches. We first train a patch segmenter that outputs patch masks which provide pixel-level localization of adversarial patches. We then propose a self adversarial training algorithm to robustify the patch segmenter. In addition, we design a robust shape completion algorithm, which is guaranteed to remove the entire patch from the images if the outputs of the patch segmenter are within a certain Hamming distance of the ground-truth patch masks. Our experiments on COCO and xView datasets demonstrate that SAC achieves superior robustness even under strong adaptive attacks with no reduction in performance on clean images, and generalizes well to unseen patch shapes, attack budgets, and unseen attack methods. Furthermore, we present the APRICOT-Mask dataset, which augments the APRICOT dataset with pixel-level annotations of adversarial patches. We show SAC can significantly reduce the targeted attack success rate of physical patch attacks. Our code is available at https://github.com/joellliu/SegmentAndComplete.",
                "ieee_keywords": [
                    "Training",
                    "Location awareness",
                    "Image segmentation",
                    "Computer vision",
                    "Shape",
                    "Detectors",
                    "Object detection"
                ],
                "author_keywords": [
                    "Adversarial attack and defense"
                ]
            },
            {
                "title": "Interpolated Joint Space Adversarial Training for Robust and Generalizable Defenses",
                "link": "https://ieeexplore.ieee.org/document/10155464/",
                "date_of_publication": null,
                "doi": "10.1109/TPAMI.2023.3286772",
                "citations": "53",
                "abstract": "Adversarial training (AT) is considered to be one of the most reliable defenses against adversarial attacks. However, models trained with AT sacrifice standard accuracy and do not generalize well to unseen attacks. Some examples of recent works show generalization improvement with adversarial samples under unseen threat models are, on-manifold threat model or neural perceptual threat model. However, the former requires exact manifold information while the latter requires algorithm relaxation. Motivated by these considerations, we propose a novel threat model called Joint Space Threat Model (JSTM), which exploits the underlying manifold information with Normalizing Flow, ensuring that the exact manifold assumption holds. Under JSTM, we develop novel adversarial attacks and defenses. Specifically, we propose the Robust Mixup strategy in which we maximize the adversity of the interpolated images and gain robustness and prevent overfitting. Our experiments show that Interpolated Joint Space Adversarial Training (IJSAT) achieves good performance in standard accuracy, robustness, and generalization. IJSAT is also flexible and can be used as a data augmentation method to improve standard accuracy and combined with many existing AT approaches can improve robustness. We demonstrate the effectiveness of our approach on three benchmark datasets, CIFAR-10/100, OM-ImageNet and CIFAR-10-C.",
                "ieee_keywords": [
                    "Robustness",
                    "Training",
                    "Manifolds",
                    "Standards",
                    "Threat modeling",
                    "Computational modeling",
                    "Data models"
                ],
                "author_keywords": [
                    "Adversarial Robustness",
                    "Adversarial Defense",
                    "Image Classification",
                    "Generative Models"
                ]
            },
            {
                "title": "Spectral Alignment of Graphs",
                "link": "https://ieeexplore.ieee.org/document/8698885/",
                "date_of_publication": null,
                "doi": "10.1109/TNSE.2019.2913233",
                "citations": "20",
                "abstract": "Graph alignment refers to the problem of finding a bijective mapping across vertices of two graphs such that, if two nodes are connected in the first graph, their images are connected in the second graph. This problem arises in many fields, such as computational biology, social sciences, and computer vision and is often cast as a quadratic assignment problem (QAP). Most standard graph alignment methods consider an optimization that maximizes the number of matches between the two graphs, ignoring the effect of mismatches. We propose a generalized graph alignment formulation that considers both matches and mismatches in a standard QAP formulation. This modification can have a major impact in aligning graphs with different sizes and heterogeneous edge densities. Moreover, we propose two methods for solving the generalized graph alignment problem based on spectral decomposition of matrices. We compare the performance of proposed methods with some existing graph alignment algorithms including Natalie2, GHOST, IsoRank, NetAlign, Klau's approach as well as a semidefinite programming-based method over various synthetic and real graph models. Our proposed method based on simultaneous alignment of multiple eigenvectors leads to consistently good performance in different graph models. In particular, in the alignment of regular graph structures, which is one of the most difficult graph alignment cases, our proposed method significantly outperforms other methods.",
                "ieee_keywords": [
                    "Optimization",
                    "Matrix decomposition",
                    "Computer vision",
                    "Standards",
                    "Linear programming",
                    "Social sciences",
                    "Approximation algorithms"
                ],
                "author_keywords": [
                    "Graph Alignment",
                    "Graph Matching",
                    "Quadratic Assignment Problem",
                    "Spectral Graph Methods."
                ]
            },
            {
                "title": "Mutual Adversarial Training: Learning Together is Better Than Going Alone",
                "link": "https://ieeexplore.ieee.org/document/9798870/",
                "date_of_publication": null,
                "doi": "10.1109/TIFS.2022.3184262",
                "citations": "1",
                "abstract": "Recent studies have shown that robustness to adversarial attacks can be transferred across deep neural networks. In other words, we can make a weak model more robust with the help of a strong teacher model. In this paper, we ask if models can “learn together” and “teach each other” to achieve better robustness instead of learning from a static teacher. We study how interactions among models enhance robustness via knowledge distillation. We propose mutual adversarial training (MAT), in which multiple models are trained together and share the knowledge of adversarial examples to achieve improved robustness. MAT allows robust models to explore a larger space of adversarial samples and find more robust feature spaces and decision boundaries. Through extensive experiments on the CIFAR-10, CIFAR-100, and mini-ImageNet datasets, we demonstrate that MAT can effectively improve model robustness and outperform state-of-the-art methods under white-box attacks. In addition, we show that MAT can also mitigate the robustness trade-off among different perturbation types. Specially, we train specialist models that learn to defend a specific perturbation type and a generalist model that learns to defend multiple perturbation types by learning from the specialists, which brings as much as 13.4% accuracy gain to AT baselines against the union of $l_{\\infty} $ , $l_{2}$ , and $l_{1}$ attacks. Our results show the effectiveness of the proposed method and demonstrate that collaborative learning is an effective strategy for designing robust models.",
                "ieee_keywords": [
                    "Robustness",
                    "Training",
                    "Perturbation methods",
                    "Computational modeling",
                    "Space exploration",
                    "Data models",
                    "Training data"
                ],
                "author_keywords": [
                    "Adversarial robustness",
                    "adversarial defense",
                    "image classification",
                    "knowledge distillation"
                ]
            },
            {
                "title": "Understanding GANs in the LQG Setting: Formulation, Generalization and Stability",
                "link": "https://ieeexplore.ieee.org/document/9081978/",
                "date_of_publication": null,
                "doi": "10.1109/JSAIT.2020.2991375",
                "citations": "10",
                "abstract": "Generative Adversarial Networks (GANs) have become a popular method to learn a probability model from data. In this paper, we provide an understanding of basic issues surrounding GANs including their formulation, generalization and stability on a simple LQG benchmark where the generator is Linear, the discriminator is Quadratic and the data has a high-dimensional Gaussian distribution. Even in this simple benchmark, the GAN problem has not been well-understood as we observe that existing state-of-the-art GAN architectures may fail to learn a proper generative distribution owing to (1) stability issues (i.e., convergence to bad local solutions or not converging at all), (2) approximation issues (i.e., having improper global GAN optimizers caused by inappropriate GAN's loss functions), and (3) generalizability issues (i.e., requiring large number of samples for training). In this setup, we propose a GAN architecture which recovers the maximum-likelihood solution and demonstrates fast generalization. Moreover, we analyze global stability of different computational approaches for the proposed GAN and highlight their pros and cons. Finally, through experiments on MNIST and CIFAR-10 datasets, we outline extensions of our model-based approach to design GANs in more complex setups than the considered Gaussian benchmark.",
                "ieee_keywords": [
                    "Gallium nitride",
                    "Stability analysis",
                    "Benchmark testing",
                    "Computer architecture",
                    "Generators",
                    "Generative adversarial networks",
                    "Information theory"
                ],
                "author_keywords": [
                    "Generative models",
                    "Wasserstein distance",
                    "PCA",
                    "stability",
                    "Lyapunov functions"
                ]
            },
            {
                "title": "Network Infusion to Infer Information Sources in Networks",
                "link": "https://ieeexplore.ieee.org/document/8408489/",
                "date_of_publication": null,
                "doi": "10.1109/TNSE.2018.2854218",
                "citations": "10",
                "abstract": "Several significant models have been developed that enable the study of diffusion of signals across biological, social and engineered networks. Within these established frameworks, the inverse problem of identifying the source of the propagated signal is challenging, owing to the numerous alternative possibilities for signal progression through the network. In real world networks, the challenge of determining sources is compounded as the true propagation dynamics are typically unknown, and when they have been directly measured, they rarely conform to the assumptions of any of the well-studied models. In this paper we introduce a method called Network Infusion (NI) that has been designed to circumvent these issues, making source inference practical for large, complex real world networks. The key idea is that to infer the source node in the network, full characterization of diffusion dynamics, in many cases, may not be necessary. This objective is achieved by creating a diffusion kernel that well-approximates standard diffusion models such as the susceptible-infected diffusion model, but lends itself to inversion, by design, via likelihood maximization or error minimization. We apply NI for both single-source and multi-source diffusion, for both single-snapshot and multi-snapshot observations, and for both homogeneous and heterogeneous diffusion setups. We prove the mean-field optimality of NI for different scenarios, and demonstrate its effectiveness over several synthetic networks. Moreover, we apply NI to a real-data application, identifying news sources in the Digg social network, and demonstrate the effectiveness of NI compared to existing methods. Finally, we propose an integrative source inference framework that combines NI with a distance centrality-based method, which leads to a robust performance in cases where the underlying dynamics are unknown.",
                "ieee_keywords": [
                    "Kernel",
                    "Social network services",
                    "Computational modeling",
                    "Inverse problems",
                    "Inference algorithms",
                    "Standards",
                    "Mathematical model"
                ],
                "author_keywords": [
                    "Information diffusion",
                    "source inference",
                    "social networks"
                ]
            }
        ]
    },
    {
        "name": "Michelle L. Mazurek",
        "publications": [
            {
                "title": "Understanding User Tradeoffs for Search in Encrypted Communication",
                "link": "https://ieeexplore.ieee.org/document/8406604/",
                "date_of_publication": "09 July 2018",
                "doi": "10.1109/EuroSP.2018.00026",
                "citations": "2",
                "abstract": "End-to-end message encryption is the only way to achieve absolute message privacy. However, searching over end-to-end encrypted messages is complicated. Several popular instant messaging tools (e.g., WhatsApp, iMessage) circumvent this inconvenience by storing the search index locally on the devices. Another approach, called searchable encryption, allows users to search encrypted messages without storing the search index locally. These approaches have inherent tradeoffs between usability and security properties, yet little is known about how general users value these tradeoffs, especially in the context of email rather than instant messaging. In this paper, we systematize these tradeoffs in order to identify key feature differences. We use these differences as the basis for a choice-based conjoint analysis experiment focused on email (n=160), in which participants make a series of choices between email services with competing features. The results allow us to quantify the relative importance of each feature. We find that users indicate high relative importance for increasing privacy and minimizing local storage requirements. While privacy is more important overall, local storage is more important than adding additional marginal privacy after an initial improvement. These results suggest that local indexing, which provides more privacy, may often be appropriate for encrypted email, but that searchable encryption, which limits local storage, may also hold promise for some users.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "I Think They're Trying to Tell Me Something: Advice Sources and Selection for Digital Security",
                "link": "https://ieeexplore.ieee.org/document/7546507/",
                "date_of_publication": "18 August 2016",
                "doi": "10.1109/SP.2016.24",
                "citations": "38",
                "abstract": "Users receive a multitude of digital-and physical-security advice every day. Indeed, if we implemented all the security advice we received, we would never leave our houses or use the Internet. Instead, users selectively choose some advice to accept and some (most) to reject, however, it is unclear whether they are effectively prioritizing what is most important or most useful. If we can understand from where and why users take security advice, we can develop more effective security interventions. As a first step, we conducted 25 semi-structured interviews of a demographically broad pool of users. These interviews resulted in several interesting findings: (1) participants evaluated digital-security advice based on the trustworthiness of the advice source, but evaluated physical-security advice based on their intuitive assessment of the advice content, (2) negative-security events portrayed in well-crafted fictional narratives with relatable characters (such as those shown in TV or movies) may be effective teaching tools for both digital-and physical-security behaviors, and (3) participants rejected advice for many reasons, including finding that the advice contains too much marketing material or threatens their privacy.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Garbage Collection Makes Rust Easier to Use: A Randomized Controlled Trial of the Bronze Garbage Collector",
                "link": "https://ieeexplore.ieee.org/document/9793872/",
                "date_of_publication": "20 June 2022",
                "doi": "10.1145/3510003.3510107",
                "citations": "2",
                "abstract": "Rust is a general-purpose programming language that is both type-and memory-safe. Rust does not use a garbage collector, but rather achieves these properties through a sophisticated, but complex, type system. Doing so makes Rust very efficient, but makes Rust relatively hard to learn and use. We designed Bronze, an optional, library-based garbage collector for Rust. To see whether Bronze could make Rust more usable, we conducted a randomized con-trolled trial with volunteers from a 633-person class, collecting data from 428 students in total. We found that for a task that required managing complex aliasing, Bronze users were more likely to complete the task in the time available, and those who did so required only about a third as much time (4 hours vs. 12 hours). We found no significant difference in total time, even though Bronze users re-did the task without Bronze afterward. Surveys indicated that ownership, borrowing, and lifetimes were primary causes of the challenges that users faced when using Rust.",
                "ieee_keywords": [
                    "Computer languages",
                    "Education",
                    "Task analysis",
                    "Programming profession",
                    "Software engineering"
                ],
                "author_keywords": [
                    "Rust",
                    "garbage collection",
                    "usability of programming languages",
                    "em-pirical study of programming languages",
                    "programming education"
                ]
            },
            {
                "title": "You Get Where You're Looking for: The Impact of Information Sources on Code Security",
                "link": "https://ieeexplore.ieee.org/document/7546508/",
                "date_of_publication": "18 August 2016",
                "doi": "10.1109/SP.2016.25",
                "citations": "113",
                "abstract": "Vulnerabilities in Android code -- including but not limited to insecure data storage, unprotected inter-component communication, broken TLS implementations, and violations of least privilege -- have enabled real-world privacy leaks and motivated research cataloguing their prevalence and impact. Researchers have speculated that appification promotes security problems, as it increasingly allows inexperienced laymen to develop complex and sensitive apps. Anecdotally, Internet resources such as Stack Overflow are blamed for promoting insecure solutions that are naively copy-pasted by inexperienced developers. In this paper, we for the first time systematically analyzed how the use of information resources impacts code security. We first surveyed 295 app developers who have published in the Google Play market concerning how they use resources to solve security-related problems. Based on the survey results, we conducted a lab study with 54 Android developers (students and professionals), in which participants wrote security-and privacy-relevant code under time constraints. The participants were assigned to one of four conditions: free choice of resources, Stack Overflow only, official Android documentation only, or books only. Those participants who were allowed to use only Stack Overflow produced significantly less secure code than those using, the official Android documentation or books, while participants using the official Android documentation produced significantly less functional code than those using Stack Overflow. To assess the quality of Stack Overflow as a resource, we surveyed the 139 threads our participants accessed during the study, finding that only 25% of them were helpful in solving the assigned tasks and only 17% of them contained secure code snippets. In order to obtain ground truth concerning the prevalence of the secure and insecure code our participants wrote in the lab study, we statically analyzed a random sample of 200,000 apps from Google Play... (Show More)",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Comparing the Usability of Cryptographic APIs",
                "link": "https://ieeexplore.ieee.org/document/7958576/",
                "date_of_publication": "26 June 2017",
                "doi": "10.1109/SP.2017.52",
                "citations": "96",
                "abstract": "Potentially dangerous cryptography errors are well-documented in many applications. Conventional wisdom suggests that many of these errors are caused by cryptographic Application Programming Interfaces (APIs) that are too complicated, have insecure defaults, or are poorly documented. To address this problem, researchers have created several cryptographic libraries that they claim are more usable, however, none of these libraries have been empirically evaluated for their ability to promote more secure development. This paper is the first to examine both how and why the design and resulting usability of different cryptographic libraries affects the security of code written with them, with the goal of understanding how to build effective future libraries. We conducted a controlled experiment in which 256 Python developers recruited from GitHub attempt common tasks involving symmetric and asymmetric cryptography using one of five different APIs. We examine their resulting code for functional correctness and security, and compare their results to their self-reported sentiment about their assigned library. Our results suggest that while APIs designed for simplicity can provide security benefits - reducing the decision space, as expected, prevents choice of insecure parameters - simplicity is not enough. Poor documentation, missing code examples, and a lack of auxiliary features such as secure key storage, caused even participants assigned to simplified libraries to struggle with both basic functional correctness and security. Surprisingly, the availability of comprehensive documentation and easy-to-use code examples seems to compensate for more complicated APIs in terms of functionally correct results and participant reactions, however, this did not extend to security results. We find it particularly concerning that for about 20% of functionally correct tasks, across libraries, participants believed their code was secure when it was not. Our results suggest that while new... (Show More)",
                "ieee_keywords": [],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Ming Lin",
        "publications": [
            {
                "title": "Multi-Agent Ergodic Coverage in Urban Environments",
                "link": "https://ieeexplore.ieee.org/document/9561257/",
                "date_of_publication": "18 October 2021",
                "doi": "10.1109/ICRA48506.2021.9561257",
                "citations": "238",
                "abstract": "An important aspect of dynamic urban coverage is how building collision avoidance is incorporated into the overall coverage mission. We consider a multi-agent urban dynamic coverage problem in which a team of flying agents uses downward facing cameras to observe the street-level environment outside of buildings. Cameras are assumed to be ineffective above a maximum altitude (lower than building height), such that agents must move around or over buildings to complete their mission. The main objective of this paper is to compare three different building avoidance strategies that are compatible with dynamic ergodic methods. To provide context for these results, we also compare our results to three other common coverage methods including: boustrophedon coverage (lawn-mower sweep), Voronoi region based coverage, and a naive grid method. All algorithms are evaluated in simulation with respect to four performance metrics (percent coverage, revisit count, revisit time, and the integral of area viewed over time), across team sizes ranging from 1 to 25 agents, and in five types of urban environments of varying density and height. We find that the relative performance of algorithms changes based on the ratio of team size to search area, as well the height and density characteristics of the urban environment.",
                "ieee_keywords": [
                    "Automation",
                    "Heuristic algorithms",
                    "Conferences",
                    "Buildings",
                    "Urban areas",
                    "Cameras",
                    "Distance measurement"
                ],
                "author_keywords": [
                    "Multi-Agent",
                    "Coverage",
                    "Lawn-mower",
                    "Ergodic",
                    "Boustrophedon",
                    "Voronoi",
                    "Urban Environment"
                ]
            },
            {
                "title": "LSwarm: Efficient Collision Avoidance for Large Swarms With Coverage Constraints in Complex Urban Scenes",
                "link": "https://ieeexplore.ieee.org/document/8767930/",
                "date_of_publication": null,
                "doi": "10.1109/LRA.2019.2929981",
                "citations": "24",
                "abstract": "In this letter, we address the problem of collision avoidance for a swarm of UAVs used for continuous surveillance of an urban environment. Our method, LSwarm, efficiently avoids collisions with static obstacles, dynamic obstacles and other agents in three-dimensional urban environments while considering coverage constraints. LSwarm calculates collision avoiding velocities that maximize the conformity of an agent to an optimal path given by a global coverage strategy and ensure sufficient resolution of the coverage data collected by each agent. Our algorithm is formulated based on optimal reciprocal collision avoidance and is scalable with respect to the size of the swarm. We evaluate the coverage performance of LSwarm in realistic simulations of a swarm of quadrotors in complex urban models. In practice, our approach can compute collision avoiding velocities for a swarm composed of tens to hundreds of agents in a few milliseconds on dense urban scenes consisting of tens of buildings.",
                "ieee_keywords": [
                    "Collision avoidance",
                    "Sensors",
                    "Surveillance",
                    "Urban areas",
                    "Multi-robot systems",
                    "Path planning",
                    "Uncertainty"
                ],
                "author_keywords": [
                    "Collision Avoidance",
                    "Path Planning for Multiple Mobile Robots or Agents",
                    "Simulation and Animation"
                ]
            },
            {
                "title": "Adversarial Differentiable Data Augmentation for Autonomous Systems",
                "link": "https://ieeexplore.ieee.org/document/9561205/",
                "date_of_publication": "18 October 2021",
                "doi": "10.1109/ICRA48506.2021.9561205",
                "citations": "4",
                "abstract": "Autonomous systems often rely on neural networks to achieve high performance on planning and control problems. Unfortunately, neural networks suffer severely when input images become degraded in ways that are not reflected in the training data. This is particularly problematic for robotic systems like autonomous vehicles (AV) for which reliability is paramount. In this work, we consider robust optimization methods for hardening control systems against image corruptions and other unexpected domain shifts. Recent work on robust optimization for neural nets has been focused largely on combating adversarial attacks. In this work, we borrow ideas from the adversarial training and data augmentation literature to enhance robustness to image corruptions and domain shifts. To this end, we train networks while augmenting image data with a battery of image degradations. Unlike traditional augmentation methods, we choose the parameters for each degradation adversarially so as to maximize system performance. By formulating image degradations in a way that is differentiable with respect to degradation parameters, we enable the use of efficient optimization methods (PGD) for choosing worst-case augmentation parameters. We demonstrate the efficacy of this method on the learning to steer task for AVs. By adversarially training against image corruptions, we produce networks that are highly robust to image corruptions. We show that the proposed differentiable augmentation schemes result in higher levels of robustness and accuracy for a range of settings as compared to baseline and state-of-the-art augmentation methods.",
                "ieee_keywords": [
                    "Degradation",
                    "Training",
                    "Neural networks",
                    "Optimization methods",
                    "Training data",
                    "Robustness",
                    "Data models"
                ],
                "author_keywords": []
            },
            {
                "title": "Monitoring Access to User Defined Areas with Multi-Agent Team in Urban Environments",
                "link": "https://ieeexplore.ieee.org/document/8901057/",
                "date_of_publication": "14 November 2019",
                "doi": "10.1109/MRS.2019.8901057",
                "citations": "1",
                "abstract": "We present an algorithm that determines where the members of a multi-agent team or swarm should be deployed in order to efficiently monitor access to a user specified region of interest. Our algorithm attempts to minimize the number of agents required to guarantee that any incursion into the region of interest is detected. The algorithm works by analyzing the geometric structure of the environment, and placing agents at advantageous positions in the environment, such as bottlenecks, to create a defensive perimeter of agents alongside physical obstacles (e.g. buildings). We demonstrate the usefulness of the algorithm through experimental simulations in an urban environment, and show how the min-cuts subroutine (used to reduce the number of agents required) can be implemented in a distributed way across the multi-agent team to enable better solutions to be found more quickly.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Reconstructing Tissue Properties From Medical Images With Application in Cancer Screening",
                "link": "https://ieeexplore.ieee.org/document/8627972/",
                "date_of_publication": null,
                "doi": "10.1109/TMRB.2019.2895785",
                "citations": "256",
                "abstract": "Purpose: In this paper, we describe a method for recovering the tissue properties directly from medical images and study the correlation of tissue (i.e., prostate) elasticity with the aggressiveness of prostate cancer using medical image analysis. Methods: We present a novel method that uses geometric and physical constraints to deduce the relative tissue elasticity parameters. Although elasticity reconstruction, or elastograph, can be used to estimate tissue elasticity, it is less suited for in-vivo measurements or deeply seated organs like prostate. We develop a method to estimate tissue elasticity values based on pairs of images, using a finite-element-based biomechanical model derived from an initial set of images, local displacements, and an optimization-based framework. Results: We demonstrate the feasibility of a statistically based classifier that automatically provides a clinical T-stage and Gleason score based on the elasticity values reconstructed from computed tomography images. Conclusion: We study the relative elasticity parameters by performing cancer grading/staging prediction and achieve up to 85% accuracy for cancer staging prediction and up to 77% accuracy for cancer grading prediction using a feature set, which includes recovered relative elasticity parameters and patient age information.",
                "ieee_keywords": [
                    "Elasticity",
                    "Strain",
                    "Computational modeling",
                    "Biomedical imaging",
                    "Stress",
                    "Image reconstruction",
                    "Cancer"
                ],
                "author_keywords": [
                    "Physically-based simulation",
                    "elasticity recovery"
                ]
            },
            {
                "title": "Small-shot Multi-modal Distillation for Vision-based Autonomous Steering",
                "link": "https://ieeexplore.ieee.org/document/10160803/",
                "date_of_publication": "04 July 2023",
                "doi": "10.1109/ICRA48891.2023.10160803",
                "citations": "24",
                "abstract": "In this paper, we propose a novel learning framework for autonomous systems that uses a small amount of “auxiliary information” that complements the learning of the main modality, called “small-shot auxiliary modality distillation network (AMD-S-Net)”. The AMD-S-Net contains a two-stream framework design that can fully extract information from different types of data (i.e., paired/unpaired multi-modality data) to distill knowledge more effectively. We also propose a novel training paradigm based on the “reset operation” that enables the teacher to explore the local loss landscape near the student domain iteratively, providing local landscape information and potential directions to discover better solutions by the student, thus achieving higher learning performance. Our experiments show that AMD-S-Net and our training paradigm outperform other SOTA methods by up to 12.7% and 18.1% improvement in autonomous steering, respectively.",
                "ieee_keywords": [
                    "Training",
                    "Automation",
                    "Autonomous systems",
                    "Data mining",
                    "Task analysis",
                    "Knowledge transfer"
                ],
                "author_keywords": []
            },
            {
                "title": "A Framework for Active Haptic Guidance Using Robotic Haptic Proxies",
                "link": "https://ieeexplore.ieee.org/document/10160996/",
                "date_of_publication": "04 July 2023",
                "doi": "10.1109/ICRA48891.2023.10160996",
                "citations": "31",
                "abstract": "Haptic feedback is an important component of creating an immersive mixed reality experience. Traditionally, haptic forces are rendered in response to the user's interactions with the virtual environment. In this work, we explore the idea of rendering haptic forces in a proactive manner, with the explicit intention to influence the user's behavior through compelling haptic forces. To this end, we present a framework for active haptic guidance in mixed reality, using one or more robotic haptic proxies to influence user behavior and deliver a safer and more immersive virtual experience. We provide details on common challenges that need to be overcome when implementing active haptic guidance, and discuss example applications that show how active haptic guidance can be used to influence the user's behavior. Finally, we apply active haptic guidance to a virtual reality navigation problem, and conduct a user study that demonstrates how active haptic guidance creates a safer and more immersive experience for users.",
                "ieee_keywords": [
                    "Automation",
                    "Navigation",
                    "Mixed reality",
                    "Virtual environments",
                    "Immersive experience",
                    "Rendering (computer graphics)",
                    "Haptic interfaces"
                ],
                "author_keywords": []
            },
            {
                "title": "Enhanced Transfer Learning for Autonomous Driving with Systematic Accident Simulation",
                "link": "https://ieeexplore.ieee.org/document/9341538/",
                "date_of_publication": "10 February 2021",
                "doi": "10.1109/IROS45743.2020.9341538",
                "citations": "4",
                "abstract": "Simulation data can be utilized to extend real-world driving data in order to cover edge cases, such as vehicle accidents. The importance of handling edge cases can be observed in the high societal costs in handling car accidents, as well as potential dangers to human drivers. In order to cover a wide and diverse range of all edge cases, we systemically parameterize and simulate the most common accident scenarios. By applying this data to autonomous driving models, we show that transfer learning on simulated data sets provide better generalization and collision avoidance, as compared to random initialization methods. Our results illustrate that information from a model trained on simulated data can be inferred to a model trained on real-world data, indicating the potential influence of simulation data in real world models and advancements in handling of anomalous driving scenarios.",
                "ieee_keywords": [
                    "Systematics",
                    "Transfer learning",
                    "Reinforcement learning",
                    "Data models",
                    "Autonomous vehicles",
                    "Accidents",
                    "Vehicles"
                ],
                "author_keywords": []
            },
            {
                "title": "A pilot exploration of systematic ideation methods and tools on design learning",
                "link": "https://ieeexplore.ieee.org/document/5480052/",
                "date_of_publication": "07 June 2010",
                "doi": "10.1109/ITHET.2010.5480052",
                "citations": "6",
                "abstract": "Today, more than ever, we need engineering graduates equipped with effective design skills and a high degree of ideation competence. This paper presents results of our pilot study on improving the ideation performance of undergraduate engineers through training in TRIZ and emphasis on sketching. This is the first study of its type to rigorously test the value of TRIZ on ideation performance. Positive study findings would provide guidance to introductory and capstone design course instructors, as well industry practitioners who intend to enhance their ideation toolkits. Our objective is to test our hypotheses that TRIZ, or sketching emphasis improves design ideation alone or in combination. A series of classroom studies using investigator-developed interventions (modules) that train students in TRIZ and Sketching Importance is run. Class participants perform ideation exercises. The results of the ideation exercises are assessed and scored for Novelty, Variety and Quantity. In the paper, we document the results of the pilot experimentation. Overall, pilot studies will help fine tune the experimental approach.",
                "ieee_keywords": [
                    "Design engineering",
                    "Process design",
                    "Engineering students",
                    "Problem-solving",
                    "Testing",
                    "Design methodology",
                    "Psychology",
                    "Educational institutions",
                    "Industrial training",
                    "Pattern analysis"
                ],
                "author_keywords": [
                    "engineering design",
                    "ideation metrics"
                ]
            },
            {
                "title": "Audio-Visual Depth and Material Estimation for Robot Navigation",
                "link": "https://ieeexplore.ieee.org/document/9981549/",
                "date_of_publication": "26 December 2022",
                "doi": "10.1109/IROS47612.2022.9981549",
                "citations": "142",
                "abstract": "Reflective and textureless surfaces such as windows, mirrors, and walls can be a challenge for scene reconstruction, due to depth discontinuities and holes. We propose an audio-visual method that uses the reflections of sound to aid in depth estimation and material classification for 3D scene reconstruction in robot navigation and AR/VR applications. The mobile phone prototype emits pulsed audio, while recording video for audio-visual classification for 3D scene reconstruction. Reflected sound and images from the video are input into our audio (EchoCNN-A) and audio-visual (EchoCNN-AV) convolutional neural networks for surface and sound source detection, depth estimation, and material classification. The inferences from these classifications enhance 3D scene reconstructions containing open spaces and reflective surfaces by depth filtering, inpainting, and placement of unmixed sound sources in the scene. Our prototype, demos, and experimental results from real-world with challenging surfaces and sound, also validated with virtual scenes, indicate high success rates on classification of material, depth estimation, and closed/open surfaces, leading to considerable improvement in 3D scene reconstruction for robot navigation.",
                "ieee_keywords": [
                    "Surface reconstruction",
                    "Three-dimensional displays",
                    "Navigation",
                    "Estimation",
                    "Prototypes",
                    "Reflection",
                    "Mobile handsets"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Niklas Elmqvist",
        "publications": [
            {
                "title": "Information Olfactation: Harnessing Scent to Convey Data",
                "link": "https://ieeexplore.ieee.org/document/8444077/",
                "date_of_publication": null,
                "doi": "10.1109/TVCG.2018.2865237",
                "citations": "28",
                "abstract": "Olfactory feedback for analytical tasks is a virtually unexplored area in spite of the advantages it offers for information recall, feature identification, and location detection. Here we introduce the concept of information olfactation as the fragrant sibling of information visualization, and discuss how scent can be used to convey data. Building on a review of the human olfactory system and mirroring common visualization practice, we propose olfactory marks, the substrate in which they exist, and their olfactory channels that are available to designers. To exemplify this idea, we present vi S cent : A six-scent stereo olfactory display capable of conveying olfactory glyphs of varying temperature and direction, as well as a corresponding software system that integrates the display with a traditional visualization display. Finally, we present three applications that make use of the viScent system: A 2D graph visualization, a 2D line and point chart, and an immersive analytics graph visualization in 3D virtual reality. We close the paper with a review of possible extensions of viScent and applications of information olfactation for general visualization beyond the examples in this paper.",
                "ieee_keywords": [
                    "Olfactory",
                    "Data visualization",
                    "Visualization",
                    "Task analysis",
                    "Two dimensional displays",
                    "Neurons",
                    "Virtual reality"
                ],
                "author_keywords": [
                    "Olfaction",
                    "smell",
                    "scent",
                    "olfactory display",
                    "immersive analytics",
                    "immersion"
                ]
            },
            {
                "title": "Towards Understanding Sensory Substitution for Accessible Visualization: An Interview Study",
                "link": "https://ieeexplore.ieee.org/document/9552177/",
                "date_of_publication": null,
                "doi": "10.1109/TVCG.2021.3114829",
                "citations": "4",
                "abstract": "For all its potential in supporting data analysis, particularly in exploratory situations, visualization also creates barriers: accessibility for blind and visually impaired individuals. Regardless of how effective a visualization is, providing equal access for blind users requires a paradigm shift for the visualization research community. To enact such a shift, it is not sufficient to treat visualization accessibility as merely another technical problem to overcome. Instead, supporting the millions of blind and visually impaired users around the world who have equally valid needs for data analysis as sighted individuals requires a respectful, equitable, and holistic approach that includes all users from the onset. In this paper, we draw on accessibility research methodologies to make inroads towards such an approach. We first identify the people who have specific insight into how blind people perceive the world: orientation and mobility (O&M) experts, who are instructors that teach blind individuals how to navigate the physical world using non-visual senses. We interview 10 O&M experts—all of them blind—to understand how best to use sensory substitution other than the visual sense for conveying spatial layouts. Finally, we investigate our qualitative findings using thematic analysis. While blind people in general tend to use both sound and touch to understand their surroundings, we focused on auditory affordances and how they can be used to make data visualizations accessible—using sonification and auralization. However, our experts recommended supporting a combination of senses—sound and touch—to make charts accessible as blind individuals may be more familiar with exploring tactile charts. We report results on both sound and touch affordances, and conclude by discussing implications for accessible visualization for blind individuals.",
                "ieee_keywords": [
                    "Data visualization",
                    "Visualization",
                    "Interviews",
                    "Blindness",
                    "Training",
                    "Navigation",
                    "Layout"
                ],
                "author_keywords": [
                    "Accessibility",
                    "blind users",
                    "sonification",
                    "visualization",
                    "spatial layouts",
                    "sound perception",
                    "MeSH Terms",
                    "Blindness",
                    "Computer Graphics",
                    "Humans",
                    "Touch",
                    "Vision, Ocular",
                    "Visually Impaired Persons"
                ]
            },
            {
                "title": "Sherpa: Leveraging User Attention for Computational Steering in Visual Analytics",
                "link": "https://ieeexplore.ieee.org/document/8973384/",
                "date_of_publication": "30 January 2020",
                "doi": "10.1109/VDS48975.2019.8973384",
                "citations": "1",
                "abstract": "We present Sherpa, a computational steering mechanism for progressive visual analytics that automatically prioritizes computations based on the analyst's navigational behavior in the data. The intuition is that navigation in data space is an indication of the analyst's interest in the data. Sherpa implementation provides computational modules, such as statistics of biological inferences about gene regulation. The position of the navigation window on the genomic sequence over time is used to prioritize computations. In a study with genomic and visualization analysts, we found that Sherpa provided comparable accuracy to the offline condition, where computations were completed prior to analysis, with shorter completion times. We also provide a second example on stock market analysis.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "VisDock: A Toolkit for Cross-Cutting Interactions in Visualization",
                "link": "https://ieeexplore.ieee.org/document/7063249/",
                "date_of_publication": null,
                "doi": "10.1109/TVCG.2015.2414454",
                "citations": "9",
                "abstract": "Standard user applications provide a range of cross-cutting interaction techniques that are common to virtually all such tools: selection, filtering, navigation, layer management, and cut-and-paste. We present VisDock, a JavaScript mixin library that provides a core set of these cross-cutting interaction techniques for visualization, including selection (lasso, paths, shape selection, etc), layer management (visibility, transparency, set operations, etc), navigation (pan, zoom, overview, magnifying lenses, etc), and annotation (point-based, region-based, data-space based, etc). To showcase the utility of the library, we have released it as Open Source and integrated it with a large number of existing web-based visualizations. Furthermore, we have evaluated VisDock using qualitative studies with both developers utilizing the toolkit to build new web-based visualizations, as well as with end-users utilizing it to explore movie ratings data. Results from these studies highlight the usability and effectiveness of the toolkit from both developer and end-user perspectives.",
                "ieee_keywords": [
                    "Visualization",
                    "Navigation",
                    "Data visualization",
                    "Libraries",
                    "Shape",
                    "Containers",
                    "Reactive power"
                ],
                "author_keywords": [
                    "Visualization system and toolkit design",
                    "interaction design",
                    "user interface",
                    "qualitative evaluation",
                    "Visualization system and toolkit design",
                    "interaction design",
                    "user interface",
                    "qualitative evaluation"
                ]
            },
            {
                "title": "AggreSet: Rich and Scalable Set Exploration using Visualizations of Element Aggregations",
                "link": "https://ieeexplore.ieee.org/document/7194854/",
                "date_of_publication": null,
                "doi": "10.1109/TVCG.2015.2467051",
                "citations": "11",
                "abstract": "Datasets commonly include multi-value (set-typed) attributes that describe set memberships over elements, such as genres per movie or courses taken per student. Set-typed attributes describe rich relations across elements, sets, and the set intersections. Increasing the number of sets results in a combinatorial growth of relations and creates scalability challenges. Exploratory tasks (e.g. selection, comparison) have commonly been designed in separation for set-typed attributes, which reduces interface consistency. To improve on scalability and to support rich, contextual exploration of set-typed data, we present AggreSet. AggreSet creates aggregations for each data dimension: sets, set-degrees, set-pair intersections, and other attributes. It visualizes the element count per aggregate using a matrix plot for set-pair intersections, and histograms for set lists, set-degrees and other attributes. Its non-overlapping visual design is scalable to numerous and large sets. AggreSet supports selection, filtering, and comparison as core exploratory tasks. It allows analysis of set relations inluding subsets, disjoint sets and set intersection strength, and also features perceptual set ordering for detecting patterns in set matrices. Its interaction is designed for rich and rapid data exploration. We demonstrate results on a wide range of datasets from different domains with varying characteristics, and report on expert reviews and a case study using student enrollment and degree data with assistant deans at a major public university.",
                "ieee_keywords": [
                    "Aggregates",
                    "Motion pictures",
                    "Data visualization",
                    "Visualization",
                    "Scalability",
                    "Filtering",
                    "Chapters"
                ],
                "author_keywords": [
                    "Multi-valued attributes",
                    "sets",
                    "visualization",
                    "set visualization",
                    "data exploration",
                    "interaction",
                    "design",
                    "scalability",
                    "Multi-valued attributes",
                    "sets",
                    "visualization",
                    "set visualization",
                    "data exploration",
                    "interaction",
                    "design",
                    "scalability"
                ]
            },
            {
                "title": "TopicLens: Efficient Multi-Level Visual Topic Exploration of Large-Scale Document Collections",
                "link": "https://ieeexplore.ieee.org/document/7539597/",
                "date_of_publication": null,
                "doi": "10.1109/TVCG.2016.2598445",
                "citations": "47",
                "abstract": "Topic modeling, which reveals underlying topics of a document corpus, has been actively adopted in visual analytics for large-scale document collections. However, due to its significant processing time and non-interactive nature, topic modeling has so far not been tightly integrated into a visual analytics workflow. Instead, most such systems are limited to utilizing a fixed, initial set of topics. Motivated by this gap in the literature, we propose a novel interaction technique called TopicLens that allows a user to dynamically explore data through a lens interface where topic modeling and the corresponding 2D embedding are efficiently computed on the fly. To support this interaction in real time while maintaining view consistency, we propose a novel efficient topic modeling method and a semi-supervised 2D embedding algorithm. Our work is based on improving state-of-the-art methods such as nonnegative matrix factorization and t-distributed stochastic neighbor embedding. Furthermore, we have built a web-based visual analytics system integrated with TopicLens. We use this system to measure the performance and the visualization quality of our proposed methods. We provide several scenarios showcasing the capability of TopicLens using real-world datasets.",
                "ieee_keywords": [
                    "Lenses",
                    "Computational modeling",
                    "Analytical models",
                    "Visual analytics",
                    "Two dimensional displays",
                    "Real-time systems"
                ],
                "author_keywords": [
                    "topic modeling",
                    "nonnegative matrix factorization",
                    "t-distributed stochastic neighbor embedding",
                    "magic lens",
                    "text analytics",
                    "MeSH Terms",
                    "Computer Graphics",
                    "Image Processing, Computer-Assisted",
                    "Models, Statistical",
                    "Stochastic Processes"
                ]
            },
            {
                "title": "ParallelSpaces: Simultaneous Exploration of Feature and Data for Hypothesis Generation",
                "link": "https://ieeexplore.ieee.org/document/7427361/",
                "date_of_publication": "10 March 2016",
                "doi": "10.1109/HICSS.2016.182",
                "citations": "82",
                "abstract": "We present ParallelSpaces, a novel method to explore bipartite datasets in both feature and data dimensions. This dyadic data is displayed as weighted bipartite graphs using scatterplots in two separated visual spaces, where each entity is positioned according to multi-dimensional properties of each entity or similarity in preferences. Selecting or navigating in one space is reflected in the other space, so that organic visual patterns can be formed to facilitate the characterization of underlying groupings. To aid visual pattern recognition we also overlay a contour plot based on kernel density estimation. We have implemented two instantiations of ParallelSpaces for (a) movie preferences, and (b) business reviews as Web-based visualizations. To validate the method, we performed a qualitative user study involving eleven participants using these Web-based tools to explore data and collect deep insights.",
                "ieee_keywords": [
                    "Motion pictures",
                    "Visualization",
                    "Data visualization",
                    "Business",
                    "Bipartite graph",
                    "Big data",
                    "Data models"
                ],
                "author_keywords": [
                    "Multimodal graphs",
                    "multivariate graphs",
                    "social network analysis",
                    "kernel density estimation"
                ]
            },
            {
                "title": "Vistrates: A Component Model for Ubiquitous Analytics",
                "link": "https://ieeexplore.ieee.org/document/8440803/",
                "date_of_publication": null,
                "doi": "10.1109/TVCG.2018.2865144",
                "citations": "21",
                "abstract": "Visualization tools are often specialized for specific tasks, which turns the user's analytical workflow into a fragmented process performed across many tools. In this paper, we present a component model design for data visualization to promote modular designs of visualization tools that enhance their analytical scope. Rather than fragmenting tasks across tools, the component model supports unification, where components—the building blocks of this model—can be assembled to support a wide range of tasks. Furthermore, the model also provides additional key properties, such as support for collaboration, sharing across multiple devices, and adaptive usage depending on expertise, from creating visualizations using dropdown menus, through instantiating components, to actually modifying components or creating entirely new ones from scratch using JavaScript or Python source code. To realize our model, we introduce V istrates , a literate computing platform for developing, assembling, and sharing visualization components. From a visualization perspective, Vistrates features cross-cutting components for visual representations, interaction, collaboration, and device responsiveness maintained in a component repository. From a development perspective, Vistrates offers a collaborative programming environment where novices and experts alike can compose component pipelines for specific analytical activities. Finally, we present several Vistrates use cases that span the full range of the classic “anytime” and “anywhere” motto for ubiquitous analysis: from mobile and on-the-go usage, through office settings, to collaborative smart environments covering a variety of tasks and devices.",
                "ieee_keywords": [
                    "Data visualization",
                    "Tools",
                    "Collaboration",
                    "Media",
                    "Substrates",
                    "Task analysis",
                    "Data analysis"
                ],
                "author_keywords": [
                    "Components",
                    "literate computing",
                    "development",
                    "exploration",
                    "dissemination",
                    "collaboration",
                    "heterogeneous devices"
                ]
            },
            {
                "title": "ConceptVector: Text Visual Analytics via Interactive Lexicon Building Using Word Embedding",
                "link": "https://ieeexplore.ieee.org/document/8023823/",
                "date_of_publication": null,
                "doi": "10.1109/TVCG.2017.2744478",
                "citations": "46",
                "abstract": "Central to many text analysis methods is the notion of a concept: a set of semantically related keywords characterizing a specific object, phenomenon, or theme. Advances in word embedding allow building a concept from a small set of seed terms. However, naive application of such techniques may result in false positive errors because of the polysemy of natural language. To mitigate this problem, we present a visual analytics system called ConceptVector that guides a user in building such concepts and then using them to analyze documents. Document-analysis case studies with real-world datasets demonstrate the fine-grained analysis provided by ConceptVector. To support the elaborate modeling of concepts, we introduce a bipolar concept model and support for specifying irrelevant words. We validate the interactive lexicon building interface by a user study and expert reviews. Quantitative evaluation shows that the bipolar lexicon generated with our methods is comparable to human-generated ones.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Observations and Reflections on Visualization Literacy in Elementary School",
                "link": "https://ieeexplore.ieee.org/document/8370203/",
                "date_of_publication": null,
                "doi": "10.1109/MCG.2018.032421650",
                "citations": "20",
                "abstract": "In this article, we share our reflections on visualization literacy and how it might be better developed in early education. We base this on lessons we learned while studying how teachers instruct, and how students acquire basic visualization principles and skills in elementary school. We use these findings to propose directions for future research on visualization literacy.",
                "ieee_keywords": [
                    "Data visualization",
                    "Visualization",
                    "Data mining",
                    "Education"
                ],
                "author_keywords": [
                    "computer graphics",
                    "visualization",
                    "literacy",
                    "data",
                    "infovis",
                    "MeSH Terms",
                    "Child",
                    "Computer Graphics",
                    "Data Visualization",
                    "Humans",
                    "Literacy",
                    "Schools",
                    "Students"
                ]
            }
        ]
    },
    {
        "name": "Tianyi Zhou",
        "publications": [
            {
                "title": "Learning to Collaborate in Decentralized Learning of Personalized Models",
                "link": "https://ieeexplore.ieee.org/document/9880456/",
                "date_of_publication": "27 September 2022",
                "doi": "10.1109/CVPR52688.2022.00954",
                "citations": "1",
                "abstract": "Learning personalized models for user-customized computer-vision tasks is challenging due to the limited private-data and computation available on each edge device. Decentralized learning (DL) can exploit the images distributed over devices on a network topology to train a global model but is not designed to train personalized models for different tasks or optimize the topology. Moreover, the mixing weights used to aggregate neighbors' gradient messages in DL can be suboptimal for personalization since they are not adaptive to different nodes/tasks and learning stages. In this paper, we dynamically update the mixing-weights to improve the personalized model for each node's task and meanwhile learn a sparse topology to reduce communication costs. Our first approach, “learning to collaborate (L2C) ”, directly optimizes the mixing weights to minimize the local validation loss per node for a predefined set of nodes/tasks. In order to produce mixing weights for new nodes or tasks, we further develop “meta-L2C‘, which learns an attention mechanism to automatically assign mixing weights by comparing two nodes' model updates. We evaluate both methods on diverse benchmarks and experimental settings for image classification. Thorough comparisons to both classical and recent methods for IID/non-IID decentralized and federated learning demonstrate our method's advantages in identifying collaborators among nodes, learning sparse topology, and producing better personalized models with low communication and computational cost.",
                "ieee_keywords": [
                    "Training",
                    "Adaptation models",
                    "Costs",
                    "Network topology",
                    "Computational modeling",
                    "Aggregates",
                    "Image edge detection"
                ],
                "author_keywords": [
                    "Self-& semi-& meta- Machine learning"
                ]
            }
        ]
    },
    {
        "name": "Yun Raymond Fu",
        "publications": [
            {
                "title": "Supplementation with glucose in PZM-3 medium improve the in vitro development of porcine transgenic cloned embryos",
                "link": "https://ieeexplore.ieee.org/document/6132103/",
                "date_of_publication": "16 January 2012",
                "doi": "10.1109/ITiME.2011.6132103",
                "citations": "54",
                "abstract": "The efficiency of producing porcine transgenic cloned embryos is still low due to lower developmental competence compared to in vivo-fertilized/cultured embryos. This phenomenon may correlates with poor culture condition. Energy substrate in culture medium play an important role in optimizing culture condition. This study pertain to investigate the effect of replacing pyruvate and lactate with glucose in PZM-3 medium at the rest part culture period on porcine embryonic development after SCNT. Porcine adult fibroblasts cells were transfected with pEGFP-C1 vector. Then transfected cells were used as donor cells for producing re-constructed embryos. Results have shown that supplement of glucose up to 5mM concentration in PZM-3 at the rest part of culture period improved the development of transgenic cloned embryos. Moreover, supplement of glucose(5mM) as energy substrate in PZM-3 at 48h of culture was optimal time. However, with regard to the percentage of EGFP-positive blastocyst, there was no significant difference between treatments. In conclusion, replacing pyruvate and lactate with glucose in PZM-3 medium at the rest part of culture period was beneficial to the development of transgenic cloned embryos and had no effect on transgene expression.",
                "ieee_keywords": [
                    "Embryo",
                    "Sugar",
                    "Substrates",
                    "In vitro",
                    "Cells (biology)",
                    "Biochemistry"
                ],
                "author_keywords": [
                    "Energy substrate",
                    "EGFP",
                    "Somatic cell nuclear transfer",
                    "Pyruvate",
                    "Lactate",
                    "Glucose"
                ]
            },
            {
                "title": "EV-Action: Electromyography-Vision Multi-Modal Action Dataset",
                "link": "https://ieeexplore.ieee.org/document/9320160/",
                "date_of_publication": "18 January 2021",
                "doi": "10.1109/FG47880.2020.00018",
                "citations": "9",
                "abstract": "Multi-modal human action analysis is a critical and attractive research topic. However, the majority of the existing datasets only provide visual modalities (i.e., RGB, depth and skeleton). To make up this, we introduce a new, largescale EV-Action dataset in this work, which consists of RGB, depth, electromyography (EMG), and two skeleton modalities. Compared with the conventional datasets, EV-Action dataset has two major improvements: (1) we deploy a motion capturing system to obtain high quality skeleton modality, which provides more comprehensive motion information including skeleton, trajectory, acceleration with higher accuracy, sampling frequency, and more skeleton markers. (2) we introduce an EMG modality which is usually used as an effective indicator in the biomechanics area, also it has yet to be well explored in motion related research. To the best of our knowledge, this is the first action dataset with EMG modality. The details of EVAction dataset are clarified, meanwhile, a simple yet effective framework for EMG-based action recognition is proposed. Moreover, state-of-the-art baselines are applied to evaluate the effectiveness of all the modalities. The obtained result clearly shows the validity of EMG modality in human action analysis tasks. We hope this dataset can make significant contributions to human motion analysis, computer vision, machine learning, biomechanics, and other interdisciplinary fields.",
                "ieee_keywords": [
                    "Electromyography",
                    "Sensors",
                    "Skeleton",
                    "Visualization",
                    "Muscles",
                    "Cameras",
                    "Trajectory"
                ],
                "author_keywords": [
                    "Action recognition",
                    "Multi view",
                    "Action Dataset",
                    "Electromyography",
                    "EMG"
                ]
            },
            {
                "title": "Decomposed contour prior for shape recognition",
                "link": "https://ieeexplore.ieee.org/document/6460247/",
                "date_of_publication": "14 February 2013",
                "doi": null,
                "citations": "86",
                "abstract": "In this paper, we address the problem of representing objects using contours for the purpose of recognition. We propose a novel segmentation method for integrating a new contour matching energy into level set based segmentation schemes. The contour matching energy is represented by major components of Elliptic Fourier shape descriptors and serves as a shape prior to guide the curve evolution. The contours in training dataset serve as templates and are utilized to infer the category of an unknown image based on matching. Our method is evaluated on the UCF sports dataset and Caltech 101 dataset. Experiments show that our method achieves promising recognition accuracy and is robust to noisy low-level features and background clutter.",
                "ieee_keywords": [
                    "Shape",
                    "Image segmentation",
                    "Level set",
                    "Noise measurement",
                    "Accuracy",
                    "Computer vision",
                    "Pattern recognition"
                ],
                "author_keywords": []
            },
            {
                "title": "Visual Semantic Reasoning for Image-Text Matching",
                "link": "https://ieeexplore.ieee.org/document/9010696/",
                "date_of_publication": "27 February 2020",
                "doi": "10.1109/ICCV.2019.00475",
                "citations": "224",
                "abstract": "Image-text matching has been a hot research topic bridging the vision and language areas. It remains challenging because the current representation of image usually lacks global semantic concepts as in its corresponding text caption. To address this issue, we propose a simple and interpretable reasoning model to generate visual representation that captures key objects and semantic concepts of a scene. Specifically, we first build up connections between image regions and perform reasoning with Graph Convolutional Networks to generate features with semantic relationships. Then, we propose to use the gate and memory mechanism to perform global semantic reasoning on these relationship-enhanced features, select the discriminative information and gradually generate the representation for the whole scene. Experiments validate that our method achieves a new state-of-the-art for the image-text matching on MS-COCO and Flickr30K datasets. It outperforms the current best method by 6.8% relatively for image retrieval and 4.8% relatively for caption retrieval on MS-COCO (Recall@1 using 1K test set). On Flickr30K, our model improves image retrieval by 12.6% relatively and caption retrieval by 5.8% relatively (Recall@1).",
                "ieee_keywords": [
                    "Semantics",
                    "Cognition",
                    "Visualization",
                    "Image representation",
                    "Feature extraction",
                    "Correlation",
                    "Image edge detection"
                ],
                "author_keywords": []
            },
            {
                "title": "Attention Bridging Network for Knowledge Transfer",
                "link": "https://ieeexplore.ieee.org/document/9008571/",
                "date_of_publication": "27 February 2020",
                "doi": "10.1109/ICCV.2019.00530",
                "citations": "11",
                "abstract": "The attention of a deep neural network obtained by back-propagating gradients can effectively explain the decision of the network. They can further be used to explicitly access to the network response to a specific pattern. Considering objects of the same category but from different domains share similar visual patterns, we propose to treat the network attention as a bridge to connect objects across domains. In this paper, we use knowledge from the source domain to guide the network's response to categories shared with the target domain. With weights sharing and domain adversary training, this knowledge can be successfully transferred by regularizing the network's response to the same category in the target domain. Specifically, we transfer the foreground prior from a simple single-label dataset to another complex multi-label dataset, leading to improvement of attention maps. Experiments about the weakly-supervised semantic segmentation task show the effectiveness of our method. Besides, we further explore and validate that the proposed method is able to improve the generalization ability of a classification network in domain adaptation and domain generalization settings.",
                "ieee_keywords": [
                    "Visualization",
                    "Task analysis",
                    "Knowledge engineering",
                    "Semantics",
                    "Training",
                    "Bridges",
                    "Knowledge transfer"
                ],
                "author_keywords": []
            },
            {
                "title": "Making Reconstruction-based Method Great Again for Video Anomaly Detection",
                "link": "https://ieeexplore.ieee.org/document/10027694/",
                "date_of_publication": "01 February 2023",
                "doi": "10.1109/ICDM54844.2022.00157",
                "citations": "1",
                "abstract": "Anomaly detection in videos is a significant yet challenging problem. Previous approaches based on deep neural networks employ either reconstruction-based or prediction-based approaches. Nevertheless, existing reconstruction-based methods 1) rely on old-fashioned convolutional autoencoders and are poor at modeling temporal dependency; 2) are prone to overfit the training samples, leading to indistinguishable reconstruction errors of normal and abnormal frames during the inference phase. To address such issues, firstly, we get inspiration from transformer and propose Spatio-Temporal Auto-Trans-Encoder, dubbed as STATE, as a new autoencoder model for enhanced consecutive frame reconstruction. Our STATE is equipped with a specifically designed learnable convolutional attention module for efficient temporal learning and reasoning. Secondly, we put forward a novel reconstruction-based input perturbation technique during testing to further differentiate anomalous frames. With the same perturbation magnitude, the testing reconstruction error of the normal frames lowers more than that of the abnormal frames, which contributes to mitigating the overfitting problem of reconstruction. Owing to the high relevance of the frame abnormality and the objects in the frame, we conduct object-level reconstruction using both the raw frame and the corresponding optical flow patches. Finally, the anomaly score is designed based on the combination of the raw and motion reconstruction errors using perturbed inputs. Extensive experiments on benchmark video anomaly detection datasets demonstrate that our approach outperforms previous reconstruction-based methods by a notable margin, and achieves state-of-the-art anomaly detection performance consistently. The code is available at https://github.com/wyzjack/MRMGA4VAD.",
                "ieee_keywords": [
                    "Convolutional codes",
                    "Training",
                    "Deep learning",
                    "Perturbation methods",
                    "Neural networks",
                    "Transformers",
                    "Cognition"
                ],
                "author_keywords": [
                    "Anomaly Detection",
                    "Transformer",
                    "Perturbation"
                ]
            },
            {
                "title": "Accurate and Fast Image Denoising via Attention Guided Scaling",
                "link": "https://ieeexplore.ieee.org/document/9479770/",
                "date_of_publication": null,
                "doi": "10.1109/TIP.2021.3093396",
                "citations": "16",
                "abstract": "Image denoising is a classical topic yet still a challenging problem, especially for reducing noise from the texture information. Feature scaling (e.g., downscale and upscale) is a widely practice in image denoising to enlarge receptive field size and save resources. However, such a common operation would lose some visual informative details. To address those problems, we propose fast and accurate image denoising via attention guided scaling (AGS). We find that the main informative feature channel and visual primitives during the scaling should keep similar. We then propose to extract the global channel-wise attention to maintain main channel information. Moreover, we propose to collect global descriptors by considering the entire spatial feature. And we then distribute the global descriptors to local positions of the scaled feature, based on their specific needs. We further introduce AGS for adversarial training, resulting in a more powerful discriminator. Extensive experiments show the effectiveness of our proposed method, where we clearly surpass all the state-of-the-art methods on most popular synthetic and real-world denoising benchmarks quantitatively and visually. We further show that our network contributes to other high-level vision applications and improves their performances significantly.",
                "ieee_keywords": [
                    "Image denoising",
                    "Noise measurement",
                    "Training",
                    "Noise reduction",
                    "Generative adversarial networks",
                    "Visualization",
                    "Task analysis"
                ],
                "author_keywords": [
                    "Image denoising",
                    "attention guided scaling",
                    "feature collection and distribution",
                    "object recognition",
                    "semantic segmentation"
                ]
            },
            {
                "title": "Discriminative metric: Schatten norm vs. vector norm",
                "link": "https://ieeexplore.ieee.org/document/6460356/",
                "date_of_publication": "14 February 2013",
                "doi": null,
                "citations": "2",
                "abstract": "The notion of metric is fundamental for the study of pattern recognition and vector 2-norm ||·|| 2 is one of the most widely used metric, i.e., Euclidean distance. However, there is often the case that the inputs are matrices, e.g., 2D images in face recognition. Since a matrix can take more structure information than its vectorization, it is highly preferable to adopt the matrix representation of the original image rather than a simple vector. In this paper, we first propose a class of discriminative metrics for matrices, i.e., Schatten p-norm, by which we can better explain that with Euclidean metric, why the differences among facial images due to impact factors, e.g., illuminations, are more significant than differences due to identity variations. Second, we propose a novel Principal Component Analysis method based on Schatten 1-norm which can be easily extended to other subspace learning methods. Extensive experiments on Yale B, CMU PIE, ORL and AR databases prove the effectiveness of our method.",
                "ieee_keywords": [
                    "Principal component analysis",
                    "Measurement",
                    "Lighting",
                    "Vectors",
                    "Databases",
                    "Face recognition",
                    "Face"
                ],
                "author_keywords": []
            },
            {
                "title": "Low-Shot Face Recognition with Hybrid Classifiers",
                "link": "https://ieeexplore.ieee.org/document/8265438/",
                "date_of_publication": "22 January 2018",
                "doi": "10.1109/ICCVW.2017.228",
                "citations": "19",
                "abstract": "In this paper, we present our solution to the MS-Celeb-1M Low-shot Face Recognition Challenge. This challenge aims to recognize 21,000 celebrities, in which 20,000 celebrities (Base Set) come with 50-100 images per person for training. But only one training image is provided for each person in the rest 1,000 celebrities (Novel Set). Given the dispersion in the number of training samples between Base Set and Novel Set, it is hard to build a single classifier that works well for both sets. To solve this problem, we propose a framework with multiple classifiers. This decomposes a single classifier for all data into multiple classifiers that each works well for a part of data. To be more specific, a Deep Convolution Neural Network (CNN) is utilized for Base Set and a Nearest Neighbor (NN) model is applied to Novel Set. The final prediction is based on a fusion of CNN results and NN results. Extensive experiments on MS-Celeb-1M Low-shot face dataset demonstrate the superiority of the proposed method. Our solution achieves 92.64% Coverage @Precision=0.99 in Novel Set while maintaining 99.58% top-1 accuracy in Base Set. This result wins the challenge in the track of without external data. Moreover, it is worth to note our result even surpasses some models using external data and can achieve the third place if compared with all participants.",
                "ieee_keywords": [
                    "Training",
                    "Data models",
                    "Training data",
                    "Face recognition",
                    "Face"
                ],
                "author_keywords": []
            },
            {
                "title": "Frame Flexible Network",
                "link": "https://ieeexplore.ieee.org/document/10204086/",
                "date_of_publication": "22 August 2023",
                "doi": "10.1109/CVPR52729.2023.01012",
                "citations": "5",
                "abstract": "Existing video recognition algorithms always conduct different training pipelines for inputs with different frame numbers, which requires repetitive training operations and multiplying storage costs. If we evaluate the model using other frames which are not used in training, we observe the performance will drop significantly (see Fig. 1), which is summarized as Temporal Frequency Deviation phenomenon. To fix this issue, we propose a general frame-work, named Frame Flexible Network (FFN), which not only enables the model to be evaluated at different frames to adjust its computation, but also reduces the memory costs of storing multiple models significantly. Concretely, FFN integrates several sets of training sequences, involves Multi-Frequency Alignment (MFAL) to learn temporal frequency invariant representations, and leverages Multi-Frequency Adaptation (MFAD) to further strengthen the representation abilities. Comprehensive empirical validations using various architectures and popular benchmarks solidly demonstrate the effectiveness and generalization of FFN (e.g., 7.08/5.15/2.17% performance gain at Frame 4/8/16 on Something-Something V1 dataset over Uniformer). Code is available at https://github.com/BeSpontaneous/FFN.",
                "ieee_keywords": [
                    "Training",
                    "Adaptation models",
                    "Computer vision",
                    "Costs",
                    "Codes",
                    "Computational modeling",
                    "Pipelines"
                ],
                "author_keywords": [
                    "Video: Action and event understanding"
                ]
            }
        ]
    },
    {
        "name": "Yanzhi Wang",
        "publications": [
            {
                "title": "Work in Progress: Mobile or FPGA? A Comprehensive Evaluation on Energy Efficiency and a Unified Optimization Framework",
                "link": "https://ieeexplore.ieee.org/document/9470459/",
                "date_of_publication": "07 July 2021",
                "doi": "10.1109/RTAS52030.2021.00060",
                "citations": "1",
                "abstract": "Efficient deployment of Deep Neural Networks (DNNs) on edge devices (i.e., FPGAs and mobile platforms) is very challenging, especially under a recent witness of the increasing DNN model size and complexity. Although various optimization approaches have been proven to be effective in many DNNs on edge devices, most state-of-the-art work focuses on ad-hoc optimizations, and there lacks a thorough study to comprehensively reveal the potentials and constraints of different edge devices when considering different optimizations. In this paper, we qualitatively and quantitatively compare the energyefficiency of FPGA-based and mobile-based DNN executions, and provide detailed analysis.",
                "ieee_keywords": [
                    "Neural networks",
                    "Real-time systems",
                    "Energy efficiency",
                    "Ad hoc networks",
                    "Complexity theory",
                    "Optimization",
                    "Field programmable gate arrays"
                ],
                "author_keywords": [
                    "DNN acceleration",
                    "FPGA",
                    "mobile device",
                    "quantization",
                    "compiler"
                ]
            },
            {
                "title": "Improving DNN Fault Tolerance using Weight Pruning and Differential Crossbar Mapping for ReRAM-based Edge AI",
                "link": "https://ieeexplore.ieee.org/document/9424332/",
                "date_of_publication": "10 May 2021",
                "doi": "10.1109/ISQED51717.2021.9424332",
                "citations": "17",
                "abstract": "Recent research demonstrated the promise of using resistive random access memory (ReRAM) as an emerging technology to perform inherently parallel analog domain in-situ matrix-vector multiplication-the intensive and key computation in deep neural networks (DNNs). However, hardware failure, such as stuck-at-fault defects, is one of the main concerns that impedes the ReRAM devices to be a feasible solution for real implementations. The existing solutions to address this issue usually require an optimization to be conducted for each individual device, which is impractical for mass-produced products (e.g., IoT devices). In this paper, we rethink the value of weight pruning in ReRAM-based DNN design from the perspective of model fault tolerance. And a differential mapping scheme is proposed to improve the fault tolerance under a high stuck-on fault rate. Our method can tolerate almost an order of magnitude higher failure rate than the traditional two-column method in representative DNN tasks. More importantly, our method does not require extra hardware cost compared to the traditional two-column mapping scheme. The improvement is universal and does not require the optimization process for each individual device.",
                "ieee_keywords": [
                    "Performance evaluation",
                    "Fault tolerance",
                    "Fault tolerant systems",
                    "Resistive RAM",
                    "Neural networks",
                    "Hardware",
                    "Task analysis"
                ],
                "author_keywords": []
            },
            {
                "title": "Achieving on-Mobile Real-Time Super-Resolution with Neural Architecture and Pruning Search",
                "link": "https://ieeexplore.ieee.org/document/9710156/",
                "date_of_publication": "28 February 2022",
                "doi": "10.1109/ICCV48922.2021.00478",
                "citations": "12",
                "abstract": "Though recent years have witnessed remarkable progress in single image super-resolution (SISR) tasks with the prosperous development of deep neural networks (DNNs), the deep learning methods are confronted with the computation and memory consumption issues in practice, especially for resource-limited platforms such as mobile devices. To overcome the challenge and facilitate the real-time deployment of SISR tasks on mobile, we combine neural architecture search with pruning search and propose an automatic search framework that derives sparse super-resolution (SR) models with high image quality while satisfying the real-time inference requirement. To decrease the search cost, we leverage the weight sharing strategy by introducing a supernet and decouple the search problem into three stages, including supernet construction, compiler-aware architecture and pruning search, and compiler-aware pruning ratio search. With the proposed framework, we are the first to achieve real-time SR inference (with only tens of milliseconds per frame) for implementing 720p resolution with competitive image quality (in terms of PSNR and SSIM) on mobile platforms (Samsung Galaxy S20).",
                "ieee_keywords": [
                    "Image quality",
                    "Deep learning",
                    "Computational modeling",
                    "Superresolution",
                    "Neural networks",
                    "Memory management",
                    "Search problems"
                ],
                "author_keywords": [
                    "Low-level and physics-based vision",
                    "Efficient training and inference methods",
                    "Vision applications and systems"
                ]
            },
            {
                "title": "ResNet Can Be Pruned 60×: Introducing Network Purification and Unused Path Removal (P-RM) after Weight Pruning",
                "link": "https://ieeexplore.ieee.org/document/9073635/",
                "date_of_publication": "30 April 2020",
                "doi": "10.1109/NANOARCH47378.2019.181304",
                "citations": "14",
                "abstract": "The state-of-art DNN structures involve high computation and great demand for memory storage which pose intensive challenge on DNN framework resources. To mitigate the challenges, weight pruning techniques has been studied. However, high accuracy solution for extreme structured pruning that combines different types of structured sparsity still waiting for unraveling due to the extremely reduced weights in DNN networks. In this paper, we propose a DNN framework which combines two different types of structured weight pruning (filter and column prune) by incorporating alternating direction method of multipliers (ADMM) algorithm for better prune performance. We are the first to find nonoptimality of ADMM process and unused weights in a structured pruned model, and further design an optimization framework which contains the first proposed Network Purification and Unused Path Removal algorithms which are dedicated to post-processing an structured pruned model after ADMM steps. Some high lights shows we achieve 232× compression on LeNet-5, 60× compression on ResNet-18 CIFAR-10 and over 5× compression on AlexNet. We share our models at anonymous link http://bit.ly/2VJ5ktv.",
                "ieee_keywords": [
                    "Convex functions",
                    "Optimization",
                    "Computational modeling",
                    "Nanoscale devices",
                    "Acceleration",
                    "Linear programming",
                    "Standards"
                ],
                "author_keywords": []
            },
            {
                "title": "Radio Frequency Fingerprinting on the Edge",
                "link": "https://ieeexplore.ieee.org/document/9372779/",
                "date_of_publication": null,
                "doi": "10.1109/TMC.2021.3064466",
                "citations": "10",
                "abstract": "Deep learning methods have been very successful at radio frequency fingerprinting tasks, predicting the identity of transmitting devices with high accuracy. We study radio frequency fingerprinting deployments at resource-constrained edge devices. We use structured pruning to jointly train and sparsify neural networks tailored to edge hardware implementations. We compress convolutional layers by a $27.2\\times$ factor while incurring a negligible prediction accuracy decrease (less than 1 percent). We demonstrate the efficacy of our approach over multiple edge hardware platforms, including a Samsung Gallaxy S10 phone and a Xilinx-ZCU104 FPGA. Our method yields significant inference speedups, $11.5\\times$ on the FPGA and $3\\times$ on the smartphone, as well as high efficiency: the FPGA processing time is $17\\times$ smaller than in a V100 GPU. To the best of our knowledge, we are the first to explore the possibility of compressing networks for radio frequency fingerprinting; as such, our experiments can be seen as a means of characterizing the informational capacity associated with this specific learning task.",
                "ieee_keywords": [
                    "Radio frequency",
                    "Hardware",
                    "Computer architecture",
                    "Field programmable gate arrays",
                    "Neural networks",
                    "Training",
                    "Graphics processing units"
                ],
                "author_keywords": [
                    "Radio frequency fingerprinting",
                    "edge computing",
                    "model pruning"
                ]
            },
            {
                "title": "Brief Industry Paper: Towards Real-Time 3D Object Detection for Autonomous Vehicles with Pruning Search",
                "link": "https://ieeexplore.ieee.org/document/9470446/",
                "date_of_publication": "07 July 2021",
                "doi": "10.1109/RTAS52030.2021.00043",
                "citations": "1",
                "abstract": "In autonomous driving, 3D object detection is es-sential as it provides basic knowledge about the environment. However, as deep learning based 3D detection methods are usually computation intensive, it is challenging to support realtime 3D object detection on edge-computing devices in selfdriving cars with limited computation and memory resources. To facilitate this, we propose a compiler-aware pruning search framework, to achieve real-time inference of 3D object detection on the resource-limited mobile devices. Specifically, a generator is applied to sample better pruning proposals in the search space based on current proposals with their performance, and an evaluator is adopted to evaluate the sampled pruning proposal performance. To accelerate the search, the evaluator employs Bayesian optimization with an ensemble of neural predictors. We demonstrate in experiments that for the first time, the pruning search framework can achieve real-time 3D object detection on mobile (Samsung Galaxy S20 phone) with state-of-the-art detection performance.",
                "ieee_keywords": [
                    "Performance evaluation",
                    "Solid modeling",
                    "Three-dimensional displays",
                    "Object detection",
                    "Search problems",
                    "Real-time systems",
                    "Mobile handsets"
                ],
                "author_keywords": [
                    "3D object detection",
                    "real-time",
                    "point cloud"
                ]
            },
            {
                "title": "All-in-One: A Highly Representative DNN Pruning Framework for Edge Devices with Dynamic Power Management",
                "link": "https://ieeexplore.ieee.org/document/10069262/",
                "date_of_publication": "22 March 2023",
                "doi": null,
                "citations": "66",
                "abstract": "During the deployment of deep neural networks (DNNs) on edge devices, many research efforts are devoted to the limited hardware resource. However, little attention is paid to the influence of dynamic power management. As edge devices typically only have a budget of energy with batteries (rather than almost unlimited energy support on servers or workstations), their dynamic power management often changes the execution frequency as in the widely-used dynamic voltage and frequency scaling (DVFS) technique. This leads to highly unstable inference speed performance, especially for computation-intensive DNN models, which can harm user experience and waste hardware resources. We firstly identify this problem and then propose All-in-One, a highly representative pruning framework to work with dynamic power management using DVFS. The framework can use only one set of model weights and soft masks (together with other auxiliary parameters of negligible storage) to represent multiple models of various pruning ratios. By re-configuring the model to the corresponding pruning ratio for a specific execution frequency (and voltage), we are able to achieve stable inference speed, i.e., keeping the difference in speed performance under various execution frequencies as small as possible. Our experiments demonstrate that our method not only achieves high accuracy for multiple models of different pruning ratios, but also reduces their variance of inference latency for various frequencies, with minimal memory consumption of only one model and one soft mask.",
                "ieee_keywords": [
                    "Performance evaluation",
                    "Power system management",
                    "Computational modeling",
                    "Neural networks",
                    "Voltage",
                    "Switches",
                    "Hardware"
                ],
                "author_keywords": []
            },
            {
                "title": "Neural Network-Based OFDM Receiver for Resource Constrained IoT Devices",
                "link": "https://ieeexplore.ieee.org/document/9945833/",
                "date_of_publication": null,
                "doi": "10.1109/IOTM.001.2200051",
                "citations": "1",
                "abstract": "Orthogonal Frequency Division Multiplexing (OFDM)-based waveforms are used for communication links in many current and emerging Internet of Things (IoT) applications, including the latest WiFi standards. For such OFDM-based transceivers, many core physical layer functions related to channel estimation, demapping, and decoding are implemented for specific choices of channel types and modulation schemes, among others. To decouple hard-wired choices from the receiver chain and thereby enhance the flexibility of IoT deployment in many novel scenarios without changing the underlying hardware, we explore a novel, modular Machine Learning (ML)-based receiver chain design. Here, ML blocks replace the individual processing blocks of an OFDM receiver, and we specifically describe this swapping for the legacy channel estimation, symbol demapping, and decoding blocks with Neural Networks (NNs). A unique aspect of this modular design is providing flexible allocation of processing functions to the legacy or ML blocks, allowing them to interchangeably coexist. Furthermore, we study the implementation cost-benefits of the proposed NNs in resource-constrained IoT devices through pruning and quantization, as well as emulation of these compressed NNs within Field Programmable Gate Arrays (FPGAs). Our evaluations demonstrate that the proposed modular NN-based receiver improves bit error rate of the traditional non-ML receiver by averagely 61 percent and 10 percent for the simulated and over-the-air datasets, respectively. We further show complexity-performance tradeoffs by presenting computational complexity comparisons between the traditional algorithms and the proposed compressed NNs.",
                "ieee_keywords": [
                    "Quantization (signal)",
                    "OFDM",
                    "Bit error rate",
                    "Channel estimation",
                    "Symbols",
                    "Receivers",
                    "Artificial neural networks",
                    "Resource management"
                ],
                "author_keywords": []
            },
            {
                "title": "Fault Sneaking Attack: a Stealthy Framework for Misleading Deep Neural Networks",
                "link": "https://ieeexplore.ieee.org/document/8806949/",
                "date_of_publication": "22 August 2019",
                "doi": null,
                "citations": "502",
                "abstract": "Despite the great achievements of deep neural networks (DNNs), the vulnerability of state-of-the-art DNNs raises security concerns of DNNs in many application domains requiring high reliability. We propose the fault sneaking attack on DNNs, where the adversary aims to misclassify certain input images into any target labels by modifying the DNN parameters. We apply ADMM (alternating direction method of multipliers) for solving the optimization problem of the fault sneaking attack with two constraints: 1) the classification of the other images should be unchanged and 2) the parameter modifications should be minimized. Specifically, the first constraint requires us not only to inject designated faults (misclassifications), but also to hide the faults for stealthy or sneaking considerations by maintaining model accuracy. The second constraint requires us to minimize the parameter modifications (using ℓ 0 norm to measure the number of modifications and ℓ 2 norm to measure the magnitude of modifications). Comprehensive experimental evaluation demonstrates that the proposed framework can inject multiple sneaking faults without losing the overall test accuracy performance.",
                "ieee_keywords": [
                    "Laser beams",
                    "Security",
                    "Semiconductor lasers",
                    "Convex functions",
                    "Neural networks",
                    "Laser theory",
                    "Optimization"
                ],
                "author_keywords": [
                    "Deep neural networks",
                    "Fault injection",
                    "ADMM"
                ]
            },
            {
                "title": "When Sorting Network Meets Parallel Bitstreams: A Fault-Tolerant Parallel Ternary Neural Network Accelerator based on Stochastic Computing",
                "link": "https://ieeexplore.ieee.org/document/9116390/",
                "date_of_publication": "15 June 2020",
                "doi": "10.23919/DATE48585.2020.9116390",
                "citations": "9",
                "abstract": "Stochastic computing (SC) has been widely used in neural networks (NNs) due to its simple hardware cost and high fault tolerance. Conventionally, SC-based NN accelerators adopt a hybrid stochastic-binary format, using an accumulative parallel counter to convert bitstreams into a binary number. This method, however, sacrifices the fault tolerance and causes a high hardware cost. In order to fully exploit the superior fault tolerance of SC, taking a ternary neural network (TNN) as an example, we propose a parallel SC-based NN accelerator purely using bitstream computation. We apply a bitonic sorting network for simultaneously implementing the accumulation and activation function with parallel bitstreams. The proposed design not only has high fault tolerance, but also achieves at least 2.8× energy efficiency improvement over the binary computing counterpart.",
                "ieee_keywords": [
                    "Artificial neural networks",
                    "Sorting",
                    "Fault tolerance",
                    "Fault tolerant systems",
                    "Encoding",
                    "Hardware",
                    "Random access memory"
                ],
                "author_keywords": [
                    "Stochastic computing",
                    "ternary neural network",
                    "bitonic sort",
                    "parallel computing"
                ]
            }
        ]
    },
    {
        "name": "Daniel Wichs",
        "publications": [
            {
                "title": "Obfuscating Compute-and-Compare Programs under LWE",
                "link": "https://ieeexplore.ieee.org/document/8104093/",
                "date_of_publication": "13 November 2017",
                "doi": "10.1109/FOCS.2017.61",
                "citations": "73",
                "abstract": "We show how to obfuscate a large and expressive class of programs, which we call compute-and-compare programs, under the learning-with-errors (LWE) assumption. Each such program CC[f, y] is parametrized by an arbitrary polynomial-time computable function f along with a target value y and we define CC[f,y](x) to output 1 if f(x) = y and 0 otherwise. In other words, the program performs an arbitrary computation f and then compares its output against a target y. Our obfuscator satisfies distributional virtual-blackbox security, which guarantees that the obfuscated program does not reveal any partial information about the function f or the target value y, as long as they are chosen from some distribution where y has sufficient pseudo-entropy given f. We also extend our result to multi-bit compute-and-compare programs MBCC[f, y, z](x) which output a message z if f(x) = y. Compute-and-compare programs are powerful enough to capture many interesting obfuscation tasks as special cases. This includes obfuscating conjunctions, and therefore we improve on the prior work of Brakerski et al. (ITCS '16) which constructed a conjunction obfuscator under a non-standard “entropic” ring-LWE assumption, while here we obfuscate a significantly broader class of programs under standard LWE. We show that our obfuscator has several interesting applications. For example, we can take any encryption scheme and publish an obfuscated plaintext equality tester that allows users to check whether a ciphertext decrypts to some target value y; as long as y has sufficient pseudo-entropy this will not harm semantic security. We can also use our obfuscator to generically upgrade attribute-based encryption to predicate encryption with one-sided attribute-hiding security, and to upgrade witness encryption to indistinguishability obfuscation which is secure for all “null circuits”. Furthermore, we show that our obfuscator gives new circular-security counterexamples for public-key bit encryption and for unb... (Show More)",
                "ieee_keywords": [
                    "Encryption",
                    "Standards",
                    "Public key",
                    "Integrated circuit modeling"
                ],
                "author_keywords": [
                    "Program Obfuscation"
                ]
            },
            {
                "title": "Laconic Function Evaluation and Applications",
                "link": "https://ieeexplore.ieee.org/document/8555164/",
                "date_of_publication": "02 December 2018",
                "doi": "10.1109/FOCS.2018.00086",
                "citations": "26",
                "abstract": "We introduce a new cryptographic primitive called laconic function evaluation (LFE). Using LFE, Alice can compress a large circuit f into a small digest. Bob can encrypt some data x under this digest in a way that enables Alice to recover f(x) without learning anything else about Bob's data. For the scheme to be laconic, we require that the size of the digest, the run-time of the encryption algorithm and the size of the ciphertext should all be small, much smaller than the circuit-size of f. We construct an LFE scheme for general circuits under the learning with errors (LWE) assumption, where the above parameters only grow polynomially with the depth but not the size of the circuit. We then use LFE to construct secure 2-party and multi-party computation (2PC, MPC) protocols with novel properties: We construct a 2-round 2PC protocol between Alice and Bob with respective inputs x A , x B in which Alice learns the output f(x A , x B ) in the second round. This is the first such protocol which is “Bob-optimized”, meaning that Alice does all the work while Bob's computation and the total communication of the protocol are smaller than the size of the circuit f or even Alice's input xA. In contrast, prior solutions based on fully homomorphic encryption are “Alice-optimized”. . We construct an MPC protocol, which allows N parties to securely evaluate a function f(x 1 , ..., x N ) over their respective inputs, where the total amount of computation performed by the parties during the protocol execution is smaller than that of evaluating the function itself! Each party has to individually pre-process the circuit f before the protocol starts and post-process the protocol transcript to recover the output after the protocol ends, and the cost of these steps is larger than the circuit size. However, this gives the first MPC where the computation performed by each party during the actual protocol execution, from the time the first protocol message is sent until the last protocol m... (Show More)",
                "ieee_keywords": [
                    "Protocols",
                    "Encryption",
                    "Iron",
                    "Databases",
                    "Public key"
                ],
                "author_keywords": [
                    "Cryptography",
                    "Lattice based cryptography",
                    "Functional encryption",
                    "Multiparty Computation"
                ]
            },
            {
                "title": "Efficient Non-Malleable Codes and Key Derivation for Poly-Size Tampering Circuits",
                "link": "https://ieeexplore.ieee.org/document/7577799/",
                "date_of_publication": null,
                "doi": "10.1109/TIT.2016.2613919",
                "citations": "8",
                "abstract": "Non-malleable codes, defined by Dziembowski, Pietrzak, and Wichs (ICS '10), provide roughly the following guarantee: if a codeword c encoding some message x is tampered to c'= f (c) such that c' ≠ c, then the tampered message x' contained in c' reveals no information about x. The nonmalleable codes have applications to immunizing cryptosystems against tampering attacks and related-key attacks. One cannot have an efficient non-malleable code that protects against all efficient tampering functions f . However, in this paper we show “the next best thing”: for any polynomial bound s given a-priori, there is an efficient non-malleable code that protects against all tampering functions f computable by a circuit of size s. More generally, for any family of tampering functions F of size |F| ≤ 2 s , there is an efficient non-malleable code that protects against all f ∈ F. The rate of our codes, defined as the ratio of message to codeword size, approaches 1. Our results are information-theoretic and our main proof technique relies on a careful probabilistic method argument using limited independence. As a result, we get an efficiently samplable family of efficient codes, such that a random member of the family is non-malleable with overwhelming probability. Alternatively, we can view the result as providing an efficient non-malleable code in the “common reference string” model. We also introduce a new notion of non-malleable key derivation, which uses randomness x to derive a secret key y = h(x) in such a way that, even if x is tampered to a different value x'= f (x), the derived key y' = h(x') does not reveal any information about y. Our results for non-malleable key derivation are analogous to those for non-malleable codes. As a useful tool in our analysis, we rely on the notion of “leakage-resilient storage” of Davi, Dziembowski, and Venturi (SCN '10), and, as a result of independent interest, we also significantly improve on the parameters of such schemes. (Show More)",
                "ieee_keywords": [
                    "Encoding",
                    "Cryptography",
                    "Probabilistic logic",
                    "Electronic mail",
                    "Resilience",
                    "Decoding"
                ],
                "author_keywords": [
                    "Non-malleable codes",
                    "key derivation",
                    "tamper-resilient cryptography"
                ]
            },
            {
                "title": "Outsourcing Private RAM Computation",
                "link": "https://ieeexplore.ieee.org/document/6979025/",
                "date_of_publication": "11 December 2014",
                "doi": "10.1109/FOCS.2014.50",
                "citations": "35",
                "abstract": "We construct the first schemes that allow a client to privately outsource arbitrary program executions to a remote server while ensuring that: (I) the client's work is small and essentially independent of the complexity of the computation being outsourced, and (II) the server's work is only proportional to the run-time of the computation on a random access machine (RAM), rather than its potentially much larger circuit size. Furthermore, our solutions are non-interactive and have the structure of reusable garbled RAM programs, addressing an open question of Lu and Ostrovsky (Eurocrypt 2013). We also construct schemes for an augmented variant of the above scenario, where the client can initially outsource a large private and persistent database to the server, and later outsource arbitrary program executions with read/write access to this database. Our solutions are built from non-reusable garbled RAM in conjunction with new types of reusable garbled circuits that are more efficient than prior solutions but only satisfy weaker security. For the basic setting without a persistent database, we can instantiate the required type of reusable garbled circuits from indistinguishability obfuscation or from functional encryption for circuits as a black-box. For the more complex setting with a persistent database, we can instantiate the required type of reusable garbled circuits using stronger notions of obfuscation. Our basic solution also requires the client to perform a one-time pre-processing step to garble a program at the cost of its RAM run-time, and we can avoid this cost using stronger notions of obfuscation. It remains an open problem to instantiate these new types of reusable garbled circuits under weaker assumptions, possibly avoiding obfuscation altogether. We show several simple extensions of our results and techniques to achieve: efficiency proportional to the input-specific RAM run-time, verifiability of outsourced RAM computation, functional encryption for RAMs,... (Show More)",
                "ieee_keywords": [
                    "Random access memory",
                    "Servers",
                    "Security",
                    "Databases",
                    "Outsourcing",
                    "Complexity theory",
                    "Protocols"
                ],
                "author_keywords": [
                    "reusable garbled circuits",
                    "reusable garbled RAM",
                    "obfuscation"
                ]
            }
        ]
    },
    {
        "name": "Xue Lin",
        "publications": [
            {
                "title": "Notice of Retraction: Application of MATLAB in Teaching Reform and Cultivation of Innovation Talents in Universities",
                "link": "https://ieeexplore.ieee.org/document/5458637/",
                "date_of_publication": "06 May 2010",
                "doi": "10.1109/ETCS.2010.133",
                "citations": "1",
                "abstract": "Retracted. Notes: Notice of Retraction: After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles. We hereby retract the content of this paper. Reasonable effort should be made to remove references to this paper. The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Work in Progress: Mobile or FPGA? A Comprehensive Evaluation on Energy Efficiency and a Unified Optimization Framework",
                "link": "https://ieeexplore.ieee.org/document/9470459/",
                "date_of_publication": "07 July 2021",
                "doi": "10.1109/RTAS52030.2021.00060",
                "citations": "1",
                "abstract": "Efficient deployment of Deep Neural Networks (DNNs) on edge devices (i.e., FPGAs and mobile platforms) is very challenging, especially under a recent witness of the increasing DNN model size and complexity. Although various optimization approaches have been proven to be effective in many DNNs on edge devices, most state-of-the-art work focuses on ad-hoc optimizations, and there lacks a thorough study to comprehensively reveal the potentials and constraints of different edge devices when considering different optimizations. In this paper, we qualitatively and quantitatively compare the energyefficiency of FPGA-based and mobile-based DNN executions, and provide detailed analysis.",
                "ieee_keywords": [
                    "Neural networks",
                    "Real-time systems",
                    "Energy efficiency",
                    "Ad hoc networks",
                    "Complexity theory",
                    "Optimization",
                    "Field programmable gate arrays"
                ],
                "author_keywords": [
                    "DNN acceleration",
                    "FPGA",
                    "mobile device",
                    "quantization",
                    "compiler"
                ]
            },
            {
                "title": "Achieving on-Mobile Real-Time Super-Resolution with Neural Architecture and Pruning Search",
                "link": "https://ieeexplore.ieee.org/document/9710156/",
                "date_of_publication": "28 February 2022",
                "doi": "10.1109/ICCV48922.2021.00478",
                "citations": "12",
                "abstract": "Though recent years have witnessed remarkable progress in single image super-resolution (SISR) tasks with the prosperous development of deep neural networks (DNNs), the deep learning methods are confronted with the computation and memory consumption issues in practice, especially for resource-limited platforms such as mobile devices. To overcome the challenge and facilitate the real-time deployment of SISR tasks on mobile, we combine neural architecture search with pruning search and propose an automatic search framework that derives sparse super-resolution (SR) models with high image quality while satisfying the real-time inference requirement. To decrease the search cost, we leverage the weight sharing strategy by introducing a supernet and decouple the search problem into three stages, including supernet construction, compiler-aware architecture and pruning search, and compiler-aware pruning ratio search. With the proposed framework, we are the first to achieve real-time SR inference (with only tens of milliseconds per frame) for implementing 720p resolution with competitive image quality (in terms of PSNR and SSIM) on mobile platforms (Samsung Galaxy S20).",
                "ieee_keywords": [
                    "Image quality",
                    "Deep learning",
                    "Computational modeling",
                    "Superresolution",
                    "Neural networks",
                    "Memory management",
                    "Search problems"
                ],
                "author_keywords": [
                    "Low-level and physics-based vision",
                    "Efficient training and inference methods",
                    "Vision applications and systems"
                ]
            },
            {
                "title": "A Shared Bicycle Intelligent Lock Control and Management System Based on Multisensor",
                "link": "https://ieeexplore.ieee.org/document/9031726/",
                "date_of_publication": null,
                "doi": "10.1109/JIOT.2020.2979899",
                "citations": "7",
                "abstract": "As a greener means of transportation, shared bicycle is becoming more popular in many cities. However, random parking seriously affects the appearance of a city and the safety of traffic. To realize a rational parking, an innovative bicycle intelligent lock based on multisensor was suggested in this article. First, a multisensor fusion and positioning algorithm was applied in the scheme of this article to judge whether the location and the positioning of the parking are proper. Next, through the radio-frequency identification technology, the system reads the information on the RF card of the intelligent lock for user authentication and used Narrowband Internet of Things (NB-IoT) to communicate between the devices and the server. Finally, through online monitoring and alarm software, the system can monitor the bicycle parking conditions in real time. In order to improve the accuracy of the event judgment, the attitude angle data were processed using the Kalman filter algorithm. The experimental results showed that a suitable direction error threshold was 10°, the average bicycle unlocking success rate was 98%, the average delay time was 4595.06 ms, and the judgment rates of inappropriate parking reached 100%. Therefore, by reminding and asking the user to park the bicycle correctly and alarming when the bicycle falls down, this system can make the parking of shared bicycles neater. The bicycle manager can also keep track of bicycle status information in time.",
                "ieee_keywords": [
                    "Bicycles",
                    "Radiofrequency identification",
                    "Hardware",
                    "Power demand",
                    "Internet of Things",
                    "Servers",
                    "Urban areas"
                ],
                "author_keywords": [
                    "Attitude sensor",
                    "Internet of Things (IoT)",
                    "Kalman filters",
                    "narrowband IoT (NB-IoT)",
                    "radio-frequency identification (RFID) technology",
                    "shared bicycle"
                ]
            },
            {
                "title": "RTMobile: Beyond Real-Time Mobile Acceleration of RNNs for Speech Recognition",
                "link": "https://ieeexplore.ieee.org/document/9218499/",
                "date_of_publication": "09 October 2020",
                "doi": "10.1109/DAC18072.2020.9218499",
                "citations": "24",
                "abstract": "Recurrent neural networks (RNNs) based automatic speech recognition has nowadays become promising and important on mobile devices such as smart phones. However, previous RNN compression techniques either suffer from hardware performance overhead due to irregularity or significant accuracy loss due to the preserved regularity for hardware friendliness. In this work, we propose RTMobile that leverages both a novel block-based pruning approach and compiler optimizations to accelerate RNN inference on mobile devices. Our proposed RTMobile is the first work that can achieve real-time RNN inference on mobile platforms. Experimental results demonstrate that RTMobile can significantly outperform existing RNN hardware acceleration methods in terms of both inference accuracy and time. Compared with prior work on FPGA, RTMobile using Adreno 640 embedded GPU on GRU can improve the energy-efficiency by 40× while maintaining the same inference time.",
                "ieee_keywords": [
                    "Acceleration",
                    "Mobile handsets",
                    "Hardware",
                    "Computational modeling",
                    "Optimization",
                    "Field programmable gate arrays",
                    "Logic gates"
                ],
                "author_keywords": [
                    "RNN",
                    "pruning",
                    "real-time acceleration",
                    "mobile"
                ]
            },
            {
                "title": "High Sensitivity Coreless Fiber Surface Plasmon Resonance Sensor Based on Au Nano Biconical Particles",
                "link": "https://ieeexplore.ieee.org/document/9580857/",
                "date_of_publication": null,
                "doi": "10.1109/JSEN.2021.3121447",
                "citations": "6",
                "abstract": "A coupling enhanced surface plasmon resonance (SPR) sensor based on coreless fiber is proposed. Compared with the common fiber, the coreless fiber is easier to perceive the changes of the external medium environment. As one-dimensional material with superior performance, the Au nano biconical particles are used to improve the sensor performance. Based on the finite element method (FEM), the electric field enhancement effect of the coupling between the SPR of the gold film and the Localized surface plasmon resonance (LSPR) of the Au nano biconical particles on the surface of the coreless fiber is analyzed, and the experimental results show that the Au nano biconical particles can improve the sensitivity. The sensitivity of the improved Au nano biconical particle sensor is 3514 nm/RIU, which is 64% higher than that of the traditional gold film SPR sensor, and the quality factor is increased by 24%. The new sensor based on Au nano biconical particles and gold film has a good application prospect in biosensor and chemical measurement. A coupling enhanced surface plasmon resonance (SPR) sensor based on coreless fiber is proposed. The experimental results show that the Au nano biconical particles can imp...View more",
                "ieee_keywords": [
                    "Sensors",
                    "Optical fiber sensors",
                    "Gold",
                    "Optical fibers",
                    "Electric fields",
                    "Optical surface waves",
                    "Nanoparticles"
                ],
                "author_keywords": [
                    "Au nano biconical",
                    "coreless fiber",
                    "coupling enhanced",
                    "LSPR",
                    "SPR"
                ]
            },
            {
                "title": "Non-Structured DNN Weight Pruning—Is It Beneficial in Any Platform?",
                "link": "https://ieeexplore.ieee.org/document/9381660/",
                "date_of_publication": null,
                "doi": "10.1109/TNNLS.2021.3063265",
                "citations": "10",
                "abstract": "Large deep neural network (DNN) models pose the key challenge to energy efficiency due to the significantly higher energy consumption of off-chip DRAM accesses than arithmetic or SRAM operations. It motivates the intensive research on model compression with two main approaches. Weight pruning leverages the redundancy in the number of weights and can be performed in a non-structured, which has higher flexibility and pruning rate but incurs index accesses due to irregular weights, or structured manner, which preserves the full matrix structure with a lower pruning rate. Weight quantization leverages the redundancy in the number of bits in weights. Compared to pruning, quantization is much more hardware-friendly and has become a “must-do” step for FPGA and ASIC implementations. Thus, any evaluation of the effectiveness of pruning should be on top of quantization. The key open question is, with quantization, what kind of pruning (non-structured versus structured) is most beneficial? This question is fundamental because the answer will determine the design aspects that we should really focus on to avoid the diminishing return of certain optimizations. This article provides a definitive answer to the question for the first time. First, we build ADMM-NN-S by extending and enhancing ADMM-NN, a recently proposed joint weight pruning and quantization framework, with the algorithmic supports for structured pruning, dynamic ADMM regulation, and masked mapping and retraining. Second, we develop a methodology for fair and fundamental comparison of non-structured and structured pruning in terms of both storage and computation efficiency. Our results show that ADMM-NN-S consistently outperforms the prior art: 1) it achieves $348\\times $ , $36\\times $ , and $8\\times $ overall weight pruning on LeNet-5, AlexNet, and ResNet-50, respectively, with (almost) zero accuracy loss and 2) we demonstrate the first fully binarized (for all layers) DNNs can be lossless in accuracy in m... (Show More)",
                "ieee_keywords": [
                    "Quantization (signal)",
                    "Redundancy",
                    "Computational modeling",
                    "Acceleration",
                    "Degradation",
                    "Random access memory",
                    "Indexes"
                ],
                "author_keywords": [
                    "Deep neural network (DNN)",
                    "hardware acceleration",
                    "quantization",
                    "weight pruning"
                ]
            },
            {
                "title": "Brief Industry Paper: Towards Real-Time 3D Object Detection for Autonomous Vehicles with Pruning Search",
                "link": "https://ieeexplore.ieee.org/document/9470446/",
                "date_of_publication": "07 July 2021",
                "doi": "10.1109/RTAS52030.2021.00043",
                "citations": "1",
                "abstract": "In autonomous driving, 3D object detection is es-sential as it provides basic knowledge about the environment. However, as deep learning based 3D detection methods are usually computation intensive, it is challenging to support realtime 3D object detection on edge-computing devices in selfdriving cars with limited computation and memory resources. To facilitate this, we propose a compiler-aware pruning search framework, to achieve real-time inference of 3D object detection on the resource-limited mobile devices. Specifically, a generator is applied to sample better pruning proposals in the search space based on current proposals with their performance, and an evaluator is adopted to evaluate the sampled pruning proposal performance. To accelerate the search, the evaluator employs Bayesian optimization with an ensemble of neural predictors. We demonstrate in experiments that for the first time, the pruning search framework can achieve real-time 3D object detection on mobile (Samsung Galaxy S20 phone) with state-of-the-art detection performance.",
                "ieee_keywords": [
                    "Performance evaluation",
                    "Solid modeling",
                    "Three-dimensional displays",
                    "Object detection",
                    "Search problems",
                    "Real-time systems",
                    "Mobile handsets"
                ],
                "author_keywords": [
                    "3D object detection",
                    "real-time",
                    "point cloud"
                ]
            },
            {
                "title": "NPAS: A Compiler-aware Framework of Unified Network Pruning and Architecture Search for Beyond Real-Time Mobile Acceleration",
                "link": "https://ieeexplore.ieee.org/document/9578043/",
                "date_of_publication": "02 November 2021",
                "doi": "10.1109/CVPR46437.2021.01403",
                "citations": "7",
                "abstract": "With the increasing demand to efficiently deploy DNNs on mobile edge devices, it becomes much more important to reduce unnecessary computation and increase the execution speed. Prior methods towards this goal, including model compression and network architecture search (NAS), are largely performed independently, and do not fully consider compiler-level optimizations which is a must-do for mobile acceleration. In this work, we first propose (i) a general category of fine-grained structured pruning applicable to various DNN layers, and (ii) a comprehensive, compiler automatic code generation framework supporting different DNNs and different pruning schemes, which bridge the gap of model compression and NAS. We further propose NPAS, a compiler-aware unified network pruning and architecture search. To deal with large search space, we propose a meta-modeling procedure based on reinforcement learning with fast evaluation and Bayesian optimization, ensuring the total number of training epochs comparable with representative NAS frameworks. Our framework achieves 6.7ms, 5.9ms, and 3.9ms ImageNet inference times with 78.2%, 75% (MobileNet-V3 level), and 71% (MobileNet-V2 level) Top-1 accuracy respectively on an off-the-shelf mobile phone, consistently outperforming prior work.",
                "ieee_keywords": [
                    "Training",
                    "Performance evaluation",
                    "Codes",
                    "Computational modeling",
                    "Computer architecture",
                    "Reinforcement learning",
                    "Network architecture"
                ],
                "author_keywords": []
            },
            {
                "title": "ESRU: Extremely Low-Bit and Hardware-Efficient Stochastic Rounding Unit Design for Low-Bit DNN Training",
                "link": "https://ieeexplore.ieee.org/document/10137222/",
                "date_of_publication": "02 June 2023",
                "doi": "10.23919/DATE56975.2023.10137222",
                "citations": "49",
                "abstract": "Stochastic rounding is crucial in the low-bit (e.g., 8-bit) training of deep neural networks (DNNs) to achieve high accuracy. One of the drawbacks of prior studies is that they require a large number of high-precision stochastic rounding units (SRUs) to guarantee low-bit DNN accuracy, which involves considerable hardware overhead. In this paper, we use extremely low-bit SRUs (ESRUs) to save a large number of hardware resources during low-bit DNN training. However, a naively designed ESRU introduces a biased distribution of random numbers, causing accuracy degradation. To address this issue, we further propose an ESRU design with a plateau-shape distribution. The plateau-shape distribution in our ESRU design is implemented with the combination of an LFSR (linear-feedback shift register) and an inverted LFSR, which avoids LFSR packing and turns an inherent LFSR drawback into an advantage in our efficient ESRU design. Experimental results using state-of-the-art DNN models demonstrate that, compared to the prior 24-bit SRU with 24-bit pseudo-random number generators (PRNG), our 8-bit ESRU with 3-bit PRNG reduces the SRU hardware resource usage by 9.75x while achieving slightly higher accuracy.",
                "ieee_keywords": [
                    "Training",
                    "Degradation",
                    "Deep learning",
                    "Neural networks",
                    "Stochastic processes",
                    "Shift registers",
                    "Hardware"
                ],
                "author_keywords": [
                    "DNNs",
                    "low-bit training",
                    "stochastic rounding"
                ]
            }
        ]
    },
    {
        "name": "Gregory D. Abowd",
        "publications": [
            {
                "title": "Behavioral Imaging and Autism",
                "link": "https://ieeexplore.ieee.org/document/6818509/",
                "date_of_publication": null,
                "doi": "10.1109/MPRV.2014.23",
                "citations": "40",
                "abstract": "Behavioral imaging encompasses the use of computational sensing and modeling techniques to measure and analyze human behavior. This article discusses a research program focused on the study of dyadic social interactions between children and their caregivers and peers. The study has resulted in a dataset containing semi-structured play interactions between children and adults. Behavioral imaging could broadly affect the quality of care for individuals with a developmental or behavioral disorder.",
                "ieee_keywords": [
                    "Pediatrics",
                    "Autism",
                    "Biomedical monitoring",
                    "Patient monitoring",
                    "Behavioral science",
                    "Sensors",
                    "Image sensing"
                ],
                "author_keywords": [
                    "behavioral imaging",
                    "autism",
                    "ASD",
                    "computer vision",
                    "assistive technologies",
                    "pervasive computing"
                ]
            },
            {
                "title": "Applying Compute-Proximal Energy Harvesting to Develop Self-Sustained Systems for Automobiles",
                "link": "https://ieeexplore.ieee.org/document/9633262/",
                "date_of_publication": null,
                "doi": "10.1109/MPRV.2021.3124738",
                "citations": "3",
                "abstract": "A major cost, inconvenience, and source of failure for adding sensors and electronic accessories to automobiles is wiring the systems into the car’s power supply. We propose an alternative approach, computer-proximal energy harvesting, which harvests power locally, so wiring is not needed. In this article, we provide an overview of the principles and empirical evaluations for harvesting energy in and around the automobile exploring the role that wind, light, vibration, and heat can play in this process. To better explore these concepts, we demonstrate two prototypes—A thermoelectric energy-based parking assistant, which is attached to the exhaust pipe, and a wind-powered external pedestrian display, which is anchored to the front bumper of a car.",
                "ieee_keywords": [
                    "Energy harvesting",
                    "Automotive engineering",
                    "Temperature measurement",
                    "Vibrations",
                    "Photovoltaic cells",
                    "Sensors",
                    "Resistance heating"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Huy L Nguyễn",
        "publications": [
            {
                "title": "Heavy Hitters via Cluster-Preserving Clustering",
                "link": "https://ieeexplore.ieee.org/document/7782918/",
                "date_of_publication": "15 December 2016",
                "doi": "10.1109/FOCS.2016.16",
                "citations": "27",
                "abstract": "In the turnstile ℓ p heavy hitters problem with parameter ε, one must maintain a high-dimensional vector x ∈ ℝ n subject to updates of the form update (i,Δ) causing the change x i ← x i + Δ, where i ε[n], Δ ∈ ℝ. Upon receiving a query, the goal is to report every \"heavy hitter\" i ∈ [n] with |x i | ≥ ε ∥x∥ p as part of a list L ⊆ [n] of size O(1/ε p ), i.e. proportional to the maximum possible number of heavy hitters. For any pε(0,2] the COUNTSKETCH of [CCFC04] solves ℓ p heavy hitters using O(ε -p lg n) words of space with O(lg n) update time, O(n lg n) query time to output L, and whose output after any query is correct with high probability (whp) 1 - 1/poly(n) [JST11, Section 4.4]. This space bound is optimal even in the strict turnstile model [JST11] in which it is promised that x i ≥ 0 for all i ∈ [n] at all points in the stream, but unfortunately the query time is very slow. To remedy this, the work [CM05] proposed the \"dyadic trick\" for the COUNTMIN sketch for p = 1 in the strict turnstile model, which to maintain whp correctness achieves suboptimal space O(ε -1 lg 2 n), worse update time O(lg 2 n), but much better query time O(ε -1 poly(lg n)). An extension to all p ∈ (0,2] appears in [KNPW11, Theorem 1], and can be obtained from [Pag13]. We show that this tradeoff between space and update time versus query time is unnecessary. We provide a new algorithm, EXPANDERSKETCH, which in the most general turnstile model achieves optimal O(ε-plog n) space, O(log n) update time, and fast O(ε-ppoly(log n)) query time, providing correctness whp. In fact, a simpler version of our algorithm for p = 1 in the strict turnstile model answers queries even faster than the \"dyadic trick\" by roughly a log n factor, dominating it in all regards. Our main innovation is an efficient reduction from the heavy hitters to a clustering problem in which each heavy hitter is encoded as some form of noisy spectral cluster in a much bigger graph, and the goal is to identify every clu... (Show More)",
                "ieee_keywords": [
                    "Clustering algorithms",
                    "Computational modeling",
                    "Partitioning algorithms",
                    "Algorithm design and analysis",
                    "Complexity theory",
                    "Estimation"
                ],
                "author_keywords": [
                    "heavy hitters",
                    "streaming",
                    "clustering"
                ]
            },
            {
                "title": "A New Framework for Distributed Submodular Maximization",
                "link": "https://ieeexplore.ieee.org/document/7782979/",
                "date_of_publication": "15 December 2016",
                "doi": "10.1109/FOCS.2016.74",
                "citations": "17",
                "abstract": "A wide variety of problems in machine learning, including exemplar clustering, document summarization, and sensor placement, can be cast as constrained submodular maximization problems. A lot of recent effort has been devoted to developing distributed algorithms for these problems. However, these results suffer from high number of rounds, suboptimal approximation ratios, or both. We develop a framework for bringing existing algorithms in the sequential setting to the distributed setting, achieving near optimal approximation ratios for many settings in only a constant number of MapReduce rounds. Our techniques also give a fast sequential algorithm for non-monotone maximization subject to a matroid constraint.",
                "ieee_keywords": [
                    "Approximation algorithms",
                    "Greedy algorithms",
                    "Distributed algorithms",
                    "Standards",
                    "Computer science",
                    "Clustering algorithms",
                    "Partitioning algorithms"
                ],
                "author_keywords": [
                    "Distributed submodular maximization",
                    "MapReduce",
                    "approximation algorithms"
                ]
            },
            {
                "title": "Constrained Submodular Maximization: Beyond 1/e",
                "link": "https://ieeexplore.ieee.org/document/7782937/",
                "date_of_publication": "15 December 2016",
                "doi": "10.1109/FOCS.2016.34",
                "citations": "23",
                "abstract": "In this work, we present a new algorithm for maximizing a non-monotone submodular function subject to a general constraint. Our algorithm finds an approximate fractional solution for maximizing the multilinear extension of the function over a down-closed polytope. The approximation guarantee is 0.372 and it is the first improvement over the 1/e approximation achieved by the unified Continuous Greedy algorithm [Feldman et al., FOCS 2011].",
                "ieee_keywords": [
                    "Optimized production technology",
                    "Approximation algorithms",
                    "Greedy algorithms",
                    "Algorithm design and analysis",
                    "Standards",
                    "Computer science",
                    "Electronic mail"
                ],
                "author_keywords": [
                    "submodular functions",
                    "maximization"
                ]
            }
        ]
    },
    {
        "name": "Alan Mislove",
        "publications": [
            {
                "title": "CRLite: A Scalable System for Pushing All TLS Revocations to All Browsers",
                "link": "https://ieeexplore.ieee.org/document/7958597/",
                "date_of_publication": "26 June 2017",
                "doi": "10.1109/SP.2017.17",
                "citations": "42",
                "abstract": "Currently, no major browser fully checks for TLS/SSL certificate revocations. This is largely due to the fact that the deployed mechanisms for disseminating revocations (CRLs, OCSP, OCSP Stapling, CRLSet, and OneCRL) are each either incomplete, insecure, inefficient, slow to update, not private, or some combination thereof. In this paper, we present CRLite, an efficient and easily-deployable system for proactively pushing all TLS certificate revocations to browsers. CRLite servers aggregate revocation information for all known, valid TLS certificates on the web, and store them in a space-efficient filter cascade data structure. Browsers periodically download and use this data to check for revocations of observed certificates in real-time. CRLite does not require any additional trust beyond the existing PKI, and it allows clients to adopt a fail-closed security posture even in the face of network errors or attacks that make revocation information temporarily unavailable. We present a prototype of name that processes TLS certificates gathered by Rapid7, the University of Michigan, and Google's Certificate Transparency on the server-side, with a Firefox extension on the client-side. Comparing CRLite to an idealized browser that performs correct CRL/OCSP checking, we show that CRLite reduces latency and eliminates privacy concerns. Moreover, CRLite has low bandwidth costs: it can represent all certificates with an initial download of 10 MB (less than 1 byte per revocation) followed by daily updates of 580 KB on average. Taken together, our results demonstrate that complete TLS/SSL revocation checking is within reach for all clients.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "WebCloud: Recruiting Social Network Users to Assist in Content Distribution",
                "link": "https://ieeexplore.ieee.org/document/6299121/",
                "date_of_publication": "13 September 2012",
                "doi": "10.1109/NCA.2012.41",
                "citations": "14",
                "abstract": "Today, the data exchanged over online social networks (OSNs) represents a significant fraction of Internet traffic. However, OSN content is different from more traditional web content, as it is more likely to be generated at the edge of the network, to be exchanged within a local geographic region, and to possess a more even popularity distribution with fewer popular objects. Unfortunately, most OSNs still use largely centralized approaches to distribute content (e.g., CDNs and web caches), resulting in lower performance due to the different workload. In this paper, we take a first step towards addressing this situation by proposing Web Cloud, a content distribution system for OSNs that works by repurposing client web browsers to help serve content to others. When a user browses content, Web Cloud tries to serve the request from one of that user's friends' browsers, instead of from the OSN directly. Unlike other systems, Web Cloud works with existing browsers and does not require any plug-ins, and therefore can be directly applied to today's OSNs. We demonstrate the practicality of Web Cloud with micro benchmarks, simulations of a Facebook deployment, a real-world deployment, and evaluations of a proof-of-concept iOS app.",
                "ieee_keywords": [
                    "Browsers",
                    "Facebook",
                    "Internet",
                    "Servers",
                    "Peer to peer computing"
                ],
                "author_keywords": []
            },
            {
                "title": "Privacy Risks with Facebook's PII-Based Targeting: Auditing a Data Broker's Advertising Interface",
                "link": "https://ieeexplore.ieee.org/document/8418598/",
                "date_of_publication": "26 July 2018",
                "doi": "10.1109/SP.2018.00014",
                "citations": "42",
                "abstract": "Sites like Facebook and Google now serve as de facto data brokers, aggregating data on users for the purpose of implementing powerful advertising platforms. Historically, these services allowed advertisers to select which users see their ads via targeting attributes. Recently, most advertising platforms have begun allowing advertisers to target users directly by uploading the personal information of the users who they wish to advertise to (e.g., their names, email addresses, phone numbers, etc.); these services are often known as custom audiences. Custom audiences effectively represent powerful linking mechanisms, allowing advertisers to leverage any PII (e.g., from customer data, public records, etc.) to target users. In this paper, we focus on Facebook's custom audience implementation and demonstrate attacks that allow an adversary to exploit the interface to infer users' PII as well as to infer their activity. Specifically, we show how the adversary can infer users' full phone numbers knowing just their email address, determine whether a particular user visited a website, and de-anonymize all the visitors to a website by inferring their phone numbers en masse. These attacks can be conducted without any interaction with the victim(s), cannot be detected by the victim(s), and do not require the adversary to spend money or actually place an ad. We propose a simple and effective fix to the attacks based on reworking the way Facebook de-duplicates uploaded information. Facebook's security team acknowledged the vulnerability and has put into place a fix that is a variant of the fix we propose. Overall, our results indicate that advertising platforms need to carefully consider the privacy implications of their interfaces.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Rolling With Confidence: Managing the Complexity of DNSSEC Operations",
                "link": "https://ieeexplore.ieee.org/document/8712408/",
                "date_of_publication": null,
                "doi": "10.1109/TNSM.2019.2916176",
                "citations": "13",
                "abstract": "The domain name system (DNS) is the naming system on the Internet. With the DNS security extensions (DNSSECs) operators can protect the authenticity of their domain using public key cryptography. DNSSEC, however, can be difficult to configure and maintain: operators need to replace keys to upgrade their algorithm, react to security breaches or follow key management policies. These tasks are not trivial. If operators do not time changes to their keys right, caching resolvers may not have access to the correct keys, potentially rendering DNS zones unavailable for minutes or hours. While best current practices give abstract guidelines on how to introduce and withdraw keys, information on how to monitor and control actual rollovers in a live environment is lacking. More specifically, it is challenging for operators to know when to introduce or withdraw keys based on the state of the network. Our main contribution is to help operators answer this question and to address this barrier for deploying DNSSEC. We develop a method with which operators can monitor the replacement of DNSSEC keys, called a rollover. Thereby, they can make confident decisions during the rollover and make sure their zone stays available at all times. We validate the method with an algorithm rollover of the Swedish TLD .se and provide an open source tool with which operators can monitor their rollover themselves.",
                "ieee_keywords": [
                    "Rollover",
                    "Monitoring",
                    "Servers",
                    "Security",
                    "Service-oriented architecture",
                    "Task analysis",
                    "Tools"
                ],
                "author_keywords": [
                    "DNS",
                    "DNSSEC",
                    "automation",
                    "key rollover",
                    "key management",
                    "monitoring"
                ]
            },
            {
                "title": "Exploring the design space of social network-based Sybil defenses",
                "link": "https://ieeexplore.ieee.org/document/6151333/",
                "date_of_publication": "13 February 2012",
                "doi": "10.1109/COMSNETS.2012.6151333",
                "citations": "27",
                "abstract": "Recently, there has been significant research interest in leveraging social networks to defend against Sybil attacks. While much of this work may appear similar at first glance, existing social network-based Sybil defense schemes can be divided into two categories: Sybil detection and Sybil tolerance. These two categories of systems both leverage global properties of the underlying social graph, but they rely on different assumptions and provide different guarantees: Sybil detection schemes are application-independent and rely only on the graph structure to identify Sybil identities, while Sybil tolerance schemes rely on application-specific information and leverage the graph structure and transaction history to bound the leverage an attacker can gain from using multiple identities. In this paper, we take a closer look at the design goals, models, assumptions, guarantees, and limitations of both categories of social network-based Sybil defense systems.",
                "ieee_keywords": [
                    "Social network services",
                    "Communities",
                    "Image edge detection",
                    "Buildings",
                    "Joining processes",
                    "Electronic mail",
                    "Protocols"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Christo Wilson",
        "publications": [
            {
                "title": "CRLite: A Scalable System for Pushing All TLS Revocations to All Browsers",
                "link": "https://ieeexplore.ieee.org/document/7958597/",
                "date_of_publication": "26 June 2017",
                "doi": "10.1109/SP.2017.17",
                "citations": "42",
                "abstract": "Currently, no major browser fully checks for TLS/SSL certificate revocations. This is largely due to the fact that the deployed mechanisms for disseminating revocations (CRLs, OCSP, OCSP Stapling, CRLSet, and OneCRL) are each either incomplete, insecure, inefficient, slow to update, not private, or some combination thereof. In this paper, we present CRLite, an efficient and easily-deployable system for proactively pushing all TLS certificate revocations to browsers. CRLite servers aggregate revocation information for all known, valid TLS certificates on the web, and store them in a space-efficient filter cascade data structure. Browsers periodically download and use this data to check for revocations of observed certificates in real-time. CRLite does not require any additional trust beyond the existing PKI, and it allows clients to adopt a fail-closed security posture even in the face of network errors or attacks that make revocation information temporarily unavailable. We present a prototype of name that processes TLS certificates gathered by Rapid7, the University of Michigan, and Google's Certificate Transparency on the server-side, with a Firefox extension on the client-side. Comparing CRLite to an idealized browser that performs correct CRL/OCSP checking, we show that CRLite reduces latency and eliminates privacy concerns. Moreover, CRLite has low bandwidth costs: it can represent all certificates with an initial download of 10 MB (less than 1 byte per revocation) followed by daily updates of 580 KB on average. Taken together, our results demonstrate that complete TLS/SSL revocation checking is within reach for all clients.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Bringing the Kid back into YouTube Kids: Detecting Inappropriate Content on Video Streaming Platforms",
                "link": "https://ieeexplore.ieee.org/document/9073060/",
                "date_of_publication": "23 April 2020",
                "doi": "10.1145/3341161.3342913",
                "citations": "4",
                "abstract": "With the advent of child-centric content-sharing platforms, such as YouTube Kids, thousands of children, from all age groups are consuming gigabytes of content on a daily basis. With PBS Kids, Disney Jr. and countless others joining in the fray, this consumption of video data stands to grow further in quantity and diversity. However, it has been observed increasingly that content unsuitable for children often slips through the cracks and lands on such platforms. To investigate this phenomenon in more detail, we collect a first of its kind dataset of inappropriate videos hosted on such children-focused apps and platforms. Alarmingly, our study finds that there is a noticeable percentage of such videos currently being watched by kids with some inappropriate videos having millions of views already. To address this problem, we develop a deep learning architecture that can flag such videos and report them. Our results show that the proposed system can be successfully applied to various types of animations, cartoons and CGI videos to detect any inappropriate content within them.",
                "ieee_keywords": [
                    "Streaming media",
                    "YouTube",
                    "Machine learning",
                    "Manuals",
                    "Histograms",
                    "Google"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Byron Wallace",
        "publications": [
            {
                "title": "Using Electronic Medical Records and Physician Data to Improve Information Retrieval for Evidence-Based Care",
                "link": "https://ieeexplore.ieee.org/document/7776328/",
                "date_of_publication": "08 December 2016",
                "doi": "10.1109/ICHI.2016.12",
                "citations": "3",
                "abstract": "Healthcare practitioners are increasingly using search functionality embedded in Electronic Medical Record (EMR) software to search for relevant evidence summaries at point of care. We introduce a learning to rank approach that exploits information carried in EMR data and UpToDate user accounts to (significantly) improve ranking results, compared to a comparable model that does not exploit such features.",
                "ieee_keywords": [
                    "Medical services",
                    "Electronic medical records",
                    "Information retrieval",
                    "Measurement",
                    "Informatics",
                    "Software",
                    "Context"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Christopher Amato",
        "publications": [
            {
                "title": "Online Planning for Target Object Search in Clutter under Partial Observability",
                "link": "https://ieeexplore.ieee.org/document/8793494/",
                "date_of_publication": "12 August 2019",
                "doi": "10.1109/ICRA.2019.8793494",
                "citations": "38",
                "abstract": "The problem of finding and grasping a target object in a cluttered, uncertain environment, target object search, is a common and important problem in robotics. One key challenge is the uncertainty of locating and recognizing each object in a cluttered environment due to noisy perception and occlusions. Furthermore, the uncertainty in localization makes manipulation difficult and uncertain. To cope with these challenges, we formulate the target object search task as a partially observable Markov decision process (POMDP), enabling the robot to reason about perceptual and manipulation uncertainty while searching. To further address the manipulation difficulty, we propose Parameterized Action Partially Observable Monte-Carlo Planning (PA-POMCP), an algorithm that evaluates manipulation actions by taking into account the effect of the robot's current belief on the success of the action execution. In addition, a novel run-time initial belief generator and a state value estimator are introduced in this paper to facilitate the PA-POMCP algorithm. Our experiments show that our methods solve the target object search task in settings where simpler methods either take more object movements or fail.",
                "ieee_keywords": [
                    "Search problems",
                    "Task analysis",
                    "Robot sensing systems",
                    "Robot kinematics",
                    "Uncertainty",
                    "Three-dimensional displays"
                ],
                "author_keywords": []
            },
            {
                "title": "Multi-Agent Reinforcement Learning Based on Representational Communication for Large-Scale Traffic Signal Control",
                "link": "https://ieeexplore.ieee.org/document/10123921/",
                "date_of_publication": null,
                "doi": "10.1109/ACCESS.2023.3275883",
                "citations": "415",
                "abstract": "Traffic signal control (TSC) is a challenging problem within intelligent transportation systems and has been tackled using multi-agent reinforcement learning (MARL). While centralized approaches are often infeasible for large-scale TSC problems, decentralized approaches provide scalability but introduce new challenges, such as partial observability. Communication plays a critical role in decentralized MARL, as agents must learn to exchange information using messages to better understand the system and achieve effective coordination. Deep MARL has been used to enable inter-agent communication by learning communication protocols in a differentiable manner. However, many deep MARL communication frameworks proposed for TSC allow agents to communicate with all other agents at all times, which can add to the existing noise in the system and degrade overall performance. In this study, we propose a communication-based MARL framework for large-scale TSC. Our framework allows each agent to learn a communication policy that dictates “which” part of the message is sent “to whom”. In essence, our framework enables agents to selectively choose the recipients of their messages and exchange variable length messages with them. This results in a decentralized and flexible communication mechanism in which agents can effectively use the communication channel only when necessary. We designed two networks, a synthetic 4×4 grid network and a real-world network based on the Pasubio neighborhood in Bologna. Our framework achieved the lowest network congestion compared to related methods, with agents utilizing ∼47−65% of the communication channel. Ablation studies further demonstrated the effectiveness of the communication policies learned within our framework. Architecture of QRC-TSC with two agents. Each agent uses a communication network (shown in the communication block) in addition to the agent network.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Q-DeckRec: A Fast Deck Recommendation System for Collectible Card Games",
                "link": "https://ieeexplore.ieee.org/document/8490446/",
                "date_of_publication": "14 October 2018",
                "doi": "10.1109/CIG.2018.8490446",
                "citations": "16",
                "abstract": "Deck building is a crucial component in playing Collectible Card Games (CCGs). The goal of deck building is to choose a fixed-sized subset of cards from a large card pool, so that they work well together in-game against specific opponents. Existing methods either lack flexibility to adapt to different opponents or require large computational resources, still making them unsuitable for any real-time or large-scale application. We propose a new deck recommendation system, named Q-DeckRec, which learns a deck search policy during a training phase and uses it to solve deck building problem instances. Our experimental results demonstrate Q-DeckRec requires less computational resources to build winning-effective decks after a training phase compared to several baseline methods.",
                "ieee_keywords": [
                    "Buildings",
                    "Search problems",
                    "Games",
                    "Optimization",
                    "Training",
                    "Genetic algorithms",
                    "Linear programming"
                ],
                "author_keywords": [
                    "deck recommendation",
                    "Q-learning",
                    "collectible card game"
                ]
            },
            {
                "title": "Learning Multi-Robot Decentralized Macro-Action-Based Policies via a Centralized Q-Net",
                "link": "https://ieeexplore.ieee.org/document/9196684/",
                "date_of_publication": "15 September 2020",
                "doi": "10.1109/ICRA40945.2020.9196684",
                "citations": "12",
                "abstract": "In many real-world multi-robot tasks, high-quality solutions often require a team of robots to perform asynchronous actions under decentralized control. Decentralized multi-agent reinforcement learning methods have difficulty learning decentralized policies because of the environment appearing to be non-stationary due to other agents also learning at the same time. In this paper, we address this challenge by proposing a macro-action-based decentralized multi-agent double deep recurrent Q-net (MacDec-MADDRQN) which trains each decentralized Q-net using a centralized Q-net for action selection. A generalized version of MacDec-MADDRQN with two separate training environments, called Parallel-MacDec-MADDRQN, is also presented to leverage either centralized or decentralized exploration. The advantages and the practical nature of our methods are demonstrated by achieving near-centralized results in simulation and having real robots accomplish a warehouse tool delivery task in an efficient way.",
                "ieee_keywords": [
                    "Robot kinematics",
                    "Training",
                    "Tools",
                    "Task analysis",
                    "Machine learning",
                    "History"
                ],
                "author_keywords": []
            },
            {
                "title": "Towards End-to-End Control of a Robot Prosthetic Hand via Reinforcement Learning",
                "link": "https://ieeexplore.ieee.org/document/9224380/",
                "date_of_publication": "15 October 2020",
                "doi": "10.1109/BioRob49111.2020.9224380",
                "citations": "2",
                "abstract": "Robot prosthetic hands intend to replicate one's lost abilities through intuitive control. So far, control methods that rely heavily on the human input such as Electromyographic (EMG) and Electroneurographic (ENG) signals have been predominantly studied. However, these methods face issues such as lack of robustness resulting in abandonment of this technology by the users. There is a need for a paradigm shift in the robot prosthetic hand control methods. With this regard, we propose an end-to-end learning of control policy for a robot prosthetic hand through reinforcement learning. Imitation learning has been fostered to help with the sparse reward setting in the hard-to-explore state-space of the problem. The results in simulation show the feasibility of successfully learning an endto-end policy for grasping objects by robot prosthetic hands, potentially increasing robustness for grasp control of future robot prosthetic hands.",
                "ieee_keywords": [
                    "Robots",
                    "Prosthetic hand",
                    "Electromyography",
                    "Trajectory",
                    "Grasping",
                    "Robustness",
                    "Entropy"
                ],
                "author_keywords": []
            },
            {
                "title": "End-to-end grasping policies for human-in-the-loop robots via deep reinforcement learning",
                "link": "https://ieeexplore.ieee.org/document/9561937/",
                "date_of_publication": "18 October 2021",
                "doi": "10.1109/ICRA48506.2021.9561937",
                "citations": "356",
                "abstract": "State-of-the-art human-in-the-loop robot grasping is hugely suffered by Electromyography (EMG) inference robustness issues. As a workaround, researchers have been looking into integrating EMG with other signals, often in an ad hoc manner. In this paper, we are presenting a method for end-to-end training of a policy for human-in-the-loop robot grasping on real reaching trajectories. For this purpose we use Reinforcement Learning (RL) and Imitation Learning (IL) in DEXTRON (DEXTerity enviRONment), a stochastic simulation environment with real human trajectories that are augmented and selected using a Monte Carlo (MC) simulation method. We also offer a success model which once trained on the expert policy data and the RL policy roll-out transitions, can provide transparency to how the deep policy works and when it is probably going to fail.",
                "ieee_keywords": [
                    "Training",
                    "Monte Carlo methods",
                    "Stochastic processes",
                    "Grasping",
                    "Reinforcement learning",
                    "Electromyography",
                    "Robustness"
                ],
                "author_keywords": []
            },
            {
                "title": "Local Advantage Actor-Critic for Robust Multi-Agent Deep Reinforcement Learning",
                "link": "https://ieeexplore.ieee.org/document/9620607/",
                "date_of_publication": "07 December 2021",
                "doi": "10.1109/MRS50823.2021.9620607",
                "citations": "4",
                "abstract": "Policy gradient methods have become popular in multi-agent reinforcement learning, but they suffer from high variance due to the presence of environmental stochasticity and exploring agents (i.e., non-stationarity), which is potentially worsened by the difficulty in credit assignment. As a result, there is a need for a method that is not only capable of efficiently solving the above two problems but also robust enough to solve a variety of tasks. To this end, we propose a new multi-agent policy gradient method, called Robust Local Advantage (ROLA) Actor-Critic. ROLA allows each agent to learn an individual action-value function as a local critic as well as ameliorating environment non-stationarity via a novel centralized training approach based on a centralized critic. By using this local critic, each agent calculates a baseline to reduce variance on its policy gradient estimation, which results in an expected advantage action-value over other agents' choices that implicitly improves credit assignment. We evaluate ROLA across diverse benchmarks and show its robustness and effectiveness over a number of state-of-the-art multi-agent policy gradient algorithms.",
                "ieee_keywords": [
                    "Training",
                    "Gradient methods",
                    "Estimation",
                    "Reinforcement learning",
                    "Benchmark testing",
                    "Robustness",
                    "Task analysis"
                ],
                "author_keywords": []
            },
            {
                "title": "Near-Optimal Adversarial Policy Switching for Decentralized Asynchronous Multi-Agent Systems",
                "link": "https://ieeexplore.ieee.org/document/8460485/",
                "date_of_publication": "13 September 2018",
                "doi": "10.1109/ICRA.2018.8460485",
                "citations": "3",
                "abstract": "A key challenge in multi-robot and multi-agent systems is generating solutions that are robust to other self-interested or even adversarial parties who actively try to prevent the agents from achieving their goals. The practicality of existing works addressing this challenge is limited to only small-scale synchronous decision-making scenarios or a single agent planning its best response against a single adversary with fixed, procedurally characterized strategies. In contrast this paper considers a more realistic class of problems where a team of asynchronous agents with limited observation and communication capabilities need to compete against multiple strategic adversaries with changing strategies. This problem necessitates agents that can coordinate to detect changes in adversary strategies and plan the best response accordingly. Our approach first optimizes a set of stratagems that represent these best responses. These optimized stratagems are then integrated into a unified policy that can detect and respond when the adversaries change their strategies. The near-optimality of the proposed framework is established theoretically as well as demonstrated empirically in simulation and hardware.",
                "ieee_keywords": [
                    "Switches",
                    "Planning",
                    "Task analysis",
                    "Robot kinematics",
                    "Probabilistic logic"
                ],
                "author_keywords": []
            },
            {
                "title": "To Ask or Not to Ask: A User Annoyance Aware Preference Elicitation Framework for Social Robots",
                "link": "https://ieeexplore.ieee.org/document/9341607/",
                "date_of_publication": "10 February 2021",
                "doi": "10.1109/IROS45743.2020.9341607",
                "citations": "129",
                "abstract": "In this paper we investigate how social robots can efficiently gather user preferences without exceeding the allowed user annoyance threshold. To do so, we use a Gazebo based simulated office environment with a TIAGo Steel robot. We then formulate the user annoyance aware preference elicitation problem as a combination of tensor completion and knapsack problems. We then test our approach on the aforementioned simulated environment and demonstrate that it can accurately estimate user preferences.",
                "ieee_keywords": [
                    "Tensors",
                    "Steel",
                    "Task analysis",
                    "Intelligent robots"
                ],
                "author_keywords": []
            },
            {
                "title": "Learning for multi-robot cooperation in partially observable stochastic environments with macro-actions",
                "link": "https://ieeexplore.ieee.org/document/8206001/",
                "date_of_publication": "14 December 2017",
                "doi": "10.1109/IROS.2017.8206001",
                "citations": "14",
                "abstract": "This paper presents a data-driven approach for multi-robot coordination in partially-observable domains based on Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs) and macro-actions (MAs). Dec-POMDPs provide a general framework for cooperative sequential decision making under uncertainty and MAs allow temporally extended and asynchronous action execution. To date, most methods assume the underlying Dec-POMDP model is known a priori or a full simulator is available during planning time. Previous methods which aim to address these issues suffer from local optimality and sensitivity to initial conditions. Additionally, few hardware demonstrations involving a large team of heterogeneous robots and with long planning horizons exist. This work addresses these gaps by proposing an iterative sampling based Expectation-Maximization algorithm (iSEM) to learn polices using only trajectory data containing observations, MAs, and rewards. Our experiments show the algorithm is able to achieve better solution quality than the state-of-the-art learning-based methods. We implement two variants of multi-robot Search and Rescue (SAR) domains (with and without obstacles) on hardware to demonstrate the learned policies can effectively control a team of distributed robots to cooperate in a partially observable stochastic environment.",
                "ieee_keywords": [
                    "Robot sensing systems",
                    "History",
                    "Manganese",
                    "Power capacitors",
                    "Stochastic processes",
                    "Decision making"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Ilias Diakonikolas",
        "publications": [
            {
                "title": "Testing Bayesian Networks",
                "link": "https://ieeexplore.ieee.org/document/8981932/",
                "date_of_publication": null,
                "doi": "10.1109/TIT.2020.2971625",
                "citations": "5",
                "abstract": "This work initiates a systematic investigation of testing high-dimensional structured distributions by focusing on testing Bayesian networks - the prototypical family of directed graphical models. A Bayesian network is defined by a directed acyclic graph, where we associate a random variable with each node. The value at any particular node is conditionally independent of all the other non-descendant nodes once its parents are fixed. Specifically, we study the properties of identity testing and closeness testing of Bayesian networks. Our main contribution is the first non-trivial efficient testing algorithms for these problems and corresponding information-theoretic lower bounds. For a wide range of parameter settings, our testing algorithms have sample complexity sublinear in the dimension and are sample-optimal, up to constant factors.",
                "ieee_keywords": [
                    "Testing",
                    "Complexity theory",
                    "Graphical models",
                    "Bayes methods",
                    "Task analysis",
                    "Random variables",
                    "Probabilistic logic"
                ],
                "author_keywords": [
                    "distribution testing",
                    "property testing",
                    "Bayesian networks",
                    "graphical models"
                ]
            },
            {
                "title": "Small Covers for Near-Zero Sets of Polynomials and Learning Latent Variable Models",
                "link": "https://ieeexplore.ieee.org/document/9317993/",
                "date_of_publication": "19 January 2021",
                "doi": "10.1109/FOCS46700.2020.00026",
                "citations": "2",
                "abstract": "Let $V$ be any vector space of multivariate degree- $d$ homogeneous polynomials with co-dimension at most $k$ , and $S$ be the set of points where all polynomials in $V$ nearly vanish. We establish a qualitatively optimal upper bound on the size of $\\epsilon$ -covers for $S$ , in the $\\ell_{2}$ -norm. Roughly speaking, we show that there exists an $\\epsilon$ -cover for $S$ of cardinality $M=(k/\\epsilon)^{O_{d}(k^{1/d})}$ . Our result is constructive yielding an algorithm to compute such an $\\epsilon$ -cover that runs in time $\\text{poly}(M)$ . Building on our structural result, we obtain significantly improved learning algorithms for several fundamental high-dimensional probabilistic models with hidden variables. These include density and parameter estimation for $k$ -mixtures of spherical Gaussians (with known common covariance), PAC learning one-hidden-layer ReLU networks with $k$ hidden units (under the Gaussian distribution), density and parameter estimation for $k$ -mixtures of linear regressions (with Gaussian covariates), and parameter estimation for $k$ -mixtures of hyperplanes. Our algorithms run in time quasi-polynomial in the parameter $k$ . Previous algorithms for these problems had running times exponential in $k^{\\Omega(1)}$ . At a high-level our algorithms for all these learning problems work as follows: By computing the low-degree moments of the hidden parameters, we are able to find a vector space of polynomials that nearly vanish on the unknown parameters. Our structural result allows us to compute a quasi-polynomial sized cover for the set of hidden parameters, which we exploit in our learning algorithms.",
                "ieee_keywords": [
                    "Estimation",
                    "Parameter estimation",
                    "Approximation algorithms",
                    "Two dimensional displays",
                    "Method of moments",
                    "Machine learning algorithms",
                    "Computational modeling"
                ],
                "author_keywords": [
                    "component",
                    "machine learning",
                    "style",
                    "styling"
                ]
            },
            {
                "title": "Rapid Approximate Aggregation with Distribution-Sensitive Interval Guarantees",
                "link": "https://ieeexplore.ieee.org/document/9458639/",
                "date_of_publication": "22 June 2021",
                "doi": "10.1109/ICDE51399.2021.00150",
                "citations": "144",
                "abstract": "Aggregating data is fundamental to data analytics, data exploration, and OLAP. Approximate query processing (AQP) techniques are often used to accelerate computation of aggregates using samples, for which confidence intervals (CIs) are widely used to quantify the associated error. CIs used in practice fall into two categories: techniques that are tight but not correct, i.e., they yield tight intervals but only offer asymptoticguarantees,makingthem unreliable, or techniques that are correct but not tight, i.e., they offer rigorous guarantees, but are overly conservative, leading to confidence intervals that are too loose to be useful. In this paper, we develop a CI technique that is both correct and tighter than traditional approaches. Starting from conservative CIs, we identify two issues they often face: pessimistic mass allocation (PMA) and phantom outlier sensitivity (PHOS). By developing a novel range-trimming technique for eliminating PHOS and pairing it with known CI techniques without PMA, we develop a technique for computing CIs with strong guarantees that requires fewer samples for the same width. We implement our techniques underneath a sampling-optimized in-memory column store and show how they accelerate queries involving aggregates on real datasets with typical speedups on the order of 10× over both traditional AQP-with-guarantees and exact methods, all while obeying accuracy constraints.",
                "ieee_keywords": [
                    "Sensitivity",
                    "Data analysis",
                    "Query processing",
                    "Conferences",
                    "Aggregates",
                    "Phantoms",
                    "Data aggregation"
                ],
                "author_keywords": [
                    "approximate query processing",
                    "aqp",
                    "olap"
                ]
            }
        ]
    },
    {
        "name": "Vikas Singh",
        "publications": [
            {
                "title": "Investigating Functional Brain Network Abnormalities via Differential Covariance Trajectory Analysis and Scan Statistics",
                "link": "https://ieeexplore.ieee.org/document/9761442/",
                "date_of_publication": "26 April 2022",
                "doi": "10.1109/ISBI52829.2022.9761442",
                "citations": "79",
                "abstract": "Rs-fMRI has been shown to be a valuable neuroimaging modality to study the pathophysiological mechanisms and effects of Alzheimer’s Disease. However, most existing brain network modeling frameworks for rs-fMRI often do not account for the combined statistical and temporal dependencies underlying dynamic functional connectivity (dFC) in a statistically robust manner, which may be limiting our understanding of altered brain organization in disease. To address these issues, we demonstrate an application of a new framework that characterizes dFC as covariance trajectories on the Riemannian manifold and employs scan statistics as a means to jointly incorporate first- and second-order statistics to localize subsets of features that contribute to group differences. Experimental results demonstrate that our approach is capable of identifying differential effects in large-scale functional networks altered in Alzheimer’s Disease in a way that overcomes statistical challenges common with many neuroimaging studies.",
                "ieee_keywords": [
                    "Neuroimaging",
                    "Manifolds",
                    "Limiting",
                    "Organizations",
                    "Brain modeling",
                    "Trajectory",
                    "Alzheimer's disease"
                ],
                "author_keywords": [
                    "Alzheimer’s Disease",
                    "Scan statistics",
                    "Riemannian manifold",
                    "Resting-state fMRI",
                    "dynamic functional connectivity"
                ]
            },
            {
                "title": "Learning Amyloid Pathology Progression from Longitudinal PIB-PET Images in Preclinical Alzheimer's Disease",
                "link": "https://ieeexplore.ieee.org/document/9098571/",
                "date_of_publication": "22 May 2020",
                "doi": "10.1109/ISBI45749.2020.9098571",
                "citations": "194",
                "abstract": "Amyloid accumulation is acknowledged to be a primary pathological event in Alzheimer's disease (AD). The literature suggests that propagation of amyloid occurs along neural pathways as a function of the disease process (prion-like transmission), but the pattern of spread in the preclinical stages of AD is still poorly understood. Previous studies have used diffusion processes to capture amyloid pathology propagation using various strategies and shown how future time-points can be predicted at the group level using a population-level structural connectivity template. But connectivity could be different between distinct subjects, and the current literature is unable to provide estimates of individual-level pathology propagation. We use a trainable network diffusion model that infers the propagation dynamics of amyloid pathology, conditioned on an individual-level connectivity network. We analyze longitudinal amyloid pathology burden in 16 gray matter (GM) regions known to be affected by AD, measured using Pittsburgh Compound B (PiB) positron emission tomography at 3 different time points for each subject. Experiments show that our model outperforms inference based on group-level trends for predicting future time points data (using individual-level connectivity networks). For group-level analysis, we find parameter differences (via permutation testing) between the models for APOE positive and APOE negative subjects.",
                "ieee_keywords": [
                    "Mathematical model",
                    "Pathology",
                    "Diseases",
                    "Predictive models",
                    "Brain modeling",
                    "Laplace equations",
                    "Market research"
                ],
                "author_keywords": [
                    "Alzheimer's disease",
                    "Network diffusion",
                    "Differential equations",
                    "PiB PET image",
                    "MRI connectivity"
                ]
            },
            {
                "title": "Multi-resolution statistical analysis on graph structured data in neuroimaging",
                "link": "https://ieeexplore.ieee.org/document/7164173/",
                "date_of_publication": "23 July 2015",
                "doi": "10.1109/ISBI.2015.7164173",
                "citations": "93",
                "abstract": "Statistical data analysis plays a major role in discovering structural and functional imaging phenotypes for mental disorders such as Alzheimer's disease (AD). The goal here is to identify, ideally early on, which regions in the brain show abnormal variations with a disorder. To make the method more sensitive, we rely on a multi-resolutional perspective of the given data. Since the underlying imaging data (such as cortical surfaces and connectomes) are naturally represented in the form of weighted graphs which lie in a non-Euclidean space, we introduce recent work from the harmonics literature to derive an effective multi-scale descriptor using wavelets on graphs that characterize the local context at each data point. Using this descriptor, we demonstrate experiments where we identify significant differences between AD and control populations using cortical surface data and tractography derived graphs/networks.",
                "ieee_keywords": [
                    "Wavelet transforms",
                    "Surface waves",
                    "Brain",
                    "Diseases",
                    "Wavelet domain",
                    "Imaging"
                ],
                "author_keywords": [
                    "wavelets",
                    "wavelets on graphs",
                    "cortical thickness",
                    "brain network",
                    "Alzheimer's disease"
                ]
            },
            {
                "title": "Performing Group Difference Testing on Graph Structured Data From GANs: Analysis and Applications in Neuroimaging",
                "link": "https://ieeexplore.ieee.org/document/9162541/",
                "date_of_publication": null,
                "doi": "10.1109/TPAMI.2020.3013433",
                "citations": "1",
                "abstract": "Generative adversarial networks (GANs) have emerged as a powerful generative model in computer vision. Given their impressive abilities in generating highly realistic images, they are also being used in novel ways in applications in the life sciences. This raises an interesting question when GANs are used in scientific or biomedical studies. Consider the setting where we are restricted to only using the samples from a trained GAN for downstream group difference analysis (and do not have direct access to the real data). Will we obtain similar conclusions? In this work, we explore if “generated” data, i.e., sampled from such GANs can be used for performing statistical group difference tests in cases versus controls studies, common across many scientific disciplines. We provide a detailed analysis describing regimes where this may be feasible. We complement the technical results with an empirical study focused on the analysis of cortical thickness on brain mesh surfaces in an Alzheimer's disease dataset. To exploit the geometric nature of the data, we use simple ideas from spectral graph theory to show how adjustments to existing GANs can yield improvements. We also give a generalization error bound by extending recent results on Neural Network Distance. To our knowledge, our work offers the first analysis assessing whether the Null distribution in “healthy versus diseased subjects” type statistical testing using data generated from the GANs coincides with the one obtained from the same analysis with real data. The code is available at https://github.com/yyxiongzju/GLapGAN .",
                "ieee_keywords": [
                    "Gallium nitride",
                    "Statistical analysis",
                    "Diseases",
                    "Machine learning",
                    "Training data",
                    "Three-dimensional displays",
                    "Training"
                ],
                "author_keywords": [
                    "Generative adversarial network",
                    "graph theory",
                    "hypothesis testing",
                    "non-euclidean",
                    "MeSH Terms",
                    "Algorithms",
                    "Brain",
                    "Humans",
                    "Image Processing, Computer-Assisted",
                    "Neural Networks, Computer",
                    "Neuroimaging"
                ]
            },
            {
                "title": "Tensorize, Factorize and Regularize: Robust Visual Relationship Learning",
                "link": "https://ieeexplore.ieee.org/document/8578210/",
                "date_of_publication": "16 December 2018",
                "doi": "10.1109/CVPR.2018.00112",
                "citations": "34",
                "abstract": "Visual relationships provide higher-level information of objects and their relations in an image - this enables a semantic understanding of the scene and helps downstream applications. Given a set of localized objects in some training data, visual relationship detection seeks to detect the most likely \"relationship\" between objects in a given image. While the specific objects may be well represented in training data, their relationships may still be infrequent. The empirical distribution obtained from seeing these relationships in a dataset does not model the underlying distribution well - a serious issue for most learning methods. In this work, we start from a simple multi-relational learning model, which in principle, offers a rich formalization for deriving a strong prior for learning visual relationships. While the inference problem for deriving the regularizer is challenging, our main technical contribution is to show how adapting recent results in numerical linear algebra lead to efficient algorithms for a factorization scheme that yields highly informative priors. The factorization provides sample size bounds for inference (under mild conditions) for the underlying [object, predicate, object] relationship learning task on its own and surprisingly outperforms (in some cases) existing methods even without utilizing visual features. Then, when integrated with an end-to-end architecture for visual relationship detection leveraging image data, we substantially improve the state-of-the-art.",
                "ieee_keywords": [
                    "Visualization",
                    "Task analysis",
                    "Semantics",
                    "Genomics",
                    "Bioinformatics",
                    "Training"
                ],
                "author_keywords": []
            },
            {
                "title": "A Projection Free Method for Generalized Eigenvalue Problem with a Nonsmooth Regularizer",
                "link": "https://ieeexplore.ieee.org/document/7410571/",
                "date_of_publication": "18 February 2016",
                "doi": "10.1109/ICCV.2015.214",
                "citations": "4",
                "abstract": "Eigenvalue problems are ubiquitous in computer vision, covering a very broad spectrum of applications ranging from estimation problems in multi-view geometry to image segmentation. Few other linear algebra problems have a more mature set of numerical routines available and many computer vision libraries leverage such tools extensively. However, the ability to call the underlying solver only as a \"black box\" can often become restrictive. Many 'human in the loop' settings in vision frequently exploit supervision from an expert, to the extent that the user can be considered a subroutine in the overall system. In other cases, there is additional domain knowledge, side or even partial information that one may want to incorporate within the formulation. In general, regularizing a (generalized) eigenvalue problem with such side information remains difficult. Motivated by these needs, this paper presents an optimization scheme to solve generalized eigenvalue problems (GEP) involving a (nonsmooth) regularizer. We start from an alternative formulation of GEP where the feasibility set of the model involves the Stiefel manifold. The core of this paper presents an end to end stochastic optimization scheme for the resultant problem. We show how this general algorithm enables improved statistical analysis of brain imaging data where the regularizer is derived from other 'views' of the disease pathology, involving clinical measurements and other image-derived representations.",
                "ieee_keywords": [
                    "Eigenvalues and eigenfunctions",
                    "Manifolds",
                    "Optimization",
                    "Computer vision",
                    "Kernel",
                    "Shape",
                    "Computers"
                ],
                "author_keywords": []
            },
            {
                "title": "Multivariate General Linear Models (MGLM) on Riemannian Manifolds with Applications to Statistical Analysis of Diffusion Weighted Images",
                "link": "https://ieeexplore.ieee.org/document/6909742/",
                "date_of_publication": "25 September 2014",
                "doi": "10.1109/CVPR.2014.352",
                "citations": "31",
                "abstract": "Linear regression is a parametric model which is ubiquitous in scientific analysis. The classical setup where the observations and responses, i.e., (x i , y i ) pairs, are Euclidean is well studied. The setting where yi is manifold valued is a topic of much interest, motivated by applications in shape analysis, topic modeling, and medical imaging. Recent work gives strategies for max-margin classifiers, principal components analysis, and dictionary learning on certain types of manifolds. For parametric regression specifically, results within the last year provide mechanisms to regress one real-valued parameter, x i ∈ R, against a manifold-valued variable, y i ∈ M. We seek to substantially extend the operating range of such methods by deriving schemes for multivariate multiple linear regression -- a manifold-valued dependent variable against multiple independent variables, i.e., f: ℝ n → M. Our variational algorithm efficiently solves for multiple geodesic bases on the manifold concurrently via gradient updates. This allows us to answer questions such as: what is the relationship of the measurement at voxel y to disease when conditioned on age and gender. We show applications to statistical analysis of diffusion weighted images, which give rise to regression tasks on the manifold GL(n)/O(n) for diffusion tensor images (DTI) and the Hilbert unit sphere for orientation distribution functions (ODF) from high angular resolution acquisition. The companion open-source code is available on nitrc.org/projects/riem_mglm. Notes: As originally published there is an error in the document PDF. The author name-order given on the document is not as intended. The author order was intended to be as follows: \"Hyunwoo J. Kim, Nagesh Adluru, Maxwell D. Collins, Moo K. Chung, Barbara B. Bendlin, Sterling C. Johnson, Richard J. Davidson, Vikas Singh.\" The article PDF remains unchanged as originally published.",
                "ieee_keywords": [
                    "Manifolds",
                    "Vectors",
                    "Shape",
                    "Linear regression",
                    "Diseases",
                    "Computational modeling",
                    "Least squares approximations"
                ],
                "author_keywords": [
                    "Multivariate general linear models",
                    "manifold statistics",
                    "diffusion weighted images",
                    "geodesic regression"
                ]
            },
            {
                "title": "Coupled Harmonic Bases for Longitudinal Characterization of Brain Networks",
                "link": "https://ieeexplore.ieee.org/document/7780645/",
                "date_of_publication": "12 December 2016",
                "doi": "10.1109/CVPR.2016.276",
                "citations": "5",
                "abstract": "There is a great deal of interest in using large scale brain imaging studies to understand how brain connectivity evolves over time for an individual and how it varies over different levels/quantiles of cognitive function. To do so, one typically performs so-called tractography procedures on diffusion MR brain images and derives measures of brain connectivity expressed as graphs. The nodes correspond to distinct brain regions and the edges encode the strength of the connection. The scientific interest is in characterizing the evolution of these graphs over time or from healthy individuals to diseased. We pose this important question in terms of the Laplacian of the connectivity graphs derived from various longitudinal or disease time points - quantifying its progression is then expressed in terms of coupling the harmonic bases of a full set of Laplacians. We derive a coupled system of generalized eigenvalue problems (and corresponding numerical optimization schemes) whose solution helps characterize the full life cycle of brain connectivity evolution in a given dataset. Finally, we show a set of results on a diffusion MR imaging dataset of middle aged people at risk for Alzheimer's disease (AD), who are cognitively healthy. In such asymptomatic adults, we find that a framework for characterizing brain connectivity evolution provides the ability to predict cognitive scores for individual subjects, and for estimating the progression of participant's brain connectivity into the future.",
                "ieee_keywords": [
                    "Laplace equations",
                    "Couplings",
                    "Diseases",
                    "Harmonic analysis",
                    "Imaging",
                    "Eigenvalues and eigenfunctions",
                    "Brain"
                ],
                "author_keywords": []
            },
            {
                "title": "On Statistical Analysis of Neuroimages with Imperfect Registration",
                "link": "https://ieeexplore.ieee.org/document/7410440/",
                "date_of_publication": "18 February 2016",
                "doi": "10.1109/ICCV.2015.83",
                "citations": "103",
                "abstract": "A variety of studies in neuroscience/neuroimaging seek to perform statistical inference on the acquired brain image scans for diagnosis as well as understanding the pathological manifestation of diseases. To do so, an important first step is to register (or co-register) all of the image data into a common coordinate system. This permits meaningful comparison of the intensities at each voxel across groups (e.g., diseased versus healthy) to evaluate the effects of the disease and/or use machine learning algorithms in a subsequent step. But errors in the underlying registration make this problematic, they either decrease the statistical power or make the follow-up inference tasks less effective/accurate. In this paper, we derive a novel algorithm which offers immunity to local errors in the underlying deformation field obtained from registration procedures. By deriving a deformation invariant representation of the image, the downstream analysis can be made more robust as if one had access to a (hypothetical) far superior registration procedure. Our algorithm is based on recent work on Scattering coefficients. Using this as a starting point, we show how results from harmonic analysis (especially, non-Euclidean wavelets) yields strategies for designing deformation and additive noise invariant representations of large 3-D brain image volumes. We present a set of results on synthetic and real brain images where we achieve robust statistical analysis even in the presence of substantial deformation errors, here, standard analysis procedures significantly under-perform and fail to identify the true signal.",
                "ieee_keywords": [
                    "Wavelet transforms",
                    "Scattering",
                    "Diseases",
                    "Brain",
                    "Algorithm design and analysis",
                    "Robustness"
                ],
                "author_keywords": []
            },
            {
                "title": "Topology-Based Kernels With Application to Inference Problems in Alzheimer's Disease",
                "link": "https://ieeexplore.ieee.org/document/5756483/",
                "date_of_publication": null,
                "doi": "10.1109/TMI.2011.2147327",
                "citations": "57",
                "abstract": "Alzheimer's disease (AD) research has recently witnessed a great deal of activity focused on developing new statistical learning tools for automated inference using imaging data. The workhorse for many of these techniques is the support vector machine (SVM) framework (or more generally kernel-based methods). Most of these require, as a first step, specification of a kernel matrix K between input examples (i.e., images). The inner product between images I i and I j in a feature space can generally be written in closed form and so it is convenient to treat K as “given.” However, in certain neuroimaging applications such an assumption becomes problematic. As an example, it is rather challenging to provide a scalar measure of similarity between two instances of highly attributed data such as cortical thickness measures on cortical surfaces. Note that cortical thickness is known to be discriminative for neurological disorders, so leveraging such information in an inference framework, especially within a multi-modal method, is potentially advantageous. But despite being clinically meaningful, relatively few works have successfully exploited this measure for classification or regression. Motivated by these applications, our paper presents novel techniques to compute similarity matrices for such topologically-based attributed data. Our ideas leverage recent developments to characterize signals (e.g., cortical thickness) motivated by the persistence of their topological features, leading to a scheme for simple constructions of kernel matrices. As a proof of principle, on a dataset of 356 subjects from the Alzheimer's Disease Neuroimaging Initiative study, we report good performance on several statistical inference tasks without any feature selection, dimensionality reduction, or parameter tuning.",
                "ieee_keywords": [
                    "Kernel",
                    "Thickness measurement",
                    "Topology",
                    "Alzheimer's disease",
                    "Neuroimaging",
                    "Accuracy",
                    "Surface treatment"
                ],
                "author_keywords": [
                    "Alzheimer's Disease Neuroimaging Initiative (ADNI)",
                    "Alzheimer's disease",
                    "cortical thickness based kernels",
                    "topological persistence",
                    "MeSH Terms",
                    "Alzheimer Disease",
                    "Cerebral Cortex",
                    "Cerebral Cortex",
                    "Databases, Factual",
                    "Fourier Analysis",
                    "Humans",
                    "Image Processing, Computer-Assisted",
                    "Neuroimaging",
                    "ROC Curve",
                    "Regression Analysis",
                    "Support Vector Machines"
                ]
            }
        ]
    },
    {
        "name": "Loris D'Antoni",
        "publications": [
            {
                "title": "Synthesizing Transducers from Complex Specifications",
                "link": "https://ieeexplore.ieee.org/document/10026579/",
                "date_of_publication": "03 February 2023",
                "doi": "10.34727/2022/isbn.978-3-85448-053-2_36",
                "citations": "11",
                "abstract": "Automating string transformations has been a driving application of program synthesis. Existing synthesizers that solve this problem produce programs in domain-specific languages (DSL) that are designed to simplify synthesis and therefore lack nice formal properties. This limitation prevents the synthesized programs from being used in verification applications (e.g., to check complex pre-post conditions) and makes the synthesizers hard to modify due to their reliance on the given DSL. We present a constraint-based approach to synthesizing transducers, a model with strong closure and decidability properties. Our approach handles three types of specifications: input-output (i) examples, (ii) types expressed as regular languages, and (iii) distances that bound how many characters the transducer can modify when processing an input string. Our work is the first to support such complex specifications and it does so by using the algorithmic properties of transducers to generate constraints that can be solved using off-the-shelf SMT solvers. Our synthesis approach can be extended to many transducer models and it can be used, thanks to closure properties of transducers, to compute repairs for partially correct transducers.",
                "ieee_keywords": [
                    "Transducers",
                    "Design automation",
                    "Synthesizers",
                    "Computational modeling",
                    "Maintenance engineering",
                    "DSL",
                    "Domain specific languages"
                ],
                "author_keywords": []
            },
            {
                "title": "HARE: Hardware accelerator for regular expressions",
                "link": "https://ieeexplore.ieee.org/document/7783747/",
                "date_of_publication": "15 December 2016",
                "doi": "10.1109/MICRO.2016.7783747",
                "citations": "41",
                "abstract": "Rapidly processing text data is critical for many technical and business applications. Traditional software-based tools for processing large text corpora use memory bandwidth inefficiently due to software overheads and thus fall far short of peak scan rates possible on modern memory systems. Prior hardware designs generally target I/O rather than memory bandwidth. In this paper, we present HARE, a hardware accelerator for matching regular expressions against large in-memory logs. HARE comprises a stall-free hardware pipeline that scans input data at a fixed rate, examining multiple characters from a single input stream in parallel in a single accelerator clock cycle. We describe a 1GHz 32-character-wide HARE design targeting ASIC implementation that processes data at 32 GB/s — matching modern memory bandwidths. This ASIC design outperforms software solutions by as much as two orders of magnitude. We further demonstrate a scaled-down FPGA proof-of-concept that operates at 100MHz with 4-wide parallelism (400 MB/s). Even at this reduced rate, the prototype outperforms grep by 1.5–20x on commonly used regular expressions.",
                "ieee_keywords": [
                    "Automata",
                    "Hardware",
                    "Pattern matching",
                    "Bandwidth",
                    "Pipelines",
                    "Software",
                    "Business"
                ],
                "author_keywords": [
                    "regular expression matching",
                    "text processing",
                    "finite automata"
                ]
            },
            {
                "title": "Minimization of Symbolic Tree Automata",
                "link": "https://ieeexplore.ieee.org/document/8576396/",
                "date_of_publication": "16 December 2018",
                "doi": null,
                "citations": "29",
                "abstract": "Symbolic tree automata allow transitions to carry predicates over rich alphabet theories, such as linear arithmetic, and therefore extend finite tree automata to operate over infinite alphabets, such as the set of rational numbers. Existing tree automata algorithms rely on the alphabet being finite, and generalizing them to the symbolic setting is not a trivial task. In this paper we study the problem of minimizing symbolic tree automata. First, we formally define and prove the properties of minimality in the symbolic setting. Second, we lift existing minimization algorithms to symbolic tree automata. Third, we present a new algorithm based on the following idea: the problem of minimizing symbolic tree automata can be reduced to the problem of minimizing symbolic (string) automata by encoding the tree structure as part of the alphabet theory. We implement and evaluate all our algorithms against existing implementations and show that the symbolic algorithms scale to large alphabets and can minimize automata over complex alphabet theories.",
                "ieee_keywords": [
                    "Automata",
                    "Minimization",
                    "HTML",
                    "Boolean algebra",
                    "Gold",
                    "Software algorithms",
                    "Software"
                ],
                "author_keywords": []
            },
            {
                "title": "Learning Syntactic Program Transformations from Examples",
                "link": "https://ieeexplore.ieee.org/document/7985680/",
                "date_of_publication": "20 July 2017",
                "doi": "10.1109/ICSE.2017.44",
                "citations": "127",
                "abstract": "Automatic program transformation tools can be valuable for programmers to help them with refactoring tasks, and for Computer Science students in the form of tutoring systems that suggest repairs to programming assignments. However, manually creating catalogs of transformations is complex and time-consuming. In this paper, we present REFAZER, a technique for automatically learning program transformations. REFAZER builds on the observation that code edits performed by developers can be used as input-output examples for learning program transformations. Example edits may share the same structure but involve different variables and subexpressions, which must be generalized in a transformation at the right level of abstraction. To learn transformations, REFAZER leverages state-of-the-art programming-by-example methodology using the following key components: (a) a novel domain-specific language (DSL) for describing program transformations, (b) domain-specific deductive algorithms for efficiently synthesizing transformations in the DSL, and (c) functions for ranking the synthesized transformations. We instantiate and evaluate REFAZER in two domains. First, given examples of code edits used by students to fix incorrect programming assignment submissions, we learn program transformations that can fix other students' submissions with similar faults. In our evaluation conducted on 4 programming tasks performed by 720 students, our technique helped to fix incorrect submissions for 87% of the students. In the second domain, we use repetitive code edits applied by developers to the same project to synthesize a program transformation that applies these edits to other locations in the code. In our evaluation conducted on 56 scenarios of repetitive edits taken from three large C# open-source projects, REFAZER learns the intended program transformation in 84% of the cases using only 2.9 examples on average.",
                "ieee_keywords": [
                    "DSL",
                    "Programming profession",
                    "Tools",
                    "C# languages",
                    "Pattern matching",
                    "Open source software"
                ],
                "author_keywords": [
                    "Program transformation",
                    "program synthesis",
                    "tutoring systems",
                    "refactoring"
                ]
            },
            {
                "title": "TraceDiff: Debugging unexpected code behavior using trace divergences",
                "link": "https://ieeexplore.ieee.org/document/8103457/",
                "date_of_publication": "13 November 2017",
                "doi": "10.1109/VLHCC.2017.8103457",
                "citations": "14",
                "abstract": "Recent advances in program synthesis offer means to automatically debug student submissions and generate personalized feedback in massive programming classrooms. When automatically generating feedback for programming assignments, a key challenge is designing pedagogically useful hints that are as effective as the manual feedback given by teachers. Through an analysis of teachers' hint-giving practices in 132 online Q&A posts, we establish three design guidelines that an effective feedback design should follow. Based on these guidelines, we develop a feedback system that leverages both program synthesis and visualization techniques. Our system compares the dynamic code execution of both incorrect and fixed code and highlights how the error leads to a difference in behavior and where the incorrect code trace diverges from the expected solution. Results from our study suggest that our system enables students to detect and fix bugs that are not caught by students using another existing visual debugging tool.",
                "ieee_keywords": [
                    "Programming",
                    "Visualization",
                    "Computer bugs",
                    "Debugging",
                    "Tools",
                    "Concrete"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Yong Jae Lee",
        "publications": [
            {
                "title": "GLIGEN: Open-Set Grounded Text-to-Image Generation",
                "link": "https://ieeexplore.ieee.org/document/10203593/",
                "date_of_publication": "22 August 2023",
                "doi": "10.1109/CVPR52729.2023.02156",
                "citations": "4",
                "abstract": "Large-scale text-to-image diffusion models have made amazing advances. However, the status quo is to use text input alone, which can impede controllability. In this work, we propose GLIGEN, Grounded-Language-to-Image Generation, a novel approach that builds upon and extends the functionality of existing pre-trained text-to-image diffusion models by enabling them to also be conditioned on grounding inputs. To preserve the vast concept knowledge of the pre-trained model, we freeze all of its weights and inject the grounding information into new trainable layers via a gated mechanism. Our model achieves open-world grounded text2img generation with caption and bounding box condition inputs, and the grounding ability generalizes well to novel spatial configurations and concepts. GLIGEN's zero-shot performance on COCO and LVIS outperforms existing supervised layout-to-image baselines by a large margin.",
                "ieee_keywords": [
                    "Graphics",
                    "Computer vision",
                    "Grounding",
                    "Image edge detection",
                    "Computational modeling",
                    "Training data",
                    "Logic gates"
                ],
                "author_keywords": [
                    "Image and video synthesis and generation"
                ]
            },
            {
                "title": "Towards Universal Fake Image Detectors that Generalize Across Generative Models",
                "link": "https://ieeexplore.ieee.org/document/10204883/",
                "date_of_publication": "22 August 2023",
                "doi": "10.1109/CVPR52729.2023.02345",
                "citations": "Abstract",
                "abstract": "With generative models proliferating at a rapid rate, there is a growing need for general purpose fake image detectors. In this work, we first show that the existing paradigm, which consists of training a deep network for real-vs-fake classification, fails to detect fake images from newer breeds of generative models when trained to detect GAN fake images. Upon analysis, we find that the resulting classifier is asymmetrically tuned to detect patterns that make an image fake. The real class becomes a ‘sink’ class holding anything that is not fake, including generated images from models not accessible during training. Building upon this discovery, we propose to perform real-vs-fake classification without learning; i.e., using a feature space not explicitly trained to distinguish real from fake images. We use nearest neighbor and linear probing as instantiations of this idea. When given access to the feature space of a large pretrained vision-language model, the very simple baseline of nearest neighbor classification has surprisingly good generalization ability in detecting fake images from a wide variety of generative models; e.g., it improves upon the SoTA [50] by +15.07 mAP and +25.90% acc when tested on unseen diffusion and autoregressive models. Our code, models, and data can be found at https://github.com/Yuheng-Li/UniversalFakeDetect",
                "ieee_keywords": [
                    "Training",
                    "Computer vision",
                    "Codes",
                    "Computational modeling",
                    "Buildings",
                    "Detectors",
                    "Generative adversarial networks"
                ],
                "author_keywords": [
                    "Image and video synthesis and generation",
                    "Footnotes",
                    "More Like This",
                    "Recent Advances of Generative Adversarial Networks in Computer Vision",
                    "IEEE Access",
                    "Published: 2019",
                    "Spatiotemporal Generative Adversarial Network-Based Dynamic Texture Synthesis for Surveillance Video Coding",
                    "IEEE Transactions on Circuits and Systems for Video Technology",
                    "Published: 2022",
                    "Show More"
                ]
            },
            {
                "title": "The Two Dimensions of Worst-case Training and Their Integrated Effect for Out-of-domain Generalization",
                "link": "https://ieeexplore.ieee.org/document/9879410/",
                "date_of_publication": "27 September 2022",
                "doi": "10.1109/CVPR52688.2022.00941",
                "citations": "1",
                "abstract": "Training with an emphasis on “hard-to-learn” components of the data has been proven as an effective method to improve the generalization of machine learning models, especially in the settings where robustness (e.g., generalization across distributions) is valued. Existing literature discussing this “hard-to-learn” concept are mainly expanded either along the dimension of the samples or the dimension of the features. In this paper, we aim to introduce a simple view merging these two dimensions, leading to a new, simple yet effective, heuristic to train machine learning models by emphasizing the worst-cases on both the sample and the feature dimensions. We name our method W2D following the concept of “Worst-case along Two Dimensions”. We validate the idea and demonstrate its empirical strength over standard benchmarks.",
                "ieee_keywords": [
                    "Training",
                    "Representation learning",
                    "Computer vision",
                    "Correlation",
                    "Merging",
                    "Force",
                    "Robustness"
                ],
                "author_keywords": [
                    "Self-& semi-& meta- Representation learning"
                ]
            },
            {
                "title": "Collaging Class-specific GANs for Semantic Image Synthesis",
                "link": "https://ieeexplore.ieee.org/document/9710212/",
                "date_of_publication": "28 February 2022",
                "doi": "10.1109/ICCV48922.2021.01415",
                "citations": "13",
                "abstract": "We propose a new approach for high resolution semantic image synthesis. It consists of one base image generator and multiple class-specific generators. The base generator generates high quality images based on a segmentation map. To further improve the quality of different objects, we create a bank of Generative Adversarial Networks (GANs) by separately training class-specific models. This has several benefits including – dedicated weights for each class; centrally aligned data for each model; additional training data from other sources, potential of higher resolution and quality; and easy manipulation of a specific object in the scene. Experiments show that our approach can generate high quality images in high resolution while having flexibility of object-level control by using class-specific generators.",
                "ieee_keywords": [
                    "Training",
                    "Image segmentation",
                    "Computer vision",
                    "Image resolution",
                    "Image synthesis",
                    "Semantics",
                    "Training data"
                ],
                "author_keywords": [
                    "Image and video synthesis"
                ]
            },
            {
                "title": "Learning Customized Visual Models with Retrieval-Augmented Knowledge",
                "link": "https://ieeexplore.ieee.org/document/10205062/",
                "date_of_publication": "22 August 2023",
                "doi": "10.1109/CVPR52729.2023.01454",
                "citations": "1",
                "abstract": "Image-text contrastive learning models such as CLIP have demonstrated strong task transfer ability. The high generality and usability of these visual models is achieved via a web-scale data collection process to ensure broad concept coverage, followed by expensive pre-training to feed all the knowledge into model weights. Alternatively, we propose React,REtrieval-Augmented CusTomization, a framework to acquire the relevant web knowledge to build customized visual models for target domains. We retrieve the most relevant image-text pairs $(\\thicksim3\\%$ of CLIP pre-training data) from the web-scale database as external knowledge and propose to customize the model by only training new modularized blocks while freezing all the original weights. The effectiveness of Reactis demonstrated via extensive experiments on classification, retrieval, detection and segmentation tasks, including zero, few, and full-shot settings. Particularly, on the zero-shot classification task, compared with CLIP, it achieves up to 5.4% improvement on ImageNet and 3.7% on the Elevaterbenchmark (20 datasets).",
                "ieee_keywords": [
                    "Knowledge engineering",
                    "Training",
                    "Visualization",
                    "Computational modeling",
                    "Semantic segmentation",
                    "Visual systems",
                    "Data models"
                ],
                "author_keywords": [
                    "Vision",
                    "language",
                    "and reasoning"
                ]
            },
            {
                "title": "Generalized Decoding for Pixel, Image, and Language",
                "link": "https://ieeexplore.ieee.org/document/10203730/",
                "date_of_publication": "22 August 2023",
                "doi": "10.1109/CVPR52729.2023.01451",
                "citations": "6",
                "abstract": "We present X-Decoder, a generalized decoding model that can predict pixel-level segmentation and language tokens seamlessly. X-Decoder takes as input two types of queries: (i) generic non-semantic queries and (ii) semantic queries induced from text inputs, to decode different pixel-level and token-level outputs in the same semantic space. With such a novel design, X-Decoder is the first work that provides a unified way to support all types of image segmentation and a variety of vision-language (VL) tasks. Without any pseudo-labeling, our design enables seamless interactions across tasks at different granularities and brings mutual benefits by learning a common and rich pixel-level understanding. After pretraining on a mixed set of a limited amount of segmentation data and millions of image-text pairs, X-Decoder exhibits strong transferability to a wide range of downstream tasks in both zero-shot and finetuning settings. Notably, it achieves (1) state-of-the-art results on open-vocabulary segmentation and referring segmentation on seven datasets; (2) better or competitive finetuned performance to other generalist and specialist models on segmentation and VL tasks; and (3) flexibility for efficient fine-tuning and novel task composition (e.g., referring captioning and image editing shown in Fig. 1). Code, demo, video and visualization are available at: https://x-decoder-vl.github.io.",
                "ieee_keywords": [
                    "Image segmentation",
                    "Computer vision",
                    "Machine vision",
                    "Computational modeling",
                    "Semantics",
                    "Predictive models",
                    "Decoding"
                ],
                "author_keywords": [
                    "Vision",
                    "language",
                    "and reasoning"
                ]
            },
            {
                "title": "Seeing the Unseen: Predicting the First-Person Camera Wearer’s Location and Pose in Third-Person Scenes",
                "link": "https://ieeexplore.ieee.org/document/9607539/",
                "date_of_publication": "24 November 2021",
                "doi": "10.1109/ICCVW54120.2021.00384",
                "citations": "1",
                "abstract": "Our goal is to predict the camera wearer’s location and pose in his/her environment based on what’s captured by the camera wearer’s first-person wearable camera. Toward this goal, we first collect a new dataset in which the camera wearer performs various activities (e.g., opening a fridge, reading a book) in different scenes with time-synchronized first-person and stationary third-person cameras. We then propose a novel deep network architecture, which takes as input the first-person video frames and empty third-person scene image (without the camera wearer) to predict the location and pose of the camera wearer. We explore and compare our approach with several intuitive baselines and show initial promising results on this novel, challenging problem.",
                "ieee_keywords": [
                    "Computer vision",
                    "Conferences",
                    "Network architecture",
                    "Cameras",
                    "Task analysis"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Bilge Mutlu",
        "publications": [
            {
                "title": "Characterizing the Effects of Haptic Rendering Parameter Variations on Perceived Kinesthetic Rendering Accuracy",
                "link": "https://ieeexplore.ieee.org/document/9517158/",
                "date_of_publication": "23 August 2021",
                "doi": "10.1109/WHC49131.2021.9517158",
                "citations": "1",
                "abstract": "To understand how the realism of a kinesthetic haptic rendering is affected by the accurate selection of the rendering model parameters, we conducted a preliminary user study where subjects compared three real-world objects to their equivalent haptic rendering. The subjects rated the rendering realism as the model parameters were varied about their nominal values. The results suggest that the required accuracy of various haptic rendering parameters is not equally important when considering the perceived realism.",
                "ieee_keywords": [
                    "Conferences",
                    "Rendering (computer graphics)",
                    "Haptic interfaces"
                ],
                "author_keywords": []
            },
            {
                "title": "Supplementary Material for Characterizing Input Methods for Human-to-Robot Demonstrations",
                "link": "https://ieeexplore.ieee.org/document/8673328/",
                "date_of_publication": "25 March 2019",
                "doi": "10.1109/HRI.2019.8673328",
                "citations": "1",
                "abstract": "In this section, we discuss some extensions of Section III and expand on the limitations mentioned in Section VI of the main article.",
                "ieee_keywords": [
                    "Robots",
                    "Instruments",
                    "Grippers",
                    "Force sensors",
                    "Task analysis",
                    "Automation"
                ],
                "author_keywords": []
            },
            {
                "title": "Assessing the Perceived Realism of Kinesthetic Haptic Renderings Under Parameter Variations",
                "link": "https://ieeexplore.ieee.org/document/9765610/",
                "date_of_publication": "05 May 2022",
                "doi": "10.1109/HAPTICS52432.2022.9765610",
                "citations": "186",
                "abstract": "Despite the large amount of research on kinesthetic haptic devices and haptic effect modeling, there is limited work assessing the perceived realism of kinesthetic model renderings. Identifying the impact of haptic effect parameters in perceived realism can help to inform the required accuracy of kinesthetic renderings. In this work, we model common kinesthetic haptic effects and evaluate the perceived realism of varying model parameters via a user study. Our results suggest that parameter accuracy requirements to achieve realistic ratings vary depending on the specific haptic parameter.",
                "ieee_keywords": [
                    "Sensitivity",
                    "Atmospheric measurements",
                    "Rendering (computer graphics)",
                    "Particle measurements",
                    "Haptic interfaces"
                ],
                "author_keywords": []
            },
            {
                "title": "Computational Tools for Human-Robot Interaction Design",
                "link": "https://ieeexplore.ieee.org/document/8673221/",
                "date_of_publication": "25 March 2019",
                "doi": "10.1109/HRI.2019.8673221",
                "citations": "8",
                "abstract": "Robots must exercise socially appropriate behavior when interacting with humans. How can we assist interaction designers to embed socially appropriate and avoid socially inappropriate behavior within human-robot interactions? We propose a multi-faceted interaction-design approach that intersects human-robot interaction and formal methods to help us achieve this goal. At the lowest level, designers create interactions from scratch and receive feedback from formal verification, while higher levels involve automated synthesis and repair of designs. In this extended abstract, we discuss past, present, and future work within each level of our design approach.",
                "ieee_keywords": [
                    "Robots",
                    "Human-robot interaction",
                    "Maintenance engineering",
                    "Programming",
                    "Task analysis",
                    "Design methodology",
                    "Electric breakdown"
                ],
                "author_keywords": [
                    "Interaction Design",
                    "Formal Methods"
                ]
            },
            {
                "title": "Recognizing Orientation Slip in Human Demonstrations",
                "link": "https://ieeexplore.ieee.org/document/9561856/",
                "date_of_publication": "18 October 2021",
                "doi": "10.1109/ICRA48506.2021.9561856",
                "citations": "87",
                "abstract": "Manipulations of a constrained object often use a non-rigid grasp that allows the object to rotate relative to the end effector. This orientation slip strategy is often present in natural human demonstrations, yet it is generally overlooked in methods to identify constraints from such demonstrations. In this paper, we present a method to model and recognize prehensile orientation slip in human demonstrations of constrained interactions. Using only observations of an end effector, we can detect the type of constraint, parameters of the constraint, and orientation slip properties. Our method uses a novel hierarchical model selection method that is informed by multiple origins of physics-based evidence. A study with eight participants shows that orientation slip occurs in natural demonstrations and confirms that it can be detected by our method.",
                "ieee_keywords": [
                    "Automation",
                    "Conferences",
                    "End effectors"
                ],
                "author_keywords": []
            },
            {
                "title": "Understanding Control Frames in Multi-Camera Robot Telemanipulation",
                "link": "https://ieeexplore.ieee.org/document/9889543/",
                "date_of_publication": "29 September 2022",
                "doi": "10.1109/HRI53351.2022.9889543",
                "citations": "64",
                "abstract": "In telemanipulation, showing the user multiple views of the remote environment can offer many benefits, although such different views can also create a problem for control. Systems must either choose a single fixed control frame, aligned with at most one of the views or switch between view-aligned control frames, enabling view-aligned control at the expense of switching costs. In this paper, we explore the trade-off between these options. We study the feasibility, benefits, and drawbacks of switching the user's control frame to align with the actively used view during telemanipulation. We additionally explore the effectiveness of explicit and implicit methods for switching control frames. Our results show that switching between multiple view-specific control frames offers significant performance gains compared to a fixed control frame. We also find personal preferences for explicit or implicit switching based on how participants planned their movements. Our findings offer concrete design guidelines for future multi-camera interfaces.",
                "ieee_keywords": [
                    "Costs",
                    "Design methodology",
                    "Switches",
                    "Performance gain",
                    "Control systems",
                    "Robots"
                ],
                "author_keywords": [
                    "telemanipulation",
                    "camera",
                    "awareness",
                    "control frame",
                    "multiple views",
                    "operator interfaces"
                ]
            },
            {
                "title": "A Method For Automated Drone Viewpoints to Support Remote Robot Manipulation",
                "link": "https://ieeexplore.ieee.org/document/9982063/",
                "date_of_publication": "26 December 2022",
                "doi": "10.1109/IROS47612.2022.9982063",
                "citations": "102",
                "abstract": "Drones can provide a minimally-constrained adapting camera view to support robot telemanipulation. Furthermore, the drone view can be automated to reduce the burden on the operator during teleoperation. However, existing approaches do not focus on two important aspects of using a drone as an automated view provider. The first is how the drone should select from a range of quality viewpoints within the workspace (e.g., opposite sides of an object). The second is how to compensate for unavoidable drone pose uncertainty in determining the viewpoint. In this paper, we provide a nonlinear optimization method that yields effective and adaptive drone viewpoints for telemanipulation with an articulated manipulator. Our first key idea is to use sparse human-in-the-loop input to toggle between multiple automatically-generated drone viewpoints. Our second key idea is to introduce optimization objectives that maintain a view of the manipulator while considering drone uncertainty and the impact on viewpoint occlusion and environment collisions. We provide an instantiation of our drone viewpoint method within a drone-manipulator remote teleoperation system. Finally, we provide an initial validation of our method in tasks where we complete common household and industrial manipulations.",
                "ieee_keywords": [
                    "Uncertainty",
                    "Service robots",
                    "Robot vision systems",
                    "Optimization methods",
                    "Manipulators",
                    "Human in the loop",
                    "Task analysis"
                ],
                "author_keywords": []
            },
            {
                "title": "Handheld Haptic Device with Coupled Bidirectional Input",
                "link": "https://ieeexplore.ieee.org/document/10224398/",
                "date_of_publication": "25 August 2023",
                "doi": "10.1109/WHC56415.2023.10224398",
                "citations": "1",
                "abstract": "Handheld kinesthetic haptic interfaces can provide greater mobility and richer tactile information as compared to traditional grounded devices. In this paper, we introduce a new handheld haptic interface which takes input using bidirectional coupled finger flexion. We present the device design motivation and design details and experimentally evaluate its performance in terms of transparency and rendering bandwidth using a handheld prototype device. In addition, we assess the device’s functional performance through a user study comparing the proposed device to a commonly used grounded input device in a set of targeting and tracking tasks.",
                "ieee_keywords": [
                    "Performance evaluation",
                    "Target tracking",
                    "Service robots",
                    "Prototypes",
                    "Process control",
                    "Bandwidth",
                    "Rendering (computer graphics)"
                ],
                "author_keywords": [
                    "Handheld",
                    "haptic device",
                    "mobile",
                    "finger flexion",
                    "bidirectional",
                    "high performance",
                    "haptic feedback"
                ]
            },
            {
                "title": "Supporting Perception of Weight through Motion-induced Sensory Conflicts in Robot Teleoperation",
                "link": "https://ieeexplore.ieee.org/document/9484207/",
                "date_of_publication": "21 July 2021",
                "doi": null,
                "citations": "35",
                "abstract": "In this paper, we design and evaluate a novel form of visually-simulated haptic feedback cue for communicating weight in robot teleoperation. We propose that a visuo-proprioceptive cue results from inconsistencies created between the user’s visual and proprioceptive senses when the robot’s movement differs from the movement of the user’s input. In a user study where participants teleoperate a six-DoF robot arm, we demonstrate the feasibility of using such a cue for communicating weight in four telemanipulation tasks to enhance user experience and task performance. CCS CONCEPTS • Human-centered computing → Interaction paradigms; • Computer systems organization → Robotics. ACM Reference Format: Pragathi Praveena, Daniel Rakita, Bilge Mutlu, and Michael Gleicher. 2020. Supporting Perception of Weight through Motion-induced Sensory Conflicts in Robot Teleoperation. In Proceedings of the 2020 ACM/IEEE International Conference on Human-Robot Interaction (HRI ’20), March 23–26, 2020, Cambridge, United Kingdom. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3319502.3374841",
                "ieee_keywords": [
                    "Human computer interaction",
                    "Visualization",
                    "Human-robot interaction",
                    "Organizations",
                    "Robot sensing systems",
                    "Manipulators",
                    "User experience"
                ],
                "author_keywords": [
                    "Communicative motion",
                    "visuo-proprioceptive cues",
                    "pseudohaptics"
                ]
            },
            {
                "title": "Informing Real-Time Corrections in Corrective Shared Autonomy Through Expert Demonstrations",
                "link": "https://ieeexplore.ieee.org/document/9472948/",
                "date_of_publication": null,
                "doi": "10.1109/LRA.2021.3094480",
                "citations": "4",
                "abstract": "Corrective Shared Autonomy is a method where human corrections are layered on top of an otherwise autonomous robot behavior. Specifically, a Corrective Shared Autonomy system leverages an external controller to allow corrections across a range of task variables (e.g., spinning speed of a tool, applied force, path) to address the specific needs of a task. However, this inherent flexibility makes the choice of what corrections to allow at any given instant difficult to determine. This choice of corrections includes determining appropriate robot state variables, scaling for these variables, and a way to allow a user to specify the corrections in an intuitive manner. This letter enables efficient Corrective Shared Autonomy by providing an automated solution based on Learning from Demonstration to both extract the nominal behavior and address these core problems. Our evaluation shows that this solution enables users to successfully complete a surface cleaning task, identifies different strategies users employed in applying corrections, and points to future improvements for our solution.",
                "ieee_keywords": [
                    "Robots",
                    "Robot kinematics",
                    "Task analysis",
                    "Force",
                    "Surface cleaning",
                    "Real-time systems",
                    "Trajectory"
                ],
                "author_keywords": [
                    "Human-Robot Collaboration",
                    "Telerobotics and Teleoperation",
                    "Learning from Demonstration"
                ]
            }
        ]
    },
    {
        "name": "Aws Albarghouthi",
        "publications": [
            {
                "title": "Computational Tools for Human-Robot Interaction Design",
                "link": "https://ieeexplore.ieee.org/document/8673221/",
                "date_of_publication": "25 March 2019",
                "doi": "10.1109/HRI.2019.8673221",
                "citations": "8",
                "abstract": "Robots must exercise socially appropriate behavior when interacting with humans. How can we assist interaction designers to embed socially appropriate and avoid socially inappropriate behavior within human-robot interactions? We propose a multi-faceted interaction-design approach that intersects human-robot interaction and formal methods to help us achieve this goal. At the lowest level, designers create interactions from scratch and receive feedback from formal verification, while higher levels involve automated synthesis and repair of designs. In this extended abstract, we discuss past, present, and future work within each level of our design approach.",
                "ieee_keywords": [
                    "Robots",
                    "Human-robot interaction",
                    "Maintenance engineering",
                    "Programming",
                    "Task analysis",
                    "Design methodology",
                    "Electric breakdown"
                ],
                "author_keywords": [
                    "Interaction Design",
                    "Formal Methods"
                ]
            },
            {
                "title": "Qubit Mapping and Routing via MaxSAT",
                "link": "https://ieeexplore.ieee.org/document/9923822/",
                "date_of_publication": "26 October 2022",
                "doi": "10.1109/MICRO56248.2022.00077",
                "citations": "275",
                "abstract": "Near-term quantum computers will operate in a noisy environment, without error correction. A critical problem for near-term quantum computing is laying out a logical circuit onto a physical device with limited connectivity between qubits. This is known as the qubit mapping and routing (QMR) problem, an intractable combinatorial problem. It is important to solve QMR as optimally as possible to reduce the amount of added noise, which may render a quantum computation useless. In this paper, we present a novel approach for optimally solving the QMR problem via a reduction to maximum satisfiability (MAXSAT). Additionally, we present two novel relaxation ideas that shrink the size of the MAXSAT constraints by exploiting the structure of a quantum circuit. Our thorough empirical evaluation demonstrates (1) the scalability of our approach compared to state-of-the-art optimal QMR techniques (solves more than 3x benchmarks with 40x speedup), (2) the significant cost reduction compared to state-of-the-art heuristic approaches (an average of ~5x swap reduction), and (3) the power of our proposed constraint relaxations.",
                "ieee_keywords": [
                    "Computers",
                    "Microarchitecture",
                    "Costs",
                    "Scalability",
                    "Qubit",
                    "Benchmark testing",
                    "Routing"
                ],
                "author_keywords": [
                    "quantum computing",
                    "qubit mapping"
                ]
            },
            {
                "title": "Semantic Robustness of Models of Source Code",
                "link": "https://ieeexplore.ieee.org/document/9825895/",
                "date_of_publication": "21 July 2022",
                "doi": "10.1109/SANER53432.2022.00070",
                "citations": "3",
                "abstract": "Deep neural networks are vulnerable to adversarial examples-small input perturbations that result in incorrect predictions. We study this problem for models of source code, where we want the neural network to be robust to source-code modifications that preserve code functionality. To facilitate training robust models, we define a powerful and generic adversary that can employ sequences of parametric, semantics-preserving program transformations. We then explore how, with such an adversary, one can train models that are robust to adversarial program transformations. We conduct a thorough evaluation of our approach and find several surprising facts: we find robust training to beat dataset augmentation in every evaluation we performed; we find that a state-of-the-art architecture (code2seq) for models of code is harder to make robust than a simpler baseline; additionally, we find code2seq to have surprising weaknesses not present in our simpler baseline model; finally, we find that robust models perform better against unseen data from different sources (as one might hope)-however, we also find that robust models are not clearly better in the cross-language transfer task. To the best of our knowledge, we are the first to study the interplay between robustness of models of code and the domain-adaptation and cross-language- transfer tasks.",
                "ieee_keywords": [
                    "Training",
                    "Codes",
                    "Perturbation methods",
                    "Neural networks",
                    "Semantics",
                    "Documentation",
                    "Data models"
                ],
                "author_keywords": [
                    "Robust training",
                    "Learning from source code",
                    "Semantics preserving transformations"
                ]
            },
            {
                "title": "Learning Differentially Private Mechanisms",
                "link": "https://ieeexplore.ieee.org/document/9519410/",
                "date_of_publication": "26 August 2021",
                "doi": "10.1109/SP40001.2021.00060",
                "citations": "11",
                "abstract": "Differential privacy is a formal, mathematical definition of data privacy that has gained traction in academia, industry, and government. The task of correctly constructing differentially private algorithms is non-trivial, and mistakes have been made in foundational algorithms. Currently, there is no automated support for converting an existing, non-private program into a differentially private version. In this paper, we propose a technique for automatically learning an accurate and differentially private version of a given non-private program. We show how to solve this difficult program synthesis problem via a combination of techniques: carefully picking representative example inputs, reducing the problem to continuous optimization, and mapping the results back to symbolic expressions. We demonstrate that our approach is able to learn foundational algorithms from the differential privacy literature and significantly outperforms natural program synthesis baselines.",
                "ieee_keywords": [
                    "Industries",
                    "Differential privacy",
                    "Privacy",
                    "Government",
                    "Security",
                    "Task analysis",
                    "Optimization"
                ],
                "author_keywords": [
                    "Differential-Privacy",
                    "Program-Synthesis"
                ]
            },
            {
                "title": "Backdoors in Neural Models of Source Code",
                "link": "https://ieeexplore.ieee.org/document/9956690/",
                "date_of_publication": "29 November 2022",
                "doi": "10.1109/ICPR56361.2022.9956690",
                "citations": "116",
                "abstract": "Deep neural networks are vulnerable to a range of adversaries. A particularly pernicious class of vulnerabilities are backdoors, where model predictions diverge in the presence of subtle triggers in inputs. An attacker can implant a backdoor by poisoning the training data to yield a desired target prediction on triggered inputs. We study backdoors in the context of deep-learning for source code. (1) We define a range of backdoor classes for source-code tasks and install backdoors using dataset poisoning. (2) We adapt and improve recent algorithms from robust statistics for our setting, showing that backdoors leave a spectral signature in the learned representation of source code, thus enabling detection of poisoned data. (3) We conduct a thorough evaluation on different architectures and languages, showing the ease of injecting backdoors and our ability to eliminate them.",
                "ieee_keywords": [
                    "Deep learning",
                    "Codes",
                    "Source coding",
                    "Neural networks",
                    "Training data",
                    "Implants",
                    "Predictive models"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Yin Li",
        "publications": [
            {
                "title": "Germanium photodiodes on pyramidal textured surface by Metal-Assisted Chemical Etching",
                "link": "https://ieeexplore.ieee.org/document/8749589/",
                "date_of_publication": "01 July 2019",
                "doi": "10.1364/CLEO_AT.2019.JTh2A.31",
                "citations": "78",
                "abstract": "We demonstrate a Ge photodiode on pyramidal textured surface by Metal-Assisted Chemical Etching (MacEtch) technique. This photodiode shows both reduced dark current and enhanced responsivity at near infrared (NIR) wavelength ranges. © 2019 The Author(s)",
                "ieee_keywords": [
                    "Germanium",
                    "Photodiodes",
                    "Surface treatment",
                    "Surface texture",
                    "Surface waves",
                    "Gold",
                    "Dark current"
                ],
                "author_keywords": []
            },
            {
                "title": "Physics to the Rescue: Deep Non-Line-of-Sight Reconstruction for High-Speed Imaging",
                "link": "https://ieeexplore.ieee.org/document/9874257/",
                "date_of_publication": null,
                "doi": "10.1109/TPAMI.2022.3203383",
                "citations": "2",
                "abstract": "Computational approach to imaging around the corner, or non-line-of-sight (NLOS) imaging, is becoming a reality thanks to major advances in imaging hardware and reconstruction algorithms. A recent development towards practical NLOS imaging, Nam et al. [1] demonstrated a high-speed non-confocal imaging system that operates at 5 Hz, 100x faster than the prior art. This enormous gain in acquisition rate, however, necessitates numerous approximations in light transport, breaking many existing NLOS reconstruction methods that assume an idealized image formation model. To bridge the gap, we present a novel deep model that incorporates the complementary physics priors of wave propagation and volume rendering into a neural network for high-quality and robust NLOS reconstruction. This orchestrated design regularizes the solution space by relaxing the image formation model, resulting in a deep model that generalizes well on real captures despite being exclusively trained on synthetic data. Further, we devise a unified learning framework that enables our model to be flexibly trained using diverse supervision signals, including target intensity images or even raw NLOS transient measurements. Once trained, our model renders both intensity and depth images at inference time in a single forward pass, capable of processing more than 5 captures per second on a high-end GPU. Through extensive qualitative and quantitative experiments, we show that our method outperforms prior physics and learning based approaches on both synthetic and real measurements. We anticipate that our method along with the fast capturing system will accelerate future development of NLOS imaging for real world applications that require high-speed imaging.",
                "ieee_keywords": [
                    "Imaging",
                    "Image reconstruction",
                    "Nonlinear optics",
                    "Physics",
                    "Transient analysis",
                    "Relays",
                    "Measurement by laser beam"
                ],
                "author_keywords": [
                    "Neural radiance field",
                    "non-line-of-sight imaging",
                    "physics-inspired deep model",
                    "non-line-of-sight reconstruction"
                ]
            },
            {
                "title": "Dual-stream Multiple Instance Learning Network for Whole Slide Image Classification with Self-supervised Contrastive Learning",
                "link": "https://ieeexplore.ieee.org/document/9578683/",
                "date_of_publication": "02 November 2021",
                "doi": "10.1109/CVPR46437.2021.01409",
                "citations": "98",
                "abstract": "We address the challenging problem of whole slide image (WSI) classification. WSIs have very high resolutions and usually lack localized annotations. WSI classification can be cast as a multiple instance learning (MIL) problem when only slide-level labels are available. We propose a MIL-based method for WSI classification and tumor detection that does not require localized annotations. Our method has three major components. First, we introduce a novel MIL aggregator that models the relations of the instances in a dual-stream architecture with trainable distance measurement. Second, since WSIs can produce large or unbalanced bags that hinder the training of MIL models, we propose to use self-supervised contrastive learning to extract good representations for MIL and alleviate the issue of prohibitive memory cost for large bags. Third, we adopt a pyramidal fusion mechanism for multiscale WSI features, and further improve the accuracy of classification and localization. Our model is evaluated on two representative WSI datasets. The classification accuracy of our model compares favorably to fully-supervised methods, with less than 2% accuracy gap across datasets. Our results also outperform all previous MIL-based methods. Additional benchmark results on standard MIL datasets further demonstrate the superior performance of our MIL aggregator on general MIL problems.",
                "ieee_keywords": [
                    "Training",
                    "Location awareness",
                    "Image resolution",
                    "Annotations",
                    "Feature extraction",
                    "Distance measurement",
                    "Pattern recognition"
                ],
                "author_keywords": []
            },
            {
                "title": "uGEMM: Unary Computing for GEMM Applications",
                "link": "https://ieeexplore.ieee.org/document/9376243/",
                "date_of_publication": null,
                "doi": "10.1109/MM.2021.3065369",
                "citations": "4",
                "abstract": "General matrix multiplication (GEMM) is pervasive in various domains, such as signal processing, computer vision, and machine learning. Conventional binary architectures for GEMM exhibit poor scalability in area and energy efficiency, due to the spatial nature of number representation and computing. On the contrary, unary computing processes data in temporal domain with extremely simple logic. However, to date, there rarely exist efficient architectures for unary GEMM. In this work, we first present uGEMM, a hardware-efficient unary GEMM architecture enabled by universally compatible arithmetic units, which simultaneously achieves input-insensitivity and high output accuracy. Next, we demonstrate that the proposed uGEMM can reliably early terminate the computation and offers dynamic energy-accuracy scaling for real-world applications via an accuracy-aware metric. Finally, to propel the future research for unary computing, we open source our unary computing simulator, UnarySim.",
                "ieee_keywords": [
                    "Logic gates",
                    "Computer architecture",
                    "Encoding",
                    "Computer vision",
                    "Microarchitecture",
                    "Generators",
                    "Energy efficiency",
                    "Signal processing"
                ],
                "author_keywords": [
                    "Unary computing",
                    "low power computer architecture",
                    "stochastic computing G (GEMM) is ubiquitous and essential in many applications",
                    "particularly emerging deep learning"
                ]
            },
            {
                "title": "UGEMM: Unary Computing Architecture for GEMM Applications",
                "link": "https://ieeexplore.ieee.org/document/9139000/",
                "date_of_publication": "13 July 2020",
                "doi": "10.1109/ISCA45697.2020.00040",
                "citations": "27",
                "abstract": "General matrix multiplication (GEMM) is universal in various applications, such as signal processing, machine learning, and computer vision. Conventional GEMM hardware architectures based on binary computing exhibit low area and energy efficiency as they scale due to the spatial nature of number representation and computing. Unary computing, on the other hand, can be performed with extremely simple processing units, often just with a single logic gate. But currently there exist no efficient architectures for unary GEMM. In this paper, we present uGEMM, an area- and energy-efficient unary GEMM architecture enabled by novel arithmetic units. The proposed design relaxes previously-imposed constraints on input bit streams-low correlation and long stream length- and achieves superior area and energy efficiency over existing unary systems. Furthermore, uGEMM's output bit streams exhibit higher accuracy and faster convergence, enabling dynamic energy-accuracy scaling on resource-constrained systems.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Gaze-enabled egocentric video summarization via constrained submodular maximization",
                "link": "https://ieeexplore.ieee.org/document/7298836/",
                "date_of_publication": "15 October 2015",
                "doi": "10.1109/CVPR.2015.7298836",
                "citations": "107",
                "abstract": "With the proliferation of wearable cameras, the number of videos of users documenting their personal lives using such devices is rapidly increasing. Since such videos may span hours, there is an important need for mechanisms that represent the information content in a compact form (i.e., shorter videos which are more easily browsable/sharable). Motivated by these applications, this paper focuses on the problem of egocentric video summarization. Such videos are usually continuous with significant camera shake and other quality issues. Because of these reasons, there is growing consensus that direct application of standard video summarization tools to such data yields unsatisfactory performance. In this paper, we demonstrate that using gaze tracking information (such as fixation and saccade) significantly helps the summarization task. It allows meaningful comparison of different image frames and enables deriving personalized summaries (gaze provides a sense of the camera wearer's intent). We formulate a summarization model which captures common-sense properties of a good summary, and show that it can be solved as a submodular function maximization with partition matroid constraints, opening the door to a rich body of work from combinatorial optimization. We evaluate our approach on a new gaze-enabled egocentric video dataset (over 15 hours), which will be a valuable standalone resource.",
                "ieee_keywords": [
                    "Feature extraction",
                    "Optimization",
                    "Cameras",
                    "Gaze tracking",
                    "Mutual information",
                    "Linear programming",
                    "Approximation methods"
                ],
                "author_keywords": []
            },
            {
                "title": "Robust Scene Inference under Noise-Blur Dual Corruptions",
                "link": "https://ieeexplore.ieee.org/document/9887772/",
                "date_of_publication": "26 September 2022",
                "doi": "10.1109/ICCP54855.2022.9887772",
                "citations": "164",
                "abstract": "Scene inference under low-light is a challenging problem due to severe noise in the captured images. One way to reduce noise is to use longer exposure during the capture. However, in the presence of motion (scene or camera motion), longer exposures lead to motion blur, resulting in loss of image information. This creates a trade-off between these two kinds of image degradations: motion blur (due to long exposure) vs. noise (due to short exposure), also referred as a dual image corruption pair in this paper. With the rise of cameras capable of capturing multiple exposures of the same scene simultaneously, it is possible to overcome this trade-off. Our key observation is that although the amount and nature of degradation varies for these different image captures, the semantic content remains the same across all images. To this end, we propose a method to leverage these multi exposure captures for robust inference under low-light and motion. Our method builds on a feature consistency loss to encourage similar results from these individual captures, and uses the ensemble of their final predictions for robust visual recognition. We demonstrate the effectiveness of our approach on simulated images as well as real captures with multiple exposures, and across the tasks of object detection and image classification. Project: https://wisionlab.com/project/noiseblurdual",
                "ieee_keywords": [
                    "Degradation",
                    "Photography",
                    "Visualization",
                    "Semantics",
                    "Object detection",
                    "Image capture",
                    "Cameras"
                ],
                "author_keywords": [
                    "Low Light",
                    "Motion Blur",
                    "Scene Inference",
                    "Object Detection",
                    "Image Classification"
                ]
            },
            {
                "title": "RegionCLIP: Region-based Language-Image Pretraining",
                "link": "https://ieeexplore.ieee.org/document/9878561/",
                "date_of_publication": "27 September 2022",
                "doi": "10.1109/CVPR52688.2022.01629",
                "citations": "38",
                "abstract": "Contrastive language-image pretraining (CLIP) using image-text pairs has achieved impressive results on image classification in both zero-shot and transfer learning set-tings. However, we show that directly applying such mod-els to recognize image regions for object detection leads to unsatisfactory performance due to a major domain shift: CLIP was trained to match an image as a whole to a text de-scription, without capturing the fine-grained alignment be-tween image regions and text spans. To mitigate this issue, we propose a new method called RegionCLIP that signifi-cantly extends CLIP to learn region-level visual representations, thus enabling fine-grained alignment between image regions and textual concepts. Our method leverages a CLIP model to match image regions with template captions, and then pretrains our model to align these region-text pairs in the feature space. When transferring our pretrained model to the open-vocabulary object detection task, our method outperforms the state of the art by 3.8 AP50 and 2.2 AP for novel categories on COCO and LVIS datasets, respectively. Further, the learned region representations support zero-shot inference for object detection, showing promising results on both COCO and LVIS datasets. Our code is available at https://github.com/microsoft/RegionCLIP.",
                "ieee_keywords": [
                    "Representation learning",
                    "Visualization",
                    "Technological innovation",
                    "Image recognition",
                    "Text recognition",
                    "Transfer learning",
                    "Object detection"
                ],
                "author_keywords": [
                    "Representation learning; Recognition: detection",
                    "categorization",
                    "retrieval; Self-& semi-& meta- Transfer/low-shot/long-tail learning; Vision + language"
                ]
            },
            {
                "title": "Interpretable and Accurate Fine-grained Recognition via Region Grouping",
                "link": "https://ieeexplore.ieee.org/document/9157365/",
                "date_of_publication": "05 August 2020",
                "doi": "10.1109/CVPR42600.2020.00869",
                "citations": "50",
                "abstract": "We present an interpretable deep model for fine-grained visual recognition. At the core of our method lies the integration of region-based part discovery and attribution within a deep neural network. Our model is trained using image-level object labels, and provides an interpretation of its results via the segmentation of object parts and the identification of their contributions towards classification. To facilitate the learning of object parts without direct supervision, we explore a simple prior of the occurrence of object parts. We demonstrate that this prior, when combined with our region-based part discovery and attribution, leads to an interpretable model that remains highly accurate. Our model is evaluated on major fine-grained recognition datasets, including CUB-200, CelebA and iNaturalist. Our results compares favourably to state-of-the-art methods on classification tasks, and outperforms previous approaches on the localization of object parts.",
                "ieee_keywords": [
                    "Birds",
                    "Visualization",
                    "Image segmentation",
                    "Dictionaries",
                    "Two dimensional displays",
                    "Pattern recognition",
                    "Object recognition"
                ],
                "author_keywords": []
            },
            {
                "title": "Spike-Based Anytime Perception",
                "link": "https://ieeexplore.ieee.org/document/10030469/",
                "date_of_publication": "06 February 2023",
                "doi": "10.1109/WACV56688.2023.00526",
                "citations": "31",
                "abstract": "In many emerging computer vision applications, it is critical to adhere to stringent latency and power constraints. The current neural network paradigm of frame-based, floating-point inference is often ill-suited to these resource-constrained applications. Spike-based perception – enabled by spiking neural networks (SNNs) – is one promising alternative. Unlike conventional neural networks (ANNs), spiking networks exhibit smooth tradeoffs between latency, power, and accuracy. SNNs are the archetype of an \"anytime algorithm\" whose accuracy improves smoothly over time. This property allows SNNs to adapt their computational investment in response to changing resource constraints. Unfortunately, mainstream algorithms for training SNNs (i.e., those based on ANN-to-SNN conversion) tend to produce models that are inefficient in practice. To mitigate this problem, we propose a set of principled optimizations that reduce latency and power consumption by 1–2 orders of magnitude in converted SNNs. These optimizations leverage a set of novel efficiency metrics designed for anytime algorithms. We also develop a state-of-the-art simulator, SaRNN, which can simulate SNNs using commodity GPU hardware and neuromorphic platforms. We hope that the proposed optimizations, metrics, and tools will facilitate the future development of spike-based vision systems.",
                "ieee_keywords": [
                    "Measurement",
                    "Training",
                    "Computer vision",
                    "Power demand",
                    "Neuromorphics",
                    "Machine vision",
                    "Neural networks"
                ],
                "author_keywords": [
                    "Algorithms: Machine learning architectures",
                    "formulations",
                    "and algorithms (including transfer)",
                    "Image recognition and understanding (object detection",
                    "categorization",
                    "segmentation",
                    "scene modeling",
                    "visual reasoning)",
                    "Embedded sensing/real-time techniques"
                ]
            }
        ]
    },
    {
        "name": "Xiaojin Zhu",
        "publications": [
            {
                "title": "Inferring air pollution by sniffing social media",
                "link": "https://ieeexplore.ieee.org/document/6921638/",
                "date_of_publication": "16 October 2014",
                "doi": "10.1109/ASONAM.2014.6921638",
                "citations": "31",
                "abstract": "The first step to deal with the significant issue of air pollution in China and elsewhere in the world is to monitor it. While more physical monitoring stations are built, current coverage is limited to large cities with most other places under-monitored. In this paper we propose a complementary approach to monitor Air Quality Index (AQI): using machine learning models to estimate AQI from social media posts. We propose a series of progressively more sophisticated machine learning models, culminating in a Markov Random Field model that utilizes the text content in social media as well as the spatiotemporal correlation among cities and days. Our extensive experiments on Sina Weibo data from 108 cities during a one-month period demonstrate the accurate AQI prediction performance of our approach.",
                "ieee_keywords": [
                    "Cities and towns",
                    "Training",
                    "Atmospheric modeling",
                    "Correlation",
                    "Spatiotemporal phenomena",
                    "Monitoring",
                    "Pollution"
                ],
                "author_keywords": []
            },
            {
                "title": "Fingerprinting 802.11 rate adaption algorithms",
                "link": "https://ieeexplore.ieee.org/document/5934893/",
                "date_of_publication": "30 June 2011",
                "doi": "10.1109/INFCOM.2011.5934893",
                "citations": "8",
                "abstract": "The effectiveness of rate adaptation algorithms is an important determinant of 802.11 wireless network performance. The diversity of algorithms that has resulted from efforts to improve rate adaptation has introduced a new dimension of variability into 802.11 wireless networks, further complicating the already difficult task of understanding and debugging 802.11 performance. To assist with this task, in this paper we present and evaluate a methodology for accurately fingerprinting 802.11 rate adaptation algorithms. Our approach uses a Support Vector Machine (SVM)-based classifier that requires only simple passive measurements of 802.11 traffic. We demonstrate that careful conversion of raw packet traces into input features for SVM is necessary for achieving high classification accuracy. We tested our classifier on the four rate adaptation algorithms available in MadWifi, cards. The classifier performs with an accuracy of 95% – 100%. We also show that the classifier is robust over a variety of network conditions if the training data includes a sufficient sampling of the range of an algorithm's behavior.",
                "ieee_keywords": [
                    "Hardware",
                    "Throughput",
                    "Fingerprint recognition"
                ],
                "author_keywords": []
            },
            {
                "title": "A Machine Learning Approach to TCP Throughput Prediction",
                "link": "https://ieeexplore.ieee.org/document/5378489/",
                "date_of_publication": null,
                "doi": "10.1109/TNET.2009.2037812",
                "citations": "104",
                "abstract": "TCP throughput prediction is an important capability for networks where multiple paths exist between data senders and receivers. In this paper, we describe a new lightweight method for TCP throughput prediction. Our predictor uses Support Vector Regression (SVR); prediction is based on both prior file transfer history and measurements of simple path properties. We evaluate our predictor in a laboratory setting where ground truth can be measured with perfect accuracy. We report the performance of our predictor for oracular and practical measurements of path properties over a wide range of traffic conditions and transfer sizes. For bulk transfers in heavy traffic using oracular measurements, TCP throughput is predicted within 10% of the actual value 87% of the time, representing nearly a threefold improvement in accuracy over prior history-based methods. For practical measurements of path properties, predictions can be made within 10% of the actual value nearly 50% of the time, approximately a 60% improvement over history-based methods, and with much lower measurement traffic overhead. We implement our predictor in a tool called PathPerf , test it in the wide area, and show that PathPerf predicts TCP throughput accurately over diverse wide area paths.",
                "ieee_keywords": [
                    "Machine learning",
                    "Throughput",
                    "Time measurement",
                    "Size measurement",
                    "History",
                    "Laboratories",
                    "Testing",
                    "Computer science",
                    "Instruments"
                ],
                "author_keywords": [
                    "Active measurements",
                    "machine learning",
                    "support vector regression",
                    "TCP throughput prediction"
                ]
            },
            {
                "title": "Cross-architecture performance prediction (XAPP) using CPU code to predict GPU performance",
                "link": "https://ieeexplore.ieee.org/document/7856640/",
                "date_of_publication": "16 February 2017",
                "doi": "10.1145/2830772.2830780",
                "citations": "37",
                "abstract": "GPUs have become prevalent and more general purpose, but GPU programming remains challenging and time consuming for the majority of programmers. In addition, it is not always clear which codes will benefit from getting ported to GPU. Therefore, having a tool to estimate GPU performance for a piece of code before writing a GPU implementation is highly desirable. To this end, we propose Cross-Architecture Performance Prediction (XAPP), a machine-learning based technique that uses only single-threaded CPU implementation to predict GPU performance. Our paper is built on the two following insights: i) Execution time on GPU is a function of program properties and hardware characteristics. ii) By examining a vast array of previously implemented GPU codes along with their CPU counterparts, we can use established machine learning techniques to learn this correlation between program properties, hardware characteristics and GPU execution time. We use an adaptive two-level machine learning solution. Our results show that our tool is robust and accurate: we achieve 26.9% average error on a set of 24 real-world kernels. We also discuss practical usage scenarios for XAPP.",
                "ieee_keywords": [
                    "Graphics processing units",
                    "Hardware",
                    "Prediction algorithms",
                    "Programming",
                    "Time measurement",
                    "Kernel",
                    "Predictive models"
                ],
                "author_keywords": [
                    "GPU",
                    "Cross-platform Prediction",
                    "Performance Modeling",
                    "Machine Learning"
                ]
            },
            {
                "title": "Crop Type Classification by Simultaneous Use of Satellite Images of Different Resolutions",
                "link": "https://ieeexplore.ieee.org/document/6588603/",
                "date_of_publication": null,
                "doi": "10.1109/TGRS.2013.2274431",
                "citations": "26",
                "abstract": "Accurate and timely identification of crop types has significant economic, agricultural, policy, and environmental applications. The existing remote sensing methods to identify crop types rely on remotely sensed images of high temporal frequency in order to utilize phenological changes in crop reflectance characteristics. However, these image sets generally have relatively low spatial resolution. This tradeoff makes it difficult to classify remotely sensed images in fragmented landscapes where field sizes are smaller than the resolution of imaging sensor. Here, we develop a method for combining high spatial resolution (high-resolution) data with images with low spatial resolution but with high time frequency to achieve a superior classification of crop types. The solution is implemented and tested on both synthetic and real data sets as a proof of concept. We show that, by incorporating high-temporal-frequency but low spatial resolution data into the classification process, up to 20% of improvement in classification accuracy can be achieved even if very few high-resolution images are available for a location. This boost in accuracy is roughly equivalent to including an additional high-resolution image to the temporal stack during the classification process. The limitations of the current algorithm include computational performance and the need for ideal crop curves. Nevertheless, the resulting boost in accuracy can help researchers create superior crop type classification maps, thereby creating the opportunity to make more informed decisions.",
                "ieee_keywords": [
                    "Agriculture",
                    "Remote sensing",
                    "Satellites",
                    "Spatial resolution",
                    "Earth",
                    "MODIS"
                ],
                "author_keywords": [
                    "Agriculture",
                    "classification algorithms",
                    "cost function",
                    "crops",
                    "geospatial analysis",
                    "image processing",
                    "pattern recognition",
                    "probability",
                    "remote sensing",
                    "vegetation mapping",
                    "Agriculture",
                    "classification algorithms",
                    "cost function",
                    "crops",
                    "geospatial analysis",
                    "image processing",
                    "pattern recognition",
                    "probability",
                    "remote sensing",
                    "vegetation mapping"
                ]
            },
            {
                "title": "Programming language support for analyzing non-persistent data",
                "link": "https://ieeexplore.ieee.org/document/7568895/",
                "date_of_publication": "15 September 2016",
                "doi": "10.1109/THS.2016.7568895",
                "citations": "75",
                "abstract": "For safety and security, surveillance cameras are widely deployed. A high percentage of the visual data, however, is never watched by humans nor analyzed by computer programs. Moreover, it is common practice to erase the data after a short duration (say, two weeks) and reuse the storage space. As a result, the data are non-persistent. Non-persistent data presents serious security risks: the unwatched and unanalyzed data may include evidence of security breaches. After the data is erased, it is no longer possible to detect the breaches nor prosecute the suspects. This paper proposes a potential solution to remedy this situation by adding automatic data sampling to a programming language. If a piece of data is marked as non-persistent, the compiler and the run-time system automatically sample and store the data, hence making a small fraction of the data persistent. The samples would allow post-event analysis to detect security breaches that are not detected earlier. The samples, due to the much smaller sizes compared with the original non-persistent data, may be analyzed using more sophisticated computer programs that are unable to keep up with the speeds of data generation.",
                "ieee_keywords": [
                    "Computers",
                    "Cameras",
                    "Security",
                    "Surveillance",
                    "Streaming media",
                    "Computer languages",
                    "Compressed sensing"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Yingyu Liang",
        "publications": [
            {
                "title": "Towards Understanding Limitations of Pixel Discretization Against Adversarial Attacks",
                "link": "https://ieeexplore.ieee.org/document/8806764/",
                "date_of_publication": "22 August 2019",
                "doi": "10.1109/EuroSP.2019.00042",
                "citations": "7",
                "abstract": "Wide adoption of artificial neural networks in various domains has led to an increasing interest in defending adversarial attacks against them. Preprocessing defense methods such as pixel discretization are particularly attractive in practice due to their simplicity, low computational overhead, and applicability to various systems. It is observed that such methods work well on simple datasets like MNIST, but break on more complicated ones like ImageNet under recently proposed strong white-box attacks. To understand the conditions for success and potentials for improvement, we study the pixel discretization defense method, including more sophisticated variants that take into account the properties of the dataset being discretized. Our results again show poor resistance against the strong attacks. We analyze our results in a theoretical framework and offer strong evidence that pixel discretization is unlikely to work on all but the simplest of the datasets. Furthermore, our arguments present insights why some other preprocessing defenses may be insecure.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Deep Online Fused Video Stabilization",
                "link": "https://ieeexplore.ieee.org/document/9706695/",
                "date_of_publication": "15 February 2022",
                "doi": "10.1109/WACV51458.2022.00094",
                "citations": "6",
                "abstract": "We present a deep neural network (DNN) that uses both sensor data (gyroscope) and image content (optical flow) to stabilize videos through unsupervised learning. The network fuses optical flow with real/virtual camera pose histories into a joint motion representation. Next, the LSTM cell infers the new virtual camera pose, which is used to generate a warping grid that stabilizes the video frames. We adopt a relative motion representation as well as a multi-stage training strategy to optimize our model without any supervision. To the best of our knowledge, this is the first DNN solution that adopts both sensor data and image content for video stabilization. We validate the proposed framework through ablation studies and demonstrate that the proposed method outperforms the state-of-art alternative solutions via quantitative evaluations and a user study. Check out our video results, code and dataset at our website.",
                "ieee_keywords": [
                    "Training",
                    "Deep learning",
                    "Fuses",
                    "Neural networks",
                    "Cameras",
                    "Gyroscopes",
                    "History"
                ],
                "author_keywords": [
                    "Computational Photography",
                    "Image and Video Synthesis"
                ]
            }
        ]
    },
    {
        "name": "Mohit Gupta",
        "publications": [
            {
                "title": "Invisible Perturbations: Physical Adversarial Examples Exploiting the Rolling Shutter Effect",
                "link": "https://ieeexplore.ieee.org/document/9578006/",
                "date_of_publication": "02 November 2021",
                "doi": "10.1109/CVPR46437.2021.01443",
                "citations": "17",
                "abstract": "Physical adversarial examples for camera-based computer vision have so far been achieved through visible artifacts — a sticker on a Stop sign, colorful borders around eyeglasses or a 3D printed object with a colorful texture. An implicit assumption here is that the perturbations must be visible so that a camera can sense them. By contrast, we contribute a procedure to generate, for the first time, physical adversarial examples that are invisible to human eyes. Rather than modifying the victim object with visible artifacts, we modify light that illuminates the object. We demonstrate how an attacker can craft a modulated light signal that adversarially illuminates a scene and causes targeted misclassifications on a state-of-the-art ImageNet deep learning model. Concretely, we exploit the radiometric rolling shutter effect in commodity cameras to create precise striping patterns that appear on images. To human eyes, it appears like the object is illuminated, but the camera creates an image with stripes that will cause ML models to output the attacker-desired classification. We conduct a range of simulation and physical experiments with LEDs, demonstrating targeted attack rates up to 84%.",
                "ieee_keywords": [
                    "Deep learning",
                    "Computer vision",
                    "Three-dimensional displays",
                    "Computational modeling",
                    "Perturbation methods",
                    "Cameras",
                    "Radiometry"
                ],
                "author_keywords": []
            },
            {
                "title": "Passive Inter-Photon Imaging",
                "link": "https://ieeexplore.ieee.org/document/9578442/",
                "date_of_publication": "02 November 2021",
                "doi": "10.1109/CVPR46437.2021.00848",
                "citations": "12",
                "abstract": "Digital camera pixels measure image intensities by converting incident light energy into an analog electrical current, and then digitizing it into a fixed-width binary representation. This direct measurement method, while conceptually simple, suffers from limited dynamic range and poor performance under extreme illumination — electronic noise dominates under low illumination, and pixel full-well capacity results in saturation under bright illumination. We propose a novel intensity cue based on measuring inter-photon timing, defined as the time delay between detection of successive photons. Based on the statistics of inter-photon times measured by a time-resolved single-photon sensor, we develop theory and algorithms for a scene brightness estimator which works over extreme dynamic range; we experimentally demonstrate imaging scenes with a dynamic range of over ten million to one. The proposed techniques, aided by the emergence of single-photon sensors such as single-photon avalanche diodes (SPADs) with picosecond timing resolution, will have implications for a wide range of imaging applications: robotics, consumer photography, astronomy, microscopy and biomedical imaging.",
                "ieee_keywords": [
                    "Photography",
                    "Current measurement",
                    "Lighting",
                    "Biomedical measurement",
                    "Dynamic range",
                    "Extraterrestrial measurements",
                    "Robot sensing systems"
                ],
                "author_keywords": []
            },
            {
                "title": "Unlocking the Performance of Proximity Sensors by Utilizing Transient Histograms",
                "link": "https://ieeexplore.ieee.org/document/10243076/",
                "date_of_publication": null,
                "doi": "10.1109/LRA.2023.3313069",
                "citations": "Abstract",
                "abstract": "We provide methods which recover planar scene geometry by utilizing the transient histograms captured by a class of close-range time-of-flight (ToF) distance sensor. A transient histogram is a one dimensional temporal waveform which encodes the arrival time of photons incident on the ToF sensor. Typically, a sensor processes the transient histogram using a proprietary algorithm to produce distance estimates, which are commonly used in several robotics applications. Our methods utilize the transient histogram directly to enable recovery of planar geometry more accurately than is possible using only proprietary distance estimates, and consistent recovery of the albedo of the planar surface, which is not possible with proprietary distance estimates alone. This is accomplished via a differentiable rendering pipeline, which simulates the transient imaging process, allowing direct optimization of scene geometry to match observations. To validate our methods, we capture 3,800 measurements of eight planar surfaces from a wide range of viewpoints, and show that our method outperforms the proprietary-distance-estimate baseline by an order of magnitude in most scenarios. We demonstrate a simple robotics application which uses our method to sense the distance to and slope of a planar surface from a sensor mounted on the end effector of a robot arm.",
                "ieee_keywords": [
                    "Sensors",
                    "Transient analysis",
                    "Histograms",
                    "Robot sensing systems",
                    "Robots",
                    "Imaging",
                    "Photonics"
                ],
                "author_keywords": [
                    "RGB-D Perception",
                    "Range Sensing",
                    "Media",
                    "More Like This",
                    "Obstacle Avoidance System for Wheeled Mobile Robots by CMOS Image Sensors",
                    "2014 Tenth International Conference on Intelligent Information Hiding and Multimedia Signal Processing",
                    "Published: 2014",
                    "Localization and Error Correction for Mobile Robot with an Image Sensor",
                    "2006 SICE-ICASE International Joint Conference",
                    "Published: 2006",
                    "Show More"
                ]
            },
            {
                "title": "Geometric Calibration of Single-Pixel Distance Sensors",
                "link": "https://ieeexplore.ieee.org/document/9779560/",
                "date_of_publication": null,
                "doi": "10.1109/LRA.2022.3176453",
                "citations": "282",
                "abstract": "Single-pixel distance sensors are a low-power, low-cost option for distance ranging, and are often attached to robots for collision detection and avoidance. The relative sensor pose, i.e., its position and orientation relative to the robot, must be known to relate its measurements to 3D scene geometry. However, sensor pose is difficult to measure accurately, which has precluded the use of single-pixel sensors from applications such as environment mapping and precise collision avoidance. In this work, we provide a calibration procedure that can accurately determine the pose of a single-pixel distance sensor given only the known motion of the robot and an unknown planar target. We establish a geometric relationship between the relative sensor pose, robot motion, and an arbitrary plane, and show that the plane and sensor parameters can be recovered via nonlinear optimization. The result is a practical procedure for sensor calibration. We evaluate the procedure in simulation and in real world experiments, and provide an open source implementation. We consider two commonly available sensors (ST VL6180X and ST VL53L3CX) and characterize them to show that while they deviate from the idealized model used in our derivation, their poses can be recovered precisely and used for effective 3D scene reconstruction.",
                "ieee_keywords": [
                    "Sensors",
                    "Robot sensing systems",
                    "Robots",
                    "Calibration",
                    "Collision avoidance",
                    "Sensor phenomena and characterization",
                    "Robot kinematics"
                ],
                "author_keywords": [
                    "Calibration and identification",
                    "localization",
                    "range sensing"
                ]
            },
            {
                "title": "Photon-Flooded Single-Photon 3D Cameras",
                "link": "https://ieeexplore.ieee.org/document/8953843/",
                "date_of_publication": "09 January 2020",
                "doi": "10.1109/CVPR.2019.00693",
                "citations": "22",
                "abstract": "Single-photon avalanche diodes (SPADs) are starting to play a pivotal role in the development of photon-efficient, long-range LiDAR systems. However, due to non-linearities in their image formation model, a high photon flux (e.g., due to strong sunlight) leads to distortion of the incident temporal waveform, and potentially, large depth errors. Operating SPADs in low flux regimes can mitigate these distortions, but, often requires attenuating the signal and thus, results in low signal-to-noise ratio. In this paper, we address the following basic question: what is the optimal photon flux that a SPAD-based LiDAR should be operated in? We derive a closed form expression for the optimal flux, which is quasi-depth-invariant, and depends on the ambient light strength. The optimal flux is lower than what a SPAD typically measures in real world scenarios, but surprisingly, considerably higher than what is conventionally suggested for avoiding distortions. We propose a simple, adaptive approach for achieving the optimal flux by attenuating incident flux based on an estimate of ambient light strength. Using extensive simulations and a hardware prototype, we show that the optimal flux criterion holds for several depth estimators, under a wide range of illumination conditions.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Single-Photon Camera Guided Extreme Dynamic Range Imaging",
                "link": "https://ieeexplore.ieee.org/document/9706657/",
                "date_of_publication": "15 February 2022",
                "doi": "10.1109/WACV51458.2022.00012",
                "citations": "4",
                "abstract": "Reconstruction of high-resolution extreme dynamic range images from a small number of low dynamic range (LDR) images is crucial for many computer vision applications. Current high dynamic range (HDR) cameras based on CMOS image sensor technology rely on multi-exposure bracketing which suffers from motion artifacts and signal-to-noise (SNR) dip artifacts in extreme dynamic range scenes. Recently, single-photon cameras (SPCs) have been shown to achieve orders of magnitude higher dynamic range for passive imaging than conventional CMOS sensors. SPCs are becoming increasingly available commercially, even in some consumer devices. Unfortunately, current SPCs suffer from low spatial resolution. To overcome the limitations of CMOS and SPC sensors, we propose a learning-based CMOS-SPC fusion method to recover high-resolution extreme dynamic range images. We compare the performance of our method against various traditional and state-of-the-art baselines using both synthetic and experimental data. Our method outperforms these baselines, both in terms of visual quality and quantitative metrics.",
                "ieee_keywords": [
                    "Performance evaluation",
                    "Computer vision",
                    "Visualization",
                    "Dynamic range",
                    "CMOS image sensors",
                    "Sensor fusion",
                    "Cameras"
                ],
                "author_keywords": [
                    "Computational Photography",
                    "Image and Video Synthesis Image Processing",
                    "Image Processing -> Image Restoration"
                ]
            },
            {
                "title": "Compressive Single-Photon 3D Cameras",
                "link": "https://ieeexplore.ieee.org/document/9878523/",
                "date_of_publication": "27 September 2022",
                "doi": "10.1109/CVPR52688.2022.01733",
                "citations": "3",
                "abstract": "Single-photon avalanche diodes (SPADs) are an emerging pixel technology for time-of-flight (ToF) 3D cameras that can capture the time-of-arrival of individual photons at picosecond resolution. To estimate depths, current SPAD-based 3D cameras measure the round-trip time of a laser pulse by building a per-pixel histogram of photon times-tamps. As the spatial and timestamp resolution of SPAD-based cameras increase, their output data rates far exceed the capacity of existing data transfer technologies. One major reason for SPAD's bandwidth-intensive operation is the tight coupling that exists between depth resolution and histogram resolution. To weaken this coupling, we propose compressive single-photon histograms (CSPH). CSPHs are a per-pixel compressive representation of the high-resolution histogram, that is built on-the-fly, as each photon is detected. They are based on a family of linear coding schemes that can be expressed as a simple matrix operation. We design different CSPH coding schemes for 3D imaging and evaluate them under different signal and background levels, laser waveforms, and illumination setups. Our results show that a well-designed CSPH can consistently reduce data rates by 1–2 orders of magnitude without compromising depth precision.",
                "ieee_keywords": [
                    "Couplings",
                    "Histograms",
                    "Three-dimensional displays",
                    "Semiconductor lasers",
                    "Cameras",
                    "Encoding",
                    "Time measurement"
                ],
                "author_keywords": [
                    "Computational photography; Physics-based vision and shape-from-X"
                ]
            },
            {
                "title": "Asynchronous Single-Photon 3D Imaging",
                "link": "https://ieeexplore.ieee.org/document/9009520/",
                "date_of_publication": "27 February 2020",
                "doi": "10.1109/ICCV.2019.00800",
                "citations": "32",
                "abstract": "Single-photon avalanche diodes (SPADs) are becoming popular in time-of-flight depth-ranging due to their unique ability to capture individual photons with picosecond timing resolution. However, ambient light (e.g., sunlight) incident on a SPAD-based 3D camera leads to severe non-linear distortions (pileup) in the measured waveform, resulting in large depth errors. We propose asynchronous single-photon 3D imaging, a family of acquisition schemes to mitigate pileup during data acquisition itself. Asynchronous acquisition temporally misaligns SPAD measurement windows and the laser cycles through deterministically predefined or randomized offsets. Our key insight is that pileup distortions can be “averaged out” by choosing a sequence of offsets that span the entire depth range. We develop a generalized image formation model and perform theoretical analysis to explore the space of asynchronous acquisition schemes and design high-performance schemes. Our simulations and experiments demonstrate an improvement in depth accuracy of up to an order of magnitude as compared to the state-of-the-art, across a wide range of imaging scenarios, including those with high ambient flux.",
                "ieee_keywords": [
                    "Photonics",
                    "Cameras",
                    "Three-dimensional displays",
                    "Histograms",
                    "Laser modes"
                ],
                "author_keywords": []
            },
            {
                "title": "Practical Coding Function Design for Time-Of-Flight Imaging",
                "link": "https://ieeexplore.ieee.org/document/8954144/",
                "date_of_publication": "09 January 2020",
                "doi": "10.1109/CVPR.2019.00166",
                "citations": "11",
                "abstract": "The depth resolution of a continuous-wave time-of-flight (CW-ToF) imaging system is determined by its coding functions. Recently, there has been growing interest in the design of new high-performance CW-ToF coding functions. However, these functions are typically designed in a hardware agnostic manner, i.e., without considering the practical device limitations, such as bandwidth, source power, digital (binary) function generation. Therefore, despite theoretical improvements, practical implementation of these functions remains a challenge. We present a constrained optimization approach for designing practical coding functions that adhere to hardware constraints. The optimization problem is non-convex with a large search space and no known globally optimal solutions. To make the problem tractable, we design an iterative, alternating least-squares algorithm, along with convex relaxation of the constraints. Using this approach, we design high-performance coding functions that can be implemented on existing hardware with minimal modifications. We demonstrate the performance benefits of the resulting functions via extensive simulations and a hardware prototype.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "iToF2dToF: A Robust and Flexible Representation for Data-Driven Time-of-Flight Imaging",
                "link": "https://ieeexplore.ieee.org/document/9609533/",
                "date_of_publication": null,
                "doi": "10.1109/TCI.2021.3126533",
                "citations": "9",
                "abstract": "Indirect Time-of-Flight (iToF) cameras are a promising depth sensing technology. However, they are prone to errors caused by multi-path interference (MPI) and low signal-to-noise ratio (SNR). Traditional methods, after denoising, mitigate MPI by estimating a transient image that encodes depths. Recently, data-driven methods that jointly denoise and mitigate MPI have become state-of-the-art without using the intermediate transient representation. In this paper, we propose to revisit the transient representation. Using data-driven priors, we interpolate/extrapolate iToF frequencies and use them to estimate the transient image. Given direct ToF (dToF) sensors capture transient images, we name our method iToF2dToF. The transient representation is flexible. It can be integrated with different rule-based depth sensing algorithms that are robust to low SNR and can deal with ambiguous scenarios that arise in practice (e.g., specular MPI, optical cross-talk). We demonstrate the benefits of iToF2dToF over previous methods in real depth sensing scenarios.",
                "ieee_keywords": [
                    "Transient analysis",
                    "Frequency measurement",
                    "Signal to noise ratio",
                    "Cameras",
                    "Optical sensors",
                    "Frequency estimation",
                    "Optical imaging"
                ],
                "author_keywords": [
                    "Depth sensing",
                    "time-of-flight"
                ]
            }
        ]
    },
    {
        "name": "Shih-Fu Chang",
        "publications": [
            {
                "title": "Combining computer and human vision into a BCI: Can the whole be greater than the sum of its parts?",
                "link": "https://ieeexplore.ieee.org/document/5627403/",
                "date_of_publication": "11 November 2010",
                "doi": "10.1109/IEMBS.2010.5627403",
                "citations": "5",
                "abstract": "Our group has been investigating the development of BCI systems for improving information delivery to a user, specifically systems for triaging image content based on what captures a user's attention. One of the systems we have developed uses single-trial EEG scores as noisy labels for a computer vision image retrieval system. In this paper we investigate how the noisy nature of the EEG-derived labels affects the resulting accuracy of the computer vision system. Specifically, we consider how the precision of the EEG scores affects the resulting precision of images retrieved by a graph-based transductive learning model designed to propagate image class labels based on image feature similarity and sparse labels.",
                "ieee_keywords": [
                    "Electroencephalography",
                    "Technical Activities Guide - TAG",
                    "Detectors",
                    "Computer vision",
                    "Testing",
                    "Image databases"
                ],
                "author_keywords": []
            },
            {
                "title": "One-Shot Learning for Function-Specific Region Segmentation in Mouse Brain",
                "link": "https://ieeexplore.ieee.org/document/8759226/",
                "date_of_publication": "11 July 2019",
                "doi": "10.1109/ISBI.2019.8759226",
                "citations": "214",
                "abstract": "A brain contains a large number of structured regions responsible for diverse functions. Detailed region annotations upon stereotaxic coordinates are highly rare, prompting the need of using one or very few available annotated results of a specific brain section to label images of broadly accessible brain section samples. Here we develop a one-shot learning approach to segment regions of mouse brains. Using the highly ordered geometry of brains, we introduce a reference mask to incorporate both the anatomical structure (visual information) and the brain atlas into brain segmentation. Using the UNet model with this reference mask, we are able to predict the region of hippocampus with high accuracy. We further implement it to segment brain images into 95 detailed regions augmented from the annotation on only one image from Allen Brain Atlas. Together, our one-shot learning method provides neuroscientists an efficient way for brain segmentation and facilitates future region-specific functional studies of brains.",
                "ieee_keywords": [
                    "Image segmentation",
                    "Training",
                    "Mice",
                    "Brain modeling",
                    "Predictive models",
                    "Hippocampus"
                ],
                "author_keywords": [
                    "One-shot learning",
                    "mouse brain",
                    "UNet",
                    "reference mask",
                    "hippocampus"
                ]
            },
            {
                "title": "Regrasping and unfolding of garments using predictive thin shell modeling",
                "link": "https://ieeexplore.ieee.org/document/7139370/",
                "date_of_publication": "02 July 2015",
                "doi": "10.1109/ICRA.2015.7139370",
                "citations": "37",
                "abstract": "Deformable objects such as garments are highly unstructured, making them difficult to recognize and manipulate. In this paper, we propose a novel method to teach a two-arm robot to efficiently track the states of a garment from an unknown state to a known state by iterative regrasping. The problem is formulated as a constrained weighted evaluation metric for evaluating the two desired grasping points during regrasping, which can also be used for a convergence criterion The result is then adopted as an estimation to initialize a regrasping, which is then considered as a new state for evaluation. The process stops when the predicted thin shell conclusively agrees with reconstruction. We show experimental results for regrasping a number of different garments including sweater, knitwear, pants, and leggings, etc.",
                "ieee_keywords": [
                    "Clothing",
                    "Grasping",
                    "Robot sensing systems",
                    "Three-dimensional displays",
                    "Grippers",
                    "Databases"
                ],
                "author_keywords": []
            },
            {
                "title": "Model-Driven Feedforward Prediction for Manipulation of Deformable Objects",
                "link": "https://ieeexplore.ieee.org/document/8255664/",
                "date_of_publication": null,
                "doi": "10.1109/TASE.2017.2766228",
                "citations": "23",
                "abstract": "Robotic manipulation of deformable objects is a difficult problem especially because of the complexity of the many different ways an object can deform. Searching such a high-dimensional state space makes it difficult to recognize, track, and manipulate deformable objects. In this paper, we introduce a predictive, model-driven approach to address this challenge, using a precomputed, simulated database of deformable object models. Mesh models of common deformable garments are simulated with the garments picked up in multiple different poses under gravity, and stored in a database for fast and efficient retrieval. To validate this approach, we developed a comprehensive pipeline for manipulating clothing as in a typical laundry task. First, the database is used for category and the pose estimation is used for a garment in an arbitrary position. A fully featured 3-D model of the garment is constructed in real time, and volumetric features are then used to obtain the most similar model in the database to predict the object category and pose. Second, the database can significantly benefit the manipulation of deformable objects via nonrigid registration, providing accurate correspondences between the reconstructed object model and the database models. Third, the accurate model simulation can also be used to optimize the trajectories for the manipulation of deformable objects, such as the folding of garments. Extensive experimental results are shown for the above tasks using a variety of different clothings. Note to Practitioners-This paper provides an open source, extensible, 3-D database for dissemination to the robotics and graphics communities. Model-driven methods are proliferating, and they need to be applied, tested, and validated in real environments. A key idea we have exploited is to have an innovative and novel use of simulation. This database will serve as infrastructure for developing advanced robotic machine learning algorithms. We want to address this machine ... (Show More)",
                "ieee_keywords": [
                    "Clothing",
                    "Object recognition",
                    "Deformable models",
                    "Robots",
                    "Predictive models",
                    "Shape",
                    "Databases"
                ],
                "author_keywords": [
                    "Deformable objects",
                    "recognition",
                    "robotic manipulation",
                    "simulation"
                ]
            },
            {
                "title": "Mobile product search with Bag of Hash Bits and boundary reranking",
                "link": "https://ieeexplore.ieee.org/document/6248030/",
                "date_of_publication": "26 July 2012",
                "doi": "10.1109/CVPR.2012.6248030",
                "citations": "21",
                "abstract": "Rapidly growing applications on smartphones have provided an excellent platform for mobile visual search. Most of previous visual search systems adopt the framework of ”Bag of Words”, in which words indicate quantized codes of visual features. In this work, we propose a novel visual search system based on ”Bag of Hash Bits” (BoHB), in which each local feature is encoded to a very small number of hash bits, instead of quantized to visual words, and the whole image is represented as bag of hash bits. The proposed BoHB method offers unique benefits in solving the challenges associated with mobile visual search, e.g., low transmission cost, cheap memory and computation on the mobile side, etc. Moreover, our BoHB method leverages the distinct properties of hashing bits such as multi-table indexing, multiple bucket probing, bit reuse, and hamming distance based ranking to achieve efficient search over gigantic visual databases. The proposed method significantly outperforms state-of-the-art mobile visual search methods like CHoG, and other (conventional desktop) visual search approaches like bag of words via vocabulary tree, or product quantization. The proposed BoHB approach is easy to implement on mobile devices, and general in the sense that it can be applied to different types of local features, hashing algorithms and image databases. We also incorporate a boundary feature in the reranking step to describe the object shapes, complementing the local features that are usually used to characterize the local details. The boundary feature can further filter out noisy results and improve the search performance, especially at the coarse category level. Extensive experiments over large-scale data sets up to 400k product images demonstrate the effectiveness of our approach.",
                "ieee_keywords": [
                    "Mobile communication",
                    "Visualization",
                    "Feature extraction",
                    "Hamming distance",
                    "Servers",
                    "Indexes"
                ],
                "author_keywords": []
            },
            {
                "title": "Few-Shot Gaze Estimation with Model Offset Predictors",
                "link": "https://ieeexplore.ieee.org/document/9747640/",
                "date_of_publication": "27 April 2022",
                "doi": "10.1109/ICASSP43922.2022.9747640",
                "citations": "1",
                "abstract": "Due to the variance of optical properties across different people, the performance of a person-agnostic gaze estimation model may not generalize well on a specific person. Though one may achieve better performance by training a person-specific model, it typically requires a large number of samples which is not available in real-life scenarios. Hence, few-shot gaze estimation method is preferred for the small number of samples from a target person. However, the key question is how to close the performance gap between a \"few-shot\" model and the \"many-shot\" model. In this paper, we propose to learn a person-specific offset predictor which outputs the difference between the person-agnostic model and the many-shot person-specific model with as few as one training sample. We adapt the knowledge to a new person by using the average of meta-learned offset predictors parameters as the initialization of the new offset predictor. Experiments show that the proposed few-shot person-specific model is not only closer to the corresponding many-shot person-specific model but also has better accuracy than the SOTA few-shot gaze estimation methods in multiple gaze datasets.",
                "ieee_keywords": [
                    "Training",
                    "Adaptation models",
                    "Conferences",
                    "Estimation",
                    "Predictive models",
                    "Signal processing",
                    "Acoustics"
                ],
                "author_keywords": [
                    "Few-shot learning",
                    "gaze estimation",
                    "initialization",
                    "consistency"
                ]
            },
            {
                "title": "Weak attributes for large-scale image retrieval",
                "link": "https://ieeexplore.ieee.org/document/6248023/",
                "date_of_publication": "26 July 2012",
                "doi": "10.1109/CVPR.2012.6248023",
                "citations": "53",
                "abstract": "Attribute-based query offers an intuitive way of image retrieval, in which users can describe the intended search targets with understandable attributes. In this paper, we develop a general and powerful framework to solve this problem by leveraging a large pool of weak attributes comprised of automatic classifier scores or other mid-level representations that can be easily acquired with little or no human labor. We extend the existing retrieval model of modeling dependency within query attributes to modeling dependency of query attributes on a large pool of weak attributes, which is more expressive and scalable. To efficiently learn such a large dependency model without overfitting, we further propose a semi-supervised graphical model to map each multiattribute query to a subset of weak attributes. Through extensive experiments over several attribute benchmarks, we demonstrate consistent and significant performance improvements over the state-of-the-art techniques. In addition, we compile the largest multi-attribute image retrieval dateset to date, including 126 fully labeled query attributes and 6,000 weak attributes of 0.26 million images.",
                "ieee_keywords": [
                    "Graphical models",
                    "Mathematical model",
                    "Yttrium",
                    "Image retrieval",
                    "Training",
                    "Equations",
                    "Humans"
                ],
                "author_keywords": []
            },
            {
                "title": "Real-time pose estimation of deformable objects using a volumetric approach",
                "link": "https://ieeexplore.ieee.org/document/6942687/",
                "date_of_publication": "06 November 2014",
                "doi": "10.1109/IROS.2014.6942687",
                "citations": "42",
                "abstract": "Pose estimation of deformable objects is a fundamental and challenging problem in robotics. We present a novel solution to this problem by first reconstructing a 3D model of the object from a low-cost depth sensor such as Kinect, and then searching a database of simulated models in different poses to predict the pose. Given noisy depth images from 360-degree views of the target object acquired from the Kinect sensor, we reconstruct a smooth 3D model of the object using depth image segmentation and volumetric fusion. Then with an efficient feature extraction and matching scheme, we search the database, which contains a large number of deformable objects in different poses, to obtain the most similar model, whose pose is then adopted as the prediction. Extensive experiments demonstrate better accuracy and orders of magnitude speed-up compared to our previous work. An additional benefit of our method is that it produces a high-quality mesh model and camera pose, which is necessary for other tasks such as regrasping and object manipulation.",
                "ieee_keywords": [
                    "Clothing",
                    "Three-dimensional displays",
                    "Solid modeling",
                    "Grasping",
                    "Robot sensing systems",
                    "Feature extraction"
                ],
                "author_keywords": []
            },
            {
                "title": "Few-Shot Object Detection with Fully Cross-Transformer",
                "link": "https://ieeexplore.ieee.org/document/9879650/",
                "date_of_publication": "27 September 2022",
                "doi": "10.1109/CVPR52688.2022.00525",
                "citations": "23",
                "abstract": "Few-shot object detection (FSOD), with the aim to detect novel objects using very few training examples, has recently attracted great research interest in the community. Metric-learning based methods have been demonstrated to be effective for this task using a two-branch based siamese network, and calculate the similarity between image regions and few-shot examples for detection. However, in previous works, the interaction between the two branches is only restricted in the detection head, while leaving the remaining hundreds of layers for separate feature extraction. Inspired by the recent work on vision transformers and vision-language transformers, we propose a novel Fully Cross-Transformer based model (FCT) for FSOD by incorporating cross-transformer into both the feature backbone and detection head. The asymmetric-batched cross-attention is proposed to aggregate the key information from the two branches with different batch sizes. Our model can improve the few-shot similarity learning between the two branches by introducing the multi-level interactions. Comprehensive experiments on both PASCAL VOC and MSCOCO FSOD benchmarks demonstrate the effectiveness of our model.",
                "ieee_keywords": [
                    "Training",
                    "Visualization",
                    "Head",
                    "Aggregates",
                    "Object detection",
                    "Benchmark testing",
                    "Feature extraction"
                ],
                "author_keywords": [
                    "Recognition: detection",
                    "categorization",
                    "retrieval; Transfer/low-shot/long-tail learning"
                ]
            },
            {
                "title": "Query Adaptive Few-Shot Object Detection with Heterogeneous Graph Convolutional Networks",
                "link": "https://ieeexplore.ieee.org/document/9710733/",
                "date_of_publication": "28 February 2022",
                "doi": "10.1109/ICCV48922.2021.00325",
                "citations": "23",
                "abstract": "Few-shot object detection (FSOD) aims to detect never-seen objects using few examples. This field sees recent improvement owing to the meta-learning techniques by learning how to match between the query image and few-shot class examples, such that the learned model can generalize to few-shot novel classes. However, currently, most of the meta-learning-based methods perform parwise matching between query image regions (usually proposals) and novel classes separately, therefore failing to take into account multiple relationships among them. In this paper, we propose a novel FSOD model using heterogeneous graph convolutional networks. Through efficient message passing among all the proposal and class nodes with three different types of edges, we could obtain context-aware proposal features and query-adaptive, multiclass-enhanced prototype representations for each class, which could help promote the pairwise matching and improve final FSOD accuracy. Extensive experimental results show that our proposed model, denoted as QA-FewDet, outperforms the current state-of-the-art approaches on the PASCAL VOC and MSCOCO FSOD benchmarks under different shots and evaluation metrics.",
                "ieee_keywords": [
                    "Measurement",
                    "Adaptation models",
                    "Computer vision",
                    "Computational modeling",
                    "Message passing",
                    "Image edge detection",
                    "Prototypes"
                ],
                "author_keywords": [
                    "Detection and localization in 2D and 3D",
                    "Recognition and classification",
                    "Scene analysis and understanding",
                    "Transfer/Low-shot/Semi/Unsupervised Learning"
                ]
            }
        ]
    },
    {
        "name": "Zhou Yu",
        "publications": [
            {
                "title": "Four-wave mixing in slow-light graphene-silicon photonic crystal waveguides",
                "link": "https://ieeexplore.ieee.org/document/6988427/",
                "date_of_publication": "18 December 2014",
                "doi": null,
                "citations": "97",
                "abstract": "We demonstrate the enhanced four-wave mixing generated in silicon photonic crystal waveguides with monolayer graphene. An enhanced high conversion efficiency and wide detuning bandwidth is observed.",
                "ieee_keywords": [
                    "Graphene",
                    "Optical waveguides",
                    "Frequency conversion",
                    "Four-wave mixing",
                    "Photonic crystals",
                    "Nonlinear optics",
                    "Photonics"
                ],
                "author_keywords": []
            },
            {
                "title": "NTIRE 2017 Challenge on Single Image Super-Resolution: Methods and Results",
                "link": "https://ieeexplore.ieee.org/document/8014883/",
                "date_of_publication": "24 August 2017",
                "doi": "10.1109/CVPRW.2017.149",
                "citations": "564",
                "abstract": "This paper reviews the first challenge on single image super-resolution (restoration of rich details in an low resolution image) with focus on proposed solutions and results. A new DIVerse 2K resolution image dataset (DIV2K) was employed. The challenge had 6 competitions divided into 2 tracks with 3 magnification factors each. Track 1 employed the standard bicubic downscaling setup, while Track 2 had unknown downscaling operators (blur kernel and decimation) but learnable through low and high res train images. Each competition had ∽100 registered participants and 20 teams competed in the final testing phase. They gauge the state-of-the-art in single image super-resolution.",
                "ieee_keywords": [
                    "Image resolution",
                    "Runtime",
                    "MATLAB",
                    "Tracking",
                    "Training",
                    "Image restoration",
                    "Testing"
                ],
                "author_keywords": []
            },
            {
                "title": "Coherent Four-Wave Mixing on Hybrid Graphene-Silicon Photonic Crystals",
                "link": "https://ieeexplore.ieee.org/document/6684291/",
                "date_of_publication": null,
                "doi": "10.1109/JSTQE.2013.2290274",
                "citations": "10",
                "abstract": "By placing monolayer graphene on silicon membrane, the effective Kerr coefficient of the hybrid media is enhanced 20 times compared to monolithic silicon. Optical four-wave mixing in graphene-silicon photonic crystal waveguide and a single mode cavity are observed at sub-milliwatt continuous wave input. This allows nonlinear functionalities including low power switching/gating, signal regeneration and parametric conversion, enhancing CMOS integrated photonic information processing on chips.",
                "ieee_keywords": [
                    "Graphene",
                    "Cavity resonators",
                    "Silicon",
                    "Optical waveguides",
                    "Educational institutions",
                    "Nonlinear optics",
                    "Photonic crystals"
                ],
                "author_keywords": [
                    "Four-wave mixing",
                    "graphene optoelectronics",
                    "photonic crystals"
                ]
            },
            {
                "title": "Ultrashort pulse mode-locking from a normal-dispersion on-chip Kerr frequency comb",
                "link": "https://ieeexplore.ieee.org/document/6989321/",
                "date_of_publication": "18 December 2014",
                "doi": "10.1364/CLEO_SI.2014.SF1I.2",
                "citations": "40",
                "abstract": "We demonstrate a broadband Kerr frequency comb and mode-locking in a globally-normal-dispersion microresonator. A record short on-chip pulse of 74-fs is directly measured. Supported by analytical theory and numerical modeling, we describe the mode-locking mechanism.",
                "ieee_keywords": [
                    "Microcavities",
                    "Optical pulse generation",
                    "Optical filters",
                    "Optical pumping",
                    "Resonant frequency"
                ],
                "author_keywords": []
            },
            {
                "title": "The Visual Object Tracking VOT2017 Challenge Results",
                "link": "https://ieeexplore.ieee.org/document/8265440/",
                "date_of_publication": "22 January 2018",
                "doi": "10.1109/ICCVW.2017.230",
                "citations": "238",
                "abstract": "The Visual Object Tracking challenge VOT2017 is the fifth annual tracker benchmarking activity organized by the VOT initiative. Results of 51 trackers are presented; many are state-of-the-art published at major computer vision conferences or journals in recent years. The evaluation included the standard VOT and other popular methodologies and a new \"real-time\" experiment simulating a situation where a tracker processes images as if provided by a continuously running sensor. Performance of the tested trackers typically by far exceeds standard baselines. The source code for most of the trackers is publicly available from the VOT page. The VOT2017 goes beyond its predecessors by (i) improving the VOT public dataset and introducing a separate VOT2017 sequestered dataset, (ii) introducing a realtime tracking experiment and (iii) releasing a redesigned toolkit that supports complex experiments. The dataset, the evaluation kit and the results are publicly available at the challenge website1.",
                "ieee_keywords": [
                    "Target tracking",
                    "Visualization",
                    "Benchmark testing",
                    "Conferences",
                    "Computer vision",
                    "Standards",
                    "Performance evaluation"
                ],
                "author_keywords": []
            },
            {
                "title": "Active metasurface devices based on correlated perovskites",
                "link": "https://ieeexplore.ieee.org/document/7734954/",
                "date_of_publication": "10 November 2016",
                "doi": "10.1109/PIERS.2016.7734954",
                "citations": "243",
                "abstract": "There has been persistent exploration of new active materials and novel device architectures to dynamically control light with larger modulation depth and increased spectral range, at faster speed, and using less power. In this presentation, I will present experimental results showing that samarium nickelate (SmNiO 3 ), a prototypical phase-change perovskite nickelate, exhibits reversible large refractive index changes over an ultra-broad spectral range, from the visible to the long-wavelength mid-infrared (λ = 400 nm-17 μm). The super broadband performance is due to strong electron correlation effects that allow extraordinarily large bandgap tuning of the order of 3 eV, and this new mechanism can be exploited to create active photonic devices.",
                "ieee_keywords": [
                    "Modulation",
                    "Photonics",
                    "Electron optics",
                    "Optical switches",
                    "Aperture antennas",
                    "Broadband communication"
                ],
                "author_keywords": []
            },
            {
                "title": "Sub 100 fs pulse generation via a Si3N4 micro-resonator based frequency comb",
                "link": "https://ieeexplore.ieee.org/document/6833777/",
                "date_of_publication": "16 June 2014",
                "doi": null,
                "citations": "34",
                "abstract": "Ultrashort optical pulses have been generated on chip from a frequency comb generated within a Si 3 N 4 micro-resonator. The pulses are measured using frequency resolved optical gating and found to have a FWHM of 74 fs.",
                "ieee_keywords": [
                    "Optical resonators",
                    "Frequency measurement",
                    "Optical variables measurement",
                    "Pulse measurements",
                    "Resonant frequency",
                    "Optical pulse generation",
                    "Optical pumping"
                ],
                "author_keywords": []
            },
            {
                "title": "Correlated perovskites as a new platform for super broadband tunable photonics",
                "link": "https://ieeexplore.ieee.org/document/7787609/",
                "date_of_publication": "19 December 2016",
                "doi": null,
                "citations": "2",
                "abstract": "We report strong and non-volatile optical modulation utilizing electron-doping induced phase change of a perovskite, SmNiO 3 . Broadband modulation (λ=400nm-17μm) is demonstrated using thin-film SmNiO 3 , and narrowband modulation is realized with metasurfaces integrated with SmNiO 3 .",
                "ieee_keywords": [
                    "Optical films",
                    "Optical reflection",
                    "Optical refraction",
                    "Optical variables control",
                    "Optical imaging",
                    "Optical modulation"
                ],
                "author_keywords": []
            },
            {
                "title": "Ultrafast phase-resolved self-acceleration and frequency-chirp in silicon chip-scale slow-light solitons",
                "link": "https://ieeexplore.ieee.org/document/6833245/",
                "date_of_publication": "16 June 2014",
                "doi": "10.1364/CLEO_SI.2013.CM3L.3",
                "citations": "24",
                "abstract": "We demonstrate the first soliton self-accelerations and frequency-shifts induced by Drude free-carrier dispersion in 1.5-mm silicon photonic crystals. Picojoule soliton center-of-mass advancement of 2-ps (about one FWHM) and wavelength blue-shift of 0.8-nm are observed via XFROG.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Groupwise Tracking of Crowded Similar-Appearance Targets from Low-Continuity Image Sequences",
                "link": "https://ieeexplore.ieee.org/document/7780478/",
                "date_of_publication": "12 December 2016",
                "doi": "10.1109/CVPR.2016.109",
                "citations": "15",
                "abstract": "Automatic tracking of large-scale crowded targets are of particular importance in many applications, such as crowded people/vehicle tracking in video surveillance, fiber tracking in materials science, and cell tracking in biomedical imaging. This problem becomes very challenging when the targets show similar appearance and the interslice/ inter-frame continuity is low due to sparse sampling, camera motion and target occlusion. The main challenge comes from the step of association which aims at matching the predictions and the observations of the multiple targets. In this paper we propose a new groupwise method to explore the target group information and employ the within-group correlations for association and tracking. In particular, the within-group association is modeled by a nonrigid 2D Thin-Plate transform and a sequence of group shrinking, group growing and group merging operations are then developed to refine the composition of each group. We apply the proposed method to track large-scale fibers from microscopy material images and compare its performance against several other multi-target tracking methods. We also apply the proposed method to track crowded people from videos with poor inter-frame continuity.",
                "ieee_keywords": [
                    "Target tracking",
                    "Two dimensional displays",
                    "Image sequences",
                    "Prediction algorithms",
                    "Imaging",
                    "Kalman filters"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Elias Bareinboim",
        "publications": [
            {
                "title": "Causal Transportability for Visual Recognition",
                "link": "https://ieeexplore.ieee.org/document/9879577/",
                "date_of_publication": "27 September 2022",
                "doi": "10.1109/CVPR52688.2022.00737",
                "citations": "7",
                "abstract": "Visual representations underlie object recognition tasks, but they often contain both robust and non-robust features. Our main observation is that image classifiers may perform poorly on out-of-distribution samples because spurious correlations between non-robust features and labels can be changed in a new environment. By analyzing procedures for out-of-distribution generalization with a causal graph, we show that standard classifiers fail because the association between images and labels is not transportable across settings. However, we then show that the causal effect, which severs all sources of confounding, remains invariant across domains. This motivates us to develop an algorithm to estimate the causal effect for image classification, which is transportable (i.e., invariant) across source and target environments. Without observing additional variables, we show that we can derive an estimand for the causal effect under empirical assumptions using representations in deep models as proxies. Theoretical analysis, empirical results, and visualizations show that our approach captures causal invariances and improves overall generalization.",
                "ieee_keywords": [
                    "Visualization",
                    "Computer vision",
                    "Correlation",
                    "Robustness",
                    "Pattern recognition",
                    "Classification algorithms",
                    "Object recognition"
                ],
                "author_keywords": [
                    "Recognition: detection",
                    "categorization",
                    "retrieval; Representation learning"
                ]
            }
        ]
    },
    {
        "name": "David Blei",
        "publications": [
            {
                "title": "Nested Hierarchical Dirichlet Processes",
                "link": "https://ieeexplore.ieee.org/document/6802355/",
                "date_of_publication": null,
                "doi": "10.1109/TPAMI.2014.2318728",
                "citations": "90",
                "abstract": "We develop a nested hierarchical Dirichlet process (nHDP) for hierarchical topic modeling. The nHDP generalizes the nested Chinese restaurant process (nCRP) to allow each word to follow its own path to a topic node according to a per-document distribution over the paths on a shared tree. This alleviates the rigid, single-path formulation assumed by the nCRP, allowing documents to easily express complex thematic borrowings. We derive a stochastic variational inference algorithm for the model, which enables efficient inference for massive collections of text documents. We demonstrate our algorithm on 1.8 million documents from The New York Times and 2.7 million documents from Wikipedia .",
                "ieee_keywords": [
                    "Indexes",
                    "Stochastic processes",
                    "Data models",
                    "Bayes methods",
                    "Atomic measurements",
                    "Random variables",
                    "Pattern analysis",
                    "Bayesian nonparametrics",
                    "Dirichlet process",
                    "topic modeling",
                    "stochastic optimization"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Carl Vondrick",
        "publications": [
            {
                "title": "Doubly Right Object Recognition: A Why Prompt for Visual Rationales",
                "link": "https://ieeexplore.ieee.org/document/10204510/",
                "date_of_publication": "22 August 2023",
                "doi": "10.1109/CVPR52729.2023.00267",
                "citations": "1",
                "abstract": "Many visual recognition models are evaluated only on their classification accuracy, a metric for which they obtain strong performance. In this paper, we investigate whether computer vision models can also provide correct rationales for their predictions. We propose a “doubly right” object recognition benchmark, where the metric requires the model to simultaneously produce both the right labels as well as the right rationales. We find that state-of-the-art visual models, such as CLIP, often provide incorrect rationales for their categorical predictions. However, by transferring the rationales from language models into visual representations through a tailored dataset, we show that we can learn a “why prompt,” which adapts large visual representations to produce correct rationales. Visualizations and empirical experiments show that our prompts significantly improve performance on doubly right object recognition, in addition to zero-shot transfer to unseen tasks and datasets.",
                "ieee_keywords": [
                    "Measurement",
                    "Visualization",
                    "Computer vision",
                    "Adaptation models",
                    "Computational modeling",
                    "Predictive models",
                    "Benchmark testing"
                ],
                "author_keywords": [
                    "Explainable computer vision"
                ]
            },
            {
                "title": "Causal Transportability for Visual Recognition",
                "link": "https://ieeexplore.ieee.org/document/9879577/",
                "date_of_publication": "27 September 2022",
                "doi": "10.1109/CVPR52688.2022.00737",
                "citations": "7",
                "abstract": "Visual representations underlie object recognition tasks, but they often contain both robust and non-robust features. Our main observation is that image classifiers may perform poorly on out-of-distribution samples because spurious correlations between non-robust features and labels can be changed in a new environment. By analyzing procedures for out-of-distribution generalization with a causal graph, we show that standard classifiers fail because the association between images and labels is not transportable across settings. However, we then show that the causal effect, which severs all sources of confounding, remains invariant across domains. This motivates us to develop an algorithm to estimate the causal effect for image classification, which is transportable (i.e., invariant) across source and target environments. Without observing additional variables, we show that we can derive an estimand for the causal effect under empirical assumptions using representations in deep models as proxies. Theoretical analysis, empirical results, and visualizations show that our approach captures causal invariances and improves overall generalization.",
                "ieee_keywords": [
                    "Visualization",
                    "Computer vision",
                    "Correlation",
                    "Robustness",
                    "Pattern recognition",
                    "Classification algorithms",
                    "Object recognition"
                ],
                "author_keywords": [
                    "Recognition: detection",
                    "categorization",
                    "retrieval; Representation learning"
                ]
            },
            {
                "title": "Multi-Level Multimodal Common Semantic Space for Image-Phrase Grounding",
                "link": "https://ieeexplore.ieee.org/document/8954271/",
                "date_of_publication": "09 January 2020",
                "doi": "10.1109/CVPR.2019.01276",
                "citations": "17",
                "abstract": "We address the problem of phrase grounding by learning a multi-level common semantic space shared by the textual and visual modalities. We exploit multiple levels of feature maps of a Deep Convolutional Neural Network, as well as contextualized word and sentence embeddings extracted from a character-based language model. Following dedicated non-linear mappings for visual features at each level, word, and sentence embeddings, we obtain multiple instantiations of our common semantic space in which comparisons between any target text and the visual content is performed with cosine similarity. We guide the model by a multi-level multimodal attention mechanism which outputs attended visual features at each level. The best level is chosen to be compared with text content for maximizing the pertinence scores of image-sentence pairs of the ground truth. Experiments conducted on three publicly available datasets show significant performance gains (20%-60% relative) over the state-of-the-art in phrase localization and set a new performance record on those datasets. We provide a detailed ablation study to show the contribution of each element of our approach and release our code on GitHub.",
                "ieee_keywords": [
                    "Location awareness",
                    "Visualization",
                    "Grounding",
                    "Semantics",
                    "Performance gain",
                    "Feature extraction",
                    "Question answering (information retrieval)"
                ],
                "author_keywords": [
                    "Vision + Language",
                    "Deep Learning",
                    "Representation Learning",
                    "Visual Reasoning"
                ]
            },
            {
                "title": "Generative Interventions for Causal Learning",
                "link": "https://ieeexplore.ieee.org/document/9577692/",
                "date_of_publication": "02 November 2021",
                "doi": "10.1109/CVPR46437.2021.00394",
                "citations": "11",
                "abstract": "We introduce a framework for learning robust visual representations that generalize to new viewpoints, backgrounds, and scene contexts. Discriminative models often learn naturally occurring spurious correlations, which cause them to fail on images outside of the training distribution. In this paper, we show that we can steer generative models to manufacture interventions on features caused by confounding factors. Experiments, visualizations, and theoretical results show this method learns robust representations more consistent with the underlying causal relationships. Our approach improves performance on multiple datasets demanding out-of-distribution generalization, and we demonstrate state-of-the-art performance generalizing from ImageNet to ObjectNet dataset.",
                "ieee_keywords": [
                    "Training",
                    "Visualization",
                    "Computer vision",
                    "Correlation",
                    "Computational modeling",
                    "Control systems",
                    "Pattern recognition"
                ],
                "author_keywords": []
            },
            {
                "title": "Adversarial Attacks are Reversible with Natural Supervision",
                "link": "https://ieeexplore.ieee.org/document/9710949/",
                "date_of_publication": "28 February 2022",
                "doi": "10.1109/ICCV48922.2021.00070",
                "citations": "3",
                "abstract": "We find that images contain intrinsic structure that enables the reversal of many adversarial attacks. Attack vectors cause not only image classifiers to fail, but also collaterally disrupt incidental structure in the image. We demonstrate that modifying the attacked image to restore the natural structure will reverse many types of attacks, providing a defense. Experiments demonstrate significantly improved robustness for several state-of-the-art models across the CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets. Our results show that our defense is still effective even if the attacker is aware of the defense mechanism. Since our defense is deployed during inference instead of training, it is compatible with pre-trained networks as well as most other defenses. Our results suggest deep networks are vulnerable to adversarial examples partly because their representations do not enforce the natural structure of images.",
                "ieee_keywords": [
                    "Training",
                    "Computer vision",
                    "Benchmark testing",
                    "Robustness",
                    "Inference algorithms",
                    "Image restoration"
                ],
                "author_keywords": [
                    "Recognition and classification",
                    "Adversarial learning"
                ]
            },
            {
                "title": "Revealing Occlusions with 4D Neural Fields",
                "link": "https://ieeexplore.ieee.org/document/9880031/",
                "date_of_publication": "27 September 2022",
                "doi": "10.1109/CVPR52688.2022.00302",
                "citations": "2",
                "abstract": "For computer vision systems to operate in dynamic situations, they need to be able to represent and reason about object permanence. We introduce a framework for learning to estimate 4D visual representations from monocular RGB-D video, which is able to persist objects, even once they become obstructed by occlusions. Unlike traditional video representations, we encode point clouds into a continuous representation, which permits the model to attend across the spatiotemporal context to resolve occlusions. On two large video datasets that we release along with this paper, our experiments show that the representation is able to successfully reveal occlusions for several tasks, without any architectural changes. Visualizations show that the attention mechanism automatically learns to follow occluded objects. Since our approach can be trained end-to-end and is easily adaptable, we believe it will be useful for handling occlusions in many video understanding tasks. Data, code, and models are available at occ1usions. cs. co1umbia. edu.",
                "ieee_keywords": [
                    "Point cloud compression",
                    "Computer vision",
                    "Visualization",
                    "Computational modeling",
                    "Data visualization",
                    "Data models",
                    "Spatiotemporal phenomena"
                ],
                "author_keywords": [
                    "Video analysis and understanding; 3D from multi-view and sensors; Deep learning architectures and techniques"
                ]
            },
            {
                "title": "What You Can Reconstruct from a Shadow",
                "link": "https://ieeexplore.ieee.org/document/10205120/",
                "date_of_publication": "22 August 2023",
                "doi": "10.1109/CVPR52729.2023.01636",
                "citations": "4",
                "abstract": "3D reconstruction is a fundamental problem in computer vision, and the task is especially challenging when the object to reconstruct is partially or fully occluded. We introduce a method that uses the shadows cast by an unobserved object in order to infer the possible 3D volumes under occlusion. We create a differentiable image formation model that allows us to jointly infer the 3D shape of an object, its pose, and the position of a light source. Since the approach is end-to-end differentiable, we are able to integrate learned priors of object geometry in order to generate realistic 3D shapes of different object categories. Experiments and visualizations show that the method is able to generate multiple possible solutions that are consistent with the observation of the shadow. Our approach works even when the position of the light source and object pose are both unknown. Our approach is also robust to real-world images where ground-truth shadow mask is unknown.",
                "ieee_keywords": [
                    "Solid modeling",
                    "Computer vision",
                    "Visualization",
                    "Three-dimensional displays",
                    "Shape",
                    "Computational modeling",
                    "Search problems"
                ],
                "author_keywords": [
                    "3D from single images"
                ]
            },
            {
                "title": "FLEX: Full-Body Grasping Without Full-Body Grasps",
                "link": "https://ieeexplore.ieee.org/document/10203567/",
                "date_of_publication": "22 August 2023",
                "doi": "10.1109/CVPR52729.2023.02029",
                "citations": "Abstract",
                "abstract": "Synthesizing 3D human avatars interacting realistically with a scene is an important problem with applications in AR/VR, video games, and robotics. Towards this goal, we address the task of generating a virtual human – hands and full body – grasping everyday objects. Existing methods approach this problem by collecting a 3D dataset of humans interacting with objects and training on this data. However, 1) these methods do not generalize to different object positions and orientations or to the presence of furniture in the scene, and 2) the diversity of their generated full-body poses is very limited. In this work, we address all the above challenges to generate realistic, diverse full-body grasps in everyday scenes without requiring any 3D full-body grasping data. Our key insight is to leverage the existence of both full-body pose and hand-grasping priors, composing them using 3D geometrical constraints to obtain full-body grasps. We empirically validate that these constraints can generate a variety of feasible human grasps that are superior to baselines both quantitatively and qualitatively. See our webpage for more details: flex.cs.columbia.edu.",
                "ieee_keywords": [
                    "Training",
                    "Video games",
                    "Solid modeling",
                    "Three-dimensional displays",
                    "Flexible printed circuits",
                    "Avatars",
                    "Grasping"
                ],
                "author_keywords": [
                    "Humans: Face",
                    "body",
                    "pose",
                    "gesture",
                    "movement",
                    "Media",
                    "More Like This",
                    "Virtual Reality Based Independent Travel Training System for Children with Intellectual Disability",
                    "2016 European Modelling Symposium (EMS)",
                    "Published: 2016",
                    "Impact of Full-Body Avatars in Immersive Multiplayer Virtual Reality Training for Police Forces",
                    "IEEE Transactions on Games",
                    "Published: 2022",
                    "Show More"
                ]
            },
            {
                "title": "Oops! Predicting Unintentional Action in Video",
                "link": "https://ieeexplore.ieee.org/document/9156404/",
                "date_of_publication": "05 August 2020",
                "doi": "10.1109/CVPR42600.2020.00100",
                "citations": "32",
                "abstract": "From just a short glance at a video, we can often tell whether a person's action is intentional or not. Can we train a model to recognize this? We introduce a dataset of in-the-wild videos of unintentional action, as well as a suite of tasks for recognizing, localizing, and anticipating its onset. We train a supervised neural network as a baseline and analyze its performance compared to human consistency on the tasks. We also investigate self-supervised representations that leverage natural signals in our dataset, and show the effectiveness of an approach that uses the intrinsic speed of video to perform competitively with highly-supervised pretraining. However, a significant gap between machine and human performance remains.",
                "ieee_keywords": [
                    "Task analysis",
                    "Visualization",
                    "Computational modeling",
                    "Analytical models",
                    "Benchmark testing",
                    "Training",
                    "Standards"
                ],
                "author_keywords": []
            },
            {
                "title": "Learning the Predictability of the Future",
                "link": "https://ieeexplore.ieee.org/document/9577709/",
                "date_of_publication": "02 November 2021",
                "doi": "10.1109/CVPR46437.2021.01242",
                "citations": "15",
                "abstract": "We introduce a framework for learning from unlabeled video what is predictable in the future. Instead of committing up front to features to predict, our approach learns from data which features are predictable. Based on the observation that hyperbolic geometry naturally and compactly encodes hierarchical structure, we propose a predictive model in hyperbolic space. When the model is most confident, it will predict at a concrete level of the hierarchy, but when the model is not confident, it learns to automatically select a higher level of abstraction. Experiments on two established datasets show the key role of hierarchical representations for action prediction. Although our representation is trained with unlabeled video, visualizations show that action hierarchies emerge in the representation.",
                "ieee_keywords": [
                    "Geometry",
                    "Computer vision",
                    "Uncertainty",
                    "Computational modeling",
                    "Predictive models",
                    "Encoding",
                    "Pattern recognition"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Suman Jana",
        "publications": [
            {
                "title": "NEUZZ: Efficient Fuzzing with Neural Program Smoothing",
                "link": "https://ieeexplore.ieee.org/document/8835342/",
                "date_of_publication": "16 September 2019",
                "doi": "10.1109/SP.2019.00052",
                "citations": "52",
                "abstract": "Fuzzing has become the de facto standard technique for finding software vulnerabilities. However, even state-of-the-art fuzzers are not very efficient at finding hard-to-trigger software bugs. Most popular fuzzers use evolutionary guidance to generate inputs that can trigger different bugs. Such evolutionary algorithms, while fast and simple to implement, often get stuck in fruitless sequences of random mutations. Gradient-guided optimization presents a promising alternative to evolutionary guidance. Gradient-guided techniques have been shown to significantly outperform evolutionary algorithms at solving high-dimensional structured optimization problems in domains like machine learning by efficiently utilizing gradients or higher-order derivatives of the underlying function. However, gradient-guided approaches are not directly applicable to fuzzing as real-world program behaviors contain many discontinuities, plateaus, and ridges where the gradient-based methods often get stuck. We observe that this problem can be addressed by creating a smooth surrogate function approximating the target program's discrete branching behavior. In this paper, we propose a novel program smoothing technique using surrogate neural network models that can incrementally learn smooth approximations of a complex, real-world program's branching behaviors. We further demonstrate that such neural network models can be used together with gradient-guided input generation schemes to significantly increase the efficiency of the fuzzing process. Our extensive evaluations demonstrate that NEUZZ significantly outperforms 10 state-of-the-art graybox fuzzers on 10 popular real-world programs both at finding new bugs and achieving higher edge coverage. NEUZZ found 31 previously unknown bugs (including two CVEs) that other fuzzers failed to find in 10 real-world programs and achieved 3X more edge coverage than all of the tested graybox fuzzers over 24 hour runs. Furthermore, NEUZZ also outperformed existi... (Show More)",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Certified Robustness to Adversarial Examples with Differential Privacy",
                "link": "https://ieeexplore.ieee.org/document/8835364/",
                "date_of_publication": "16 September 2019",
                "doi": "10.1109/SP.2019.00044",
                "citations": "190",
                "abstract": "Adversarial examples that fool machine learning models, particularly deep neural networks, have been a topic of intense research interest, with attacks and defenses being developed in a tight back-and-forth. Most past defenses are best effort and have been shown to be vulnerable to sophisticated attacks. Recently a set of certified defenses have been introduced, which provide guarantees of robustness to norm-bounded attacks. However these defenses either do not scale to large datasets or are limited in the types of models they can support. This paper presents the first certified defense that both scales to large networks and datasets (such as Google's Inception network for ImageNet) and applies broadly to arbitrary model types. Our defense, called PixelDP, is based on a novel connection between robustness against adversarial examples and differential privacy, a cryptographically-inspired privacy formalism, that provides a rigorous, generic, and flexible foundation for defense.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "HVLearn: Automated Black-Box Analysis of Hostname Verification in SSL/TLS Implementations",
                "link": "https://ieeexplore.ieee.org/document/7958596/",
                "date_of_publication": "26 June 2017",
                "doi": "10.1109/SP.2017.46",
                "citations": "36",
                "abstract": "Show in Context Google Scholar 31. D. Angluin, \"Learning Regular Sets from Queries and Counterexamples\", Inf. Comput., vol. 75, no. 2, pp. 87-106, 1987. Show in Context CrossRef Google Scholar 32. Apache Software Foundation. Apache HttpComponents-HttpComponents HttpClient Overview, [online] Available: https://hc.apache.org/httpcomponents-client-ga/. Google Scholar 33. G. Argyros, I. Stais, S. Jana, A. D. Keromytis and A. Kiayias, \"SFAD-iff: Automated Evasion Attacks and Fingerprinting Using Black-box Differential Automata Learning\", Proceedings of the ACM SIGSAC Conference on Computerand Communications Security, pp. 1690-1701, 2016. Show in Context Google Scholar 34. J. L. Balcázar, J. Díaz, R. Gavalda and O. Watanabe, Algorithms for Learning Finite Automata from Queries: A Unified View, Springer, pp. 53-72, 1997. Show in Context Google Scholar 35. D. Bleichenbacher, \"Chosen ciphertext attacks against protocols based on the RSA encryption standard PKCS# 1\", Proceedings of the Annual InternationalCryptology Conference on Advances in Cryptology, pp. 1-12, 1998. Show in Context Google Scholar 36. C. Brubaker, S. Jana, B. Ray, S. Khurshid and V. Shmatikov, \"Using Frankencerts for Automated Adversarial Testing of Certificate Validation in SSL/TLS Implementations\", Proceedings of the IEEE Symposium on Security and Privacy, pp. 114-129, 2014. Show in Context CrossRef Google Scholar 37. D. Brumley and D. Boneh, \"Remote Timing Attacks Are Practical\", Proceedings of the USENIX Conference on Security Symposium, pp. 1-1, 2003. Show in Context Google Scholar 38. Y. Chen and Z. Su, \"Guided Differential Testing of Certificate Validation in SSL/TLS Implementations\", Proceedings of the Joint Meeting on Foundations of Software Engineering, pp. 793-804, 2015. Show in Context CrossRef Google Scholar 39. Y. Chen and Z. Su, \"Guided Differential Testing of Certificate Validation in SSL/TLS Implementations\", Proceedings of the Joint Meeting on Foundations of Software Engineering, pp. 793-804, 2015. Show in Context CrossRef Google Scholar 40. J. Clark and P. C. Van, \"Oorschot. SoK: SSL and HTTPS: Revisiting Past Challenges and Evaluating Certificate Trust Model Enhancements\", Proceedings of the IEEE Symposium on Security and Privacy, pp. 511-525, 2013. Show in Context Google Scholar 41. J. De Ruiter and E. Poll, \"Protocol State Fuzzing of TLS Implementations\", Proceedings of the USENIX Conference on Security Symposium, pp. 193-206, 2015. Show in Context Google Scholar 42. Docjar. HostnameChecker, [online] Available: http://www.docjar.com/docs/api/sun/security/util/HostnameChecker.html. Google Scholar 43. T. Duong and J. Rizzo, Here Come The ⊕Ninjas, 2011. Show in Context Google Scholar 44. T. Duong and J. Rizzo, The CRIME Attack, 2012. Show in Context Google Scholar 45. S. Fahl, M. Harbach, T. Muders, L. Baumgärtner, B. Freisleben and M. Smith, \"Why Eve and Mallory Love Android: An Analysis of Android SSL (in)Security\", Proceedings of the ACM SIGSAC Conference on Computerand Communications Security, pp. 50-61, 2012. Show in Context CrossRef Google Scholar 46. N. J. A. Fardan and K. G. Paterson, \"Lucky thirteen: Breaking the tls and dtls record protocols\", Proceedings of the IEEE Symposium on Security and Privacy, pp. 526-540, 2013. Show in Context CrossRef Google Scholar 47. P. Fiterău-Broştean, R. Janssen and F. Vaandrager, \"Learning Fragments of the TCP Network Protocol\", Proceedings of the International Conference on Formal Methods for Industrial Critical Systems, pp. 78-93, 2014. Show in Context Google Scholar 48. P. Fiterău-Broştean, R. Janssen and F. Vaandrager, \"Combining Model Learning and Model Checking to Analyze TCP Implementations\", Proceedings of the International Conference on Computer Aided Verification, pp. 454-471, 2016. Show in Context Google Scholar 49. S. Fujiwara, G. V. Bochmann, F. Khendek, M. Amalou and A. Ghedamsi, \"Test Selection Based on Finite State Models\", IEEE Transactions on software engineering, vol. 17, no. 6, pp. 591-603, 1991. Show in Context View Article Google Scholar 50. M. Georgiev, S. Iyengar, S. Jana, R. Anubhai, D. Boneh and V. Shmatikov, \"The Most Dangerous Code in the World: Validating SSL Certificates in Non-browser Software\", Proceedings of the ACM SIGSAC Conference on Computerand Communications Security, pp. 38-49, 2012. Show in Context CrossRef Google Scholar 51. GNU Compilers. Gcov-Using the GNU Compiler Collection (GCC), [online] Available: https://gcc.gnu.org/onlinedocs/gcc-4.8.1/gcc/Gcov.html. Show in Context Google Scholar 52. B. He, V. Rastogi, Y. Cao, Y. Chen, V. Venkatakrishnan, R. Yang, et al., \"Vetting SSL usage in applications with SSLint\", Proceedings of the IEEE Symposium on Security and Privacy, pp. 519-534, 2015. Show in Context CrossRef Google Scholar 53. D. Kaminsky, M. L. Patterson and L. Sassaman, \"PKI Layer Cake: New Collision Attacks Against the Global x.509 Infrastructure\", Proceedings of the International Conference on Financial Cryptography and Data Security, pp. 289-303, 2010. Show in Context CrossRef Google Scholar 54. M. J. Kearns and U. V. Vazirani, An Introduction to Computational Learning Theory, MIT Press, 1994. Show in Context Google Scholar 55. D. Kozen, \"Lower Bounds for Natural Proof Systems\", Proceedings of the Annual Symposium on Foundations of Computer Science, pp. 254-266, 1977. Show in Context CrossRef Google Scholar 56. A. Langley, Apple's SSL/TLS Bug, 2014, [online] Available: https://goo.gl/DzRLNq. Show in Context Google Scholar 57. A. Lenstra, J. P. Hughes, M. Augier, J. W. Bos, T. Kleinjung and C. Wachter, \"Ron was wrong Whit is right\", International Association for Cryptologic Research, 2012. Show in Context Google Scholar 58. Oracle. Java Cryptography Architecture Oracle Providers Documentation, [online] Available: https://docs.oracle.com/javase/7/docs/technotes/guides/security/SunProviders.html. Google Scholar 59. H. Raffelt, B. Steffen and T. Berg, \"LearnLib: A Library for Automata Learning and Experimentation\", Proceedings of the International Workshop on Formal Methods for Industrial Critical Systems, pp. 62-71, 2005. Show in Context CrossRef Google Scholar 60. M. Sipser, Introduction to the Theory of Computation, Thomson Course Technology Boston, 2006. Show in Context Google Scholar 61. J. Somorovsky, \"Systematic Fuzzing and Testing of TLS Libraries\", Proceedings of the ACM SIGSAC Conference on Computerand Communications Security, pp. 1492-1504, 2016. Show in Context CrossRef Google Scholar",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "NEZHA: Efficient Domain-Independent Differential Testing",
                "link": "https://ieeexplore.ieee.org/document/7958601/",
                "date_of_publication": "26 June 2017",
                "doi": "10.1109/SP.2017.27",
                "citations": "36",
                "abstract": "Differential testing uses similar programs as cross-referencing oracles to find semantic bugs that do not exhibit explicit erroneous behaviors like crashes or assertion failures. Unfortunately, existing differential testing tools are domain-specific and inefficient, requiring large numbers of test inputs to find a single bug. In this paper, we address these issues by designing and implementing NEZHA, an efficient input-format-agnostic differential testing framework. The key insight behind NEZHA's design is that current tools generate inputs by simply borrowing techniques designed for finding crash or memory corruption bugs in individual programs (e.g., maximizing code coverage). By contrast, NEZHA exploits the behavioral asymmetries between multiple test programs to focus on inputs that are more likely to trigger semantic bugs. We introduce the notion of δ-diversity, which summarizes the observed asymmetries between the behaviors of multiple test applications. Based on δ-diversity, we design two efficient domain-independent input generation mechanisms for differential testing, one gray-box and one black-box. We demonstrate that both of these input generation schemes are significantly more efficient than existing tools at finding semantic bugs in real-world, complex software. NEZHA's average rate of finding differences is 52 times and 27 times higher than that of Frankencerts and Mucerts, two popular domain-specific differential testing tools that check SSL/TLS certificate validation implementations, respectively. Moreover, performing differential testing with NEZHA results in 6 times more semantic bugs per tested input, compared to adapting state-of-the-art general-purpose fuzzers like American Fuzzy Lop (AFL) to differential testing by running them on individual test programs for input generation. NEZHA discovered 778 unique, previously unknown discrepancies across a wide variety of applications (ELF and XZ parsers, PDF viewers and SSL/TLS libraries), many of which... (Show More)",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Neutaint: Efficient Dynamic Taint Analysis with Neural Networks",
                "link": "https://ieeexplore.ieee.org/document/9152790/",
                "date_of_publication": "30 July 2020",
                "doi": "10.1109/SP40000.2020.00022",
                "citations": "15",
                "abstract": "Dynamic taint analysis (DTA) is widely used by various applications to track information flow during runtime execution. Existing DTA techniques use rule-based taint-propagation, which is neither accurate (i.e., high false positive rate) nor efficient (i.e., large runtime overhead). It is hard to specify taint rules for each operation while covering all corner cases correctly. Moreover, the overtaint and undertaint errors can accumulate during the propagation of taint information across multiple operations. Finally, rule-based propagation requires each operation to be inspected before applying the appropriate rules resulting in prohibitive performance overhead on large real-world applications.In this work, we propose Neutaint, a novel end-to-end approach to track information flow using neural program embeddings. The neural program embeddings model the target's programs computations taking place between taint sources and sinks, which automatically learns the information flow by observing a diverse set of execution traces. To perform lightweight and precise information flow analysis, we utilize saliency maps to reason about most influential sources for different sinks. Neutaint constructs two saliency maps, a popular machine learning approach to influence analysis, to summarize both coarse-grained and fine-grained information flow in the neural program embeddings.We compare Neutaint with 3 state-of-the-art dynamic taint analysis tools. The evaluation results show that Neutaint can achieve 68% accuracy, on average, which is 10% improvement while reducing 40× runtime overhead over the second-best taint tool Libdft on 6 real world programs. Neutaint also achieves 61% more edge coverage when used for taint-guided fuzzing indicating the effectiveness of the identified influential bytes. We also evaluate Neutaint's ability to detect real world software attacks. The results show that Neutaint can successfully detect different types of vulnerabilities including buffer/heap/int... (Show More)",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Precise Detection of Kernel Data Races with Probabilistic Lockset Analysis",
                "link": "https://ieeexplore.ieee.org/document/10179366/",
                "date_of_publication": "21 July 2023",
                "doi": "10.1109/SP46215.2023.10179366",
                "citations": "33",
                "abstract": "Finding data races is critical for ensuring security in modern kernel development. However, finding data races in the kernel is challenging because it requires jointly searching over possible combinations of system calls and concurrent execution schedules. Kernel race testing systems typically perform this search by executing groups of fuzzer seeds from a corpus and applying a combination of schedule fuzzing and dynamic race prediction on traces. However, predicting which combinations of seeds can expose races in the kernel is difficult as fuzzer seeds will usually follow different execution paths when executed concurrently due to inter-thread communications and synchronization.To address this challenge, we introduce a new analysis for kernel race prediction, Probabilistic Lockset Analysis (PLA) that addresses the challenges posed by race prediction for the kernel. PLA leverages the observation that system calls almost always perform certain memory accesses to shared memory to perform their function. PLA uses randomized concurrent trace sampling to identify memory accesses that are performed consistently and estimates the probability of races between them subject to kernel lock synchronization. By prioritizing high probability races, PLA is able to make accurate predictions.We evaluate PLA against comparable kernel race testing methods and show it finds races at a 3× higher rate over 24 hours. We use PLA to find 183 races in linux kernel v5.18-rc5, including 102 harmful races. PLA is able to find races that have severe security impact in heavily tested core kernel modules, including use-after-free in memory management, OOB write in network cryptography, and leaking kernel heap memory information. Some of these vulnerabilities have been overlooking by existing systems for years: one of the races found by PLA involving an OOB write has been present in the kernel since 2013 (version v3.14-rc1) and has been designated a high severity CVE. (Show More)",
                "ieee_keywords": [
                    "Schedules",
                    "Privacy",
                    "Linux",
                    "Memory management",
                    "Programmable logic arrays",
                    "Fuzzing",
                    "Probabilistic logic"
                ],
                "author_keywords": [
                    "systems-security",
                    "kernel-security",
                    "concurrent-program-testing",
                    "software-testing"
                ]
            },
            {
                "title": "Learning Approximate Execution Semantics From Traces for Binary Function Similarity",
                "link": "https://ieeexplore.ieee.org/document/10002189/",
                "date_of_publication": null,
                "doi": "10.1109/TSE.2022.3231621",
                "citations": "323",
                "abstract": "Detecting semantically similar binary functions – a crucial capability with broad security usages including vulnerability detection, malware analysis, and forensics – requires understanding function behaviors and intentions. This task is challenging as semantically similar functions can be compiled to run on different architectures and with diverse compiler optimizations or obfuscations. Most existing approaches match functions based on syntactic features without understanding the functions’ execution semantics. We present Trex , a transfer-learning-based framework, to automate learning approximate execution semantics explicitly from functions’ traces collected via forced-execution (i.e., by violating the control flow semantics) and transfer the learned knowledge to match semantically similar functions. While it is known that forced-execution traces are too imprecise to be directly used to detect semantic similarity, our key insight is that these traces can instead be used to teach an ML model approximate execution semantics of diverse instructions and their compositions. We thus design a pretraining task, which trains the model to learn approximate execution semantics from the two modalities (i.e., forced-executed code and traces) of the function. We then finetune the pretrained model to match semantically similar functions. We evaluate Trex on 1,472,066 functions from 13 popular software projects, compiled to run on 4 architectures (x86, x64, ARM, and MIPS), and with 4 optimizations ( O0 - O3 ) and 5 obfuscations. Trex outperforms the state-of-the-art solutions by 7.8%, 7.2%, and 14.3% in cross-architecture, optimization, and obfuscation function matching, respectively, while running 8× faster. Ablation studies suggest that the pretraining significantly boosts the function matching performance, underscoring the importance of learning execution semantics. Our case studies demonstrate the practical use-cases of Trex – on 180 real-world firmware images, Trex ... (Show More)",
                "ieee_keywords": [
                    "Semantics",
                    "Task analysis",
                    "Computer architecture",
                    "Optimization",
                    "Codes",
                    "Behavioral sciences",
                    "Computational modeling"
                ],
                "author_keywords": [
                    "Binary analysis",
                    "large language models",
                    "software security"
                ]
            },
            {
                "title": "Vision Paper: Grand Challenges in Resilience: Autonomous System Resilience through Design and Runtime Measures",
                "link": "https://ieeexplore.ieee.org/document/9133332/",
                "date_of_publication": null,
                "doi": "10.1109/OJCS.2020.3006807",
                "citations": "10",
                "abstract": "In this article, we put forward the substantial challenges in cyber resilience in the domain of autonomous systems and outline foundational solutions to address these challenges. These solutions fall into two broad themes: resilience-by-design and resilience-by-reaction. We use several application drivers from autonomous systems to motivate the challenges in cyber resilience and to demonstrate the benefit of the solutions. We focus on some autonomous systems in the near horizon (autonomous ground and aerial vehicles) and also a little more distant (autonomous rescue and relief). For resilience-by-design, we focus on design methods in software that are needed for our cyber systems to be resilient. In contrast, for resilience-by-reaction, we discuss how to make systems resilient by responding, reconfiguring, or recovering at runtime when failures happen. We also discuss the notion of adaptive execution to improve resilience, execution transparently and adaptively among available execution platforms (mobile/embedded, edge, and cloud). For each of the two themes, we survey the current state, and the desired state and ways to get there. We conclude the paper by looking at the research challenges we will have to solve in the short and the mid-term to make the vision of resilient autonomous systems a reality. This article came out of discussions that started at the NSF-sponsored Grand Challenges in Resilience Workshop held at Purdue in 2019 with the co-authors contributing to going into the depth of the issues and then this article.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Effective Seed Scheduling for Fuzzing with Graph Centrality Analysis",
                "link": "https://ieeexplore.ieee.org/document/9833761/",
                "date_of_publication": "27 July 2022",
                "doi": "10.1109/SP46214.2022.9833761",
                "citations": "4",
                "abstract": "Seed scheduling, the order in which seeds are selected, can greatly affect the performance of a fuzzer. Existing approaches schedule seeds based on their historical mutation data, but ignore the structure of the underlying Control Flow Graph (CFG). Examining the CFG can help seed scheduling by revealing the potential edge coverage gain from mutating a seed. An ideal strategy will schedule seeds based on a count of all reachable and feasible edges from a seed through mutations, but computing feasibility along all edges is prohibitively expensive. Therefore, a seed scheduling strategy must approximate this count. We observe that an approximate count should have 3 properties -(i) it should increase if there are more edges reachable from a seed; (ii) it should decrease if mutation history information suggests an edge is hard to reach or is located far away from currently visited edges; and (iii) it should be efficient to compute over large CFGs. We observe that centrality measures from graph analysis naturally provide these three properties and therefore can efficiently approximate the likelihood of reaching unvisited edges by mutating a seed. We therefore build a graph called the edge horizon graph that connects seeds to their closest unvisited nodes and compute the seed node’s centrality to measure the potential edge coverage gain from mutating a seed. We implement our approach in K-Scheduler and compare with many popular seed scheduling strategies. We find that K-Scheduler increases feature coverage by 25.89% compared to Entropic and edge coverage by 4.21% compared to the next-best AFL-based seed scheduler, in arithmetic mean on 12 Google FuzzBench programs. It also finds 3 more previously-unknown bugs than the next-best AFL-based seed scheduler.",
                "ieee_keywords": [
                    "Schedules",
                    "Privacy",
                    "Processor scheduling",
                    "Gain measurement",
                    "Fuzzing",
                    "Internet",
                    "Security"
                ],
                "author_keywords": [
                    "Fuzzing",
                    "Seed-Scheduler",
                    "Katz-Centrality",
                    "Graph-Analysis"
                ]
            },
            {
                "title": "DeepTest: Automated Testing of Deep-Neural-Network-Driven Autonomous Cars",
                "link": "https://ieeexplore.ieee.org/document/8453089/",
                "date_of_publication": "02 September 2018",
                "doi": "10.1145/3180155.3180220",
                "citations": "355",
                "abstract": "Recent advances in Deep Neural Networks (DNNs) have led to the development of DNN-driven autonomous cars that, using sensors like camera, LiDAR, etc., can drive without any human intervention. Most major manufacturers including Tesla, GM, Ford, BMW, and Waymo/Google are working on building and testing different types of autonomous vehicles. The lawmakers of several US states including California, Texas, and New York have passed new legislation to fast-track the process of testing and deployment of autonomous vehicles on their roads. However, despite their spectacular progress, DNNs, just like traditional software, often demonstrate incorrect or unexpected corner-case behaviors that can lead to potentially fatal collisions. Several such real-world accidents involving autonomous cars have already happened including one which resulted in a fatality. Most existing testing techniques for DNN-driven vehicles are heavily dependent on the manual collection of test data under different driving conditions which become prohibitively expensive as the number of test conditions increases. In this paper, we design, implement, and evaluate DeepTest, a systematic testing tool for automatically detecting erroneous behaviors of DNN-driven vehicles that can potentially lead to fatal crashes. First, our tool is designed to automatically generated test cases leveraging real-world changes in driving conditions like rain, fog, lighting conditions, etc. DeepTest systematically explore different parts of the DNN logic by generating test inputs that maximize the numbers of activated neurons. DeepTest found thousands of erroneous behaviors under different realistic driving conditions (e.g., blurring, rain, fog, etc.) many of which lead to potentially fatal crashes in three top performing DNNs in the Udacity self-driving car challenge.",
                "ieee_keywords": [
                    "Neurons",
                    "Autonomous automobiles",
                    "Software",
                    "Testing",
                    "Automobiles",
                    "Sensors",
                    "Computer architecture"
                ],
                "author_keywords": [
                    "deep learning",
                    "testing",
                    "self-driving cars",
                    "deep neural networks",
                    "autonomous vehicle",
                    "neuron coverage"
                ]
            }
        ]
    },
    {
        "name": "Changxi Zheng",
        "publications": [
            {
                "title": "Two-Color and 3D Phase-Amplitude Modulation Holograms",
                "link": "https://ieeexplore.ieee.org/document/8426414/",
                "date_of_publication": "09 August 2018",
                "doi": null,
                "citations": "74",
                "abstract": "We report a high-efficiency dielectric metasurface with continuous and arbitrary control of both amplitude and phase of one or two colors simultaneously. We numerically and experimentally demonstrate 2D and 3D holograms using such metasurfaces.",
                "ieee_keywords": [
                    "Holography",
                    "Three-dimensional displays",
                    "Two dimensional displays",
                    "Image reconstruction",
                    "Image color analysis",
                    "Lenses",
                    "Interference"
                ],
                "author_keywords": []
            },
            {
                "title": "Ultra-broadband Nanophotonics via Adaptive Inverse Design",
                "link": "https://ieeexplore.ieee.org/document/9573023/",
                "date_of_publication": "29 October 2021",
                "doi": null,
                "citations": "77",
                "abstract": "We present an inverse design method for achieving unprecedented performance and ultra wide bandwidth based on direct adaptive refinement of the device geometry. We experimentally demonstrate a 90/10 splitter with more than 200 nm bandwidth.",
                "ieee_keywords": [
                    "Integrated optics",
                    "Performance evaluation",
                    "Geometry",
                    "Nanophotonics",
                    "Design methodology",
                    "Waveguide lasers",
                    "Bandwidth"
                ],
                "author_keywords": [
                    "(130.3120) Integrated optics devices",
                    "(230.3120) Integrated optics devices",
                    "(230.7370) Waveguides"
                ]
            },
            {
                "title": "Inverse Geometric Design of Fabrication-Robust Nanophotonic Waveguides",
                "link": "https://ieeexplore.ieee.org/document/9191966/",
                "date_of_publication": "10 September 2020",
                "doi": null,
                "citations": "96",
                "abstract": "We present an inverse design method making waveguides with high performance and high robustness to fabrication errors. As an example, we show a 1-to-4 mode converter with > -1.5 dB conversion efficiency under geometric variations within fabrication tolerances.",
                "ieee_keywords": [
                    "Fabrication",
                    "Geometry",
                    "Optical waveguides",
                    "Optimization",
                    "Design methodology",
                    "Robustness",
                    "Scattering"
                ],
                "author_keywords": []
            },
            {
                "title": "Dynamic Sliding Window for Realtime Denoising Networks",
                "link": "https://ieeexplore.ieee.org/document/9747168/",
                "date_of_publication": "27 April 2022",
                "doi": "10.1109/ICASSP43922.2022.9747168",
                "citations": "447",
                "abstract": "Realtime speech denoising has been long studied. Almost all existing methods process the incoming data stream using a sliding window of fixed-size. Yet, we show that the use of fixed-size sliding window may lead to an accumulating lag, especially in presence of other background computing processes that may occupy CPU resources. In response, we propose a new sliding window strategy and a lightweight neural network to leverage it. Our experiments show that the proposed approach achieves denoising quality on a par with the stateof-the-art realtime denoising models. More importantly, our approach is faster, maintaining a stable realtime performance even when the available computing power fluctuates.",
                "ieee_keywords": [
                    "Conferences",
                    "Computational modeling",
                    "Noise reduction",
                    "Neural networks",
                    "Signal processing",
                    "Acoustics",
                    "Task analysis"
                ],
                "author_keywords": [
                    "realtime",
                    "denoising",
                    "sliding window",
                    "neural network",
                    "data stream"
                ]
            },
            {
                "title": "High-efficiency amplitude-phase modulation holograms based on dielectric metasurfaces",
                "link": "https://ieeexplore.ieee.org/document/8083110/",
                "date_of_publication": "26 October 2017",
                "doi": null,
                "citations": "356",
                "abstract": "We report a high-efficiency dielectric metasurface with continuous and arbitrary control of both amplitude and phase. We experimentally demonstrated the advantages of complete wavefront control by comparing amplitude-phase modulation metasurface holograms to phase-only metasurface holograms.",
                "ieee_keywords": [
                    "Holography",
                    "Dielectrics",
                    "Two dimensional displays",
                    "Interference",
                    "Laser beams",
                    "Polarization"
                ],
                "author_keywords": []
            },
            {
                "title": "Mechanics-Aware Modeling of Cloth Appearance",
                "link": "https://ieeexplore.ieee.org/document/8812922/",
                "date_of_publication": null,
                "doi": "10.1109/TVCG.2019.2937301",
                "citations": "9",
                "abstract": "Micro-appearance models have brought unprecedented fidelity and details to cloth rendering. Yet, these models neglect fabric mechanics: when a piece of cloth interacts with the environment, its yarn and fiber arrangement usually changes in response to external contact and tension forces. Since subtle changes of a fabric's microstructures can greatly affect its macroscopic appearance, mechanics-driven appearance variation of fabrics has been a phenomenon that remains to be captured. We introduce a mechanics-aware model that adapts the microstructures of cloth yarns in a physics-based manner. Our technique works on two distinct physical scales: using physics-based simulations of individual yarns, we capture the rearrangement of yarn-level structures in response to external forces. These yarn structures are further enriched to obtain appearance-driving fiber-level details. The cross-scale enrichment is made practical through a new parameter fitting algorithm for simulation, an augmented procedural yarn model coupled with a custom-design regression neural network. We train the network using a dataset generated by joint simulations at both the yarn and the fiber levels. Through several examples, we demonstrate that our model is capable of synthesizing photorealistic cloth appearance in a mechanically plausible way.",
                "ieee_keywords": [
                    "Yarn",
                    "Fabrics",
                    "Computational modeling",
                    "Microstructure",
                    "Training",
                    "Geometry",
                    "Neural networks"
                ],
                "author_keywords": [
                    "Cloth appearance",
                    "cloth mechanics"
                ]
            },
            {
                "title": "DeepCAD: A Deep Generative Network for Computer-Aided Design Models",
                "link": "https://ieeexplore.ieee.org/document/9710909/",
                "date_of_publication": "28 February 2022",
                "doi": "10.1109/ICCV48922.2021.00670",
                "citations": "17",
                "abstract": "Deep generative models of 3D shapes have received a great deal of research interest. Yet, almost all of them generate discrete shape representations, such as voxels, point clouds, and polygon meshes. We present the first 3D generative model for a drastically different shape representation— describing a shape as a sequence of computer-aided design (CAD) operations. Unlike meshes and point clouds, CAD models encode the user creation process of 3D shapes, widely used in numerous industrial and engineering design tasks. However, the sequential and irregular structure of CAD operations poses significant challenges for existing 3D generative models. Drawing an analogy between CAD operations and natural language, we propose a CAD generative network based on the Transformer. We demonstrate the performance of our model for both shape autoencoding and random shape generation. To train our network, we create a new CAD dataset consisting of 178,238 models and their CAD construction sequences. We have made this dataset publicly available to promote future research on this topic.",
                "ieee_keywords": [
                    "Point cloud compression",
                    "Solid modeling",
                    "Computer vision",
                    "Three-dimensional displays",
                    "Design automation",
                    "Shape",
                    "Computational modeling"
                ],
                "author_keywords": [
                    "Neural generative models",
                    "3D from a single image and shape-from-x",
                    "Datasets and evaluation",
                    "Representation learning"
                ]
            },
            {
                "title": "FishGym: A High-Performance Physics-based Simulation Framework for Underwater Robot Learning",
                "link": "https://ieeexplore.ieee.org/document/9812066/",
                "date_of_publication": "12 July 2022",
                "doi": "10.1109/ICRA46639.2022.9812066",
                "citations": "271",
                "abstract": "Bionic underwater robots have demonstrated their superiority in many applications. Yet, training their intelligence for a variety of tasks that mimic the behavior of underwater creatures poses a number of challenges in practice, mainly due to lack of a large amount of available training data as well as the high cost in real physical environment. Alternatively, simulation has been considered as a viable and important tool for acquiring datasets in different environments, but it mostly targeted rigid and soft body systems. There is currently dearth of work for more complex fluid systems interacting with immersed solids that can be efficiently and accurately simulated for robot training purposes. In this paper, we propose a new platform called “FishGym”, which can be used to train fish-like underwater robots. The framework consists of a robotic fish modeling module using articulated body with skinning, a GPU-based high-performance localized two-way coupled fluid-structure interaction simulation module that handles both finite and infinitely large domains, as well as a reinforcement learning module. We leveraged existing training methods with adaptations to underwater fish-like robots and obtained learned control policies for multiple benchmark tasks. The training results are demonstrated with reasonable motion trajectories, with comparisons and analyses to empirical models as well as known real fish swimming behaviors to highlight the advantages of the proposed platform.",
                "ieee_keywords": [
                    "Training",
                    "Autonomous underwater vehicles",
                    "Adaptation models",
                    "Fluids",
                    "Reinforcement learning",
                    "Benchmark testing",
                    "Fish"
                ],
                "author_keywords": []
            },
            {
                "title": "One Man’s Trash Is Another Man’s Treasure: Resisting Adversarial Examples by Adversarial Examples",
                "link": "https://ieeexplore.ieee.org/document/9156804/",
                "date_of_publication": "05 August 2020",
                "doi": "10.1109/CVPR42600.2020.00049",
                "citations": "9",
                "abstract": "Modern image classification systems are often built on deep neural networks, which suffer from adversarial examples--images with deliberately crafted, imperceptible noise to mislead the network's classification. To defend against adversarial examples, a plausible idea is to obfuscate the network's gradient with respect to the input image. This general idea has inspired a long line of defense methods. Yet, almost all of them have proven vulnerable. We revisit this seemingly flawed idea from a radically different perspective. We embrace the omnipresence of adversarial examples and the numerical procedure of crafting them, and turn this harmful attacking process into a useful defense mechanism. Our defense method is conceptually simple: before feeding an input image for classification, transform it by finding an adversarial example on a pre-trained external model. We evaluate our method against a wide range of possible attacks. On both CIFAR-10 and Tiny ImageNet datasets, our method is significantly more robust than state-of-the-art methods. Particularly, in comparison to adversarial training, our method offers lower training cost as well as stronger robustness.",
                "ieee_keywords": [
                    "Training",
                    "Robustness",
                    "Perturbation methods",
                    "Neural networks",
                    "Transforms",
                    "Mathematical model",
                    "Numerical models"
                ],
                "author_keywords": []
            },
            {
                "title": "Linear Semantics in Generative Adversarial Networks",
                "link": "https://ieeexplore.ieee.org/document/9578156/",
                "date_of_publication": "02 November 2021",
                "doi": "10.1109/CVPR46437.2021.00923",
                "citations": "9",
                "abstract": "Generative Adversarial Networks (GANs) are able to generate high-quality images, but it remains difficult to explicitly specify the semantics of synthesized images. In this work, we aim to better understand the semantic representation of GANs, and thereby enable semantic control in GAN’s generation process. Interestingly, we find that a well-trained GAN encodes image semantics in its internal feature maps in a surprisingly simple way: a linear transformation of feature maps suffices to extract the generated image semantics. To verify this simplicity, we conduct extensive experiments on various GANs and datasets; and thanks to this simplicity, we are able to learn a semantic segmentation model for a trained GAN from a small number (e.g., 8) of labeled images. Last but not least, leveraging our finding, we propose two few-shot image editing approaches, namely Semantic-Conditional Sampling and Semantic Image Editing. Given a trained GAN and as few as eight semantic annotations, the user is able to generate diverse images subject to a user-provided semantic layout, and control the synthesized image semantics. We have made the code publicly available 1 .",
                "ieee_keywords": [
                    "Training",
                    "Image segmentation",
                    "Annotations",
                    "Face recognition",
                    "Semantics",
                    "Layout",
                    "Process control"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Shuran Song",
        "publications": [
            {
                "title": "Visual Perspective Taking for Opponent Behavior Modeling",
                "link": "https://ieeexplore.ieee.org/document/9562028/",
                "date_of_publication": "18 October 2021",
                "doi": "10.1109/ICRA48506.2021.9562028",
                "citations": "1",
                "abstract": "In order to engage in complex social interaction, humans learn at a young age to infer what others see and cannot see from a different point-of-view, and learn to predict others’ plans and behaviors. These abilities have been mostly lacking in robots, sometimes making them appear awkward and socially inept. Here we propose an end-to-end long-term visual prediction framework for robots to begin to acquire both these critical cognitive skills, known as Visual Perspective Taking (VPT) and Theory of Behavior (TOB). We demonstrate our approach in the context of visual hide-and-seek – a game that represents a cognitive milestone in human development. Unlike traditional visual predictive model that generates new frames from immediate past frames, our agent can directly predict to multiple future timestamps (25 s), extrapolating by 175% beyond the training horizon. We suggest that visual behavior modeling and perspective taking skills will play a critical role in the ability of physical robots to fully integrate into real-world multi-agent activities.",
                "ieee_keywords": [
                    "Training",
                    "Visualization",
                    "Automation",
                    "Conferences",
                    "Games",
                    "Predictive models",
                    "Robots"
                ],
                "author_keywords": []
            },
            {
                "title": "Scene Editing as Teleoperation: A Case Study in 6DoF Kit Assembly",
                "link": "https://ieeexplore.ieee.org/document/9982158/",
                "date_of_publication": "26 December 2022",
                "doi": "10.1109/IROS47612.2022.9982158",
                "citations": "177",
                "abstract": "Studies in robot teleoperation have been centered around action specifications-from continuous joint control to discrete end-effector pose control. However, these “robot-centric” interfaces often require skilled operators with extensive robotics expertise. To make teleoperation accessible to nonexpert users, we propose the framework “Scene Editing as Teleoperation” (SEaT), where the key idea is to transform the traditional “robot-centric” interface into a “scene-centric” interface-instead of controlling the robot, users focus on specifying the task's goal by manipulating digital twins of the real-world objects. As a result, a user can perform teleoperation without any expert knowledge of the robot hardware. To achieve this goal, we utilize a category-agnostic scene-completion algorithm that translates the real-world workspace (with unknown objects) into a manipulable virtual scene representation and an action-snapping algorithm that refines the user input before generating the robot's action plan. To train the algorithms, we procedurely generated a large-scale, diverse kit-assembly dataset that contains object-kit pairs that mimic real-world object-kitting tasks. Our experiments in simulation and on a real-world system demonstrate that our framework improves both the efficiency and success rate for 6DoF kit-assembly tasks. A user study demonstrates that SEaT framework participants achieve a higher task success rate and report a lower subjective workload compared to an alternative robot-centric interface.",
                "ieee_keywords": [
                    "Transforms",
                    "Hardware",
                    "End effectors",
                    "Planning",
                    "Digital twins",
                    "Task analysis",
                    "Robots"
                ],
                "author_keywords": []
            },
            {
                "title": "AdaGrasp: Learning an Adaptive Gripper-Aware Grasping Policy",
                "link": "https://ieeexplore.ieee.org/document/9560833/",
                "date_of_publication": "18 October 2021",
                "doi": "10.1109/ICRA48506.2021.9560833",
                "citations": "7",
                "abstract": "This paper aims to improve robots’ versatility and adaptability by allowing them to use a large variety of end- effector tools and quickly adapt to new tools. We propose AdaGrasp, a method to learn a single grasping policy that generalizes to novel grippers. By training on a large collection of grippers, our algorithm is able to acquire generalizable knowledge of how different grippers should be used in various tasks. Given a visual observation of the scene and the gripper, AdaGrasp infers the possible grasp poses and their grasp scores by computing the cross convolution between the shape encodings of the gripper and scene. Intuitively, this cross convolution operation can be considered as an efficient way of exhaustively matching the scene geometry with gripper geometry under different grasp poses (i.e., translations and orientations), where a good \"match\" of 3D geometry will lead to a successful grasp. We validate our methods in both simulation and real- world environments. Our experiment shows that AdaGrasp significantly outperforms the existing multi-gripper grasping policy method, especially when handling cluttered environments and partial observations. Code and Data are available at https://adagrasp.cs.columbia.edu.",
                "ieee_keywords": [
                    "Geometry",
                    "Training",
                    "Visualization",
                    "Three-dimensional displays",
                    "Convolution",
                    "Shape",
                    "Grasping"
                ],
                "author_keywords": []
            },
            {
                "title": "Dynamic Grasping with Reachability and Motion Awareness",
                "link": "https://ieeexplore.ieee.org/document/9636057/",
                "date_of_publication": "16 December 2021",
                "doi": "10.1109/IROS51168.2021.9636057",
                "citations": "9",
                "abstract": "Grasping in dynamic environments presents a unique set of challenges. A stable and reachable grasp can become unreachable and unstable as the target object moves, motion planning needs to be adaptive and in real time, the delay in computation makes prediction necessary. In this paper, we present a dynamic grasping framework that is reachability-aware and motion-aware. Specifically, we model the reachability space of the robot using a signed distance field which enables us to quickly screen unreachable grasps. Also, we train a neural network to predict the grasp quality conditioned on the current motion of the target. Using these as ranking functions, we quickly filter a large grasp database to a few grasps in real time. In addition, we present a seeding approach for arm motion generation that utilizes solution from previous time step. This quickly generates a new arm trajectory that is close to the previous plan and prevents fluctuation. We implement a recurrent neural network (RNN) for modelling and predicting the object motion. Our extensive experiments demonstrate the importance of each of these components and we validate our pipeline on a real robot.",
                "ieee_keywords": [
                    "Recurrent neural networks",
                    "Fluctuations",
                    "Dynamics",
                    "Pipelines",
                    "Grasping",
                    "Predictive models",
                    "Real-time systems"
                ],
                "author_keywords": []
            },
            {
                "title": "TANDEM3D: Active Tactile Exploration for 3D Object Recognition",
                "link": "https://ieeexplore.ieee.org/document/10161091/",
                "date_of_publication": "04 July 2023",
                "doi": "10.1109/ICRA48891.2023.10161091",
                "citations": "74",
                "abstract": "Tactile recognition of 3D objects remains a challenging task. Compared to 2D shapes, the complex geometry of 3D surfaces requires richer tactile signals, more dexterous actions, and more advanced encoding techniques. In this work, we propose TANDEM3D, a method that applies a co-training framework for exploration and decision making to 3D object recognition with tactile signals. Starting with our previous work, which introduced a co-training paradigm for 2D recognition problems, we introduce a number of advances that enable us to scale up to 3D. TANDEM3D is based on a novel encoder that builds 3D object representation from contact positions and normals using PointNet++. Furthermore, by enabling 6DOF movement, TANDEM3D explores and collects discriminative touch information with high efficiency. Our method is trained entirely in simulation and validated with real-world experiments. Compared to state-of-the-art baselines, TANDEM3D achieves higher accuracy and a lower number of actions in recognizing 3D objects and is also shown to be more robust to different types and amounts of sensor noise.",
                "ieee_keywords": [
                    "Geometry",
                    "Three-dimensional displays",
                    "Shape",
                    "Decision making",
                    "Fingers",
                    "Tactile sensors",
                    "Encoding"
                ],
                "author_keywords": []
            },
            {
                "title": "Cloth Funnels: Canonicalized-Alignment for Multi-Purpose Garment Manipulation",
                "link": "https://ieeexplore.ieee.org/document/10161546/",
                "date_of_publication": "04 July 2023",
                "doi": "10.1109/ICRA48891.2023.10161546",
                "citations": "55",
                "abstract": "Automating garment manipulation is challenging due to extremely high variability in object configurations. To reduce this intrinsic variation, we introduce the task of “canonicalized-alignment” that simplifies downstream applications by reducing the possible garment configurations. This task can be considered as “cloth state funnel” that manipulates arbitrarily configured clothing items into a predefined deformable configuration (i.e. canonicalization) at an appropriate rigid pose (i.e. alignment). In the end, the cloth items will result in a compact set of structured and highly visible configurations - which are desirable for downstream manipulation skills. To enable this task, we propose a novel canonicalized-alignment objective that effectively guides learning to avoid adverse local minima during learning. Using this objective, we learn a multi-arm, multi-primitive policy that strategically chooses between dynamic flings and quasi-static pick and place actions to achieve efficient canonicalized-alignment. We evaluate this approach on a real-world ironing and folding system that relies on this learned policy as the common first step. Empirically, we demonstrate that our task-agnostic canonicalized-alignment can enable even simple manually -designed policies to work well where they were pre-viously inadequate, thus bridging the gap between automated non-deformable manufacturing and deformable manipulation.",
                "ieee_keywords": [
                    "Automation",
                    "Clothing",
                    "Pipelines",
                    "Manufacturing",
                    "Complexity theory",
                    "Task analysis"
                ],
                "author_keywords": []
            },
            {
                "title": "CoWs on Pasture: Baselines and Benchmarks for Language-Driven Zero-Shot Object Navigation",
                "link": "https://ieeexplore.ieee.org/document/10203853/",
                "date_of_publication": "22 August 2023",
                "doi": "10.1109/CVPR52729.2023.02219",
                "citations": "2",
                "abstract": "For robots to be generally useful, they must be able to find arbitrary objects described by people (i.e., be language-driven) even without expensive navigation training on in-domain data (i.e., perform zero-shot inference). We explore these capabilities in a unified setting: language- driven zero-shot object navigation (L-ZSON). Inspired by the recent success of open-vocabulary models for image classification, we investigate a straightforward framework, CLIP on Wheels (CoW), to adapt open-vocabulary models to this task without fine-tuning. To better evaluate L-ZSON, we introduce the Pasturebenchmark, which considers finding uncommon objects, objects described by spatial and appearance attributes, and hidden objects described relative to visible objects. We conduct an in-depth empirical study by directly deploying 22 CoW baselines across Habitat, Robothor,and Pasture. In total, we evaluate over 90k navigation episodes and find that (1) CoW baselines often struggle to leverage language descriptions but are proficient at finding uncommon objects. (2) A simple Co W, with CLIP-based object localization and classical exploration-and no additional training-matches the navigation efficiency of a state-of-the-art ZSON method trained for 500M steps on HabitatMp3d data. This same CoW provides a 15.6 percentage point improvement in success over a state-of-the-art ROBOTHOR ZSON model. 1 1 For code, data, and videos, see cow.cs.columbia.edu/",
                "ieee_keywords": [
                    "Training",
                    "Location awareness",
                    "Adaptation models",
                    "Navigation",
                    "Wheels",
                    "Cows",
                    "Pattern recognition"
                ],
                "author_keywords": [
                    "Embodied vision: Active agents",
                    "simulation"
                ]
            },
            {
                "title": "Universal Manipulation Policy Network for Articulated Objects",
                "link": "https://ieeexplore.ieee.org/document/9681198/",
                "date_of_publication": null,
                "doi": "10.1109/LRA.2022.3142397",
                "citations": "8",
                "abstract": "We introduce the Universal Manipulation Policy Network (UMPNet) – a single image-based policy network that infers closed-loop action sequences for manipulating articulated objects. To infer a wide range of action trajectories, the policy supports 6DoF action representation and varying trajectory length. To handle a diverse set of objects, the policy learns from objects with different articulation structures and generalizes to unseen objects or categories. The policy is trained with self-guided exploration without any human demonstrations, scripted policy, or pre-defined goal conditions. To support effective multi-step interaction, we introduce a novel Arrow-of-Time action attribute that indicates whether an action will change the object state back to the past or forward into the future. With the Arrow-of-Time inference at each interaction step, the learned policy is able to select actions that consistently lead towards or away from a given state, thereby, enabling both effective state exploration and goal-conditioned manipulation.",
                "ieee_keywords": [
                    "Trajectory",
                    "Task analysis",
                    "Three-dimensional displays",
                    "Visualization",
                    "Affordances",
                    "Robots",
                    "End effectors"
                ],
                "author_keywords": [
                    "Deep learning in grasping and manipulation",
                    "perception for grasping and manipulation"
                ]
            },
            {
                "title": "TANDEM: Learning Joint Exploration and Decision Making With Tactile Sensors",
                "link": "https://ieeexplore.ieee.org/document/9839393/",
                "date_of_publication": null,
                "doi": "10.1109/LRA.2022.3193466",
                "citations": "3",
                "abstract": "Inspired by the human ability to perform complex manipulation in the complete absence of vision (like retrieving an object from a pocket), the robotic manipulation field is motivated to develop new methods for tactile-based object interaction. However, tactile sensing presents the challenge of being an active sensing modality: a touch sensor provides sparse, local data, and must be used in conjunction with effective exploration strategies in order to collect information. In this work, we focus on the process of guiding tactile exploration, and its interplay with task-related decision making. We propose TANDEM (TActile exploration aNd DEcision Making), an architecture to learn efficient exploration strategies in conjunction with decision making. Our approach is based on separate but co-trained modules for exploration and discrimination. We demonstrate this method on a tactile object recognition task, where a robot equipped with a touch sensor must explore and identify an object from a known set based on binary contact signals alone. TANDEM achieves higher accuracy with fewer actions than alternative methods and is also shown to be more robust to sensor noise.",
                "ieee_keywords": [
                    "Object recognition",
                    "Task analysis",
                    "Decision making",
                    "Sensors",
                    "Manipulators",
                    "Training",
                    "Tactile sensors"
                ],
                "author_keywords": [
                    "Force and tactile sensing",
                    "reinforcement learning",
                    "recognition",
                    "deep learning",
                    "tactile exploration"
                ]
            },
            {
                "title": "FishGym: A High-Performance Physics-based Simulation Framework for Underwater Robot Learning",
                "link": "https://ieeexplore.ieee.org/document/9812066/",
                "date_of_publication": "12 July 2022",
                "doi": "10.1109/ICRA46639.2022.9812066",
                "citations": "271",
                "abstract": "Bionic underwater robots have demonstrated their superiority in many applications. Yet, training their intelligence for a variety of tasks that mimic the behavior of underwater creatures poses a number of challenges in practice, mainly due to lack of a large amount of available training data as well as the high cost in real physical environment. Alternatively, simulation has been considered as a viable and important tool for acquiring datasets in different environments, but it mostly targeted rigid and soft body systems. There is currently dearth of work for more complex fluid systems interacting with immersed solids that can be efficiently and accurately simulated for robot training purposes. In this paper, we propose a new platform called “FishGym”, which can be used to train fish-like underwater robots. The framework consists of a robotic fish modeling module using articulated body with skinning, a GPU-based high-performance localized two-way coupled fluid-structure interaction simulation module that handles both finite and infinitely large domains, as well as a reinforcement learning module. We leveraged existing training methods with adaptations to underwater fish-like robots and obtained learned control policies for multiple benchmark tasks. The training results are demonstrated with reasonable motion trajectories, with comparisons and analyses to empirical models as well as known real fish swimming behaviors to highlight the advantages of the proposed platform.",
                "ieee_keywords": [
                    "Training",
                    "Autonomous underwater vehicles",
                    "Adaptation models",
                    "Fluids",
                    "Reinforcement learning",
                    "Benchmark testing",
                    "Fish"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Rocco Anthony Servedio",
        "publications": [
            {
                "title": "The Perils of Being Unhinged: On the Accuracy of Classifiers Minimizing a Noise-Robust Convex Loss",
                "link": "https://ieeexplore.ieee.org/document/9931054/",
                "date_of_publication": null,
                "doi": "10.1162/neco_a_01502",
                "citations": "1",
                "abstract": "van Rooyen, Menon, and Williamson (2015) introduced a notion of convex loss functions being robust to random classification noise and established that the “unhinged” loss function is robust in this sense. In this letter, we study the accuracy of binary classifiers obtained by minimizing the unhinged loss and observe that even for simple linearly separable data distributions, minimizing the unhinged loss may only yield a binary classifier with accuracy no better than random guessing.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "A Regularity Lemma, and Low-Weight Approximators, for Low-Degree Polynomial Threshold Functions",
                "link": "https://ieeexplore.ieee.org/document/5497883/",
                "date_of_publication": "01 July 2010",
                "doi": "10.1109/CCC.2010.28",
                "citations": "9",
                "abstract": "We give a \"regularity lemma\" for degree-d polynomial threshold functions (PTFs) over the Boolean cube {-1,1} n . Roughly speaking, this result shows that every degree-d PTF can be decomposed into a constant number of subfunctions such that almost all of the subfunctions are close to being regular PTFs. Here a \"regular\" PTF is a PTF sign(p(x)) where the influence of each variable on the polynomial p(x) is a small fraction of the total influence of p. As an application of this regularity lemma, we prove that for any constants d ≥ 1, ϵ > 0, every degree-d PTF over n variables can be approximated to accuracy eps by a constant degree PTF that has integer weights of total magnitude O(n d ). This weight bound is shown to be optimal up to logarithmic factors.",
                "ieee_keywords": [
                    "Polynomials",
                    "Boolean functions",
                    "Computational complexity",
                    "Decision trees",
                    "Computer science",
                    "USA Councils",
                    "Bipartite graph",
                    "Combinatorial mathematics",
                    "Complexity theory",
                    "Coordinate measuring machines"
                ],
                "author_keywords": [
                    "regularity lemma",
                    "polynomial threshold function",
                    "Boolean function"
                ]
            },
            {
                "title": "Beyond Trace Reconstruction: Population Recovery from the Deletion Channel",
                "link": "https://ieeexplore.ieee.org/document/8948611/",
                "date_of_publication": "06 January 2020",
                "doi": "10.1109/FOCS.2019.00050",
                "citations": "11",
                "abstract": "Population recovery is the problem of learning an unknown distribution over an unknown set of n-bit strings, given access to independent draws from the distribution that have been independently corrupted according to some noise channel. Recent work has intensively studied such problems both for the bit-flip noise channel and for the erasure noise channel. In this paper we initiate the study of population recovery under the deletion channel, in which each bit b is independently deleted with some fixed probability and the surviving bits are concatenated and transmitted. This is a far more challenging noise model than bit-flip~noise or erasure noise; indeed, even the simplest case in which the population is of size 1 (corresponding to a trivial probability distribution supported on a single string) corresponds to the trace reconstruction problem, which is a challenging problem that has received much recent attention. In this work we give algorithms and lower bounds for population recovery under the deletion channel when the population size is some value ℓ > 1. As our main sample complexity upper bound, we show that for any population size ℓ = o(log n / log log n), a population of ℓ strings from {o,1} n can be learned under deletion channel noise using 2 n(1/2+o(1)) samples. On the lower bound side, we show that at least n Ω(ℓ) samples are required to perform population recovery under the deletion channel when the population size is ℓ, for all ℓ ≤ n 1/2-ε . Our upper bounds are obtained via a robust multivariate generalization of a polynomial-based analysis, due to Krasikov and Roddity [KR97], of how the k-deck of a bit-string uniquely identifies the string; this is a very different approach from recent algorithms for trace reconstruction (the ℓ = 1 case). Our lower bounds build on moment-matching results of Roos[Roos:00] and Daskalakis and Papadimitriou[DP15].",
                "ieee_keywords": [
                    "Sociology",
                    "Statistics",
                    "Complexity theory",
                    "Upper bound",
                    "Noise measurement",
                    "DNA"
                ],
                "author_keywords": [
                    "lower bounds",
                    "foundations of machine learning",
                    "population recovery",
                    "trace reconstruction",
                    "randomized algorithms",
                    "deletion channel"
                ]
            },
            {
                "title": "Deterministic Approximate Counting for Juntas of Degree-2 Polynomial Threshold Functions",
                "link": "https://ieeexplore.ieee.org/document/6875492/",
                "date_of_publication": "14 August 2014",
                "doi": "10.1109/CCC.2014.31",
                "citations": "4",
                "abstract": "Let g : {-1, 1} k → {-1, 1} be any Boolean function and q 1 , . . . , q k be any degree-2 polynomials over {-1, 1} n . We give a deterministic algorithm which, given as input explicit descriptions of g, q 1 , . . ., q k and an accuracy parameter ϵ > 0, approximates Pr x~ {-1, 1} n [g(sign(q 1 (x)), . . . , sign(q k (x))) = 1] to within an additive ±ϵ. For any constant ϵ > 0 and k ≥ 1 the running time of our algorithm is a fixed polynomial in n (in fact this is true even for some not-too-small ϵ = 0 n (1) and not-too-large k = ω n (1)). This is the first fixed polynomial-time algorithm that can deterministically approximately count satisfying assignments of a natural class of depth-3 Boolean circuits. Our algorithm extends a recent result [1] which gave a deterministic approximate counting algorithm for a single degree-2 polynomial threshold function sign(q(x)), corresponding to the k = 1 case of our result. Note that even in the k = 1 case it is NP-hard to determine whether Pr x~ {-1, 1} n [sign(q(x)) = 1] is nonzero, so any sort of multiplicative approximation is almost certainly impossible even for efficient randomized algorithms. Our algorithm and analysis requires several novel technical ingredients that go significantly beyond the tools required to handle the k = 1 case in [1]. One of these is a new multidimensional central limit theorem for degree-2 polynomials in Gaussian random variables which builds on recent Malliavin-calculus-based results from probability theory. We use this CLT as the basis of a new decomposition technique for k-tuples of degree-2 Gaussian polynomials and thus obtain an efficient deterministic approximate counting algorithm for the Gaussian distribution, i.e., an algorithm for estimating Pr x~N (0, 1) n [g(sign(q 1 (x)), . . . , sign(q k (x))) = 1]. Finally, a third new ingredient is a “regularity lemma” for k-tuples of degree-d polynomial threshold functions. This generalizes both the regularity lemmas of [2], [3] (which apply... (Show More)",
                "ieee_keywords": [
                    "Polynomials",
                    "Approximation algorithms",
                    "Approximation methods",
                    "Random variables",
                    "Eigenvalues and eigenfunctions",
                    "Covariance matrices",
                    "Calculus"
                ],
                "author_keywords": [
                    "Approximate counting",
                    "derandomization",
                    "polynomial threshold function"
                ]
            },
            {
                "title": "New Algorithms and Lower Bounds for Monotonicity Testing",
                "link": "https://ieeexplore.ieee.org/document/6979013/",
                "date_of_publication": "11 December 2014",
                "doi": "10.1109/FOCS.2014.38",
                "citations": "19",
                "abstract": "We consider the problem of testing whether an unknown Boolean function f : {- 1, 1} n → {-1, 1} is monotone versus ε-far from every monotone function. The two main results of this paper are a new lower bound and a new algorithm for this well-studied problem. Lower bound: We prove an Ω̅(n 1/5 ) lower bound on the query complexity of any non-adaptive two-sided error algorithm for testing whether an unknown Boolean function f is monotone versus constant-far from monotone. This gives an exponential improvement on the previous lower bound of Ω(log n) due to Fischer et al. [1]. We show that the same lower bound holds for monotonicity testing of Boolean-valued functions over hypergrid domains {1,···, m} n for all m ≥ 2. Upper bound: We present an O(n 5/6 ) poly(1/ε)-query algorithm that tests whether an unknown Boolean function f is monotone versus ε-far from monotone. Our algorithm, which is non-adaptive and makes one-sided error, is a modified version of the algorithm of Chakrabarty and Seshadhri[2], which makes O(n 7/8 ) poly(1/ε) queries.",
                "ieee_keywords": [
                    "Testing",
                    "Boolean functions",
                    "Complexity theory",
                    "Random variables",
                    "Upper bound",
                    "Algorithm design and analysis",
                    "Vectors"
                ],
                "author_keywords": [
                    "Boolean functions",
                    "Property testing",
                    "Monotonicity testing"
                ]
            },
            {
                "title": "Learning Sums of Independent Integer Random Variables",
                "link": "https://ieeexplore.ieee.org/document/6686157/",
                "date_of_publication": "19 December 2013",
                "doi": "10.1109/FOCS.2013.31",
                "citations": "18",
                "abstract": "Let bS = bX_1 + ·s + bX_n be a sum of n independent integer random variables bX_i, where each bX_i is supported on 0, 1, ·, k-1 but otherwise may have an arbitrary distribution (in particular the bX_i's need not be identically distributed). How many samples are required to learn the distribution bS to high accuracy? In this paper we show that the answer is completely independent of n, and moreover we give a computationally efficient algorithm which achieves this low sample complexity. More precisely, our algorithm learns any such bS to ε-accuracy (with respect to the total variation distance between distributions) using poly(k, 1/ε) samples, independent of n. Its running time is poly(k, 1/ε) in the standard word RAM model. Thus we give a broad generalization of the main result of DDS12stoc which gave a similar learning result for the special case k=2 (when the distribution bS is a Poisson Binomial Distribution). Prior to this work, no nontrivial results were known for learning these distributions even in the case k=3. A key difficulty is that, in contrast to the case of k = 2, sums of independent 0, 1, 2-valued random variables may behave very differently from (discretized) normal distributions, and in fact may be rather complicated - they are not log-concave, they can be θ(n)-modal, there is no relationship between Kolmogorov distance and total variation distance for the class, etc. Nevertheless, the heart of our learning result is a new limit theorem which characterizes what the sum of an arbitrary number of arbitrary independent 0, 1, ·, k-1-valued random variables may look like. Previous limit theorems in this setting made strong assumptions on the \"shift invariance\" of the random variables bX_i in order to force a discretized normal limit. We believe that our new limit theorem, as the first result for truly arbitrary sums of independent 0, 1, ·, k-1-valued random variables, is of independent interest.",
                "ieee_keywords": [
                    "Random variables",
                    "Digital TV",
                    "Gaussian distribution",
                    "Accuracy",
                    "Complexity theory",
                    "Approximation methods",
                    "Standards"
                ],
                "author_keywords": [
                    "limit theorem",
                    "discrete distribution learning",
                    "sums of independent integer random variables"
                ]
            },
            {
                "title": "Learning Sums of Independent Random Variables with Sparse Collective Support",
                "link": "https://ieeexplore.ieee.org/document/8555114/",
                "date_of_publication": "02 December 2018",
                "doi": "10.1109/FOCS.2018.00036",
                "citations": "2",
                "abstract": "We study the learnability of sums of independent integer random variables given a bound on the size of the union of their supports. For a A ⊂Z + ubset A of non-negative integers, a sum of independent random variables with collective support A (called an \"A-sum\" in this paper) is a distribution S = X 1 + ... + X N where the X i 's are mutually independent (but not necessarily identically distributed) integer random variables all of whose supports are contained in A. We give two main algorithmic results for learning such distributions: 1) For the case |A|=3, we give an algorithm for learning A-sums to accuracy ε that uses poly(1/ε) samples and runs in time poly(1/ε), independent of N and of the elements of A. 2) For an arbitrary constant k>=4, if A = {a 1 ,...,a k } with 0<;=a 1 <; ... <; a k , we give an algorithm that uses poly(1/ε)*log log a k samples (independent of N) and runs in time poly(1/ε, log a k ). We prove an essentially matching lower bound: if |A| = 4, then any algorithm must use Ω(log log a 4 ) samples even for learning to constant accuracy. We also give similar-in-spirit (but quantitatively very different) algorithmic results, and essentially matching lower bounds, for the case in which A is not known to the learner. Our learning algorithms employ new limit theorems which may be of independent interest. Our algorithms and lower bounds together settle the question of how the sample complexity of learning sums of independent integer random variables scales with the elements in the union of their supports, both in the known-support and unknown-support settings. Finally, all our algorithms easily extend to the \"semi-agnostic\" learning model, in which training data is generated from a distribution that is only c*ε-close to some A-sum for a constant c>0.",
                "ieee_keywords": [
                    "Random variables",
                    "Complexity theory",
                    "Computer science",
                    "Hidden Markov models",
                    "Standards",
                    "Unsupervised learning",
                    "Tools"
                ],
                "author_keywords": [
                    "distribution learning",
                    "sums of independent random variables",
                    "central limit theorems",
                    "unsupervised learning",
                    "sample complexity"
                ]
            },
            {
                "title": "An Average-Case Depth Hierarchy Theorem for Boolean Circuits",
                "link": "https://ieeexplore.ieee.org/document/7354441/",
                "date_of_publication": "17 December 2015",
                "doi": "10.1109/FOCS.2015.67",
                "citations": "15",
                "abstract": "We prove an average-case depth hierarchy theorem for Boolean circuits over the standard basis of AND, OR, and NOT gates. Our hierarchy theorem says that for every d ≥ 2, there is an explicit n-variable Boolean function f, computed by a linear-size depth-d formula, which is such that any depth-(d - 1) circuit that agrees with f on (1/2 + o n (1)) fraction of all inputs must have size exp(n Ω(1/d) ). This answers an open question posed by Hastad in his Ph.D. thesis [Has86b]. Our average-case depth hierarchy theorem implies that the polynomial hierarchy is infinite relative to a random oracle with probability 1, confirming a conjecture of Hastad [Has86a], Cai [Cai86], and Babai [Bab87]. We also use our result to show that there is no “approximate converse” to the results of Linial, Mansour, Nisan [LMN93] and Boppana [Bop97] on the total influence of constant-depth circuits, thus answering a question posed by Kalai [Kal12] and Hatami [Hat14]. A key ingredient in our proof is a notion of random projections which generalize random restrictions.",
                "ieee_keywords": [
                    "Polynomials",
                    "Correlation",
                    "Complexity theory",
                    "Boolean functions",
                    "Logic gates",
                    "Computational modeling",
                    "Integrated circuit modeling"
                ],
                "author_keywords": [
                    "Small-depth circuits",
                    "average-case",
                    "depth hierarchy theorem",
                    "Polynomial Hierarchy",
                    "random projections"
                ]
            },
            {
                "title": "Deterministic Search for CNF Satisfying Assignments in Almost Polynomial Time",
                "link": "https://ieeexplore.ieee.org/document/8104112/",
                "date_of_publication": "13 November 2017",
                "doi": "10.1109/FOCS.2017.80",
                "citations": "2",
                "abstract": "We consider the fundamental derandomization problem of deterministically finding a satisfying assignment to a CNF formula that has many satisfying assignments. We give a deterministic algorithm which, given an n-variable poly(n)-clause CNF formula F that has at least ε2 n satisfying assignments, runs in time n(Õ(log log n) 2 ) for ε ≥ 1/polylog(n) and outputs a satisfying assignment of F. Prior to our work the fastest known algorithm for this problem was simply to enumerate over all seeds of a pseudorandom generator for CNFs; using the best known PRGs for CNFs [DETT10], this takes time n Ω̃(log n) even for constant ε. Our approach is based on a new general framework relating deterministic search and deterministic approximate counting, which we believe may find further applications.",
                "ieee_keywords": [
                    "Approximation algorithms",
                    "Search problems",
                    "Generators",
                    "Computer science",
                    "Complexity theory",
                    "Runtime",
                    "Standards"
                ],
                "author_keywords": [
                    "Unconditional derandomization",
                    "CNF satisfiability",
                    "deterministic approximate counting"
                ]
            },
            {
                "title": "Fooling Intersections of Low-Weight Halfspaces",
                "link": "https://ieeexplore.ieee.org/document/8104113/",
                "date_of_publication": "13 November 2017",
                "doi": "10.1109/FOCS.2017.81",
                "citations": "4",
                "abstract": "A weight-t halfspace is a Boolean function f(x) = sign(w 1 x 1 + ⋯ + w n x n - θ) where each w i is an integer in {-t, . . . , t}. We give an explicit pseudorandom generator that δ-fools any intersection of k weight-t halfspaces with seed length poly(log n, log k, t, 1/δ). In particular, our result gives an explicit PRG that fools any intersection of any quasipoly(n) number of halfspaces of any polylog(n) weight to any 1/polylog(n) accuracy using seed length polylog(n). Prior to this work no explicit PRG with non-trivial seed length was known even for fooling intersections of n weight-1 halfspaces to constant accuracy. The analysis of our PRG fuses techniques from two different lines of work on unconditional pseudorandomness for different kinds of Boolean functions. We extend the approach of Harsha, Klivans and Meka [HKM12] for fooling intersections of regular halfspaces, and combine this approach with results of Bazzi [Baz07] and Razborov [Raz09] on bounded independence fooling CNF formulas. Our analysis introduces new couplingbased ingredients into the standard Lindeberg method for establishing quantitative central limit theorems and associated pseudorandomness results.",
                "ieee_keywords": [
                    "Generators",
                    "Boolean functions",
                    "Standards",
                    "Complexity theory",
                    "Computer science",
                    "Random variables",
                    "Approximation algorithms"
                ],
                "author_keywords": [
                    "Unconditional derandomization",
                    "intersection of halfspaces",
                    "Lindeberg method"
                ]
            }
        ]
    },
    {
        "name": "Xi Chen",
        "publications": [
            {
                "title": "VisDrone-MOT2019: The Vision Meets Drone Multiple Object Tracking Challenge Results",
                "link": "https://ieeexplore.ieee.org/document/9022306/",
                "date_of_publication": "05 March 2020",
                "doi": "10.1109/ICCVW.2019.00028",
                "citations": "11",
                "abstract": "The Vision Meets Drone Multiple Object Tracking (MOT) Challenge 2019 is the second annual activity focusing on evaluating multi-object tracking algorithms on drones, held in conjunction with the 17-th International Conference on Computer Vision (ICCV 2019). Results of 12 submitted MOT algorithms on the collected drone-based dataset are presented. Meanwhile, we also report the results of 6 state-of-the-art MOT algorithms, and provide a comprehensive analysis and discussion of the results. The results of all submissions are publicly available at the website: http://www.aiskyeye.com/. The challenge results show that MOT on drones is far from being solved. We believe the challenge can largely boost the research and development in MOT on drone platforms.",
                "ieee_keywords": [
                    "Drones",
                    "Target tracking",
                    "Object tracking",
                    "Task analysis",
                    "Feature extraction",
                    "Computer vision"
                ],
                "author_keywords": [
                    "Multiple Object Tracking",
                    "Drone",
                    "Benchmark"
                ]
            },
            {
                "title": "Inactivation of Bacillus Spores Using a Low-Temperature Atmospheric Plasma Brush",
                "link": "https://ieeexplore.ieee.org/document/5466067/",
                "date_of_publication": null,
                "doi": "10.1109/TPS.2010.2049129",
                "citations": "14",
                "abstract": "The plasma sporicidal effects on Bacillus atrophaeus spores were studied using a low-temperature atmospheric plasma brush in terms of the following conditions: 1) plasma conditions; 2) plasma gas compositions; 3) plasma exposure time; and 4) the types of supporting media. It was found that the plasma brush with O_2 addition was very effective in inactivating the Bacillus spores. With 0.33, 0.7, and 1 vol % (5, 10, and 15 sccm) oxygen addition, the plasma exposure time of achieving a 99.9999% reduction of the spores was less than 4, 2.5, and 1.5 min, respectively. It was noted that the plasma inactivation efficiency was also dependent on the type of supporting media, on which the spores were seeded. With plasma exposure, significant damages to sporal structure were observed by scanning electron microscopic (SEM) examination and leakage of intracellular proteins and DNAs were detected by monitoring the light absorbance at wavelengths of 280 and 260 nm, respectively. Results obtained in this paper indicate that the low-temperature atmospheric plasma technology is promising in various decontamination applications such as sterilization of daily used heat-sensitive items and emergency treatment of biological warfare agents.",
                "ieee_keywords": [
                    "Fungi",
                    "Plasma waves",
                    "Brushes",
                    "Scanning electron microscopy",
                    "Atrophy",
                    "Proteins",
                    "Leak detection",
                    "Monitoring",
                    "Atmospheric waves",
                    "Plasma applications"
                ],
                "author_keywords": [
                    "Atmospheric gas plasmas",
                    "decontamination",
                    "glow discharge",
                    "spores",
                    "sterilization"
                ]
            },
            {
                "title": "An Efficient Optimization Design for 1 MHz Ultrasonic Transmitting Transducer",
                "link": "https://ieeexplore.ieee.org/document/9328167/",
                "date_of_publication": null,
                "doi": "10.1109/JSEN.2021.3052375",
                "citations": "9",
                "abstract": "Due to the low efficiency of traditional design method, an efficient optimization design method is developed to design and fabricate high-performance ultrasonic transmitting transducer (UTT) based on PiezoCAD software and particle swarm optimization (PSO) algorithm. Based on the data of design and performance parameters of UTT obtained by PiezoCAD software, the genetic algorithm-based back-propagation neural networks (GA-BPNNs) are established to describe their mapping relationship. The optimality criterions are established based on the performance parameters including center frequency (CF) and maximum echo amplitude (MEA). Then, based on the established GA-BPNNs and optimality criterions, the optimization design method of UTT is developed under the framework of PSO algorithm. According to the desired performance, the design parameters of UTT, including the thickness and diameter of PZT-4 (Pb(ZrxTi1-x)O3), the acoustic impedance of backing, are optimized by the developed method, and they are 2 mm, 30 mm and 3 MRayl, respectively. Based on the optimized design parameters, the CF and MEA simulated by PiezoCAD are 1.05 MHz and -45.1 dB, which are in good agreement with the designed ones (1 MHz and -45 dB). In addition, the CF of the fabricated UTT is 1.07 MHz, and its insertion loss is -7.2 dB at 1.075MHz, which implies that the fabricated UTT has excellent performance. Therefore, the developed optimization design method is effective to fabricate high-performance UTT.",
                "ieee_keywords": [
                    "Optimization",
                    "Acoustics",
                    "Design methodology",
                    "Software",
                    "Impedance",
                    "Transducers",
                    "Genetic algorithms"
                ],
                "author_keywords": [
                    "Ultrasonic transmitting transducer",
                    "back-propagation neural network",
                    "optimization design",
                    "particle swarm optimization algorithm"
                ]
            },
            {
                "title": "Non-Invasive Heart Rate Estimation From Ballistocardiograms Using Bidirectional LSTM Regression",
                "link": "https://ieeexplore.ieee.org/document/9422793/",
                "date_of_publication": null,
                "doi": "10.1109/JBHI.2021.3077002",
                "citations": "7",
                "abstract": "Non-invasive heart rate estimation is of great importance in daily monitoring of cardiovascular diseases. In this paper, a bidirectional long short term memory (bi-LSTM) regression network is developed for non-invasive heart rate estimation from the ballistocardiograms (BCG) signals. The proposed deep regression model provides an effective solution to the existing challenges in BCG heart rate estimation, such as the mismatch between the BCG signals and ground-truth reference, multi-sensor fusion and effective time series feature learning. Allowing label uncertainty in the estimation can reduce the manual cost of data annotation while further improving the heart rate estimation performance. Compared with the state-of-the-art BCG heart rate estimation methods, the strong fitting and generalization ability of the proposed deep regression model maintains better robustness to noise ( e.g. , sensor noise) and perturbations ( e.g. , body movements) in the BCG signals and provides a more reliable solution for long term heart rate monitoring.",
                "ieee_keywords": [
                    "Estimation",
                    "Heart rate variability",
                    "Monitoring",
                    "Heart beat",
                    "Hydraulic systems",
                    "Electrocardiography",
                    "Transducers"
                ],
                "author_keywords": [
                    "Ballistocardiograms",
                    "deep feature learning",
                    "bidirectional LSTM",
                    "heart rate estimation",
                    "hydraulic bed sensor",
                    "MeSH Terms",
                    "Ballistocardiography",
                    "Data Curation",
                    "Heart Rate",
                    "Humans",
                    "Monitoring, Physiologic",
                    "Movement"
                ]
            },
            {
                "title": "A Cost-Sensitive Dense Network for Fault Diagnosis under Data Imbalance",
                "link": "https://ieeexplore.ieee.org/document/10058443/",
                "date_of_publication": "13 March 2023",
                "doi": "10.1109/ICSMD57530.2022.10058443",
                "citations": "60",
                "abstract": "Intelligent Fault Diagnosis (IFD) is crucial to guarantee the secure and stable functioning of mechanical equipment. The development of deep learning is continuously injecting vitality into IFD. However, in real-world industrial scenarios, obtaining sufficient fault data is difficult, and the fault data is much less than the normal data. Therefore, existing deep learning methods degrade performance when dealing with imbalanced fault diagnosis tasks, which poses a significant challenge to IFD under imbalanced data. To solve the above issue, a cost-sensitive dense network (CSD-Net) based on the improved dense convolutional neural network (DenseNet) and adaptive weighted cross-entropy (AWCE) is proposed, which includes a fault classification module as well as a cost adaptive module. Specifically, the improved DenseNet is used as a feature extractor in the fault classification module to obtain a more efficient feature extraction capability with fewer training parameters. The scaled exponential linear units (SELU) activation function serves to increase the stability of the model. In the cost adaptation module, AWCE adaptively assigns more appropriate misclassification costs to each class to lessen the effects of data imbalance. Eventually, experiments with different levels of class imbalance are designed and confirmed the primacy and efficacy of the proposed method.",
                "ieee_keywords": [
                    "Fault diagnosis",
                    "Training",
                    "Deep learning",
                    "Adaptation models",
                    "Costs",
                    "Adaptive systems",
                    "Feature extraction"
                ],
                "author_keywords": [
                    "intelligent fault diagnosis",
                    "DenseNet",
                    "data imbalance",
                    "cost-sensitive learning"
                ]
            },
            {
                "title": "Local binary pattern network: A deep learning approach for face recognition",
                "link": "https://ieeexplore.ieee.org/document/7532955/",
                "date_of_publication": "19 August 2016",
                "doi": "10.1109/ICIP.2016.7532955",
                "citations": "49",
                "abstract": "Deep learning is well known as a method to extract hierarchical representations of data. In this paper a novel unsupervised deep learning based methodology, named Local Binary Pattern Network (LBPNet), is proposed to efficiently extract and compare high-level over-complete features in multilayer hierarchy. The LBPNet retains the same topology of Convolutional Neural Network (CNN) - one of the most well studied deep learning architectures - whereas the trainable kernels are replaced by the off-the-shelf computer vision descriptor (i.e., LBP). This enables the LBPNet to achieve a high recognition accuracy without requiring any costly model learning approach on massive data. Through extensive numerical experiments using the public benchmarks (i.e., FERET and LFW), LBPNet has shown that it is comparable to other unsupervised methods.",
                "ieee_keywords": [
                    "Feature extraction",
                    "Principal component analysis",
                    "Face",
                    "Machine learning",
                    "Face recognition",
                    "Computer architecture",
                    "Kernel"
                ],
                "author_keywords": [
                    "Deep learning",
                    "Local Binary Pattern",
                    "PCA",
                    "Convolutional Neural Network"
                ]
            },
            {
                "title": "Compute-and-Forward for Uplink Non-Orthogonal Multiple Access",
                "link": "https://ieeexplore.ieee.org/document/8374813/",
                "date_of_publication": null,
                "doi": "10.1109/LWC.2018.2844852",
                "citations": "8",
                "abstract": "Successive interference cancellation (SIC) has been regarded as the de facto decoding method in the non-orthogonal multiple access (NOMA) system. However, SIC often requires the paired users to have significantly different received power levels for the uplink scenario. This is problematic in terms of fairness and outage probabilities when the paired users are close to each other. To address this challenge, we propose a new decoding method for uplink NOMA based on compute-and-forward. In particular, we show that our method achieves better fairness and smaller average outage probabilities while enjoying essentially the same complexity as SIC decoding.",
                "ieee_keywords": [
                    "Decoding",
                    "NOMA",
                    "Interference cancellation",
                    "Power system reliability",
                    "Probability",
                    "Uplink",
                    "Signal to noise ratio"
                ],
                "author_keywords": [
                    "5G wireless network",
                    "compute-and-forward",
                    "non-orthogonal multiple access",
                    "receiver complexity"
                ]
            },
            {
                "title": "The study of portable remote multi-life-parameter monitoring network",
                "link": "https://ieeexplore.ieee.org/document/6026728/",
                "date_of_publication": "22 September 2011",
                "doi": "10.1109/HEALTH.2011.6026728",
                "citations": "215",
                "abstract": "With the development of modern signal processing and computer network technology, it brings new challenges to remote health care structure. This paper discusses a prototype which realizes the functions of physiological signal collection and calculation, GPS global positioning, and data transmission through GPRS. The new type of portable multi-life-parameter physiologic monitoring terminal is based on embedded design method. It consists of physiological signal collection unit, GPS unit, data storage unit, data analysis unit, LCD (320*240) display unit and data communication unit. The physiological signal collection unit includes ECG, temperature, blood pressure and blood sugar part. The server system deals with physiological database, user information, interacting of physician-patient with audio and video, computer automatic diagnosis, wired or GPRS data transmission. We expect to solve the shortcoming of poor operability, large volume and high price of the traditional biological monitoring devices. Multi-life-parameter of the patients can be monitored remotely and wirelessly, with the advantages of good operability, strong scalability, small size, low cost, high reliability and real-time processing.",
                "ieee_keywords": [
                    "Biomedical monitoring",
                    "Monitoring",
                    "Servers",
                    "Electrocardiography",
                    "Temperature measurement",
                    "Global Positioning System",
                    "Temperature sensors"
                ],
                "author_keywords": [
                    "Telemedicine",
                    "physiological parameters",
                    "GPS",
                    "GPRS",
                    "monitoring terminal"
                ]
            },
            {
                "title": "On the Complexity of Optimal Lottery Pricing and Randomized Mechanisms",
                "link": "https://ieeexplore.ieee.org/document/7354467/",
                "date_of_publication": "17 December 2015",
                "doi": "10.1109/FOCS.2015.93",
                "citations": "19",
                "abstract": "We study the optimal lottery problem and the optimal mechanism design problem in the setting of a single unit-demand buyer with item values drawn from independent distributions. Optimal solutions to both problems are characterized by a linear program with exponentially many variables. For the menu size complexity of the optimal lottery problem, we present an explicit, simple instance with distributions of support size 2, and show that exponentially many lotteries are required to achieve the optimal revenue. We also show that, when distributions have support size 2 and share the same high value, the simpler scheme of item pricing can achieve the same revenue as the optimal menu of lotteries. The same holds for the case of two items with support size 2 (but not necessarily the same high value). For the computational complexity of the optimal mechanism design problem, we show that unless the polynomial-time hierarchy collapses (more exactly, PNP = P#P), there is no universal efficient randomized algorithm to implement an optimal mechanism even when distributions have support size 3.",
                "ieee_keywords": [
                    "Pricing",
                    "Complexity theory",
                    "Standards",
                    "Algorithm design and analysis",
                    "Polynomials",
                    "Additives",
                    "Cost accounting"
                ],
                "author_keywords": [
                    "Lottery pricing",
                    "optimal mechanism design",
                    "unit-demand buyer"
                ]
            },
            {
                "title": "Monostatic MIMO Backscatter Communications",
                "link": "https://ieeexplore.ieee.org/document/9110880/",
                "date_of_publication": null,
                "doi": "10.1109/JSAC.2020.3000823",
                "citations": "22",
                "abstract": "Backscatter communications have two major antenna configurations: the bistatic configuration, in which the reader employs two different sets of antennas to transmit and receive, and the monostatic configuration, in which the reader employs one set of antennas for both transmitting and receiving. In this paper, we provide a comprehensive study on the MIMO techniques for the $M \\times L$ monostatic channel. Particularly we study on the joint design of the query and the coding matrices. We show that the maximum achievable diversity order of the monostatic channel is the diversity order achieved by the block-lever unitary query and orthogonal space-time block code (BUTQ-OSTBC) design pair, which is $\\frac {ML}{2}$ , exactly the half of the diversity order of the conventional MIMO channel. Then we show that uniform query, the simplest query approach, cannot achieve the maximum achievable diversity order in the monostatic channel. We generalize BUTQ-OSTBC to the general augmenting approach, and show that unitary matrix is optimal in terms of SER performance among all possible query matrices, when OSTBC is employed. The above results further indicate that in the backscatter channel, additional diversity can be obtained by varying the query signals over time slots within the channel coherent time, which is quite different from the results from the conventional MIMO channels. We verify our results by Monte Carlo simulations.",
                "ieee_keywords": [
                    "Backscatter",
                    "MIMO communication",
                    "Transmitting antennas",
                    "Receiving antennas",
                    "Encoding",
                    "Diversity reception",
                    "Array signal processing"
                ],
                "author_keywords": [
                    "Backscatter communications",
                    "wirelessly powered communications",
                    "monostatic",
                    "MIMO",
                    "space-time coding"
                ]
            }
        ]
    },
    {
        "name": "Xiangyu Zhang",
        "publications": [
            {
                "title": "Detecting Backdoors in Pre-trained Encoders",
                "link": "https://ieeexplore.ieee.org/document/10205024/",
                "date_of_publication": "22 August 2023",
                "doi": "10.1109/CVPR52729.2023.01569",
                "citations": "1",
                "abstract": "Self-supervised learning in computer vision trains on unlabeled data, such as images or (image, text) pairs, to obtain an image encoder that learns high-quality embeddings for input data. Emerging backdoor attacks towards encoders expose crucial vulnerabilities of self-supervised learning, since downstream classifiers (even further trained on clean data) may inherit backdoor behaviors from en-coders. Existing backdoor detection methods mainly focus on supervised learning settings and cannot handle pre-trained encoders especially when input labels are not available. In this paper, we propose DECREE, the first back-door detection approach for pre-trained encoders, requiring neither classifier headers nor input labels. We evaluate DECREE on over 400 encoders trojaned under 3 paradigms. We show the effectiveness of our method on image encoders pre-trained on ImageNet and OpenAI's CLIP 400 million image-text pairs. Our method consistently has a high detection accuracy even if we have only limited or no access to the pre-training dataset. Code is available at https://github.com/GiantSeaweed/DECREE.",
                "ieee_keywords": [
                    "Threat modeling",
                    "Computer vision",
                    "Codes",
                    "Supervised learning",
                    "Self-supervised learning",
                    "Pattern recognition",
                    "Behavioral sciences"
                ],
                "author_keywords": [
                    "Adversarial attack and defense"
                ]
            },
            {
                "title": "ImU: Physical Impersonating Attack for Face Recognition System with Natural Style Changes",
                "link": "https://ieeexplore.ieee.org/document/10179360/",
                "date_of_publication": "21 July 2023",
                "doi": "10.1109/SP46215.2023.10179360",
                "citations": "51",
                "abstract": "This paper presents a novel physical impersonating attack against face recognition systems. It aims at generating consistent style changes across multiple pictures of the attacker under different conditions and poses. Additionally, the style changes are required to be physically realizable by make-up and can induce the intended misclassification. To achieve the goal, we develop novel techniques to embed multiple pictures of the same physical person to vectors in the StyleGAN’s latent space, such that the embedded latent vectors have some implicit correlations to make the search for consistent style changes feasible. Our digital and physical evaluation results show our approach can allow an outsider attacker to successfully impersonate the insiders with consistent and natural changes.",
                "ieee_keywords": [
                    "Privacy",
                    "Correlation",
                    "Face recognition",
                    "Security"
                ],
                "author_keywords": [
                    "impersontation",
                    "physical-attack",
                    "face-recognition",
                    "stylegan"
                ]
            },
            {
                "title": "PMP: Cost-effective Forced Execution with Probabilistic Memory Pre-planning",
                "link": "https://ieeexplore.ieee.org/document/9152685/",
                "date_of_publication": "30 July 2020",
                "doi": "10.1109/SP40000.2020.00035",
                "citations": "4",
                "abstract": "Malware is a prominent security threat and exposing malware behavior is a critical challenge. Recent malware often has payload that is only released when certain conditions are satisfied. It is hence difficult to fully disclose the payload by simply executing the malware. In addition, malware samples may be equipped with cloaking techniques such as VM detectors that stop execution once detecting that the malware is being monitored. Forced execution is a highly effective method to penetrate malware self-protection and expose hidden behavior, by forcefully setting certain branch outcomes. However, an existing state-of-the-art forced execution technique X-Force is very heavyweight, requiring tracing individual instructions, reasoning about pointer alias relations on-the-fly, and repairing invalid pointers by on-demand memory allocation. We develop a light-weight and practical forced execution technique. Without losing analysis precision, it avoids tracking individual instructions and on-demand allocation. Under our scheme, a forced execution is very similar to a native one. It features a novel memory pre-planning phase that pre-allocates a large memory buffer, and then initializes the buffer, and variables in the subject binary, with carefully crafted values in a random fashion before the real execution. The pre-planning is designed in such a way that dereferencing an invalid pointer has a very large chance to fall into the pre-allocated region and hence does not cause any exception, and semantically unrelated invalid pointer dereferences highly likely access disjoint (pre-allocated) memory regions, avoiding state corruptions with probabilistic guarantees. Our experiments show that our technique is 84 times faster than X-Force, has 6.5X and 10% fewer false positives and negatives for program dependence detection, respectively, and can expose 98% more malicious behaviors in 400 recent malware samples.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Model Orthogonalization: Class Distance Hardening in Neural Networks for Better Security",
                "link": "https://ieeexplore.ieee.org/document/9833688/",
                "date_of_publication": "27 July 2022",
                "doi": "10.1109/SP46214.2022.9833688",
                "citations": "3",
                "abstract": "The distance between two classes for a deep learning classifier can be measured by the level of difficulty in flipping all (or majority of) samples in a class to the other. The class distances of many pre-trained models in the wild are very small and do not align well with humans’ intuition (e.g., classes turtle and bird have smaller distance than classes cat and dog), making the models vulnerable to backdoor attacks, which aim to cause misclassification by stamping a specific pattern to inputs. We propose a novel model hardening technique called model orthogonalization which is an add-on training step to pretrained models, including clean models, poisoned models, and adversarially trained models. It can substantially enlarge class distances with reasonable training cost and without much accuracy degradation. Our evaluation on 5 datasets with 22 model structures show that our technique can enlarge class distances by 177.63% on average with less than 1% accuracy loss, outperforming existing hardening techniques such as adversarial training, universal adversarial perturbation, and directly using generated backdoors. It reduces 80% false positives for a state-of-the-art backdoor scanner as the enlarged class distances allow the scanner to easily distinguish clean and poisoned models, and substantially outperforms three existing techniques in removing injected backdoors.",
                "ieee_keywords": [
                    "Training",
                    "Degradation",
                    "Deep learning",
                    "Privacy",
                    "Costs",
                    "Perturbation methods",
                    "Neural networks"
                ],
                "author_keywords": []
            },
            {
                "title": "PAD: Programming third-party web advertisement censorship",
                "link": "https://ieeexplore.ieee.org/document/8115637/",
                "date_of_publication": "23 November 2017",
                "doi": "10.1109/ASE.2017.8115637",
                "citations": "2",
                "abstract": "In the current online advertisement delivery, an ad slot on a publisher's website may go through multiple layers of bidding and reselling until the final ad content is delivered. The publishers have little control on the ads being displayed on their web pages. As a result, website visitors may suffer from unwanted ads such as malvertising, intrusive ads, and information disclosure ads. Unfortunately, the visitors often blame the publisher for their unpleasant experience and switch to competitor websites. In this paper, we propose a novel programming support system for ad delivery, called PAD, for publisher programmers, who specify their policies on regulating third-party ads shown on their websites. PAD features an expressive specification language and a novel persistent policy enforcement runtime that can self-install and self-protect throughout the entire ad delegation chain. It also provides an ad-specific memory protection scheme that prevents malvertising by corrupting malicious payloads. Our experiments show that PAD has negligible runtime overhead. It effectively suppresses a set of malvertising cases and unwanted ad behaviors reported in the real world, without affecting normal functionalities and regular ads.",
                "ieee_keywords": [
                    "Internet",
                    "Runtime",
                    "Browsers",
                    "Advertising",
                    "Trojan horses"
                ],
                "author_keywords": []
            },
            {
                "title": "OSPREY: Recovery of Variable and Data Structure via Probabilistic Analysis for Stripped Binary",
                "link": "https://ieeexplore.ieee.org/document/9519451/",
                "date_of_publication": "26 August 2021",
                "doi": "10.1109/SP40001.2021.00051",
                "citations": "911",
                "abstract": "Recovering variables and data structure information from stripped binary is a prominent challenge in binary program analysis. While various state-of-the-art techniques are effective in specific settings, such effectiveness may not generalize. This is mainly because the problem is inherently uncertain due to the information loss in compilation. Most existing techniques are deterministic and lack a systematic way of handling such uncertainty. We propose a novel probabilistic technique for variable and structure recovery. Random variables are introduced to denote the likelihood of an abstract memory location having various types and structural properties such as being a field of some data structure. These random variables are connected through probabilistic constraints derived through program analysis. Solving these constraints produces the posterior probabilities of the random variables, which essentially denote the recovery results. Our experiments show that our technique substantially outperforms a number of state-of-the-art systems, including IDA, Ghidra, Angr, and Howard. Our case studies demonstrate the recovered information improves binary code hardening and binary decompilation.",
                "ieee_keywords": [
                    "Privacy",
                    "Uncertainty",
                    "Systematics",
                    "Binary codes",
                    "Probabilistic logic",
                    "Data structures",
                    "Random variables"
                ],
                "author_keywords": [
                    "Binary-Analysis",
                    "Reverse-Engineering",
                    "Type-Inference",
                    "Probabilistic-Analysis"
                ]
            },
            {
                "title": "Demo abstract: Diagnostic tracing of wireless sensor networks with TinyTracer",
                "link": "https://ieeexplore.ieee.org/document/5779087/",
                "date_of_publication": "27 May 2011",
                "doi": null,
                "citations": "99",
                "abstract": "Run-time debugging tools are required to detect and diagnose post-deployment failures in wireless sensor networks. Reproducing a failure from the trace of past events can play a crucial role in diagnosis. We describe TinyTracer, an efficient interprocedural control-flow tracing tool that generates the trace of all interleaving concurrent events as well as the control-flow paths taken. TinyTracer enables reproducing failures at a later stage, allowing the programmer to diagnose failures effectively. In this demo, we demonstrate the ease of use of TinyTracer. We see TinyTracer as an important tool for post-deployment diagnosis, which can enable future research on trace-based debugging approaches for wireless sensor networks.",
                "ieee_keywords": [
                    "Wireless sensor networks",
                    "Engines",
                    "Instruments",
                    "Debugging",
                    "Ash",
                    "Random access memory",
                    "USA Councils"
                ],
                "author_keywords": []
            },
            {
                "title": "CPC: Automatically Classifying and Propagating Natural Language Comments via Program Analysis",
                "link": "https://ieeexplore.ieee.org/document/9284079/",
                "date_of_publication": "21 December 2020",
                "doi": null,
                "citations": "156",
                "abstract": "Code comments provide abundant information that have been leveraged to help perform various software engineering tasks, such as bug detection, specification inference, and code synthesis. However, developers are less motivated to write and update comments, making it infeasible and error-prone to leverage comments to facilitate software engineering tasks. In this paper, we propose to leverage program analysis to systematically derive, refine, and propagate comments. For example, by propagation via program analysis, comments can be passed on to code entities that are not commented such that code bugs can be detected leveraging the propagated comments. Developers usually comment on different aspects of code elements like methods, and use comments to describe various contents, such as functionalities and properties. To more effectively utilize comments, a fine-grained and elaborated taxonomy of comments and a reliable classifier to automatically categorize a comment are needed. In this paper, we build a comprehensive taxonomy and propose using program analysis to propagate comments. We develop a prototype CPC, and evaluate it on 5 projects. The evaluation results demonstrate 41573 new comments can be derived by propagation from other code locations with 88% accuracy. Among them, we can derive precise functional comments for 87 native methods that have neither existing comments nor source code. Leveraging the propagated comments, we detect 37 new bugs in open source large projects, 30 of which have been confirmed and fixed by developers, and 304 defects in existing comments (by looking at inconsistencies between existing and propagated comments), including 12 incomplete comments and 292 wrong comments. This demonstrates the effectiveness of our approach. Our user study confirms propagated comments align well with existing comments in terms of quality.",
                "ieee_keywords": [
                    "Computer bugs",
                    "Taxonomy",
                    "Natural languages",
                    "Prototypes",
                    "Reliability",
                    "Task analysis",
                    "Software engineering"
                ],
                "author_keywords": []
            },
            {
                "title": "Debugging with Intelligence via Probabilistic Inference",
                "link": "https://ieeexplore.ieee.org/document/8453198/",
                "date_of_publication": "02 September 2018",
                "doi": "10.1145/3180155.3180237",
                "citations": "6",
                "abstract": "We aim to debug a single failing execution without the assistance from other passing/failing runs. In our context, debugging is a process with substantial uncertainty - lots of decisions have to be made such as what variables shall be inspected first. To deal with such uncertainty, we propose to equip machines with human-like intelligence. Specifically, we develop a highly automated debugging technique that aims to couple human-like reasoning (e.g., dealing with uncertainty and fusing knowledge) with program semantics based analysis, to achieve benefits from the two and mitigate their limitations. We model debugging as a probabilistic inference problem, in which the likelihood of each executed statement instance and variable being correct/faulty is modeled by a random variable. Human knowledge, human-like reasoning rules and program semantics are modeled as conditional probability distributions, also called probabilistic constraints. Solving these constraints identifies the most likely faulty statements. Our results show that the technique is highly effective. It can precisely identify root causes for a set of real-world bugs in a very small number of interactions with developers, much smaller than a recent proposal that does not encode human intelligence. Our user study also confirms that it substantially improves human productivity.",
                "ieee_keywords": [
                    "Debugging",
                    "Probabilistic logic",
                    "Uncertainty",
                    "Cognition",
                    "Semantics",
                    "Python",
                    "Tools"
                ],
                "author_keywords": [
                    "Debugging",
                    "Probabilistic Inference",
                    "Python"
                ]
            },
            {
                "title": "Cross-Layer Retrofitting of UAVs Against Cyber-Physical Attacks",
                "link": "https://ieeexplore.ieee.org/document/8462886/",
                "date_of_publication": "13 September 2018",
                "doi": "10.1109/ICRA.2018.8462886",
                "citations": "18",
                "abstract": "As a rapidly growing cyber-physical platform, unmanned aerial vehicles are facing more security threats as their capabilities and applications continue to expand. Adversaries with detailed knowledge about the vehicle could orchestrate sophisticated attacks that are not easily detected or handled by the vehicle's control system. In this work, we purpose a generic security framework, termed BlueBox, capable of detecting and handling a variety of cyber-physical attacks. To demonstrate an application of BlueBox in practice, we retrofitted an off-the-shelf quadcopter. A series of attacks were then launched by embedding malicious code in the control software and by altering the vehicle's hardware with the specific targeting of sensors, controller, motors, vehicle dynamics, and operating system. Experimental results verified that BlueBox was capable of both detecting a variety of cyber-physical attacks, while also providing the means in which to recover from such attacks.",
                "ieee_keywords": [
                    "Software",
                    "Hardware",
                    "Security",
                    "Sensor fusion",
                    "Vehicle dynamics",
                    "Control systems"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Dongyan Xu",
        "publications": [
            {
                "title": "Cross-Layer Retrofitting of UAVs Against Cyber-Physical Attacks",
                "link": "https://ieeexplore.ieee.org/document/8462886/",
                "date_of_publication": "13 September 2018",
                "doi": "10.1109/ICRA.2018.8462886",
                "citations": "18",
                "abstract": "As a rapidly growing cyber-physical platform, unmanned aerial vehicles are facing more security threats as their capabilities and applications continue to expand. Adversaries with detailed knowledge about the vehicle could orchestrate sophisticated attacks that are not easily detected or handled by the vehicle's control system. In this work, we purpose a generic security framework, termed BlueBox, capable of detecting and handling a variety of cyber-physical attacks. To demonstrate an application of BlueBox in practice, we retrofitted an off-the-shelf quadcopter. A series of attacks were then launched by embedding malicious code in the control software and by altering the vehicle's hardware with the specific targeting of sensors, controller, motors, vehicle dynamics, and operating system. Experimental results verified that BlueBox was capable of both detecting a variety of cyber-physical attacks, while also providing the means in which to recover from such attacks.",
                "ieee_keywords": [
                    "Software",
                    "Hardware",
                    "Security",
                    "Sensor fusion",
                    "Vehicle dynamics",
                    "Control systems"
                ],
                "author_keywords": []
            },
            {
                "title": "vHaul: Towards Optimal Scheduling of Live Multi-VM Migration for Multi-tier Applications",
                "link": "https://ieeexplore.ieee.org/document/7214077/",
                "date_of_publication": "20 August 2015",
                "doi": "10.1109/CLOUD.2015.67",
                "citations": "12",
                "abstract": "Live virtual machine (VM) migration enables seamless movement of an online server from one location to another to achieve failure recovery, load balancing, and system maintenance. Beyond single VM migration, a multi-tier application involves a group of correlated VMs and its live migration will require careful scheduling of the migrations of the member VMs. Our observations from extensive experiments using a variety of multi-tier applications suggest that, in a dedicated data center with dedicated migration links, different migration strategies result in distinct performance impacts on a multi-tier application. The root cause of the problem is the inter-dependence between functional components of a multitier application. We leverage these observations in vHaul, a system that coordinates multi-VM migration to approximate the optimal scheduling. Our evaluation of a vHaul prototype on Xen suggests that vHaul yields the optimal multi-VM live migration schedules. Further, our application-level evaluation using Apache Olio, a web 2.0 cloud application, shows that the optimal migration schedule produced by vHaul outperforms the worst-case schedule by 43% in application throughput. Moreover, the optimal schedule significantly reduces service latency during migration by up to 70%.",
                "ieee_keywords": [
                    "Web servers",
                    "Databases",
                    "Degradation",
                    "Throughput",
                    "Bandwidth",
                    "Optimal scheduling"
                ],
                "author_keywords": [
                    "Virtualization",
                    "Cloud Computing",
                    "Live Migration"
                ]
            },
            {
                "title": "LEAPS: Detecting Camouflaged Attacks with Statistical Learning Guided by Program Analysis",
                "link": "https://ieeexplore.ieee.org/document/7266838/",
                "date_of_publication": "17 September 2015",
                "doi": "10.1109/DSN.2015.34",
                "citations": "26",
                "abstract": "Currently cyber infrastructures are facing increasingly stealthy attacks that implant malicious payloads under the cover of benign programs. Existing attack detection approaches based on statistical learning methods may generate misleading decision boundaries when processing noisy data with such a mixture of benign and malicious behaviors. On the other hand, attack detection based on formal program analysis may lack completeness or adaptivity when modelling attack behaviors. In light of these limitations, we have developed LEAPS, an attack detection system based on supervised statistical learning to classify benign and malicious system events. Furthermore, we leverage control flow graphs inferred from the system event logs to enable automatic pruning of the training data, which leads to a more accurate classification model when applied to the testing data. Our extensive evaluation shows that, compared with pure statistical learning models, LEAPS achieves consistently higher accuracy when detecting real-world camouflaged attacks with benign program cover-up.",
                "ieee_keywords": [
                    "Statistical learning",
                    "Training",
                    "Payloads",
                    "Libraries",
                    "Hidden Markov models",
                    "Data models",
                    "Feature extraction"
                ],
                "author_keywords": [
                    "Attack Detection",
                    "Statistical Learning",
                    "Program Analysis"
                ]
            },
            {
                "title": "Flight Recovery of MAVs with Compromised IMU",
                "link": "https://ieeexplore.ieee.org/document/8968145/",
                "date_of_publication": "28 January 2020",
                "doi": "10.1109/IROS40897.2019.8968145",
                "citations": "4",
                "abstract": "Micro Aerial Vehicles (MAVs) rely on onboard attitude and position sensors for autonomous flight. Due to their size, weight, and power (SWaP) constraints, most modern MAVs use miniaturized inertial measurement units (IMUs) to provide attitude feedback, which is critical for flight stabilization and control. However, recent adversarial attack studies have demonstrated that many commonly used IMUs are vulnerable to attacks exploiting their physical characteristics. Conventional redundancy-based approaches are not effective against such attacks because redundant IMUs have the same or similar physical vulnerabilities. In this paper, we present a novel fault-tolerant solution for IMU compromised scenarios, using separate position and heading information to restore the failed attitude states. Rather than adding more IMU alternatives for recovery, the proposed method is intended to minimize any modifications to the existing system and control program. Thus, it is particularly useful for vehicles that have tight SWaP constraints while requiring simultaneous high performance and safety demands. To execute the recovery logic properly, a robust estimator was designed for fine-grained detection and isolation of the faulty sensors. The effectiveness of the proposed approach was validated on a quadcopter MAV through both simulation and experimental flight tests.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "DRIP: A framework for purifying trojaned kernel drivers",
                "link": "https://ieeexplore.ieee.org/document/6575342/",
                "date_of_publication": "08 August 2013",
                "doi": "10.1109/DSN.2013.6575342",
                "citations": "3",
                "abstract": "Kernel drivers are usually provided in the form of loadable kernel extensions, which can be loaded/unloaded dynamically at runtime and execute with the same privilege as the core operating system kernel. The unrestricted security access from the drivers to the kernel is nevertheless a double-edged sword that makes them susceptible targets of trojan attacks. Given a benign driver, it is now easy to implant malicious logic with existing hacking tools. Once implanted, such malicious logic is difficult to detect. In this paper we propose DRIP, a framework for detecting and eliminating malicious logic embedded in a kernel driver through iteratively eliminating unnecessary kernel API invocations from the driver. When provided with the binary of a trojaned driver, DRIP generates a purified driver with benign functionalities preserved and malicious ones eliminated. Our evaluation shows that DRIP successfully eliminates malicious effects of trojaned drivers in the system, with the purified drivers maintaining or even improving their performance over the trojaned drivers.",
                "ieee_keywords": [
                    "Kernel",
                    "Testing",
                    "Context",
                    "Runtime",
                    "Monitoring",
                    "Communication channels"
                ],
                "author_keywords": [
                    "System Security",
                    "Kernel Drivers",
                    "Trojan Detection"
                ]
            },
            {
                "title": "Fuzzing SGX Enclaves via Host Program Mutations",
                "link": "https://ieeexplore.ieee.org/document/10190488/",
                "date_of_publication": "31 July 2023",
                "doi": "10.1109/EuroSP57164.2023.00035",
                "citations": "36",
                "abstract": "Intel Software Guard eXtension (SGX) is the cornerstone of Confidential Computing, enabling runtime code and data integrity and confidentiality via enclaves. Unfortunately, memory-unsafe and type-unsafe programming languages, such as C/C++, are commonly used to develop enclave implementations. As a result, a memory corruption or a data race within enclaves could lead to different attacks against the enclaves, such as Return-Of-Programming (ROP) and data leakage, breaking the hardware security guarantee provided by SGX. To automatically identify these issues in existing enclave implementations, in this paper, we propose FuzzSGX, an input and program mutation-based fuzzer for Intel SGX enclave implementations. FuzzSGX provides an enclave fuzzing runtime, FuzzSGX Runtime, a drop-in library for Intel SGX SDK, enabling code coverage and sanitization within enclaves. To explore the host app-enclave boundary, FuzzSGX conducts static analysis and symbolic execution on existing host apps and enclave implementations to generate promising fuzzing programs, fuzzing both ECALLs and OCALLs. We evaluate FuzzSGX using 30 popular SGX applications and enclave implementations and find 93 bugs among these SGX projects, including data races, null pointer dereferences, out-of-bound accesses, division-by-zero, etc. FuzzSGX achieves 3.2x higher code coverage and finds 48.2% more bugs by directly targeting the host appenclave boundary by using program mutations, compared to state-of-the-art fuzzers.",
                "ieee_keywords": [
                    "Runtime",
                    "Codes",
                    "Hardware security",
                    "Data integrity",
                    "Computer bugs",
                    "Graphene",
                    "Static analysis"
                ],
                "author_keywords": [
                    "Fuzzing",
                    "SGX",
                    "Enclaves"
                ]
            },
            {
                "title": "vSnoop: Improving TCP Throughput in Virtualized Environments via Acknowledgement Offload",
                "link": "https://ieeexplore.ieee.org/document/5645469/",
                "date_of_publication": "29 November 2010",
                "doi": "10.1109/SC.2010.57",
                "citations": "30",
                "abstract": "Virtual machine (VM) consolidation has become a common practice in clouds, Grids, and datacenters. While this practice leads to higher CPU utilization, we observe its negative impact on the TCP throughput of the consolidated VMs: As more VMs share the same core/CPU, the CPU scheduling latency for each VM increases significantly. Such increase leads to slower progress of TCP transmissions to the VMs. To address this problem, we propose an approach called vSnoop, where the driver domain of a host acknowledges TCP packets on behalf of the guest VMs - whenever it is safe to do so. Our evaluation of a Xen-based prototype indicates that vSnoop constantly achieves TCP throughput improvement for VMs (of orders of magnitude in some scenarios). We further show that the higher TCP throughput leads to improvement in application- level performance, via experiments with a two-tier online auction application and two suites of MPI benchmarks.",
                "ieee_keywords": [
                    "Driver circuits",
                    "Throughput",
                    "Receivers",
                    "Bridges",
                    "Scheduling",
                    "Semantics",
                    "Clouds"
                ],
                "author_keywords": []
            },
            {
                "title": "Learn-to-Recover: Retrofitting UAVs with Reinforcement Learning-Assisted Flight Control Under Cyber-Physical Attacks",
                "link": "https://ieeexplore.ieee.org/document/9196611/",
                "date_of_publication": "15 September 2020",
                "doi": "10.1109/ICRA40945.2020.9196611",
                "citations": "26",
                "abstract": "In this paper, we present a generic fault-tolerant control (FTC) strategy via reinforcement learning (RL). We demonstrate the effectiveness of this method on quadcopter unmanned aerial vehicles (UAVs). The fault-tolerant control policy is trained to handle actuator and sensor fault/attack. Unlike traditional FTC, this policy does not require fault detection and diagnosis (FDD) nor tailoring the controller for specific attack scenarios. Instead, the policy is running simultaneously alongside the stabilizing controller without the need for on- detection activation. The effectiveness of the policy is compared with traditional active and passive FTC strategies against actuator and sensor faults. We compare their performance in position control tasks via simulation and experiments on quadcopters. The result shows that the strategy can effectively tolerate different types of attacks/faults and maintain the vehicle's position, outperforming the other two methods.",
                "ieee_keywords": [
                    "Actuators",
                    "Fault tolerance",
                    "Fault tolerant systems",
                    "Vehicle dynamics",
                    "Learning (artificial intelligence)",
                    "Training",
                    "Solid modeling"
                ],
                "author_keywords": []
            },
            {
                "title": "FACE-CHANGE: Application-Driven Dynamic Kernel View Switching in a Virtual Machine",
                "link": "https://ieeexplore.ieee.org/document/6903605/",
                "date_of_publication": "22 September 2014",
                "doi": "10.1109/DSN.2014.52",
                "citations": "8",
                "abstract": "Kernel minimization has already been established as a practical approach to reducing the trusted computing base. Existing solutions have largely focused on whole-system profiling - generating a globally minimum kernel image that is being shared by all applications. However, since different applications use only part of the kernel's code base, the minimized kernel still includes an unnecessarily large attack surface. Furthermore, once the static minimized kernel is generated, it is not flexible enough to adapt to an altered execution environment (e.g., new workload). FACE-CHANGE is a virtualization-based system to facilitate dynamic switching at runtime among multiple minimized kernels, each customized for an individual application. Based on precedent profiling results, FACE-CHANGE transparently presents a customized kernel view for each application to confine its reach ability of kernel code. In the event that the application exceeds this boundary, FACE-CHANGE is able to recover the missing code and back trace its attack/exception provenance to analyze the anomalous behavior.",
                "ieee_keywords": [
                    "Kernel",
                    "Context",
                    "Runtime",
                    "Indexes",
                    "Switches",
                    "Minimization",
                    "Loading"
                ],
                "author_keywords": [
                    "Attack Surface Minimization",
                    "Attack Provenance",
                    "Virtualization"
                ]
            },
            {
                "title": "PGPatch: Policy-Guided Logic Bug Patching for Robotic Vehicles",
                "link": "https://ieeexplore.ieee.org/document/9833567/",
                "date_of_publication": "27 July 2022",
                "doi": "10.1109/SP46214.2022.9833567",
                "citations": "3",
                "abstract": "Automated program repair (APR) methods aim to identify patches for a given bug and apply them with minimal human intervention. To date, existing APR approaches focus on repairing software bugs, such as memory safety bugs. However, our analysis of popular robotic vehicle (RV) control software shows that most of their bugs are not memory bugs but rather logic bugs. These bugs, while not causing software crashes, can cause an RV to reach an undesired physical state (e.g., hitting the ground). To fix these logic bugs, we introduce PGPatch, a policy-guided program repair framework for RV control programs, which identifies the correct patch for a given logic bug and applies it without human intervention. PGPatch takes, as input, existing or new logic formulas used to discover logic bugs. It then leverages the formulas using a dedicated dynamic analysis to classify the previously known logic bugs into a patch type. It next uses a customized algorithm, based on the identified patch type and violated formula, to produce a source code patch as output. Lastly, it creates repeatable tests to verify the patch’s completeness, ensuring that the patch is correct and does not degrade the RV’s performance. We evaluate PGPatch on selected bug cases from three popular RV control software and find that it correctly fixes 258 out of 297 logic bugs (86.9%). We additionally recruit 18 experienced RV developers and users and conduct a user study that demonstrates how using PGPatch makes fixing bugs in RV software significantly quicker and less error-prone.",
                "ieee_keywords": [
                    "Privacy",
                    "Codes",
                    "Heuristic algorithms",
                    "Computer bugs",
                    "Maintenance engineering",
                    "Software",
                    "Safety"
                ],
                "author_keywords": [
                    "automated-program-repair",
                    "Robotic-Vehicle",
                    "Logic-bug",
                    "Patching"
                ]
            }
        ]
    },
    {
        "name": "Aniket Bera",
        "publications": [
            {
                "title": "IEEE VR 2023 Workshops",
                "link": "https://ieeexplore.ieee.org/document/10108719/",
                "date_of_publication": "01 May 2023",
                "doi": "10.1109/VRW58643.2023.00078",
                "citations": "43",
                "abstract": "Published: 2022 Three-Dimensional Displays: A Review and Applications Analysis IEEE Transactions on Broadcasting Published: 2011 Show More",
                "ieee_keywords": [
                    "Computer vision",
                    "Affective computing",
                    "Three-dimensional displays",
                    "Extended reality",
                    "Conferences",
                    "Collaboration",
                    "User interfaces"
                ],
                "author_keywords": []
            },
            {
                "title": "Contextualized Styling of Images for Web Interfaces using Reinforcement Learning",
                "link": "https://ieeexplore.ieee.org/document/10019633/",
                "date_of_publication": "23 January 2023",
                "doi": "10.1109/ISM55400.2022.00037",
                "citations": "67",
                "abstract": "Content personalization is one of the foundations of today’s digital marketing. Often the same image needs to be adapted for different design schemes for content that is created for different occasions, geographic locations or other aspects of the target population. We present a novel reinforcement learning (RL) based method for automatically stylizing images to complement the design scheme of media, e.g., interactive websites, apps, or posters. Our approach considers attributes related to the design of the media and adapts the style of the input image to match the context. We do so using a preferential reward system in the RL framework that learns a reward function using human feedback. We conducted several user studies to evaluate our approach and demonstrate that we are able to effectively adapt image styles to different design schemes. In user studies, images stylized through our approach were the most preferred variation across a majority of our experiments. Additionally, we also release a dataset consisting of perceptual associations of web context with the associated image style.",
                "ieee_keywords": [
                    "Training",
                    "Scalability",
                    "Impedance matching",
                    "Sociology",
                    "Reinforcement learning",
                    "Media",
                    "Statistics"
                ],
                "author_keywords": [
                    "reinforcement learning",
                    "image enhancement",
                    "context",
                    "image modification",
                    "content variant generation"
                ]
            },
            {
                "title": "EWareNet: Emotion-Aware Pedestrian Intent Prediction and Adaptive Spatial Profile Fusion for Social Robot Navigation",
                "link": "https://ieeexplore.ieee.org/document/10161504/",
                "date_of_publication": "04 July 2023",
                "doi": "10.1109/ICRA48891.2023.10161504",
                "citations": "93",
                "abstract": "We present EWareNet, a novel intent and affect-aware social robot navigation algorithm among pedestrians. Our approach predicts the trajectory-based pedestrian intent from gait sequence, which is then used for intent-guided navigation taking into account social and proxemic constraints. We propose a transformer-based model that works on commodity RGB-D cameras mounted onto a moving robot. Our intent prediction routine is integrated into a mapless navigation scheme and makes no assumptions about the environment of pedestrian motion. Our navigation scheme consists of a novel obstacle profile representation methodology that is dynamically adjusted based on the pedestrian pose, intent, and affect. The navigation scheme is based on a reinforcement learning algorithm that takes pedestrian intent and robot's impact on pedestrian intent into consideration, in addition to the environmental configuration. We outperform current state-of-art algorithms for intent prediction from 3D gaits.",
                "ieee_keywords": [
                    "Pedestrians",
                    "Three-dimensional displays",
                    "Navigation",
                    "Heuristic algorithms",
                    "Social robots",
                    "Robot vision systems",
                    "Reinforcement learning"
                ],
                "author_keywords": []
            },
            {
                "title": "AZTR: Aerial Video Action Recognition with Auto Zoom and Temporal Reasoning",
                "link": "https://ieeexplore.ieee.org/document/10160564/",
                "date_of_publication": "04 July 2023",
                "doi": "10.1109/ICRA48891.2023.10160564",
                "citations": "1",
                "abstract": "We propose a novel approach for aerial video action recognition. Our method is designed for videos captured using UAVs and can run on edge or mobile devices. We present a learning-based approach that uses customized auto zoom to automatically identify the human target and scale it appropriately. This makes it easier to extract the key features and reduces the computational overhead. We also present an efficient temporal reasoning algorithm to capture the action information along the spatial and temporal domains within a controllable computational cost. Our approach has been implemented and evaluated both on the desktop with high-end GPUs and on the low power Robotics RB5 Platform for robots and drones. In practice, we achieve $6.1-7.4 \\%$ improvement over SOTA in Top-1 accuracy on the RoCoG-v2 dataset, 8.3-10.4% improvement on the UAV-Human dataset and 3.2% improvement on the Drone Action dataset.",
                "ieee_keywords": [
                    "Automation",
                    "Feature extraction",
                    "Cognition",
                    "Mobile handsets",
                    "Computational efficiency",
                    "Robots",
                    "Drones"
                ],
                "author_keywords": []
            },
            {
                "title": "Placing Human Animations into 3D Scenes by Learning Interaction- and Geometry-Driven Keyframes",
                "link": "https://ieeexplore.ieee.org/document/10030961/",
                "date_of_publication": "06 February 2023",
                "doi": "10.1109/WACV56688.2023.00038",
                "citations": "37",
                "abstract": "We present a novel method for placing a 3D human animation into a 3D scene while maintaining any human-scene interactions in the animation. We use the notion of computing the most important meshes in the animation for the interaction with the scene, which we call \"keyframes.\" These keyframes allow us to better optimize the placement of the animation into the scene such that interactions in the animations (standing, laying, sitting, etc.) match the affordances ofthe scene (e.g., standing on the floor or laying in a bed). We compare our method, which we call PAAK, with prior approaches, including POSA, PROX ground truth, and a motion synthesis method, and highlight the benefits of our method with a perceptual study. Human raters preferred our PAAK method over the PROX ground truth data 64.6% of the time. Additionally, in direct comparisons, the raters preferred PAAK over competing methods including 61.5% compared to POSA. Our project website is available at https://gamma.umd.edu/paak/.",
                "ieee_keywords": [
                    "Computer vision",
                    "Three-dimensional displays",
                    "Affordances",
                    "Animation",
                    "Floors"
                ],
                "author_keywords": [
                    "Algorithms: Computational photography",
                    "image and video synthesis",
                    "Arts/games/social media",
                    "Virtual/augmented reality"
                ]
            }
        ]
    },
    {
        "name": "Chunyi Peng",
        "publications": [
            {
                "title": "Leveraging Synergies Between AI and Networking to Build Next Generation Edge Networks",
                "link": "https://ieeexplore.ieee.org/document/10061749/",
                "date_of_publication": "13 March 2023",
                "doi": "10.1109/CIC56439.2022.00013",
                "citations": "170",
                "abstract": "Networking and Artificial Intelligence (AI) are two of the most transformative information technologies over the last few decades. Building upon the synergies of these two powerful technologies, we envision designing next generation of edge networks to be highly efficient, reliable, robust and secure. To this end, in this paper, we delve into interesting and fundamental research challenges and opportunities that span two major broad and symbiotic areas: AI for Networks and Networks for AI. The former deals with the development of new AI tools and techniques that can enable the next generation AI-assisted networks; while the latter focuses on developing networking techniques and tools that will facilitate the vision of distributed intelligence, resulting in a virtuous research cycle where advances in one will help accelerate advances in the other. A wide range of applications will be further discussed to illustrate the importance of the foundational advances developed in these two areas.",
                "ieee_keywords": [
                    "Wireless communication",
                    "Symbiosis",
                    "Privacy",
                    "Uncertainty",
                    "Program processors",
                    "Reliability engineering",
                    "Security"
                ],
                "author_keywords": []
            },
            {
                "title": "Nascent: Tackling Caller-ID Spoofing in 4G Networks via Efficient Network-Assisted Validation",
                "link": "https://ieeexplore.ieee.org/document/8737567/",
                "date_of_publication": "17 June 2019",
                "doi": "10.1109/INFOCOM.2019.8737567",
                "citations": "1",
                "abstract": "Caller-ID spoofing deceives the callee into believing a call is originating from another user. Spoofing has been strategically used in the now-pervasive telephone fraud, causing substantial monetary loss and sensitive data leakage. Unfortunately, caller-ID spoofing is feasible even when user authentication is in place. State-of-the-art solutions either exhibit high overhead or require extensive upgrades, and thus are unlikely to be deployed in the near future. In this paper, we seek an effective and efficient solution for 4G (and conceptually 5G) carrier networks to detect (and block) caller-ID spoofing. Specifically, we propose Nascent, Network-assisted caller ID authentication, to validate the caller-ID used during call setup which may not match the previously-authenticated ID. Nascent functionality is split between data-plane gateways and call control session functions. By leveraging existing communication interfaces between the two and authentication data already available at the gateways, Nascent only requires small, standard-compatible patches to the existing 4G infrastructure. We prototype and experimentally evaluate three variants of Nascent in traditional and Network Functions Virtualization (NFV) deployments. We demonstrate that Nascent significantly reduces overhead compared to the state-of-the-art, without sacrificing effectiveness.",
                "ieee_keywords": [
                    "Authentication",
                    "Logic gates",
                    "IP networks",
                    "Instant messaging",
                    "Telephone sets",
                    "Long Term Evolution",
                    "Protocols"
                ],
                "author_keywords": []
            },
            {
                "title": "ViViSnoop: Someone is snooping your typing without seeing it!",
                "link": "https://ieeexplore.ieee.org/document/8228624/",
                "date_of_publication": "21 December 2017",
                "doi": "10.1109/CNS.2017.8228624",
                "citations": "5",
                "abstract": "In the paper, we present ViViSnoop, a novel video-assisted keystroke inference attack which snoops the victim user's typed input without visually seeing it. Instead, it infers the typed input from the vibration extracted from the video capturing the desk where the physical or virtual keyboard is placed. ViViSnoop is built on the fact that keystrokes on the desk incur mechanical vibrations which are subtle but still extractable through computer vision processing. To further exploit subtle vibration patterns for accurate and reliable keystroke inference, ViViSnoop incorporates a suite of techniques such as vibration-specific video processing to enhance raw vibration quality, a novel virtual sensing array technique to develop fine-grained location signatures, and a two-phase classifier to achieve high accuracy and efficiency. Our extensive evaluation shows that ViViSnoop realizes high-accuracy keystroke inference. ViViSnoop achieves around 55% single-character accuracy in inferring passwords (random inputs). In word and sentence inference, it achieves 71.4% and 59.4% accuracy using top-1 choices and even almost 100% and 75% using top-10 choices. This means there is almost no difficulty in understanding user inputs. It imposes a covert and serious threat to leak user typing in public/office spaces.",
                "ieee_keywords": [
                    "Vibrations",
                    "Keyboards",
                    "Cameras",
                    "Feature extraction",
                    "Sensors",
                    "Video equipment",
                    "Portable computers"
                ],
                "author_keywords": []
            },
            {
                "title": "Device-Customized Multi-Carrier Network Access on Commodity Smartphones",
                "link": "https://ieeexplore.ieee.org/document/8476210/",
                "date_of_publication": null,
                "doi": "10.1109/TNET.2018.2869492",
                "citations": "4",
                "abstract": "Accessing multiple carrier networks (T-Mobile, Sprint, AT&T, and so on) offers a promising paradigm for smartphones to boost its mobile network quality. However, the current practice does not achieve the full potential of this approach because it has not utilized fine-grained, cellular-specific domain knowledge. Our experiments and code analysis discover three implementation-independent issues: 1) it may not trigger the anticipated switch when the serving carrier network is poor; 2) the switch takes a much longer time than needed; and 3) the device fails to choose the high-quality network (e.g., selecting 3G rather than 4G). To address them, we propose iCellular, which exploits low-level cellular information at the device to improve multi-carrier access. iCellular is proactive and adaptive in its multi-carrier selection by leveraging existing end-device mechanisms and standards-complaint procedures. It performs adaptive monitoring to ensure responsive selection and minimal service disruption and enhances carrier selection with online learning and runtime decision fault prevention. It is readily deployable on smartphones without infrastructure/hardware modifications. We implement iCellular on commodity phones and harness the efforts of Project Fi to assess multi-carrier access over two U.S. carriers: T-Mobile and Sprint. Our evaluation shows that, iCellular boosts the devices' throughput with up to 3.74× throughput improvement, 6.9× suspension reduction, and 1.9× latency decrement over the state of the art, with moderate CPU, and memory and energy overheads.",
                "ieee_keywords": [
                    "Switches",
                    "5G mobile communication",
                    "Google",
                    "Smart phones",
                    "3G mobile communication",
                    "Throughput"
                ],
                "author_keywords": [
                    "4G mobile communication",
                    "5G mobile communication",
                    "multi-carrier network access",
                    "project Fi"
                ]
            },
            {
                "title": "VPPlus: Exploring the Potentials of Video Processing for Live Video Analytics at the Edge",
                "link": "https://ieeexplore.ieee.org/document/9812896/",
                "date_of_publication": "05 July 2022",
                "doi": "10.1109/IWQoS54832.2022.9812896",
                "citations": "2",
                "abstract": "Edge-assisted video analytics is gaining momentum. In this work, we tackle an important problem to compress video content live streamed from the device to the edge without scarifying accuracy and timeliness of its video analytics. We find that on-device processing can be tuned over a larger configuration space for more video compression, which was largely overlooked. Inspired by our pilot study, we design VPPlus to fulfill the potentials to compress the video as much as we can, while preserving analytical accuracy. VPPlus incorporates two core modules – offline profiling and online adaptation – to generate proper feedback automatically and quickly to tune on-device processing. We validate the effectiveness and efficiency of VPPlususing five object detection tasks over two popular datasets; VPPlus outperforms the state-of-art approaches in almost all the cases.",
                "ieee_keywords": [
                    "Visual analytics",
                    "Image edge detection",
                    "Quality of service",
                    "Object detection",
                    "Video compression",
                    "Streaming media",
                    "Task analysis"
                ],
                "author_keywords": [
                    "Video analytics",
                    "edge computing",
                    "on-device processing",
                    "video compression"
                ]
            },
            {
                "title": "A Machine Learning Based Approach to Mobile Network Analysis",
                "link": "https://ieeexplore.ieee.org/document/8487371/",
                "date_of_publication": "11 October 2018",
                "doi": "10.1109/ICCCN.2018.8487371",
                "citations": "1",
                "abstract": "In this paper, we present our recent work in progress on 4G mobile network analysis. In order to provide an in-depth study on the closed network operations, we advocate a novel approach via two-level, device-centric machine learning that can open up the system behaviors and facilitate fine-grained analysis . We describe our proposed approach, and use the latency analysis on two popular mobile apps (Web browsing and Instant Messaging) to illustrate how our scheme works. We further preliminary results and discuss the open issues.",
                "ieee_keywords": [
                    "Long Term Evolution",
                    "Protocols",
                    "Instant messaging",
                    "Runtime",
                    "Base stations",
                    "Servers"
                ],
                "author_keywords": []
            },
            {
                "title": "OPA: One-Predict-All For Efficient Deployment",
                "link": "https://ieeexplore.ieee.org/document/10228928/",
                "date_of_publication": "29 August 2023",
                "doi": "10.1109/INFOCOM53939.2023.10228928",
                "citations": "1",
                "abstract": "Deep neural network (DNN) is the de facto standard for running a variety of computer vision applications over mobile and embedded systems. Prior to deployment, a DNN is specialized by training to fit the target use scenario (depending on computing power and visual data input). To handle its costly training and meet diverse deployment needs, a \"Train Once, Deploy Everywhere\" paradigm has been recently proposed by training one super-network and selecting one out of many sub-networks (part of the super-network) for the target scenario; This empowers efficient DNN deployment at low training cost (training once). However, the existing studies tackle some deployment factors like computing power and source data but largely overlook the impact of their runtime dynamics (say, time-varying visual contents and GPU/CPU workloads). In this work, we propose OPA to cover all these deployment factors, particularly those along with runtime dynamics in visual data contents and computing resources. To quickly and accurately learn which sub-network runs \"best\" in the dynamic deployment scenario, we devise a \"One-Predict-All\" approach with no need to run all the candidate sub-networks. Instead, we first develop a shallow sub-network to test the water and then use its test results to predict the performance of all other deeper sub-networks. We have implemented and evaluated OPA. Compared to the state-of-the-art, OPA has achieved up to 26% higher Top-1 accuracy for a given latency requirement.",
                "ieee_keywords": [
                    "Training",
                    "Visualization",
                    "Computer vision",
                    "Runtime",
                    "Embedded systems",
                    "Costs",
                    "Artificial neural networks"
                ],
                "author_keywords": []
            },
            {
                "title": "Breaking Geographic Routing Among Connected Vehicles",
                "link": "https://ieeexplore.ieee.org/document/10202631/",
                "date_of_publication": "09 August 2023",
                "doi": "10.1109/DSN58367.2023.00018",
                "citations": "16",
                "abstract": "Geographic routing for connected vehicles enables vehicles and roadside infrastructure to exchange information about traffic conditions and road hazards based on their geographic positions. Its security is thus critical to traffic efficiency and road safety. In this paper, we conduct a security analysis of one standardized geographic routing protocol - GeoNetworking-and unfortunately find that its packet forwarding algorithms are vulnerable to two simple attacks. The first inter-area interception attack disturbs the victim vehicle's routing decision making and intercepts packets transmitted from one area to another. The second intra-area blockage attack intervenes packet forwarding within an area by impersonating a packet forwarder in a contention based flooding process; The attacker injects fake packets to its nearby peers and prevents vehicles within an area from receiving the broadcast packets. We use an open-source simulator to evaluate the effectiveness of proof-of-concept attacks and assess their attack damages under the settings released in public field tests. The first attack achieves an inter-area interception rate up to 99.9% (>35% in all test cases); The second attack reaches an intra-area packet blockage rate between 35% and 39%, which implies that about one-third vehicles within an area fail to receive broadcast packets. These attacks cause unnecessary traffic jams and collisions which could be avoided if GeoNetworking is properly secured. We further propose standard-compatible solutions to mitigating both attacks and conduct a preliminary evaluation to validate their effectiveness.",
                "ieee_keywords": [
                    "Connected vehicles",
                    "Vehicle routing",
                    "Decision making",
                    "Routing",
                    "Routing protocols",
                    "Road safety",
                    "Security"
                ],
                "author_keywords": [
                    "Geographic Routing",
                    "Connected Vehicles",
                    "GeoNetworking",
                    "Interception Attacks"
                ]
            },
            {
                "title": "ChromaCode: A Fully Imperceptible Screen-Camera Communication System",
                "link": "https://ieeexplore.ieee.org/document/8917661/",
                "date_of_publication": null,
                "doi": "10.1109/TMC.2019.2956493",
                "citations": "13",
                "abstract": "Hidden screen-camera communication techniques emerge as a new paradigm that embeds data imperceptibly into regular videos while remaining unobtrusive to human viewers. Three key goals on imperceptible, high rate, and reliable communication are desirable but conflicting, and existing solutions usually made a trade-off among them. In this paper, we present the design and implementation of CHROMACODE, a screen-camera communication system that achieves all three goals simultaneously. In our design, we consider for the first time color space for perceptually uniform lightness modifications. On this basis, we design an outcome-based adaptive embedding scheme, which adapts to both pixel lightness and regional texture. Last, we propose a concatenated code scheme for robust coding and devise multiple techniques to overcome various screen-camera channel errors. Our prototype and experiments demonstrate that CHROMACODE achieves remarkable raw throughputs of >700 kbps, data goodputs of 120 kbps with BER of 0.05, and with fully imperceptible flicker for viewing proved by user study, which significantly outperforms previous works.",
                "ieee_keywords": [
                    "Videos",
                    "Image color analysis",
                    "Reliability",
                    "Throughput",
                    "Advertising",
                    "Mobile computing",
                    "Communication systems"
                ],
                "author_keywords": [
                    "Screen-camera communication",
                    "hidden visible communication",
                    "non-intrusive visible communication"
                ]
            },
            {
                "title": "A Close Look at 5G in the Wild: Unrealized Potentials and Implications",
                "link": "https://ieeexplore.ieee.org/document/10229016/",
                "date_of_publication": "29 August 2023",
                "doi": "10.1109/INFOCOM53939.2023.10229016",
                "citations": "2",
                "abstract": "This paper reports our in-depth measurement study of 5G experience with three US operators (AT&T, Verizon and T-Mobile). We not only quantitively characterize 5G coverage, availability and performance (over both mmWave and Sub-6GHz bands), but also identify several performance issues and analyze their root causes. We see that real 5G experience is not that satisfactory as anticipated. It is mainly because faster 5G is not used as it can and should. We have several surprising findings: Despite huge speed potentials (say, up to several hundreds of Mbps), more than half are not realized in practice; Such under-utilization is mainly stemmed from current practice and policies that manage radio resource in a performance-oblivious manner; 5G is even less used where 5G is co-deployed over both mmWave and Sub-6GHz bands; Transiently missing 5G is not uncommon and its negative impacts last much longer. Inspired by our findings, we design a patch solution called 5GBoost to fix the problems identified in legacy 5G operations. Our preliminary evaluation validates its effectiveness to realize more 5G potentials.",
                "ieee_keywords": [
                    "Costs",
                    "5G mobile communication",
                    "Pain",
                    "Software",
                    "Millimeter wave communication"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Ninghui Li",
        "publications": [
            {
                "title": "Generating Summary Risk Scores for Mobile Applications",
                "link": "https://ieeexplore.ieee.org/document/6720107/",
                "date_of_publication": null,
                "doi": "10.1109/TDSC.2014.2302293",
                "citations": "38",
                "abstract": "One of Android's main defense mechanisms against malicious apps is a risk communication mechanism which, before a user installs an app, warns the user about the permissions the app requires, trusting that the user will make the right decision. This approach has been shown to be ineffective as it presents the risk information of each app in a “stand-alone” fashion and in a way that requires too much technical knowledge and time to distill useful information. We discuss the desired properties of risk signals and relative risk scores for Android apps in order to generate another metric that users can utilize when choosing apps. We present a wide range of techniques to generate both risk signals and risk scores that are based on heuristics as well as principled machine learning techniques. Experimental results conducted using real-world data sets show that these methods can effectively identify malware as very risky, are simple to understand, and easy to use.",
                "ieee_keywords": [
                    "Androids",
                    "Humanoid robots",
                    "Malware",
                    "Google",
                    "Computational modeling",
                    "Smart phones",
                    "Biological system modeling"
                ],
                "author_keywords": [
                    "Risk",
                    "mobile",
                    "malware",
                    "data mining"
                ]
            },
            {
                "title": "Slicing: A New Approach for Privacy Preserving Data Publishing",
                "link": "https://ieeexplore.ieee.org/document/5645625/",
                "date_of_publication": null,
                "doi": "10.1109/TKDE.2010.236",
                "citations": "173",
                "abstract": "Several anonymization techniques, such as generalization and bucketization, have been designed for privacy preserving microdata publishing. Recent work has shown that generalization loses considerable amount of information, especially for high-dimensional data. Bucketization, on the other hand, does not prevent membership disclosure and does not apply for data that do not have a clear separation between quasi-identifying attributes and sensitive attributes. In this paper, we present a novel technique called slicing, which partitions the data both horizontally and vertically. We show that slicing preserves better data utility than generalization and can be used for membership disclosure protection. Another important advantage of slicing is that it can handle high-dimensional data. We show how slicing can be used for attribute disclosure protection and develop an efficient algorithm for computing the sliced data that obey the ℓ-diversity requirement. Our workload experiments confirm that slicing preserves better utility than generalization and is more effective than bucketization in workloads involving the sensitive attribute. Our experiments also demonstrate that slicing can be used to prevent membership disclosure.",
                "ieee_keywords": [
                    "Correlation",
                    "Diseases",
                    "Privacy",
                    "Publishing",
                    "Joining processes",
                    "Data privacy",
                    "Partitioning algorithms"
                ],
                "author_keywords": [
                    "Privacy preservation",
                    "data anonymization",
                    "data publishing",
                    "data security."
                ]
            },
            {
                "title": "SymCerts: Practical Symbolic Execution for Exposing Noncompliance in X.509 Certificate Validation Implementations",
                "link": "https://ieeexplore.ieee.org/document/7958595/",
                "date_of_publication": "26 June 2017",
                "doi": "10.1109/SP.2017.40",
                "citations": "25",
                "abstract": "Show in Context CrossRef Google Scholar 37. S. Löwe, \"Cpachecker with explicit-value analysis based on cegar and interpolation\" in TACAS, Springer-Verlag, 2013. Show in Context CrossRef Google Scholar 38. G. Brat, K. Havelund, S. Park and W. Visser, \"Model checking programs\", IEEE International Conference on Automated Software Engineering (ASE), pp. 3-12, 2000. Show in Context Google Scholar 39. M. J. C. Gordon and T. F. Melham, Introduction to HOL: A Theorem Proving Environment for Higher Order Logic, New York, NY, USA:Cambridge University Press, 1993. Show in Context Google Scholar 40. J. Jaffar, V. Murali, J. Navas and A. Santosa, \"Tracer: A symbolic execution tool for verification\" in Computer Aided Verification ser. Lecture Notes in Computer Science, Springer Berlin Heidelberg, vol. 7358, pp. 758-766, 2012. Show in Context CrossRef Google Scholar 41. E. M. Clarke, S. Jha and W. Marrero, \"Verifying security protocols with brutus\", TOSEM, vol. 9, no. 4, 2000. Show in Context CrossRef Google Scholar 42. P. Godefroid, \"Model checking for programming languages using verisoft\", POPL. ACM, pp. 174-186, 1997. Show in Context CrossRef Google Scholar 43. C. Brubaker, S. Jana, B. Ray, S. Khurshid and V. Shmatikov, \"Using frankencerts for automated adversarial testing of certificate validation in SSL/TLS implementations\", Security and Privacy (SP) 2014 IEEE Symposium on. IEEE, pp. 114-129, 2014. Show in Context CrossRef Google Scholar 44. J. C. King, \"Symbolic execution and program testing\", Communications of the ACM, vol. 19, no. 7, pp. 385-394, 1976. Show in Context CrossRef Google Scholar 45. C. Cadar, D. Dunbar and D. R. Engler, \"Klee: Unassisted and automatic generation of high-coverage tests for complex systems programs\", OSDI, pp. 209-224, 2008. Show in Context Google Scholar 46. C. Cadar, P. Godefroid, S. Khurshid, C. S. Påsäreanu, K. Sen, N. Tillmann, et al., \"Symbolic execution for software testing in practice: preliminary assessment\", ICSE, pp. 1066-1071, 2011. Show in Context CrossRef Google Scholar 47. C. Cadar and K. Sen, \"Symbolic execution for software testing: three decades later\", Communications of the ACM, vol. 56, no. 2, pp. 82-90, 2013. Show in Context CrossRef Google Scholar 48. V. Kuznetsov, J. Kinder, S. Bucur and G. Candea, \"Efficient state merging in symbolic execution\", Proceedings of the 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation ser. PLDI'12, pp. 193-204, 2012. Show in Context CrossRef Google Scholar 49. HTTPS client is here for the Photon!, 2015, [online] Available: https://community.particle.io/t/https-client-is-here-for-the-photon-by-the-glowfi-sh-team/15934. Show in Context Google Scholar 50. spark/firmware/communication/lib, 2016, [online] Available: https://github.com/spark/firmware/tree/master/communication/lib. Show in Context Google Scholar 51. Arduino/libraries/ESP8266WiFi/src/include/ssl.h, 2016, [online] Available: https://github.com/esp8266/Arduino/blob/master/libraries/ESP8266WiFi/src/include/ssl.h. Show in Context Google Scholar 52. micropython/extmod, 2017, [online] Available: https://github.com/micropython/micropython/tree/master/extmod. Show in Context Google Scholar 53. CVE-2016–6303, [online] Available: https://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2016–6303. Show in Context Google Scholar 54. CVE-2016–7052, [online] Available: https://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2016–7052. Show in Context Google Scholar 55. CVE-2016–6305, [online] Available: https://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2016–6305. Show in Context Google Scholar 56. P.-H. Kamp, \"Please put openssl out of its misery\", Queue, vol. 12, no. 3, pp. 20:20-20:23, Apr. 2014. Show in Context Google Scholar 57. how to install curl and libcurl, [online] Available: https://curl.haxx.se/docs/install.html. Show in Context Google Scholar 58. ipsvd-intemet protocol service daemons-installation, [online] Available: http://smarden.org/ipsyd/install.html. Show in Context Google Scholar 59. Package: gatling (0.12cvs20120114–4) high performance web server and file server, [online] Available: https://packages.debian.org/wheezy/gatling. Show in Context Google Scholar 60. A Python library that encapsulates wolfSSL's wolfCrypt APL., [online] Available: https://pypi.python.org/pypi/wolfcrypt/0.2.0. Show in Context Google Scholar 61. mbed TLS (PolarSSL) wrapper, [online] Available: https://pypi.python.org/pypi/python-mbedtls/0.6. Show in Context Google Scholar 62. W. M. McKeeman, \"Differential testing for software\", DIGITAL TECH-NICAL JOURNAL, vol. 10, no. 1, pp. 100-107, 1998. Show in Context Google Scholar 63. R. B. Evans and A. Savoia, \"Differential testing: A new approach to change detection\", The 6th Joint Meeting on European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering: Companion Papers ser. ESEC-FSE companion'07, pp. 549-552, 2007. Show in Context CrossRef Google Scholar 64. C. Barrett, R. Sebastiani, S. Seshia and C. Tinelli, \"Satisfiability modulo theories\" in Handbook of Satisfiabllity, IOS Press, vol. 185, pp. 825-885, February 2009. Show in Context Google Scholar 65. V. Ganesh and D. L. Dill, \"A decision procedure for bit-vectors and arrays\", Proceedings of the 19th International Conference on Computer Aided Verification ser. CAV'07, pp. 519-531, 2007. Show in Context CrossRef Google Scholar 66. L. S. Huang, A. Rice, E. Ellingsen and C. Jackson, \"Analyzing Forged SSL Certificates in the Wild\", Security and Privacy (SP) 2014 IEEE Symposium on. IEEE, pp. 83-97, 2014. Show in Context View Article Google Scholar 67. C. Hlauschek, M. Gruber, F. Fankhauser and C. Schanes, \"Prying open Pandora's box: KCI attacks against TLS\", 9th USENIX Workshop on Offensive Technologies (WOOT 15), 2015. Show in Context Google Scholar 68. A. Bates, J. Pletcher, T. Nichols, B. Hollembaek, D. Tian, K. R. Butler, et al., \"Securing ssl certificate verification through dynamic linking\", Proceedings of the 2014 ACM SIGSAC Conference on Computerand Communications Security, pp. 394-405, 2014. Show in Context CrossRef Google Scholar 69. M. Georgiev, S. Iyengar, S. Jana, R. Anubhai, D. Boneh and V. Shmatikov, \"The most dangerous code in the world: validating SSL certificates in non-browser software\", Proceedings of the 2012 ACM conference on Computerand communications security, pp. 38-49, 2012. Show in Context CrossRef Google Scholar 70. K. Bhargavan, A. D. Lavaud, C. Fournet, A. Pironti and P. Y. Strub, \"Triple handshakes and cookie cutters: Breaking and fixing authentication over TLS\", Security and Privacy (SP) 2014 IEEE Symposium on. IEEE, pp. 98-113, 2014. Show in Context CrossRef Google Scholar 71. B. He, V. Rastogi, Y. Cao, Y. Chen, V. Venkatakrishnan, R. Yang, et al., \"Vetting ssl usage in applications with sslint\", 2015 IEEE Symposium on Security and Privacy, pp. 519-534, 2015. Show in Context Google Scholar 72. I. Yun, C. Min, X. Si, Y. Jang, T. Kim and M. Naik, \"Apisan: Sanitizing api usages through semantic cross-checking\", 25th USENIX Security Symposium (USENIX Security 16), pp. 363-378, Aug. 2016. Show in Context Google Scholar 73. C. Cadar, V. Ganesh, P. M. Pawlowski, D. L. Dill and D. R. Engler, \"Exe: automatically generating inputs of death\", TISSEC, vol. 12, no. 2, 2008. Show in Context CrossRef Google Scholar 74. K. Sen, D. Marinov and G. Agha, \"Cute: A concolic unit testing engine for c\", ESEC/FSE. ACM, 2005. Show in Context Google Scholar 75. M. Das, S. Lerner and M. Seigle, \"Esp: Path-sensitive program verification in polynomial time\", PLDI, 2002. Show in Context CrossRef Google Scholar 76. P. Godefroid, M. Y. Levin and D. Molnar, \"Sage: whitebox fuzzing for security testing\", Queue, vol. 10, no. 1, pp. 20, 2012. Show in Context CrossRef Google Scholar 77. V. Chipounov, V. Kuznetsov and G. Candea, \"S2e: a platform for invivo multi-path analysis of software systems\", ASPLOS, 2012. Show in Context CrossRef Google Scholar 78. P. Godefroid, N. Klarlund and K. Sen, \"DART: Directed automated random testing\", PLDI. ACM, 2005. Show in Context CrossRef Google Scholar 79. P. Godefroid, P. De Halleux, A. V. Nori, S. K. Rajamani, W. Schulte, N. Tillmann, et al., \"Automating software testing using program analysis\", Software IEEE, vol. 25, no. 5, pp. 30-37, 2008. Show in Context View Article Google Scholar 80. D. A. Ramos and D. R. Engler, \"Practical low-effort equivalence verification of real code\", Proceedings of the 23rd International Conference on Computer Aided Verification ser. CAV'll, pp. 669-685, 2011. Show in Context Google Scholar 81. D. A. Ramos and D. Engler, \"Under-constrained symbolic execution: Correctness checking for real code\", 24th USENIX Security Symposium (USENIX Security 15), pp. 49-64, Aug. 2015. Show in Context Google Scholar 82. L. Pedrosa, A. Fogel, N. Kothari, R. Govindan, R. Mahajan and T. Millstein, \"Analyzing protocol implementations for interoperability\", NSDI, 2015. Show in Context Google Scholar 83. M. Canini, D. Venzano, P. Perešini, D. Kostić and J. Rexford, \"A nice way to test openflow applications\", NSDI, 2012. Show in Context Google Scholar 84. M. Kuzniar, P. Peresini, M. Canini, D. Venzano and D. Kostic, \"A soft way for openflow switch interoperability testing\", Proceedings of the 8th International Conference on Emerging Networking Experiments and Technologies ser. CoNEXT'12, pp. 265-276, 2012. Show in Context CrossRef Google Scholar 85. C. Min, S. Kashyap, B. Lee, C. Song and T. Kim, \"Cross-checking semantic correctness: The case of finding file system bugs\", Proceedings of the 25th Symposium on Operating Systems Principles ser. SOSP'15, pp. 361-377, 2015. Show in Context CrossRef Google Scholar 86. L. D'Antoni and M. Veanes, \"Minimization of symbolic automata\", SIGPLAN Not., vol. 49, no. 1, pp. 541-553, Jan. 2014. Show in Context CrossRef Google Scholar 87. G. Argyros, I. Stais, A. Kiayias and A. D. Keromytis, \"Back in black: towards formal black box analysis of sanitizers and filters\", Security and Privacy (SP) 2016 IEEE Symposium on. IEEE, pp. 91-109, 2016. Show in Context View Article Google Scholar 88. George Argyros, Ioannis Stais, Suman Jana, Angelos D. Keromytis and Aggelos Kiayias, \"SFADiff: Automated Evasion Attacks and Fingerprinting Using Black-box Differential Automata Learning\", Proceedings of the 23rd ACM Conference on Computerand Communications Security, Oct 2016. Show in Context CrossRef Google Scholar 89. C. Adams and S. Lloyd, Understanding PKI: Concepts Standards and Deployment Considerations, Boston, MA, USA:Addison-Wesley Longman Publishing Co., Inc., 2002. Show in Context Google Scholar 90. G. Nelson and D. C. Oppen, \"Fast decision procedures based on congruence closure\", J. ACM, vol. 27, no. 2, pp. 356-364, Apr. 1980. Show in Context CrossRef Google Scholar 91. S. Legg, \"ASN.1 Module Definition for the LDAP and X.500 Component Matching Rules\", RFC 3727, Mar. 2013, [online] Available: https://rfc-editor.org/rfc/rfc3727.txt. Show in Context Google Scholar 92. \"ASN.1 Translation\", RFC 6025, Oct. 2015, [online] Available: https://rfc-editor.org/rfc/rfc6025.txt. Show in Context Google Scholar 93. ASN.1 JavaScript decoder, [online] Available: https://lapo.it/asnljs/. Show in Context Google Scholar 94. wolfSSL ChangeLog, 2016, [online] Available: https://www.wolfssl.com/wolfSSL/Docs-wolfssl-changelog.html. Show in Context Google Scholar 95. mbed TLS 2.2.0 2.1.3 1.3.15 and PolarSSL 1.2.18 released 2015, [online] Available: https://tls.mbed.org/tech-updates/releases/mbedtls-2.2.0–2.1.3-1.3.15-and-polarssl.1.2.18-released. Show in Context Google Scholar 96. Certificate Template Extensions: Application Policy, [online] Available: https://technet.microsoft.com/en-us/library/cc731792(v=ws.11).aspx. Show in Context Google Scholar 97. C. Young, Flawed MatrixSSL Code Highlights Need for Better IoT Update Practices, 2016, [online] Available: http://www.tripwire.com/state-of-security/security-data-protection/cyber-security/flawed-matrixssl-code-highlights-need-for-better-iot-update-practices/. Show in Context Google Scholar 98. IPv6 Ready Logo Program, 2016, [online] Available: https://www.ipv6ready.org. Show in Context Google Scholar 99. High Definition Logos, 2016, [online] Available: http://www.digitaleurope.org/Services/High-Definition-Logos. Show in Context Google Scholar 100. T. Liang, A. Reynolds, C. Tinelli, C. Barrett and M. Deters, A DPLL(T) Theory Solver for a Theory of Strings and Regular Expressions, Cham:Springer International Publishing, pp. 646-662, 2014. Show in Context Google Scholar 101. Vulnerability Note VU#396440-MatrixSSL contains multiple vulnerabilities, 2016, [online] Available: http://www.kb.cert.org/vuls/id/396440. Show in Context Google Scholar 102. S. Chong, J. Guttman, A. Datta, A. C. Myers, B. Pierce, P. Schaumont, et al., \"Report on the NSF workshop on formal methods for security\", CoRR, vol. abs/1608.00678, 2016. Show in Context Google Scholar",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Mobile Data Collection and Analysis with Local Differential Privacy",
                "link": "https://ieeexplore.ieee.org/document/8788771/",
                "date_of_publication": "08 August 2019",
                "doi": "10.1109/MDM.2019.00-80",
                "citations": "8",
                "abstract": "Local Differential Privacy (LDP), where each user perturbs her data locally before sending to an untrusted party, is a new and promising privacy-preserving model for mobile data collection and analysis. LDP has been deployed in many real products recently by several major software and Internet companies, including Google, Apple and Microsoft. This seminar talk first introduces the rationale of LDP model behind these deployed systems to collect and analyze usage data privately, then surveys the current research landscape in LDP, and finally identifies several open problems and research directions in this community.",
                "ieee_keywords": [
                    "Differential privacy",
                    "Perturbation methods",
                    "Frequency estimation",
                    "Data collection",
                    "Estimation",
                    "Seminars"
                ],
                "author_keywords": [
                    "Mobile data collection, data analysis, privacy, local differential privacy"
                ]
            },
            {
                "title": "A Probabilistic Discriminative Model for Android Malware Detection with Decompiled Source Code",
                "link": "https://ieeexplore.ieee.org/document/6894210/",
                "date_of_publication": null,
                "doi": "10.1109/TDSC.2014.2355839",
                "citations": "101",
                "abstract": "Mobile devices are an important part of our everyday lives, and the Android platform has become a market leader. In recent years a number of approaches for Android malware detection have been proposed, using permissions, source code analysis, or dynamic analysis. In this paper, we propose to use a probabilistic discriminative model based on regularized logistic regression for Android malware detection. Through extensive experimental evaluation, we demonstrate that it can generate probabilistic outputs with highly accurate classification results. In particular, we propose to use Android API calls as features extracted from decompiled source code, and analyze and explore issues in feature granularity, feature representation, feature selection, and regularization. We show that the probabilistic discriminative model also works well with permissions, and substantially outperforms the state-of-the-art methods for Android malware detection with application permissions. Furthermore, the discriminative learning model achieves the best detection results by combining both decompiled source code and application permissions. To the best of our knowledge, this is the first research that proposes probabilistic discriminative model for Android malware detection with a thorough study of desired representation of decompiled source code and is the first research work for Android malware detection task that combines both analysis of decompiled source code and application permissions.",
                "ieee_keywords": [
                    "Feature extraction",
                    "Malware",
                    "Androids",
                    "Humanoid robots",
                    "Smart phones",
                    "Probabilistic logic",
                    "Measurement"
                ],
                "author_keywords": []
            },
            {
                "title": "Effective Risk Communication for Android Apps",
                "link": "https://ieeexplore.ieee.org/document/6684532/",
                "date_of_publication": null,
                "doi": "10.1109/TDSC.2013.58",
                "citations": "44",
                "abstract": "The popularity and advanced functionality of mobile devices has made them attractive targets for malicious and intrusive applications (apps). Although strong security measures are in place for most mobile systems, the area where these systems often fail is the reliance on the user to make decisions that impact the security of a device. As our prime example, Android relies on users to understand the permissions that an app is requesting and to base the installation decision on the list of permissions. Previous research has shown that this reliance on users is ineffective, as most users do not understand or consider the permission information. We propose a solution that leverages a method to assign a risk score to each app and display a summary of that information to users. Results from four experiments are reported in which we examine the effects of introducing summary risk information and how best to convey such information to a user. Our results show that the inclusion of risk-score information has significant positive effects in the selection process and can also lead to more curiosity about security-related information.",
                "ieee_keywords": [
                    "Privacy",
                    "Security",
                    "Androids",
                    "Humanoid robots",
                    "Smart phones",
                    "Mobile communication"
                ],
                "author_keywords": [
                    "Risk communication",
                    "usability",
                    "mobile security"
                ]
            },
            {
                "title": "Intrusion Detection Systems for IoT",
                "link": "https://ieeexplore.ieee.org/document/10016461/",
                "date_of_publication": null,
                "doi": "10.1002/9781119892199.ch13",
                "citations": "Editor(s): Keith Gremban; Ananthram Swami; Robert Douglass; Stephan Gerali",
                "abstract": "This chapter focuses on techniques to detect attacks on internet of things (IoT) devices. It reviews intrusion detection systems (IDSes) proposed for IoT devices and categorizes the IDSes according to the research challenges they aim to address and their core techniques. The chapter also categorizes the IDSes based on the threats that they aim to prevent, such as routing attacks in IPv6 over low‐power wireless personal area networks (6LoWPAN). It describes the IDSes concerning: from where the IDS collects logs to be analyzed (i.e. host‐based or network‐based); the type of architectures the IDS uses (i.e. centralized, decentralized, or distributed); and the type of detection mechanism that the IDS relies on (i.e. signature‐based, anomaly‐based, or hybrid). The IDSes that deal with complex attacks should enable the protection of IoT devices from advanced threats. Page(s): 237 - 258 Copyright Year: 2023 Edition: 1 ISBN Information: DOI: 10.1002/9781119892199.ch13 Publisher: Wiley-IEEE Press Authors Keywords IEEE Keywords Internet of Things , Protocols , Temperature measurement , IEEE 802.15 Standard , Edge computing , Computer architecture , Clouds Metrics More Like This Future Edge Cloud and Edge Computing for Internet of Things Applications Published: 2018 Secure Quantum Steganography Protocol for Fog Cloud Internet of Things Published: 2018 Show More",
                "ieee_keywords": [
                    "Internet of Things",
                    "Protocols",
                    "Temperature measurement",
                    "IEEE 802.15 Standard",
                    "Edge computing",
                    "Computer architecture",
                    "Clouds"
                ],
                "author_keywords": []
            },
            {
                "title": "Closeness: A New Privacy Measure for Data Publishing",
                "link": "https://ieeexplore.ieee.org/document/5072216/",
                "date_of_publication": null,
                "doi": "10.1109/TKDE.2009.139",
                "citations": "128",
                "abstract": "The k-anonymity privacy requirement for publishing microdata requires that each equivalence class (i.e., a set of records that are indistinguishable from each other with respect to certain “identifying” attributes) contains at least k records. Recently, several authors have recognized that k-anonymity cannot prevent attribute disclosure. The notion of ℓ-diversity has been proposed to address this; ℓ-diversity requires that each equivalence class has at least ℓ well-represented (in Section 2) values for each sensitive attribute. In this paper, we show that ℓ-diversity has a number of limitations. In particular, it is neither necessary nor sufficient to prevent attribute disclosure. Motivated by these limitations, we propose a new notion of privacy called “closeness.” We first present the base model t-closeness, which requires that the distribution of a sensitive attribute in any equivalence class is close to the distribution of the attribute in the overall table (i.e., the distance between the two distributions should be no more than a threshold t). We then propose a more flexible privacy model called (n,t)-closeness that offers higher utility. We describe our desiderata for designing a distance measure between two probability distributions and present two distance measures. We discuss the rationale for using closeness as a privacy measure and illustrate its advantages through examples and experiments.",
                "ieee_keywords": [
                    "Data privacy",
                    "Publishing",
                    "Data security",
                    "Probability distribution",
                    "Information security",
                    "Diseases",
                    "Remuneration",
                    "Databases"
                ],
                "author_keywords": [
                    "Privacy preservation",
                    "data anonymization",
                    "data publishing",
                    "data security."
                ]
            },
            {
                "title": "On the Complexity of Authorization in RBAC under Qualification and Security Constraints",
                "link": "https://ieeexplore.ieee.org/document/5590260/",
                "date_of_publication": null,
                "doi": "10.1109/TDSC.2010.55",
                "citations": "18",
                "abstract": "In practice, assigning access permissions to users must satisfy a variety of constraints motivated by business and security requirements. Here, we focus on Role-Based Access Control (RBAC) systems, in which access permissions are assigned to roles and roles are then assigned to users. User-role assignment is subject to role-based constraints, such as mutual exclusion constraints, prerequisite constraints, and role-cardinality constraints. Also, whether a user is qualified for a role depends on whether his/her qualification satisfies the role's requirements. In other words, a role can only be assigned to a certain set of qualified users. In this paper, we study fundamental problems related to access control constraints and user-role assignment, such as determining whether there are conflicts in a set of constraints, verifying whether a user-role assignment satisfies all constraints, and how to generate a valid user-role assignment for a system configuration. Computational complexity results and/or algorithms are given for the problems we consider.",
                "ieee_keywords": [
                    "Computational complexity",
                    "Access control",
                    "Formal verification"
                ],
                "author_keywords": [
                    "Access control",
                    "RBAC",
                    "formal methods",
                    "computational complexity."
                ]
            },
            {
                "title": "Analyzing Operational Behavior of Stateful Protocol Implementations for Detecting Semantic Bugs",
                "link": "https://ieeexplore.ieee.org/document/8023160/",
                "date_of_publication": "31 August 2017",
                "doi": "10.1109/DSN.2017.36",
                "citations": "9",
                "abstract": "Network protocol implementations must comply with their specifications that include properties describing the correct operational behavior of the protocol in response to different temporal orderings of network events. Due to inconsistent interpretations of the specification, developers can unknowingly introduce semantic bugs, which cause the implementations to violate the respective properties. Detecting such bugs in stateful protocols becomes significantly difficult as their operations depend on their internal state machines and the complex interactions between the protocol logic. In this paper, we present an automated tool to help developers analyze their protocol implementations and detect semantic bugs violating the temporal properties of the protocols. Given an implementation, our tool (1) extracts the implemented finite state machine (FSM) of the protocol from the source code by symbolically exploring the code and (2) determines whether the extracted FSM violates given temporal properties by using an off-the-shelf model checker. We demonstrated the efficacy of our tool by applying it on 6 protocol implementations. We detected 11 semantic bugs (2 with security implications) when we analyzed these implementations against properties obtained from their publicly available specifications.",
                "ieee_keywords": [
                    "Protocols",
                    "Computer bugs",
                    "Semantics",
                    "Tools",
                    "Security",
                    "Model checking",
                    "Servers"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Dan Goldwasser",
        "publications": [
            {
                "title": "Learning from the Ones that Got Away: Detecting New Forms of Phishing Attacks",
                "link": "https://ieeexplore.ieee.org/document/8440723/",
                "date_of_publication": null,
                "doi": "10.1109/TDSC.2018.2864993",
                "citations": "47",
                "abstract": "Phishing attacks continue to pose a major threat for computer system defenders, often forming the first step in a multi-stage attack. There have been great strides made in phishing detection; however, some phishing emails appear to pass through filters by making simple structural and semantic changes to the messages. We tackle this problem through the use of a machine learning classifier operating on a large corpus of phishing and legitimate emails. We design SAFe-PC (Semi-Automated Feature generation for Phish Classification), a system to extract features, elevating some to higher level features, that are meant to defeat common phishing email detection strategies. To evaluate SAFe-PC , we collect a large corpus of phishing emails from the central IT organization at a tier-1 university. The execution of SAFe-PC on the dataset exposes hitherto unknown insights on phishing campaigns directed at university users. SAFe-PC detects more than 70 percent of the emails that had eluded our production deployment of Sophos, a state-of-the-art email filtering tool. It also outperforms SpamAssassin, a commonly used email filtering tool. We also developed an online version of SAFe-PC, that can be incrementally retrained with new samples. Its detection performance improves with time as new samples are collected, while the time to retrain the classifier stays constant.",
                "ieee_keywords": [
                    "Phishing",
                    "Electronic mail",
                    "Feature extraction",
                    "Training",
                    "Online services",
                    "Blacklisting",
                    "Machine learning"
                ],
                "author_keywords": [
                    "Phishing attacks",
                    "machine learning security",
                    "online learning",
                    "university-based phishing campaigns"
                ]
            },
            {
                "title": "Do You Do Yoga? Understanding Twitter Users' Types and Motivations using Social and Textual Information",
                "link": "https://ieeexplore.ieee.org/document/9364605/",
                "date_of_publication": "30 March 2021",
                "doi": "10.1109/ICSC50631.2021.00067",
                "citations": "115",
                "abstract": "Leveraging social media data to understand people's lifestyle choices is an exciting domain to explore but requires a multiview formulation of the data. In this paper, we propose a joint embedding model based on the fusion of neural networks with attention mechanism by incorporating social and textual information of users to understand their activities and motivations. We use well-being related tweets from Twitter, focusing on ‘Yoga’. We demonstrate our model on two downstream tasks: (i) finding user type such as either practitioner or promotional (promoting yoga studio/gym), other; (ii) finding user motivation i.e. health benefit, spirituality, love to tweet/retweet about yoga but do not practice yoga. Notes: This article was originally incorrectly tagged as not presented at the conference. It is now included as part of the conference record.",
                "ieee_keywords": [
                    "Social networking (online)",
                    "Computational modeling",
                    "Blogs",
                    "Neural networks",
                    "Semantics",
                    "Predictive models",
                    "Task analysis"
                ],
                "author_keywords": [
                    "joint embedding",
                    "user representation",
                    "social media",
                    "yoga"
                ]
            },
            {
                "title": "KNOD: Domain Knowledge Distilled Tree Decoder for Automated Program Repair",
                "link": "https://ieeexplore.ieee.org/document/10172873/",
                "date_of_publication": "14 July 2023",
                "doi": "10.1109/ICSE48619.2023.00111",
                "citations": "2",
                "abstract": ", Deep Learning Metrics Footnotes More Like This Prediction and Visualisation of Viral Genome Antigen Using Deep Learning & Artificial Intelligence 2021 5th International Conference on Computing Methodologies and Communication (ICCMC) Published: 2021 Combined IASI-NG and MWS Observations for the Retrieval of Cloud Liquid and Ice Water Path: A Deep Learning Artificial Intelligence Approach IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing Published: 2022 Show More References References is not available for this document.",
                "ieee_keywords": [
                    "Training",
                    "Codes",
                    "Source coding",
                    "Computer bugs",
                    "Semantics",
                    "Syntactics",
                    "Benchmark testing"
                ],
                "author_keywords": [
                    "Automated Program Repair",
                    "Abstract Syntax Tree",
                    "Deep Learning"
                ]
            },
            {
                "title": "Analyzing Large Collections of Open-Ended Feedback From MOOC Learners Using LDA Topic Modeling and Qualitative Analysis",
                "link": "https://ieeexplore.ieee.org/document/9373927/",
                "date_of_publication": null,
                "doi": "10.1109/TLT.2021.3064798",
                "citations": "16",
                "abstract": "There is a large variation in background and purpose of massive open online course (MOOC) learners. To improve the overall MOOC learning experience, it is important to identify which MOOC characteristics are most important for learners. For this purpose, in this article, we analyzed about 150 000 open-ended learner responses from 810 MOOCs to three postcourse survey questions about their learning experience: (Q1) What was your most favorite part and why? (Q2) What your least favorite part and why? (Q3) How could the course be improved? We used the latent Dirichlet allocation topic model to identify prominent topics present in learner responses to each question. We determined the theme of each identified topic through qualitative analysis. Our results show that the following aspects of MOOCs can significantly impact the learning experience: quality of course content, accurate description of prerequisites and required time commitment in course syllabus, quality of assessment and feedback, meaningful interaction with peers and educators, engaging instructor and videos, accessibility of learning materials, and usability of platform.",
                "ieee_keywords": [
                    "Electronic learning",
                    "Computer aided instruction",
                    "Analytical models",
                    "Data models",
                    "Mathematical model",
                    "Interviews",
                    "Coherence"
                ],
                "author_keywords": [
                    "Classroom feedback systems",
                    "distance learning",
                    "large text archives",
                    "machine learning"
                ]
            },
            {
                "title": "Interactive Learning for Identifying Relevant Tweets to Support Real-time Situational Awareness",
                "link": "https://ieeexplore.ieee.org/document/8807283/",
                "date_of_publication": null,
                "doi": "10.1109/TVCG.2019.2934614",
                "citations": "8",
                "abstract": "Various domain users are increasingly leveraging real-time social media data to gain rapid situational awareness. However, due to the high noise in the deluge of data, effectively determining semantically relevant information can be difficult, further complicated by the changing definition of relevancy by each end user for different events. The majority of existing methods for short text relevance classification fail to incorporate users' knowledge into the classification process. Existing methods that incorporate interactive user feedback focus on historical datasets. Therefore, classifiers cannot be interactively retrained for specific events or user-dependent needs in real-time. This limits real-time situational awareness, as streaming data that is incorrectly classified cannot be corrected immediately, permitting the possibility for important incoming data to be incorrectly classified as well. We present a novel interactive learning framework to improve the classification process in which the user iteratively corrects the relevancy of tweets in real-time to train the classification model on-the-fly for immediate predictive improvements. We computationally evaluate our classification model adapted to learn at interactive rates. Our results show that our approach outperforms state-of-the-art machine learning models. In addition, we integrate our framework with the extended Social Media Analytics and Reporting Toolkit (SMART) 2.0 system, allowing the use of our interactive learning framework within a visual analytics system tailored for real-time situational awareness. To demonstrate our framework's effectiveness, we provide domain expert feedback from first responders who used the extended SMART 2.0 system.",
                "ieee_keywords": [
                    "Real-time systems",
                    "Social networking (online)",
                    "Computational modeling",
                    "Visual analytics",
                    "Machine learning",
                    "Adaptation models",
                    "Training"
                ],
                "author_keywords": [
                    "Interactive machine learning",
                    "human-computer interaction",
                    "social media analytics",
                    "emergency/disaster management",
                    "situational awareness"
                ]
            },
            {
                "title": "ACE – An Anomaly Contribution Explainer for Cyber-Security Applications",
                "link": "https://ieeexplore.ieee.org/document/9005989/",
                "date_of_publication": "24 February 2020",
                "doi": "10.1109/BigData47090.2019.9005989",
                "citations": "4",
                "abstract": "In this paper we introduce Anomaly Contribution Explainer or ACE, a tool to explain security anomaly detection models in terms of the model features through a regression framework, and its variant, ACE-KL, which highlights the important anomaly contributors. ACE and ACE-KL provide insights in diagnosing which attributes significantly contribute to an anomaly by building a specialized linear model to locally approximate the anomaly score that a black-box model generates. We conducted experiments with these anomaly detection models to detect security anomalies on both synthetic data and real data. In particular, we evaluate performance on three public data sets: CERT insider threat, netflow logs, and Android malware. The experimental results are encouraging: our methods consistently identify the correct contributing feature in the synthetic data where ground truth is available; similarly, for real data sets, our methods point a security analyst in the direction of the underlying causes of an anomaly, including in one case leading to the discovery of previously overlooked network scanning activity. We have made our source code publicly available.",
                "ieee_keywords": [
                    "Anomaly detection",
                    "Predictive models",
                    "Biological system modeling",
                    "Feature extraction",
                    "Machine learning",
                    "Computer security"
                ],
                "author_keywords": [
                    "anomaly detection",
                    "model explanation",
                    "model interpretability",
                    "cyber-security"
                ]
            },
            {
                "title": "Automated Attack Synthesis by Extracting Finite State Machines from Protocol Specification Documents",
                "link": "https://ieeexplore.ieee.org/document/9833673/",
                "date_of_publication": "27 July 2022",
                "doi": "10.1109/SP46214.2022.9833673",
                "citations": "5",
                "abstract": "Automated attack discovery techniques, such as attacker synthesis or model-based fuzzing, provide powerful ways to ensure network protocols operate correctly and securely. Such techniques, in general, require a formal representation of the protocol, often in the form of a finite state machine (FSM). Unfortunately, many protocols are only described in English prose, and implementing even a simple network protocol as an FSM is time-consuming and prone to subtle logical errors. Automatically extracting protocol FSMs from documentation can significantly contribute to increased use of these techniques and result in more robust and secure protocol implementations.In this work we focus on attacker synthesis as a representative technique for protocol security, and on RFCs as a representative format for protocol prose description. Unlike other works that rely on rule-based approaches or use off-the-shelf NLP tools directly, we suggest a data-driven approach for extracting FSMs from RFC documents. Specifically, we use a hybrid approach consisting of three key steps: (1) large-scale word-representation learning for technical language, (2) focused zero-shot learning for mapping protocol text to a protocol-independent information language, and (3) rule-based mapping from protocol-independent information to a specific protocol FSM. We show the generalizability of our FSM extraction by using the RFCs for six different protocols: BGPv4, DCCP, LTP, PPTP, SCTP and TCP. We demonstrate how automated extraction of an FSM from an RFC can be applied to the synthesis of attacks, with TCP and DCCP as case-studies. Our approach shows that it is possible to automate attacker synthesis against protocols by using textual specifications such as RFCs.",
                "ieee_keywords": [
                    "Privacy",
                    "Protocols",
                    "Automata",
                    "Documentation",
                    "Fuzzing",
                    "Security"
                ],
                "author_keywords": [
                    "attack-synthesis",
                    "network-security",
                    "NLP"
                ]
            },
            {
                "title": "Does Yoga Make You Happy? Analyzing Twitter User Happiness using Textual and Temporal Information",
                "link": "https://ieeexplore.ieee.org/document/9378461/",
                "date_of_publication": "19 March 2021",
                "doi": "10.1109/BigData50022.2020.9378461",
                "citations": "1",
                "abstract": "Although yoga is a multi-component practice to hone the body and mind and be known to reduce anxiety and depression, there is still a gap in understanding people's emotional state related to yoga in social media. In this study, we investigate the causal relationship between practicing yoga and being happy by incorporating textual and temporal information of users using Granger causality. To find out causal features from the text, we measure two variables (i) Yoga activity level based on content analysis and (ii) Happiness level based on emotional state. To understand users' yoga activity, we propose a joint embedding model based on the fusion of neural networks with attention mechanism by leveraging users' social and textual information. For measuring the emotional state of yoga users (target domain), we suggest a transfer learning approach to transfer knowledge from an attention-based neural network model trained on a source domain. Our experiment on Twitter dataset demonstrates that there are 1447 users where \"yoga Granger-causes happiness\".",
                "ieee_keywords": [
                    "Knowledge engineering",
                    "Social networking (online)",
                    "Neural networks",
                    "Blogs",
                    "Transfer learning",
                    "Big Data",
                    "Depression"
                ],
                "author_keywords": [
                    "social media",
                    "yoga",
                    "granger causality",
                    "emotion",
                    "transfer learning"
                ]
            },
            {
                "title": "Understanding COVID-19 Vaccine Campaign on Facebook using Minimal Supervision",
                "link": "https://ieeexplore.ieee.org/document/10021123/",
                "date_of_publication": "26 January 2023",
                "doi": "10.1109/BigData55660.2022.10021123",
                "citations": "49",
                "abstract": "In the age of social media, where billions of internet users share information and opinions, the negative impact of pandemics is not limited to the physical world. It provokes a surge of incomplete, biased, and incorrect information, also known as an infodemic. This global infodemic jeopardizes measures to control the pandemic by creating panic, vaccine hesitancy, and fragmented social response. Platforms like Facebook allow advertisers to adapt their messaging to target different demographics and help alleviate or exacerbate the infodemic problem depending on their content. In this paper, we propose a minimally supervised multi-task learning framework for understanding messaging on Facebook related to the COVID vaccine by identifying ad themes and moral foundations. Furthermore, we perform a more nuanced thematic analysis of messaging tactics of vaccine campaigns on social media so that policymakers can make better decisions on pandemic control.",
                "ieee_keywords": [
                    "COVID-19",
                    "Limiting",
                    "Social networking (online)",
                    "Pandemics",
                    "Big Data",
                    "Multitasking",
                    "Vaccines"
                ],
                "author_keywords": [
                    "COVID-19 vaccine",
                    "social media",
                    "facebook ads",
                    "minimal supervision",
                    "weak labeling"
                ]
            },
            {
                "title": "Interpretable Engagement Models for MOOCs Using Hinge-Loss Markov Random Fields",
                "link": "https://ieeexplore.ieee.org/document/8590724/",
                "date_of_publication": null,
                "doi": "10.1109/TLT.2018.2889953",
                "citations": "2",
                "abstract": "Maintaining and cultivating student engagement is critical for learning. Understanding factors affecting student engagement can help in designing better courses and improving student retention. The large number of participants in massive open online courses (MOOCs) and data collected from their interactions on the MOOC open up avenues for studying student engagement at scale. In this work, we develop an interpretable statistical relational learning model for understanding student engagement in online courses using a complex combination of behavioral, linguistic, structural, and temporal cues. We show how to abstract student engagement types of active, passive, and disengagement as meaningful latent variables using logical rules in our model connecting student behavioral signals with student success in MOOCs. We demonstrate that the latent formulation for engagement helps in predicting two measures of student success: performance, their final grade in the course, and survival, their continued presence in the course till the end, across seven MOOCs. Furthermore in order to initiate better instructor interventions, we need to be able to predict student success early in the course. We demonstrate that we can predict student success early in the course reliably using the latent model. We also demonstrate the utility of our models in predicting student success in new courses, by training our models on one course and testing on another course. We show that the latent abstractions are helpful in predicting student success and engagement reliably in new MOOCs that have not yet gathered student interaction data. We then perform a closer quantitative analysis of different features derived from student interactions on the MOOC and identify student activities that are good indicators of student success at different points in the course. Through a qualitative analysis of the latent engagement variable values, we demonstrate their utility in understanding students' engagement leve... (Show More)",
                "ieee_keywords": [
                    "Predictive models",
                    "Analytical models",
                    "Data models",
                    "Discussion forums",
                    "Linguistics",
                    "Education",
                    "Adaptation models"
                ],
                "author_keywords": [
                    "Latent engagement models",
                    "student engagement",
                    "graphical models",
                    "statistical relational models",
                    "course success prediction"
                ]
            }
        ]
    },
    {
        "name": "Changhee Jung",
        "publications": [
            {
                "title": "A Safety-Performance Metric Enabling Computational Awareness in Autonomous Robots",
                "link": "https://ieeexplore.ieee.org/document/10197452/",
                "date_of_publication": null,
                "doi": "10.1109/LRA.2023.3300251",
                "citations": "54",
                "abstract": "This letter takes a first step towards the analysis of safety and performance critical computational tasks for autonomous robots. Our contribution is a safety-performance (SP) metric that ensures safety first and then rewards improved performance of real-time computational tasks, building on the notion of “nominal safety” which defines timely computation as critical to safety. To fully utilize the computing capacity of heterogeneous processing units (e.g., CPU + GPU), a computational task graph model called the Stochastic Heterogeneous Parallel Directed Acyclic Graph (SHP-DAG) is adopted to capture the uncertain nature of robotic applications and their required computation. Compared to state-of-the-art task models, SHP-DAG avoids the pessimism of deterministic worst-case execution time (WCET), instead modeling the execution times of tasks by probability distributions. Our SP metric is defined upon this task model, which allows us to apply the FIFO and CFS schedulers of the Linux kernel on complex robotic computational tasks and compare the SP metric with baseline metrics, average and worst-case makespan. Extensive experimental results on NVIDIA Jetson AGX Xavier hardware demonstrate that the proposed SP metric is appropriate for managing computational tasks in a manner that balances safety and performance in robotic systems.",
                "ieee_keywords": [
                    "Task analysis",
                    "Safety",
                    "Measurement",
                    "Robots",
                    "Computational modeling",
                    "Real-time systems",
                    "Timing"
                ],
                "author_keywords": [
                    "Autonomous robots",
                    "safety management",
                    "scheduling algorithms",
                    "software performance",
                    "uncertainty"
                ]
            },
            {
                "title": "CapOS: Capacitor Error Resilience for Energy Harvesting Systems",
                "link": "https://ieeexplore.ieee.org/document/9870858/",
                "date_of_publication": null,
                "doi": "10.1109/TCAD.2022.3202861",
                "citations": "1",
                "abstract": "Published: 1999 Impact of thyristor controlled series capacitor on bulk power system reliability IEEE Power Engineering Society Summer Meeting, Published: 2002 Show More References References is not available for this document.",
                "ieee_keywords": [
                    "Capacitors",
                    "Checkpointing",
                    "Nonvolatile memory",
                    "Energy harvesting",
                    "Registers",
                    "Power system reliability",
                    "Internet of Things"
                ],
                "author_keywords": [
                    "Capacitor",
                    "energy harvesting",
                    "reliability"
                ]
            },
            {
                "title": "Unbounded Hardware Transactional Memory for a Hybrid DRAM/NVM Memory System",
                "link": "https://ieeexplore.ieee.org/document/9251977/",
                "date_of_publication": "11 November 2020",
                "doi": "10.1109/MICRO50266.2020.00051",
                "citations": "8",
                "abstract": "Persistent memory programming requires failure atomicity. To achieve this in an efficient manner, recent proposals use hardware-based logging for atomic-durable updates and hardware transactional memory (HTM) for isolation. Although the unbounded HTMs are promising for both performance and programmability reasons, none of the previous studies satisfies the practical requirements. They either require unrealistic hard-ware overheads or do not allow transactions to exceed on-chip cache boundaries. Furthermore, it has never been possible to use both DRAM and NVM in HTM, though it is becoming a popular persistency model. To this end, this study proposes UHTM, unbounded hardware transactional memory for DRAM and NVM hybrid memory systems. UHTM combines the cache coherence protocol and address-signatures to detect conflicts in the entire memory space. This approach improves concurrency by significantly reducing the false-positive rates of previous studies. More importantly, UHTM allows both DRAM and NVM data to interact with each other in transactions without compromising the consistency guarantee. This is rendered possible by UHTM's hybrid version management that provides an undo-based log for DRAM and a redo-based log for NVM. The experimental results show that UHTM outperforms the state-of-the-art durable HTM, which is LLC-bounded, by 56% on average and up to 818%.",
                "ieee_keywords": [
                    "Protocols",
                    "Nonvolatile memory",
                    "Memory management",
                    "Random access memory",
                    "Programming",
                    "Hardware",
                    "System-on-chip"
                ],
                "author_keywords": [
                    "Persistent Memory",
                    "Transaction",
                    "Conflict Detection",
                    "Version Management",
                    "Hybrid Memory",
                    "Hardware Transactional Memory"
                ]
            },
            {
                "title": "Compiler-Directed High-Performance Intermittent Computation with Power Failure Immunity",
                "link": "https://ieeexplore.ieee.org/document/9804619/",
                "date_of_publication": "29 June 2022",
                "doi": "10.1109/RTAS54340.2022.00012",
                "citations": "2",
                "abstract": "This paper introduces power failure immunity (PFI), an essential program execution property for energy harvesting systems to achieve efficient intermittent computation. PFI ensures program code regions never fail more than once i.e., at most single in-region outage, during intermittent computation as if they are immunized after the first power outage. To enforce PFI automatically for such batteryless systems that use a tiny energy buffer instead, we present its compiler-directed enforcement. The compiler leverages a precise static analysis to partition the program into recoverable regions with the energy buffer size in mind so that their execution can be completed—using the full energy buffered in a single charge cycle—regardless of program execution paths. In this way, no matter how unstable the energy harvesting source is, no region fails more than once.In the virtue of PFI, this paper presents ROCKCLIMB, a high-performance and rollback-free intermittent computation scheme. It guarantees that PFI-enforced regions never fail, i.e., there is no in-region outage at all. To achieve this, ROCKCLIMB checks if the fully buffered energy is secured at each region boundary. If it is not secured, ROCKCLIMB waits until the energy buffer is fully charged, before executing the following region. In particular, the rollback-free nature of ROCKCLIMB obviates the need to log memory writes—required for rollback recovery—since no region is power-interrupted. As a result, PFI+ROCKCLIMB achieves rollback-free and memory-log-free intermittent computation, ensuring forward execution progress and maximizing it even in the presence of frequent power outages. Our real board experiments demonstrate that PFI+ROCKCLIMB outperforms the state-of-the-art work by 5%—550% on average in various energy harvesting conditions.",
                "ieee_keywords": [
                    "Program processors",
                    "Codes",
                    "Static analysis",
                    "Real-time systems",
                    "Power system reliability",
                    "Computational efficiency",
                    "Energy harvesting"
                ],
                "author_keywords": []
            },
            {
                "title": "Featherweight Soft Error Resilience for GPUs",
                "link": "https://ieeexplore.ieee.org/document/9923801/",
                "date_of_publication": "26 October 2022",
                "doi": "10.1109/MICRO56248.2022.00030",
                "citations": "1",
                "abstract": "This paper presents Flame, a hardware/software co-designed resilience scheme for protecting GPUs against soft errors. For low-cost yet high-performance resilience, Flame uses acoustic sensors and idempotent processing for error detection and recovery, respectively. That is, Flame seeks to correct any sensor-detected errors by re-executing the idempotent region where they occurred. To achieve this, it is essential for each idempotent region to ensure the absence of errors before moving on to the next region. This is so-called soft error verification that takes sensors’ worst-case detection latency (WCDL) to verify each region finished. Rather than waiting for WCDL at each region end, which incurs too much performance overhead, Flame proposes WCDL-aware warp scheduling that can hide the error verification delay (i.e., WCDL) with GPU’s inherent massive warp-level parallelism. When a warp hits each idempotent region boundary, Flame deschedules the warp and switches to one of the other ready warps—as if the region boundary were a regular long-latency operation triggering the warp switching. By leveraging GPU’s inherent ability for the latency hiding, Flame can completely eliminate the verification delay without significant hardware modification. The experimental results demonstrate that the performance overhead of Flame is near zero, i.e., 0.6% on average for 34 GPU benchmark applications.",
                "ieee_keywords": [
                    "Microarchitecture",
                    "Costs",
                    "Fires",
                    "Graphics processing units",
                    "Switches",
                    "Parallel processing",
                    "Acoustic sensors"
                ],
                "author_keywords": []
            },
            {
                "title": "Automatic Permission Check Analysis for Linux Kernel",
                "link": "https://ieeexplore.ieee.org/document/9750908/",
                "date_of_publication": null,
                "doi": "10.1109/TDSC.2022.3165368",
                "citations": "1",
                "abstract": "Permission checks play an essential role in operating system security by providing access control to privileged functionalities. However, it is challenging for kernel developers to scalably verify the soundness of existing checks due to the large codebase and complexity of the kernel. In fact, Linux kernel contains millions of lines of code with hundreds of permission checks, and even worse, its complexity is fast-growing. This paper presents PeX, a static Pe rmission check error detector for Linu X , which takes as input a kernel source code and reports any missing, inconsistent, and redundant permission checks. PeX uses KIRIN (Kernel InteRface based Indirect call aNalysis), a novel, precise, and scalable indirect call analysis technique. Over the interprocedural control flow graph built by KIRIN, PeX automatically identifies permission checks and infers the mappings between permission checks and privileged functions. For each privileged function, PeX examines all possible paths to the function to check if necessary permission checks are correctly enforced. We evaluated PeX on the latest stable Linux kernel v4.18.5 for three types of permission checks: Discretionary Access Controls (DAC), Capabilities, and Linux Security Modules (LSM). PeX reported 45 new permission check errors, 17 of which have been confirmed by the kernel developers.",
                "ieee_keywords": [
                    "Kernel",
                    "Linux",
                    "Permission",
                    "Codes",
                    "Access control",
                    "Computer science",
                    "Computer bugs"
                ],
                "author_keywords": [
                    "Linux kernel",
                    "static analysis",
                    "permission check",
                    "bug detection"
                ]
            },
            {
                "title": "DevFuzz: Automatic Device Model-Guided Device Driver Fuzzing",
                "link": "https://ieeexplore.ieee.org/document/10179293/",
                "date_of_publication": "21 July 2023",
                "doi": "10.1109/SP46215.2023.10179293",
                "citations": "101",
                "abstract": "The security of device drivers is critical for the entire operating system’s reliability. Yet, it remains very challenging to validate if a device driver can properly handle potentially malicious input from a hardware device. Unfortunately, existing symbolic execution-based solutions often do not scale, while fuzzing solutions require real devices or manual device models, leaving many device drivers under-tested and insecure.This paper presents DevFuzz, a new model-guided device driver fuzzing framework that does not require a physical device. DevFuzz uses symbolic execution to automatically generate the probe model that can guide a fuzzer to properly initialize a device driver under test. DevFuzz also leverages both static and dynamic program analyses to construct MMIO, PIO, and DMA device models to improve the effectiveness of fuzzing further. DevFuzz successfully tested 191 device drivers of various bus types (PCI, USB, RapidIO, I2C) from different operating systems (Linux, FreeBSD, and Windows) and detected 72 bugs, 41 of which have been patched and merged into the mainstream.",
                "ieee_keywords": [
                    "Privacy",
                    "Operating systems",
                    "Linux",
                    "Computer bugs",
                    "Fuzzing",
                    "Universal Serial Bus",
                    "Device drivers"
                ],
                "author_keywords": []
            }
        ]
    },
    {
        "name": "Milind Kulkarni",
        "publications": [
            {
                "title": "Treelogy: a benchmark suite for tree traversal applications",
                "link": "https://ieeexplore.ieee.org/document/7581286/",
                "date_of_publication": "10 October 2016",
                "doi": "10.1109/IISWC.2016.7581286",
                "citations": "1",
                "abstract": "An interesting class of irregular algorithms are tree traversal algorithms, which repeatedly traverse spatial trees to perform efficient computations. Optimizing tree traversal algorithms requires understanding specific characteristics of these algorithms which affect their behavior and govern which types of optimizations are likely to perform well. In this work, we present a set of tree traversal applications, drawn from multiple domains, called Treelogy. This benchmark suite can be studied through the use of an ontology that characterizes tree traversal algorithms according to structural properties of the algorithm. We describe how each of these attributes constrains or allows different kinds of traversal optimizations. We characterize the applications of Treelogy both according to our structural ontology as well as by various applicationand input-dependent metrics that measure the irregularity of these applications. We then show the scalability of these applications implemented using generic frameworks for GPU, shared memory, and distributed memory systems. We also find that for certain input distributions, Barnes-Hut and 2-point correlation yield better performance when implemented with tree types other than the \"standard\" tree.",
                "ieee_keywords": [
                    "Optimization",
                    "Ontologies",
                    "Kernel",
                    "Measurement",
                    "Data mining",
                    "Hardware",
                    "Processor scheduling"
                ],
                "author_keywords": []
            },
            {
                "title": "Programming language support for analyzing non-persistent data",
                "link": "https://ieeexplore.ieee.org/document/7568895/",
                "date_of_publication": "15 September 2016",
                "doi": "10.1109/THS.2016.7568895",
                "citations": "75",
                "abstract": "For safety and security, surveillance cameras are widely deployed. A high percentage of the visual data, however, is never watched by humans nor analyzed by computer programs. Moreover, it is common practice to erase the data after a short duration (say, two weeks) and reuse the storage space. As a result, the data are non-persistent. Non-persistent data presents serious security risks: the unwatched and unanalyzed data may include evidence of security breaches. After the data is erased, it is no longer possible to detect the breaches nor prosecute the suspects. This paper proposes a potential solution to remedy this situation by adding automatic data sampling to a programming language. If a piece of data is marked as non-persistent, the compiler and the run-time system automatically sample and store the data, hence making a small fraction of the data persistent. The samples would allow post-event analysis to detect security breaches that are not detected earlier. The samples, due to the much smaller sizes compared with the original non-persistent data, may be analyzed using more sophisticated computer programs that are unable to keep up with the speeds of data generation.",
                "ieee_keywords": [
                    "Computers",
                    "Cameras",
                    "Security",
                    "Surveillance",
                    "Streaming media",
                    "Computer languages",
                    "Compressed sensing"
                ],
                "author_keywords": []
            },
            {
                "title": "Beyond Big Data -- Rethinking Programming Languages for Non-persistent Data",
                "link": "https://ieeexplore.ieee.org/document/7450559/",
                "date_of_publication": "09 April 2016",
                "doi": "10.1109/CCBD.2015.16",
                "citations": "1",
                "abstract": "In \"Big Data\" research, the data acquired from many sources are fused and analyzed to obtain valuable and sometimes unexpected information. Even though the volumes of data are unprecedented, the data are usually stored for post-experiment analysis and for sharing among scientists. Typical scenarios implicitly assume that the data are stored and can be re-analyzed later. The reality is, unfortunately, not so ideal because the data may be \"non-persistent\" and allow only one-time use. We propose to reformulate how big data programs are developed, and introduce the notion of data-carrying programs that are, in a sense, self-validating. By writing these programs in a specially-defined language, and transforming them to store sample data, programs can save enough data to provide high-confidence validation of their results.",
                "ieee_keywords": [
                    "Big data",
                    "Computer languages",
                    "Radiation detectors",
                    "Cloud computing",
                    "Cameras",
                    "Roads",
                    "Databases"
                ],
                "author_keywords": [
                    "big data",
                    "sampling",
                    "non-persistent data",
                    "programming language"
                ]
            },
            {
                "title": "Orion: Scaling Genomic Sequence Matching with Fine-Grained Parallelization",
                "link": "https://ieeexplore.ieee.org/document/7013024/",
                "date_of_publication": "19 January 2015",
                "doi": "10.1109/SC.2014.42",
                "citations": "10",
                "abstract": "Gene sequencing instruments are producing huge volumes of data, straining the capabilities of current database searching algorithms and hindering efforts of researchers analyzing large collections of data to obtain greater insights. In the space of parallel genomic sequence search, most of the popular software packages, like mpiBLAST, use the database segmentation approach, wherein the entire database is sharded and searched on different nodes. However this approach does not scale well with the increasing length of individual query sequences as well as the rapid growth in size of sequence databases. In this paper, we propose a fine-grained parallelism technique, called Orion, that divides the input query into an adaptive number of fragments and shards the database. Our technique achieves higher parallelism (and hence speedup) and load balancing than database sharding alone, while maintaining 100% accuracy. We show that it is 12.3X faster than mpiBLAST for solving a relevant comparative genomics problem.",
                "ieee_keywords": [
                    "Databases",
                    "Parallel processing",
                    "Genomics",
                    "Bioinformatics",
                    "DNA",
                    "Organisms"
                ],
                "author_keywords": []
            },
            {
                "title": "PySE: Automatic Worst-Case Test Generation by Reinforcement Learning",
                "link": "https://ieeexplore.ieee.org/document/8730198/",
                "date_of_publication": "06 June 2019",
                "doi": "10.1109/ICST.2019.00023",
                "citations": "15",
                "abstract": "Stress testing is an important task in software testing, which examines the behavior of a program under a heavy load. Symbolic execution is a useful tool to find out the worst-case input values for the stress testing. However, symbolic execution does not scale to a large program, since the number of paths to search grows exponentially with an input size. So far, such a scalability issue has been mostly managed by pruning out unpromising paths in the middle of searching based on heuristics, but this kind of work easily eliminates the true worst case as well, providing sub-optimal one only. Another way to achieve scalability is to learn a branching policy of worst-case complexity from small scale tests and apply it to a large scale. However, use cases of such a method are restricted to programs whose worst-case branching policy has a simple pattern. To address such limitations, we propose PySE that uses symbolic execution to collect the behaviors of a given branching policy, and updates the policy using a reinforcement learning approach through multiple executions. PySE's branching policy keeps evolving in a way that the length of an execution path increases in the long term, and ultimately reaches the worst-case complexity. PySE can also learn the worst-case branching policy of a complex or irregular pattern, using an artificial neural network in a fully automatic way. Experiment results demonstrate that PySE can effectively find a path of worst-case complexity for various Python benchmark programs and scales.",
                "ieee_keywords": [
                    "Complexity theory",
                    "Testing",
                    "Explosions",
                    "Stress",
                    "Python",
                    "History",
                    "Genetic algorithms"
                ],
                "author_keywords": [
                    "Machine learning",
                    "Q-learning",
                    "Symbolic execution",
                    "Worst-case complexity",
                    "Stress testing"
                ]
            },
            {
                "title": "XSTRESSOR : Automatic Generation of Large-Scale Worst-Case Test Inputs by Inferring Path Conditions",
                "link": "https://ieeexplore.ieee.org/document/8730162/",
                "date_of_publication": "06 June 2019",
                "doi": "10.1109/ICST.2019.00011",
                "citations": "6",
                "abstract": "An important part of software testing is generation of worst-case test inputs, which exercise a program under extreme loads. For such a task, symbolic execution is a useful tool with its capability to reason about all possible execution paths of a program, including the one with the worst case behavior. However, symbolic execution suffers from the path explosion problem and frequent calls to a constraint solver, which make it impractical to be used at a large scale. To address the issue, this paper presents XSTRESSOR that is able to generate test inputs that can run specific loops in a program with the worst-case complexity in a large scale. XSTRESSOR synthetically generates the path condition for the large-scale, worst-case execution from a predictive model that is built from a set of small scale tests. XSTRESSOR avoids the scaling problem of prior techniques by limiting full-blown symbolic execution and run-time calls to constraint solver to small scale tests only. We evaluate XSTRESSOR against WISE and SPF-WCA, the most closely related tools to generate worst-case test inputs. Results show that XSTRESSOR can generate the test inputs faster than WISE and SPF-WCA, and also scale to much larger input sizes.",
                "ieee_keywords": [
                    "Generators",
                    "Explosions",
                    "Stress",
                    "Predictive models",
                    "Testing",
                    "Complexity theory",
                    "Software"
                ],
                "author_keywords": [
                    "Symbolic execution",
                    "Worst-case complexity",
                    "Automatic program testing",
                    "Stress testing",
                    "Program Synthesis"
                ]
            },
            {
                "title": "Treelogy: A benchmark suite for tree traversals",
                "link": "https://ieeexplore.ieee.org/document/7975294/",
                "date_of_publication": "13 July 2017",
                "doi": "10.1109/ISPASS.2017.7975294",
                "citations": "5",
                "abstract": "An interesting class of irregular algorithms is tree traversal algorithms, which repeatedly traverse various trees to perform efficient computations. Tree traversal algorithms form the algorithmic kernels in an important set of applications in scientific computing, computer graphics, bioinformatics, and data mining, etc. There has been increasing interest in understanding tree traversal algorithms, optimizing them, and applying them in a wide variety of settings. Crucially, while there are many possible optimizations for tree traversal algorithms, which optimizations apply to which algorithms is dependent on algorithmic characteristics. In this work, we present a suite of tree traversal kernels, drawn from diverse domains, called Treelogy, to explore the connection between tree traversal algorithms and state-of-the-art optimizations. We characterize these algorithms by developing an ontology based on their structural properties. The attributes extracted through our ontology, for a given traversal kernel, can aid in quick analysis of the suitability of platform- and application-specific as well as independent optimizations. We provide reference implementations of these kernels for three platforms: shared memory multicores, distributed memory systems, and GPUs, and evaluate their scalability.",
                "ieee_keywords": [
                    "Optimization",
                    "Kernel",
                    "Ontologies",
                    "Benchmark testing",
                    "Correlation",
                    "Algorithm design and analysis",
                    "Data mining"
                ],
                "author_keywords": []
            },
            {
                "title": "μSETL: A set based programming abstraction for wireless sensor networks",
                "link": "https://ieeexplore.ieee.org/document/5779051/",
                "date_of_publication": "27 May 2011",
                "doi": null,
                "citations": "3",
                "abstract": "Metrics Footnotes More Like This Programming languages for Wireless Sensor Networks: A comparative study 2015 2nd International Conference on Computing for Sustainable Global Development (INDIACom) Published: 2015 Behavioural Specification of Wireless Sensor Network Applications 2007 First International Global Information Infrastructure Symposium Published: 2007 Show More References References is not available for this document.",
                "ieee_keywords": [
                    "Programming",
                    "Wireless sensor networks",
                    "Monitoring",
                    "Temperature sensors",
                    "Program processors",
                    "Base stations",
                    "Set theory"
                ],
                "author_keywords": [
                    "Wireless Sensor Networks",
                    "Programming Abstractions"
                ]
            },
            {
                "title": "Efficient Collaborative Approximation in MapReduce without Missing Rare Keys",
                "link": "https://ieeexplore.ieee.org/document/8064056/",
                "date_of_publication": "12 October 2017",
                "doi": "10.1109/ICCAC.2017.15",
                "citations": "107",
                "abstract": "Recent proposals extend MapReduce, a widely-used Big Data processing framework, with sampling to improve performance by producing approximate results with statistical error bounds. However, because these systems perform global uniform sampling across the entire key space of input data, they may completely miss rare keys which may be unacceptable in some applications. Well-known stratified sampling avoids missing rare keys by obtaining the same number of samples for each key which also achieves good performance by sampling popular keys infrequently and rare keys more often. While online stratified sampling has been done in centralized settings, straightforward extension to MapReduce's distributed setting cannot easily leverage the number of per-key samples seen globally by all the Mappers to reduce the sampling rate of each Mapper in the future. Because there are hundreds of Mappers in a typical MapReduce job, such feedback can drastically reduce oversampling and improve performance. We present MaDSOS (MapReduce with Distributed Stratified Online Sampling) which makes two contributions: (1) Instead of a fixed n per-key samples and the resultant sampling rates, we propose a telescoping algorithm that uses fixed sampling rates of the form 1/2^k and, between n and 2n samples. (2) We propose a collaborative feedback scheme, that is enabled by the specific form of sampling rates and the leniency in the sample counts, to efficiently cut the sampling rates, and thus oversampling, once the desired number of samples have been seen globally. For our MapReduce benchmarks, MaDSOS improves performance by 59% over Hadoop while guaranteeing never to miss rare keys and achieves 2.5% per-key error compared to 100% worst-case error under global sampling at a fixed rate for all the keys.",
                "ieee_keywords": [
                    "Reservoirs",
                    "Collaboration",
                    "Urban areas",
                    "Decision support systems",
                    "Error analysis",
                    "Conferences"
                ],
                "author_keywords": [
                    "Distributed Stratified Online Sampling",
                    "Map-Reduce"
                ]
            },
            {
                "title": "General transformations for GPU execution of tree traversals",
                "link": "https://ieeexplore.ieee.org/document/6877443/",
                "date_of_publication": "14 August 2014",
                "doi": "10.1145/2503210.2503223",
                "citations": "13",
                "abstract": ", Propulsion , Optimization , Servers , Semantics , Legged locomotion INSPEC: Controlled Indexing multiprocessing systems , parallel algorithms , tree data structures INSPEC: Non-Controlled Indexing GPU execution , programmer-friendly GPU computing environments , offloading workloads , GPU memory hierarchy , pointer-based dynamic data structures , irregular algorithms , kd-tree traversal algorithm , Barnes-Hut tree traversal algorithm , application-specific semantics , general-purpose techniques , algorithmic structure , CPU versions Author Keywords vectorization , tree traversals , GPU , irregular programs Metrics Footnotes More Like This An efficient parallel algorithm for high dimensional similarity join Proceedings of the First Merged International Parallel Processing Symposium and Symposium on Parallel and Distributed Processing Published: 1998 Astrophysical N-body simulations using hierarchical tree data structures Supercomputing '92:Proceedings of the 1992 ACM/IEEE Conference on Supercomputing Published: 1992 Show More References References is not available for this document.",
                "ieee_keywords": [
                    "Educational institutions",
                    "Abstracts",
                    "Propulsion",
                    "Optimization",
                    "Servers",
                    "Semantics",
                    "Legged locomotion"
                ],
                "author_keywords": [
                    "vectorization",
                    "tree traversals",
                    "GPU",
                    "irregular programs"
                ]
            }
        ]
    },
    {
        "name": "Bruno Ribeiro",
        "publications": [
            {
                "title": "An S-band Automatically Tunable Bandpass Filter Based on a Machine Learning Approach",
                "link": "https://ieeexplore.ieee.org/document/9443583/",
                "date_of_publication": "03 June 2021",
                "doi": "10.1109/WAMICON47156.2021.9443583",
                "citations": "1",
                "abstract": "This paper presents a novel automatic tuning mechanism that eliminates hand-tuning and is suitable for electronically-tunable microwave filters. The proposed method is based on a machine learning approach using physics-based filter characteristic parameters like resonant frequency, bandwidth, insertion loss, and return loss. The whole tuning process is done automatically and does not require any pre-tuning or human expertise. Furthermore, unlike single-frequency post-production tuning techniques, the presented methodology is applicable to continuously-tunable filters covering a wide frequency range. This method is experimentally tested on an S-band evanescent-mode electronically-tunable bandpass filter that can tune from 2-3. 5GHz. To the best of the author's knowledge, this is the first demonstration of such an automatic tuning mechanism where the user can specify any frequency of interest and the filter tunes automatically to that frequency within the entire operating range of the filter.",
                "ieee_keywords": [
                    "Band-pass filters",
                    "Wireless communication",
                    "Resonator filters",
                    "Resonant frequency",
                    "Machine learning",
                    "Insertion loss",
                    "Bandwidth"
                ],
                "author_keywords": [
                    "automatic tuning",
                    "contactless cavity resonator",
                    "machine learning",
                    "microwave tunable filter."
                ]
            },
            {
                "title": "GrAPL Keynote 2",
                "link": "https://ieeexplore.ieee.org/document/8778370/",
                "date_of_publication": "29 July 2019",
                "doi": "10.1109/IPDPSW.2019.00049",
                "citations": "24",
                "abstract": "Provides an abstract of the keynote presentation and may include a brief professional biography of the presenter. The complete presentation was not made available for publication as part of the conference proceedings.",
                "ieee_keywords": [
                    "Representation learning",
                    "Graph neural networks",
                    "Deep learning",
                    "Data mining",
                    "Computational modeling",
                    "Turning",
                    "Probabilistic logic"
                ],
                "author_keywords": []
            },
            {
                "title": "Graph Pattern Mining and Learning through User-Defined Relations",
                "link": "https://ieeexplore.ieee.org/document/8594979/",
                "date_of_publication": "30 December 2018",
                "doi": "10.1109/ICDM.2018.00170",
                "citations": "2",
                "abstract": "In this work we propose R-GPM, a parallel computing framework for graph pattern mining (GPM) through a user-defined subgraph relation. More specifically, we enable the computation of statistics of patterns through their subgraph classes, generalizing traditional GPM methods. R-GPM provides efficient estimators for these statistics by employing a MCMC sampling algorithm combined with several optimizations. We provide both theoretical guarantees and empirical evaluations of our estimators in application scenarios such as stochastic optimization of deep high-order graph neural network models and pattern (motif) counting. We also propose and evaluate optimizations that enable improvements of our estimators accuracy, while reducing their computational costs in up to 3-orders-of-magnitude. Finally, we show that R-GPM is scalable, providing near-linear speedups.",
                "ieee_keywords": [
                    "Task analysis",
                    "Computational modeling",
                    "Optimization",
                    "Data mining",
                    "Computer science",
                    "Neural networks",
                    "Distance measurement"
                ],
                "author_keywords": [
                    "Graph mining",
                    "subgraph pattern",
                    "sampling",
                    "random walk"
                ]
            },
            {
                "title": "On New Group Popularity Prediction in Event-Based Social Networks",
                "link": "https://ieeexplore.ieee.org/document/8713911/",
                "date_of_publication": null,
                "doi": "10.1109/TNSE.2019.2916893",
                "citations": "8",
                "abstract": "Event-based social networks (EBSN) have recently emerged as an important complement to online social networks. They enjoy the advantages of both online social networks and offline social communities: offline social events can be conveniently organized online, and users interact with each other face-to-face in the organized offline events. Although previous work has shown that member and structural features are important to the future popularity of groups in the EBSN, it is not yet clear how different member roles and the interplay between them contribute to group popularity. In this paper, we study a real-world dataset from Meetup-a popular EBSN platform-and propose a deep-neural-network-based method to predict the popularity of new Meetup groups. Our method uses group-level features specific to EBSNs, such as time and location of events in a group, as well as the structural features internal to a group, such as the inferred member roles in a group and social substructures among members. Empirically, our approach reduces the normalized root-mean-squared error of the popularity prediction (measured in RSVPs) of a group's future events by up to 12%, against the state-of-the-art baselines. Through case studies, our method also identifies member and structure patterns that are most predictive of a group's future popularity. Our study provides new understanding about what makes a group successful in the EBSN.",
                "ieee_keywords": [
                    "Feature extraction",
                    "Measurement",
                    "Chemistry",
                    "Mathematical model",
                    "Facebook",
                    "Twitter"
                ],
                "author_keywords": [
                    "Event-based social networks",
                    "group popularity prediction",
                    "circular fingerprints",
                    "role discovery"
                ]
            }
        ]
    },
    {
        "name": "Yexiang Xue",
        "publications": [
            {
                "title": "DESK: A Robotic Activity Dataset for Dexterous Surgical Skills Transfer to Medical Robots",
                "link": "https://ieeexplore.ieee.org/document/8967760/",
                "date_of_publication": "28 January 2020",
                "doi": "10.1109/IROS40897.2019.8967760",
                "citations": "16",
                "abstract": "Datasets are an essential component for training effective machine learning models. In particular, surgical robotic datasets have been key to many advances in semi-autonomous surgeries, skill assessment, and training. Simulated surgical environments can enhance the data collection process by making it faster, simpler and cheaper than real systems. In addition, combining data from multiple robotic domains can provide rich and diverse training data for transfer learning algorithms. In this paper, we present the DESK (DExterous Surgical SKills) dataset. It comprises a set of surgical robotic skills collected during a surgical training task using three robotic platforms: the Taurus II robot, Taurus II simulated robot, and the YuMi robot. This dataset was used to test the idea of transferring knowledge across different domains (e.g. from Taurus to YuMi robot) for a surgical gesture classification task with seven gestures/surgemes. We explored two different scenarios: 1) No transfer and 2) Domain transfer (simulated Taurus to real Taurus and YuMi robots). We conducted extensive experiments with three supervised learning models and provided baselines in each of these scenarios. Results show that using simulation data during training enhances the performance on the real robots, where limited real data is available. In particular, we obtained an accuracy of 55% on the real Taurus data using a model that is trained only on the simulator data, but that accuracy improved to 82% when the ratio of real to simulated data was increased to 0.18 in the training set.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Transferring Dexterous Surgical Skill Knowledge between Robots for Semi-autonomous Teleoperation",
                "link": "https://ieeexplore.ieee.org/document/8956396/",
                "date_of_publication": "13 January 2020",
                "doi": "10.1109/RO-MAN46459.2019.8956396",
                "citations": "14",
                "abstract": "In the future, deployable, teleoperated surgical robots can save the lives of critically injured patients in battlefield environments. These robotic systems will need to have autonomous capabilities to take over during communication delays and unexpected environmental conditions during critical phases of the procedure. Understanding and predicting the next surgical actions (referred as “surgemes”) is essential for autonomous surgery. Most approaches for surgeme recognition cannot cope with the high variability associated with austere environments and thereby cannot “transfer” well to field robotics. We propose a methodology that uses compact image representations with kinematic features for surgeme recognition in the DESK dataset. This dataset offers samples for surgical procedures over different robotic platforms with a high variability in the setup. We performed surgeme classification in two setups: 1) No transfer, 2) Transfer from a simulated scenario to two real deployable robots. Then, the results were compared with recognition accuracies using only kinematic data with the same experimental setup. The results show that our approach improves the recognition performance over kinematic data across different domains. The proposed approach produced a transfer accuracy gain up to 20% between the simulated and the real robot, and up to 31% between the simulated robot and a different robot. A transfer accuracy gain was observed for all cases, even those already above 90%.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "DESERTS: DElay-tolerant SEmi-autonomous Robot Teleoperation for Surgery",
                "link": "https://ieeexplore.ieee.org/document/9561399/",
                "date_of_publication": "18 October 2021",
                "doi": "10.1109/ICRA48506.2021.9561399",
                "citations": "10",
                "abstract": "Telesurgery can be hindered by high-latency and low-bandwidth communication networks, often found in austere settings. Even delays of less than one second are known to negatively impact surgeries. To tackle the effects of connectivity associated with telerobotic surgeries, we propose the DESERTS framework. DESERTS provides a novel simulator interface where the surgeon can operate directly on a virtualized reality simulation and the activities are mirrored in a remote robot, almost simultaneously. Thus, the surgeon can perform the surgery uninterrupted, while high-level commands are extracted from his motions and are sent to a remote robotic agent. The simulated setup mirrors the remote environment, including an alpha-blended view of the remote scene. The framework abstracts the actions into atomic surgical maneuvers (surgemes) which eliminate the need to transmit compressed video information. This system uses a deep learning based architecture to perform live recognition of the surgemes executed by the operator. The robot then executes the received surgemes, thereby achieving semi-autonomy. The framework’s performance was tested on a peg transfer task. We evaluated the accuracy of the recognition and execution module independently as well as during live execution. Furthermore, we assessed the framework’s performance in the presence of increasing delays. Notably, the system maintained a task success rate of 87% from no-delays to 5 seconds of delay.",
                "ieee_keywords": [
                    "Minimally invasive surgery",
                    "Surgery",
                    "Bandwidth",
                    "Delays",
                    "Mirrors",
                    "Task analysis",
                    "Robots"
                ],
                "author_keywords": []
            },
            {
                "title": "Dexterous Skill Transfer between Surgical Procedures for Teleoperated Robotic Surgery",
                "link": "https://ieeexplore.ieee.org/document/9515453/",
                "date_of_publication": "23 August 2021",
                "doi": "10.1109/RO-MAN50785.2021.9515453",
                "citations": "1",
                "abstract": "In austere environments, teleoperated surgical robots could save the lives of critically injured patients if they can perform complex surgical maneuvers under limited communication bandwidth. The bandwidth requirement is reduced by transferring atomic surgical actions (referred to as “surgemes”) instead of the low-level kinematic information. While such a policy reduces the bandwidth requirement, it requires accurate recognition of the surgemes. In this paper, we demonstrate that transfer learning across surgical tasks can boost the performance of surgeme recognition. This is demonstrated by using a network pre-trained with peg-transfer data from Yumi robot to learn classification on debridement on data from Taurus robot. Using a pre-trained network improves the classification accuracy achieves a classification accuracy of 76% with only 8 sequences in target domain, which is 22.5% better than no-transfer scenario. Additionally, ablations on transfer learning indicate that transfer learning requires 40% less data compared to no-transfer to achieve same classification accuracy. Further, the convergence rate of the transfer learning setup is significantly higher than the no-transfer setup trained only on the target domain.",
                "ieee_keywords": [
                    "Learning systems",
                    "Medical robotics",
                    "Transfer learning",
                    "Training data",
                    "Surgery",
                    "Kinematics",
                    "Bandwidth"
                ],
                "author_keywords": []
            },
            {
                "title": "ASAP: A Semi-Autonomous Precise System for Telesurgery During Communication Delays",
                "link": "https://ieeexplore.ieee.org/document/10026257/",
                "date_of_publication": null,
                "doi": "10.1109/TMRB.2023.3239674",
                "citations": "1",
                "abstract": "In remote, rural, and disadvantaged areas, telesurgery can be severely hindered by limitations of communication infrastructure. In conventional telesurgery, delays as small as 300ms can produce fatal surgical errors. To mitigate the effect of communication delays during telesurgery, we introduce a semi-autonomous system that decouples the user interaction from the robot execution. This system uses a physics-based simulator where a surgeon can demonstrate individual surgical subtasks, with immediate graphical feedback. Each subtask is performed asynchronously, unaffected by communication latency, jitter, and packet loss. A surgical step recognition module extracts the intended actions from the observed surgeon-simulation interaction. The remote robot can perform each one of these actions autonomously. The action recognition system leveraged a transfer learning approach that minimized the data needed during training, and most of the learning is obtained from simulated data. We tested this system in two tasks: fluid-submerged peg transfer (resembling bleeding events) and surgical debridement. The system showed robustness to delays of up to 5 seconds, maintaining a performance rate of 87% for peg transfer and 88% for debridement. Also, the framework reduced the completion time under delays by 45% and 11% during peg transfer and debridement, respectively.",
                "ieee_keywords": [
                    "Robots",
                    "Delays",
                    "Surgery",
                    "Task analysis",
                    "Transfer learning",
                    "Medical robotics",
                    "Biomimetics"
                ],
                "author_keywords": [
                    "Medical robotics",
                    "telesurgical robotics",
                    "human robot interaction",
                    "deep learning",
                    "transfer learning"
                ]
            },
            {
                "title": "Imitation Refinement for X-ray Diffraction Signal Processing",
                "link": "https://ieeexplore.ieee.org/document/8683723/",
                "date_of_publication": "17 April 2019",
                "doi": "10.1109/ICASSP.2019.8683723",
                "citations": "3",
                "abstract": "Many real-world tasks involve identifying signals from data satisfying background or prior knowledge. In domains like materials discovery, due to the flaws and biases in raw experimental data, the identification of X-ray diffraction (XRD) signals often requires significant (manual) expert work to find refined signals that are similar to the ideal theoretical ones. Automatically refining the raw XRD signals utilizing simulated theoretical data is thus desirable. We propose imitation refinement, a novel approach to refine imperfect input signals, guided by a pre-trained classifier incorporating prior knowledge from simulated theoretical data, such that the refined signals imitate the ideal ones. The classifier is trained on the ideal simulated data to classify signals and learns an embedding space where each class is represented by a prototype. The refiner learns to refine the imperfect signals with small modifications, such that their embeddings are closer to the corresponding prototypes. We show that the refiner can be trained in both supervised and unsupervised fashions. We further illustrate the effectiveness of the proposed approach both qualitatively and quantitatively in an X-ray diffraction signal refinement task in materials discovery.",
                "ieee_keywords": [],
                "author_keywords": []
            },
            {
                "title": "Task Detection in Continual Learning via Familiarity Autoencoders",
                "link": "https://ieeexplore.ieee.org/document/9945326/",
                "date_of_publication": "18 November 2022",
                "doi": "10.1109/SMC53654.2022.9945326",
                "citations": "183",
                "abstract": "Continual learning requires the ability to reliably transfer previously learned knowledge to new tasks without disrupting established competencies. Methods such as Progressive Neural Network [1] accomplish high-quality transfer learning while nullifying the insidious problem of catastrophic forgetting. However, most module-based continual learning systems require task labels during operation – a constraint that limits their application in many real-world conditions where task indicators are opaque. This paper proposes a task detector neural algorithm to acquire task information while maintaining immunity to forgetting. Our proposed task detector allows progressive neural networks (and many similar systems) to operate without task labels during test time. Our task detector is built from familiarity autoencoders which recognize the nature of the required task from input data. We demonstrate the generality and effectiveness of our approach through experiments in video game playing and automated image repair. Our results show near-perfect task recognition in all domains (>.99 F1), rewards above published single-task scores in MinAtar, and realistic image repairs on damaged human face pictures. The performance of our integrated method is nearly identical to the progressive systems equipped with ground-truth task labels 1",
                "ieee_keywords": [
                    "Training",
                    "Video games",
                    "Image recognition",
                    "Face recognition",
                    "Neural networks",
                    "Transfer learning",
                    "Detectors"
                ],
                "author_keywords": []
            },
            {
                "title": "Sequential Prediction with Logic Constraints for Surgical Robotic Activity Recognition",
                "link": "https://ieeexplore.ieee.org/document/9515358/",
                "date_of_publication": "23 August 2021",
                "doi": "10.1109/RO-MAN50785.2021.9515358",
                "citations": "3",
                "abstract": "Many real-world time-sensitive and high-stake applications (e.g., surgical, rescue, and recovery robotics) exhibit sequential nature; thus, applying Recurrent Neural Network (RNN)-based sequential models is an attractive approach to detect robotic activity. One limitation of such approaches is data scarcity. As a result, limited training samples may lead to over-fitting, producing incorrect predictions during deployment. Nevertheless, abundant domain knowledge may still be available, which may help formulate logic constraints. In this paper, we propose a novel way to integrate domain knowledge into RNN-based sequential prediction. We build a Markov Logic Network (MLN)-based classifier that automatically learns constraint weights from data. We propose two methods to incorporate this MLN-based prediction: (i) PriorLayer, in which the values of the hidden layer of the RNN are combined with weights learned from logic constraints in an additional neural network layer, and (ii) Conflation, in which class probabilities from RNN predictions and constraint weights are combined based on the conflation of class probabilities. We evaluate robotic activity classification methods on a simulated OpenAI Gym environment and a real-world DESK dataset for surgical robotics. We observe that our proposed MLN-based approaches boost the performance of LSTM-based networks. In particular, MLN boosts the accuracy of LSTM from 71% to 84% on the Gym dataset and from 68% to 72% on the Taurus robot dataset. Furthermore, MLN (i.e., PriorLayer) shows regularization capability where it improves accuracy in initial LSTM training while avoiding over-fitting early, thus improves the final classification accuracy on unseen data. The code is available at https://github.com/masud99r/prediction-with-logic-constraints.",
                "ieee_keywords": [
                    "Training",
                    "Recurrent neural networks",
                    "Systematics",
                    "Markov processes",
                    "Activity recognition",
                    "Robots"
                ],
                "author_keywords": []
            },
            {
                "title": "Bootstrap Advantage Estimation for Policy Optimization in Reinforcement Learning",
                "link": "https://ieeexplore.ieee.org/document/10069727/",
                "date_of_publication": "23 March 2023",
                "doi": "10.1109/ICMLA55696.2022.00041",
                "citations": "35",
                "abstract": "This paper proposes an advantage estimation approach based on data augmentation for policy optimization. Unlike using data augmentation on the input to learn value and policy function as existing methods use, our method uses data augmentation to compute a bootstrap advantage estimation. This Bootstrap Advantage Estimation (BAE) is then used for learning and updating the gradient of policy and value function. To demonstrate the effectiveness of our approach, we conducted experiments on several environments. These environments are from three benchmarks: Procgen, Deepmind Control, and Pybullet, which include both image and vector-based observations; discrete and continuous action spaces. We observe that our method reduces the policy and the value loss better than the Generalized advantage estimation (GAE) method and eventually improves cumulative return. Furthermore, our method performs better than two recently proposed data augmentation techniques (RAD and DRAC). Overall, our method performs better empirically than baselines in sample efficiency and generalization, where the agent is tested in unseen environments.",
                "ieee_keywords": [
                    "Estimation",
                    "Reinforcement learning",
                    "Aerospace electronics",
                    "Benchmark testing",
                    "Optimization"
                ],
                "author_keywords": [
                    "Deep Reinforcement Learning",
                    "Advantage Estimation",
                    "Generalization in Reinforcement Learning"
                ]
            }
        ]
    }
]